{
  "generated": "2025-06-27T16:13:26.770Z",
  "sites": {
    "hyperbeam": {
      "name": "Hyperbeam",
      "baseUrl": "https://hyperbeam.arweave.net",
      "pages": [
        {
          "url": "https://hyperbeam.arweave.net/build/introduction/what-is-hyperbeam.html",
          "title": "What is HyperBEAM",
          "content": "HyperBEAM - Documentation permaweb/HyperBEAM Build on HyperBEAM Build on HyperBEAM Intro to HyperBEAM Intro to HyperBEAM Table of contents Implementing AO-Core A Robust and Scalable Foundation Intro to AO-Core Getting Started Getting Started Core Capabilities Building ao Processes Pathing in HyperBEAM Devices Devices Overview Core Devices Building Devices Reference Reference FAQ Glossary Troubleshooting Run a Node Table of contents Implementing AO-Core A Robust and Scalable Foundation What is HyperBEAM?¶ HyperBEAM is the primary, production-ready implementation of the AO-Core protocol, built on the robust Erlang/OTP framework. It serves as a decentralized operating system, powering the AO Computer—a scalable, trust-minimized, distributed supercomputer built on permanent storage of Arweave. Implementing AO-Core¶ HyperBEAM transforms the abstract concepts of AO-Core—Messages, Devices, and Paths—into a concrete, operational system. It provides the runtime environment and essential services to execute these computations across a network of distributed nodes. Messages: Modular Data Packets¶ In HyperBEAM, every interaction within the AO Computer is handled as a message. A message is a binary item or a map of functions. These cryptographically-linked data units are the foundation for communication, allowing processes to trigger computations, query state, and transfer value. HyperBEAM nodes are responsible for routing and processing these messages according to the rules of the AO-Core protocol. Devices: Extensible Execution Engines¶ HyperBEAM introduces a uniquely modular architecture centered around Devices. These pluggable components are Erlang modules that define specific computational logic—like running WASM, managing state, or relaying data—allowing for unprecedented flexibility. This design allows developers to extend the system by creating custom Devices to fit their specific computational needs. Paths: Composable Pipelines¶ HyperBEAM exposes a powerful HTTP API that uses structured URL patterns to interact with processes and data. This pathing mechanism allows developers to create verifiable data pipelines, composing functionality from multiple devices into a single, atomic request. The URL bar effectively becomes a command-line interface for AO's trustless compute environment. A Robust and Scalable Foundation¶ Built on the Erlang/OTP framework, HyperBEAM provides a robust and secure foundation that leverages the BEAM virtual machine for exceptional concurrency, fault tolerance, and scalability. This abstracts away underlying hardware, allowing diverse nodes to contribute resources without compatibility issues. The system governs how nodes coordinate and interact, forming a decentralized network that is resilient and permissionless. June 24, 2025 var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith(\"__tabbed_\"))",
          "estimatedWords": 376,
          "lastModified": "2025-06-27T16:11:42.467Z",
          "siteKey": "hyperbeam",
          "siteName": "Hyperbeam",
          "depth": 3,
          "crawledAt": "2025-06-27T16:11:42.467Z"
        },
        {
          "url": "https://hyperbeam.arweave.net/",
          "title": "Hyperbeam",
          "content": "Hyperbeam A Decentralized Operating System. Built on AO. Scroll For More. Build Build a new generation of apps and services for cyberspace. Fully trustless. Onchain. Boot Power your onchain compute and get paid. What is hyperBEAM? Hyperbeam. Powering the decentralized supercomputer: AO. Access, build, and lease hardware for applications and services at any scale. Your gateway to AO, a decentralized supercomputer network built on top of Arweave. AO and Arweave power a cyberspace which guarantees the rights of users, outside of the control of any individual or group. Hyper Parallel Ditch shared memory, embrace autonomous state. Execute concurrent processes without compromise. Async Communicate via asynchronous message passing for unheard of throughput. Distributed Get resilient compute in your terminal with one command. Learn More About Hyperbeam What Do I Do With Hyperbeam? 01 Monetize Your Hardware. Access a shared economy for hardware in the new cyberspace. All while earning $AO Offer compute to AO processes and their users, earning fees in return. Run your own gateway. Empower builders to launch trust-minimized, serverless WASM functions using built-in TEE integrations. Coming Soon: Offer support for GPUs. Run Your Node Sorry, your browser doesn’t support embedded video. 02 Apps for a cyberspace that guarantees user rights. Build freely. Deploy Once. Use it permanently. Access everywhere. Create a Permaweb App Sorry, your browser doesn't support embedded video. 03 Modular, scalable devices for the new frontier. Create for the infrastructure that unlocks access to 7,000,000+ smart contracts. Create on Hyperbeam Sorry, your browser doesn't support embedded video. Made with in San Francisco lol jk in cyberspace. Hungry to eat glass all day, Join Us.",
          "estimatedWords": 268,
          "lastModified": "2025-06-27T16:11:42.527Z",
          "siteKey": "hyperbeam",
          "siteName": "Hyperbeam",
          "depth": 4,
          "crawledAt": "2025-06-27T16:11:42.527Z"
        },
        {
          "url": "https://hyperbeam.arweave.net/what-is-hyperbeam.html",
          "title": "404 - Not found",
          "content": "HyperBEAM - Documentation permaweb/HyperBEAM Build on HyperBEAM Run a Node 404 - Not found var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith(\"__tabbed_\"))",
          "estimatedWords": 16,
          "lastModified": "2025-06-27T16:11:43.080Z",
          "siteKey": "hyperbeam",
          "siteName": "Hyperbeam",
          "depth": 4,
          "crawledAt": "2025-06-27T16:11:43.081Z"
        },
        {
          "url": "https://hyperbeam.arweave.net/what-is-ao-core.html",
          "title": "404 - Not found",
          "content": "HyperBEAM - Documentation permaweb/HyperBEAM Build on HyperBEAM Run a Node 404 - Not found var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith(\"__tabbed_\"))",
          "estimatedWords": 16,
          "lastModified": "2025-06-27T16:11:43.336Z",
          "siteKey": "hyperbeam",
          "siteName": "Hyperbeam",
          "depth": 4,
          "crawledAt": "2025-06-27T16:11:43.337Z"
        },
        {
          "url": "https://hyperbeam.arweave.net/hyperbeam-capabilities.html",
          "title": "404 - Not found",
          "content": "HyperBEAM - Documentation permaweb/HyperBEAM Build on HyperBEAM Run a Node 404 - Not found var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith(\"__tabbed_\"))",
          "estimatedWords": 16,
          "lastModified": "2025-06-27T16:11:43.783Z",
          "siteKey": "hyperbeam",
          "siteName": "Hyperbeam",
          "depth": 4,
          "crawledAt": "2025-06-27T16:11:43.783Z"
        },
        {
          "url": "https://hyperbeam.arweave.net/building-on-ao.html",
          "title": "404 - Not found",
          "content": "HyperBEAM - Documentation permaweb/HyperBEAM Build on HyperBEAM Run a Node 404 - Not found var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith(\"__tabbed_\"))",
          "estimatedWords": 16,
          "lastModified": "2025-06-27T16:11:44.450Z",
          "siteKey": "hyperbeam",
          "siteName": "Hyperbeam",
          "depth": 4,
          "crawledAt": "2025-06-27T16:11:44.450Z"
        },
        {
          "url": "https://hyperbeam.arweave.net/pathing-in-hyperbeam.html",
          "title": "404 - Not found",
          "content": "HyperBEAM - Documentation permaweb/HyperBEAM Build on HyperBEAM Run a Node 404 - Not found var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith(\"__tabbed_\"))",
          "estimatedWords": 16,
          "lastModified": "2025-06-27T16:11:44.801Z",
          "siteKey": "hyperbeam",
          "siteName": "Hyperbeam",
          "depth": 4,
          "crawledAt": "2025-06-27T16:11:44.801Z"
        },
        {
          "url": "https://hyperbeam.arweave.net/devices/hyperbeam-devices.html",
          "title": "404 - Not found",
          "content": "HyperBEAM - Documentation permaweb/HyperBEAM Build on HyperBEAM Run a Node 404 - Not found var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith(\"__tabbed_\"))",
          "estimatedWords": 16,
          "lastModified": "2025-06-27T16:11:45.207Z",
          "siteKey": "hyperbeam",
          "siteName": "Hyperbeam",
          "depth": 4,
          "crawledAt": "2025-06-27T16:11:45.207Z"
        },
        {
          "url": "https://hyperbeam.arweave.net/devices/meta-at-1-0.html",
          "title": "404 - Not found",
          "content": "HyperBEAM - Documentation permaweb/HyperBEAM Build on HyperBEAM Run a Node 404 - Not found var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith(\"__tabbed_\"))",
          "estimatedWords": 16,
          "lastModified": "2025-06-27T16:11:45.556Z",
          "siteKey": "hyperbeam",
          "siteName": "Hyperbeam",
          "depth": 4,
          "crawledAt": "2025-06-27T16:11:45.556Z"
        },
        {
          "url": "https://hyperbeam.arweave.net/devices/building-devices.html",
          "title": "404 - Not found",
          "content": "HyperBEAM - Documentation permaweb/HyperBEAM Build on HyperBEAM Run a Node 404 - Not found var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith(\"__tabbed_\"))",
          "estimatedWords": 16,
          "lastModified": "2025-06-27T16:11:46.117Z",
          "siteKey": "hyperbeam",
          "siteName": "Hyperbeam",
          "depth": 4,
          "crawledAt": "2025-06-27T16:11:46.117Z"
        },
        {
          "url": "https://hyperbeam.arweave.net/reference/faq.html",
          "title": "404 - Not found",
          "content": "HyperBEAM - Documentation permaweb/HyperBEAM Build on HyperBEAM Run a Node 404 - Not found var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith(\"__tabbed_\"))",
          "estimatedWords": 16,
          "lastModified": "2025-06-27T16:11:46.441Z",
          "siteKey": "hyperbeam",
          "siteName": "Hyperbeam",
          "depth": 4,
          "crawledAt": "2025-06-27T16:11:46.441Z"
        },
        {
          "url": "https://hyperbeam.arweave.net/reference/glossary.html",
          "title": "404 - Not found",
          "content": "HyperBEAM - Documentation permaweb/HyperBEAM Build on HyperBEAM Run a Node 404 - Not found var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith(\"__tabbed_\"))",
          "estimatedWords": 16,
          "lastModified": "2025-06-27T16:11:46.948Z",
          "siteKey": "hyperbeam",
          "siteName": "Hyperbeam",
          "depth": 4,
          "crawledAt": "2025-06-27T16:11:46.948Z"
        },
        {
          "url": "https://hyperbeam.arweave.net/reference/troubleshooting.html",
          "title": "404 - Not found",
          "content": "HyperBEAM - Documentation permaweb/HyperBEAM Build on HyperBEAM Run a Node 404 - Not found var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith(\"__tabbed_\"))",
          "estimatedWords": 16,
          "lastModified": "2025-06-27T16:11:47.195Z",
          "siteKey": "hyperbeam",
          "siteName": "Hyperbeam",
          "depth": 4,
          "crawledAt": "2025-06-27T16:11:47.195Z"
        },
        {
          "url": "https://hyperbeam.arweave.net/run/running-a-hyperbeam-node.html",
          "title": "Running a HyperBEAM Node",
          "content": "HyperBEAM - Documentation permaweb/HyperBEAM Build on HyperBEAM Run a Node Run a Node Running a HyperBEAM Node Running a HyperBEAM Node Table of contents System Dependencies Prerequisites for Running Starting a Basic Node Optional Build Profiles Node Configuration Verify Installation Running for Production (Mainnet) Stopping the Node (rebar3 shell) Next Steps Configuring Your Machine TEE Nodes Joining/Running a Router Reference Reference FAQ Glossary Troubleshooting Table of contents System Dependencies Prerequisites for Running Starting a Basic Node Optional Build Profiles Node Configuration Verify Installation Running for Production (Mainnet) Stopping the Node (rebar3 shell) Next Steps Running a HyperBEAM Node¶ This guide provides the basics for running your own HyperBEAM node, installing dependencies, and connecting to the AO network. System Dependencies¶ To successfully build and run a HyperBEAM node, your system needs several software dependencies installed. macOSLinux (Debian/Ubuntu)Windows (WSL) Install core dependencies using Homebrew: brew install cmake git pkg-config openssl ncurses Install core dependencies using apt: sudo apt-get update && sudo apt-get install -y --no-install-recommends \\ build-essential \\ cmake \\ git \\ pkg-config \\ ncurses-dev \\ libssl-dev \\ sudo \\ curl \\ ca-certificates Using the Windows Subsystem for Linux (WSL) with a distribution like Ubuntu is recommended. Follow the Linux (Debian/Ubuntu) instructions within your WSL environment. Erlang/OTP¶ HyperBEAM is built on Erlang/OTP. You need version OTP 27 installed (check the rebar.config or project documentation for specific version requirements, typically OTP 27). Installation methods: macOS (brew)Linux (apt) brew install erlang@27 sudo apt install erlang=1:27.* Source Build Download from erlang.org and follow the build instructions for your platform. Rebar3¶ Rebar3 is the build tool for Erlang projects. Installation methods: macOS (brew)Linux / macOS (Direct Download) brew install rebar3 Get the rebar3 binary from the official website. Place the downloaded rebar3 file in your system's PATH (e.g., /usr/local/bin) and make it executable (chmod +x rebar3). Node.js¶ Node.js might be required for certain JavaScript-related tools or dependencies. Node version 22+ is required. Installation methods: macOS (brew)Linux (apt)asdf (Recommended) brew install node # Check your distribution's recommended method, might need nodesource repo sudo apt install nodejs npm asdf-vm with the asdf-nodejs plugin is recommended. asdf plugin add nodejs https://github.com/asdf-vm/asdf-nodejs.git asdf install nodejs latest # ensures you get the latest version (22+) asdf global nodejs latest Rust¶ Rust is needed if you intend to work with or build components involving WebAssembly (WASM) or certain Native Implemented Functions (NIFs) used by some devices (like ~snp@1.0). The recommended way to install Rust on all platforms is via rustup: curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh source \"$HOME/.cargo/env\" # Or follow the instructions provided by rustup Prerequisites for Running¶ Before starting a node, ensure you have: Installed the system dependencies mentioned above. Cloned the HyperBEAM repository (git clone ...). Compiled the source code (rebar3 compile in the repo directory). An Arweave wallet keyfile (e.g., generated via Wander). The path to this file is typically set via the hb_key configuration option (see Configuring Your HyperBEAM Node). Starting a Basic Node¶ The simplest way to start a HyperBEAM node for development or testing is using rebar3 from the repository's root directory: rebar3 shell This command: Starts the Erlang Virtual Machine (BEAM) with all HyperBEAM modules loaded. Initializes the node with default settings (from hb_opts.erl). Starts the default HTTP server (typically on port 8734), making the node accessible. Drops you into an interactive Erlang shell where you can interact with the running node. This basic setup is suitable for local development and exploring HyperBEAM's functionalities. Optional Build Profiles¶ HyperBEAM uses build profiles to enable optional features, often requiring extra dependencies. To run a node with specific profiles enabled, use rebar3 as ... shell: Available Profiles (Examples): genesis_wasm: Enables Genesis WebAssembly support. rocksdb: Enables the RocksDB storage backend. http3: Enables HTTP/3 support. Example Usage: # Start with RocksDB profile rebar3 as rocksdb shell # Start with RocksDB and Genesis WASM profiles rebar3 as rocksdb, genesis_wasm shell Note: Choose profiles before starting the shell, as they affect compile-time options. Node Configuration¶ HyperBEAM offers various configuration options (port, key file, data storage, logging, etc.). These are primarily set using a config.flat file and can be overridden by environment variables or command-line arguments. See the dedicated Configuring Your HyperBEAM Node guide for detailed information on all configuration methods and options. Verify Installation¶ To quickly check if your node is running and accessible, you can send a request to its ~meta@1.0 device (assuming default port 8734): curl http://localhost:8734/~meta@1.0/info A JSON response containing node information indicates success. Running for Production (Mainnet)¶ While you can connect to the main AO network using the rebar3 shell for testing purposes (potentially using specific configurations or helper functions like hb:start_mainnet/1 if available and applicable), the standard and recommended method for a stable production deployment (like running on the mainnet) is to build and run a release. 1. Build the Release: From the root of the HyperBEAM repository, build the release package. You might include specific profiles needed for your mainnet setup (e.g., rocksdb if you intend to use it): # Build release with default profile rebar3 release # Or, build with specific profiles (example) # rebar3 as rocksdb release This command compiles the project and packages it along with the Erlang Runtime System (ERTS) and all dependencies into a directory, typically _build/default/rel/hb. 2. Configure the Release: Navigate into the release directory (e.g., cd _build/default/rel/hb). Ensure you have a correctly configured config.flat file here. See the configuration guide for details on setting mainnet parameters (port, key file location, store path, specific peers, etc.). Environment variables can also be used to override settings in the release's config.flat when starting the node. 3. Start the Node: Use the generated start script (bin/hb) to run the node: # Start the node in the foreground (logs to console) ./bin/hb console # Start the node as a background daemon ./bin/hb start # Check the status ./bin/hb ping ./bin/hb status # Stop the node ./bin/hb stop Consult the generated bin/hb script or Erlang/OTP documentation for more advanced start-up options (e.g., attaching a remote shell). Running as a release provides a more robust, isolated, and manageable way to operate a node compared to running directly from the rebar3 shell. Stopping the Node (rebar3 shell)¶ To stop the node running within the rebar3 shell, press Ctrl+C twice or use the Erlang command q().. Next Steps¶ Configure Your Node: Deep dive into configuration options. TEE Nodes: Learn about running nodes in Trusted Execution Environments for enhanced security. Routers: Understand how to configure and run a router node. June 24, 2025 var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith(\"__tabbed_\"))",
          "estimatedWords": 1079,
          "lastModified": "2025-06-27T16:11:47.534Z",
          "siteKey": "hyperbeam",
          "siteName": "Hyperbeam",
          "depth": 2,
          "crawledAt": "2025-06-27T16:11:47.534Z"
        },
        {
          "url": "https://hyperbeam.arweave.net/running-a-hyperbeam-node.html",
          "title": "404 - Not found",
          "content": "HyperBEAM - Documentation permaweb/HyperBEAM Build on HyperBEAM Run a Node 404 - Not found var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith(\"__tabbed_\"))",
          "estimatedWords": 16,
          "lastModified": "2025-06-27T16:11:47.935Z",
          "siteKey": "hyperbeam",
          "siteName": "Hyperbeam",
          "depth": 3,
          "crawledAt": "2025-06-27T16:11:47.935Z"
        },
        {
          "url": "https://hyperbeam.arweave.net/configuring-your-machine.html",
          "title": "404 - Not found",
          "content": "HyperBEAM - Documentation permaweb/HyperBEAM Build on HyperBEAM Run a Node 404 - Not found var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith(\"__tabbed_\"))",
          "estimatedWords": 16,
          "lastModified": "2025-06-27T16:11:48.326Z",
          "siteKey": "hyperbeam",
          "siteName": "Hyperbeam",
          "depth": 3,
          "crawledAt": "2025-06-27T16:11:48.326Z"
        },
        {
          "url": "https://hyperbeam.arweave.net/tee-nodes.html",
          "title": "404 - Not found",
          "content": "HyperBEAM - Documentation permaweb/HyperBEAM Build on HyperBEAM Run a Node 404 - Not found var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith(\"__tabbed_\"))",
          "estimatedWords": 16,
          "lastModified": "2025-06-27T16:11:49.077Z",
          "siteKey": "hyperbeam",
          "siteName": "Hyperbeam",
          "depth": 3,
          "crawledAt": "2025-06-27T16:11:49.077Z"
        },
        {
          "url": "https://hyperbeam.arweave.net/joining-running-a-router.html",
          "title": "404 - Not found",
          "content": "HyperBEAM - Documentation permaweb/HyperBEAM Build on HyperBEAM Run a Node 404 - Not found var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith(\"__tabbed_\"))",
          "estimatedWords": 16,
          "lastModified": "2025-06-27T16:11:49.344Z",
          "siteKey": "hyperbeam",
          "siteName": "Hyperbeam",
          "depth": 3,
          "crawledAt": "2025-06-27T16:11:49.344Z"
        },
        {
          "url": "https://hyperbeam.arweave.net/build/reference/troubleshooting.html",
          "title": "Developer Troubleshooting Guide",
          "content": "HyperBEAM - Documentation permaweb/HyperBEAM Build on HyperBEAM Build on HyperBEAM Intro to HyperBEAM Intro to AO-Core Getting Started Getting Started Core Capabilities Building ao Processes Pathing in HyperBEAM Devices Devices Overview Core Devices Building Devices Reference Reference FAQ Glossary Troubleshooting Troubleshooting Table of contents Process Execution Fails Memory Errors in Compute Unit Getting Help Run a Node Table of contents Process Execution Fails Memory Errors in Compute Unit Getting Help Developer Troubleshooting Guide¶ This guide addresses common issues you might encounter when developing processes for HyperBEAM. Process Execution Fails¶ Symptoms: Errors when deploying or executing processes Solutions: Check both HyperBEAM and CU logs for specific error messages Verify that the WASM module is correctly compiled and valid Test with a simple example process to isolate the issue Adjust memory limits if the process requires more resources Memory Errors in Compute Unit¶ Symptoms: Out of memory errors or excessive memory usage during process execution Solutions: Adjust the PROCESS_WASM_MEMORY_MAX_LIMIT environment variable Enable garbage collection by setting an appropriate GC_INTERVAL_MS Monitor memory usage and adjust limits as needed If on a low-memory system, reduce concurrent process execution Getting Help¶ If you're still experiencing issues after trying these troubleshooting steps: Check the GitHub repository for known issues Join the Discord community for support Open an issue on GitHub with detailed information about your problem June 24, 2025 var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith(\"__tabbed_\"))",
          "estimatedWords": 225,
          "lastModified": "2025-06-27T16:11:49.397Z",
          "siteKey": "hyperbeam",
          "siteName": "Hyperbeam",
          "depth": 3,
          "crawledAt": "2025-06-27T16:11:49.397Z"
        },
        {
          "url": "https://hyperbeam.arweave.net/introduction/what-is-hyperbeam.html",
          "title": "404 - Not found",
          "content": "HyperBEAM - Documentation permaweb/HyperBEAM Build on HyperBEAM Run a Node 404 - Not found var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith(\"__tabbed_\"))",
          "estimatedWords": 16,
          "lastModified": "2025-06-27T16:11:50.073Z",
          "siteKey": "hyperbeam",
          "siteName": "Hyperbeam",
          "depth": 4,
          "crawledAt": "2025-06-27T16:11:50.073Z"
        },
        {
          "url": "https://hyperbeam.arweave.net/introduction/what-is-ao-core.html",
          "title": "404 - Not found",
          "content": "HyperBEAM - Documentation permaweb/HyperBEAM Build on HyperBEAM Run a Node 404 - Not found var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith(\"__tabbed_\"))",
          "estimatedWords": 16,
          "lastModified": "2025-06-27T16:11:50.432Z",
          "siteKey": "hyperbeam",
          "siteName": "Hyperbeam",
          "depth": 4,
          "crawledAt": "2025-06-27T16:11:50.432Z"
        },
        {
          "url": "https://hyperbeam.arweave.net/faq.html",
          "title": "404 - Not found",
          "content": "HyperBEAM - Documentation permaweb/HyperBEAM Build on HyperBEAM Run a Node 404 - Not found var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith(\"__tabbed_\"))",
          "estimatedWords": 16,
          "lastModified": "2025-06-27T16:11:50.677Z",
          "siteKey": "hyperbeam",
          "siteName": "Hyperbeam",
          "depth": 4,
          "crawledAt": "2025-06-27T16:11:50.677Z"
        },
        {
          "url": "https://hyperbeam.arweave.net/glossary.html",
          "title": "404 - Not found",
          "content": "HyperBEAM - Documentation permaweb/HyperBEAM Build on HyperBEAM Run a Node 404 - Not found var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith(\"__tabbed_\"))",
          "estimatedWords": 16,
          "lastModified": "2025-06-27T16:11:51.250Z",
          "siteKey": "hyperbeam",
          "siteName": "Hyperbeam",
          "depth": 4,
          "crawledAt": "2025-06-27T16:11:51.250Z"
        },
        {
          "url": "https://hyperbeam.arweave.net/troubleshooting.html",
          "title": "404 - Not found",
          "content": "HyperBEAM - Documentation permaweb/HyperBEAM Build on HyperBEAM Run a Node 404 - Not found var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith(\"__tabbed_\"))",
          "estimatedWords": 16,
          "lastModified": "2025-06-27T16:11:51.605Z",
          "siteKey": "hyperbeam",
          "siteName": "Hyperbeam",
          "depth": 4,
          "crawledAt": "2025-06-27T16:11:51.605Z"
        }
      ],
      "lastCrawled": "2025-06-27T16:13:26.770Z",
      "stats": {
        "totalPages": 24,
        "averageWords": 95,
        "duration": 9833,
        "requestCount": 24,
        "averageResponseTime": 381.375,
        "pagesPerSecond": 2.4407607037526695
      }
    },
    "ao": {
      "name": "AO Cookbook",
      "baseUrl": "https://cookbook_ao.arweave.net",
      "pages": [
        {
          "url": "https://cookbook_ao.arweave.net/welcome/ao-core-introduction.html",
          "title": "Introduction to AO-Core",
          "content": "Introduction to AO-Core ​AO-Core is a protocol and standard for distributed computation that forms the foundation of the AO computer. Inspired by and built upon concepts from the Erlang language, AO-Core embraces the actor model for concurrent, distributed systems. Unlike traditional blockchain systems, AO-Core defines a flexible, powerful computation protocol that enables a wide range of applications beyond just running Lua programs.What is AO-Core? ​AO-Core is the fundamental protocol of the AO computer that:Defines standards for trustless computation distributed across the worldProvides mathematical guarantees about program executionEnables composable, modular development through devicesSupports various execution environments beyond just LuaImplements the actor model for concurrent, message-passing computationThe Actor Model in AO ​AO references the actor model of computation where:Each actor (or process) is an independent unit of computationActors communicate exclusively through message passingActors can create other actors, send messages, and make local decisionsThe system is inherently concurrent and distributedThis approach, inspired by Erlang, provides natural scalability and resilience in distributed systems.Key Features of AO-Core ​Resilient: There is no single point of failure. AO-Core exists across many machines distributed worldwide, making it immune to physical destruction or tampering.Permanent: Computations following the AO-Core protocol are stored permanently on Arweave, allowing you to recall and continue your work at any time.Permissionless: No registration is required to use AO-Core. Your right to use it is guaranteed by the underlying protocol.Trustless: The state of your computations is mathematically guaranteed, allowing you to build services that don't require trust in any central authority.Beyond Just Processes ​While AO Processes (smart contracts built using the AO-Core protocol) are powerful for creating autonomous agents, AO-Core itself enables much more:Serverless functions with trustworthy guaranteesHybrid applications combining smart contract and serverless functionalityCustom execution environments through different devicesComposable systems using the path languageNext Steps ​In the following sections, we'll explore how AO Processes build on top of the AO-Core protocol, and how you can get started building your own applications in this powerful environment.",
          "estimatedWords": 320,
          "lastModified": "2025-06-27T16:11:53.750Z",
          "siteKey": "ao",
          "siteName": "AO Cookbook",
          "depth": 2,
          "crawledAt": "2025-06-27T16:11:53.750Z"
        },
        {
          "url": "https://cookbook_ao.arweave.net/llms-explanation.html",
          "title": "LLMs Documentation",
          "content": "LLMs Documentation ​llms.txt: ​Structured overview of the ao ecosystem.Ideal for AI tools navigating documentation or answering general questions.Suited for agents with web search capabilities.llms-full.txt: ​Complete technical documentation.Designed for in-depth analysis, troubleshooting, or chatbot integration.Provides exhaustive details for complex queries.INFOThe llms-full.txt file only contains content from references and release notes, as testing showed this focused approach performs better with current AI models.",
          "estimatedWords": 61,
          "lastModified": "2025-06-27T16:11:54.332Z",
          "siteKey": "ao",
          "siteName": "AO Cookbook",
          "depth": 3,
          "crawledAt": "2025-06-27T16:11:54.332Z"
        },
        {
          "url": "https://cookbook_ao.arweave.net/tutorials/index.html",
          "title": "Tutorials",
          "content": "Tutorials ​Here, we've created a series of tutorials to help you get started with aos and build your first processes. These tutorials include interactive guides, code snippets, and examples to help you get comfortable with the aos environment.List of Tutorials ​Getting Started - An Interactive GuideBots and Games",
          "estimatedWords": 48,
          "lastModified": "2025-06-27T16:11:55.534Z",
          "siteKey": "ao",
          "siteName": "AO Cookbook",
          "depth": 3,
          "crawledAt": "2025-06-27T16:11:55.534Z"
        },
        {
          "url": "https://cookbook_ao.arweave.net/tutorials/begin/index.html",
          "title": "Begin An Interactive Tutorial",
          "content": "Begin: An Interactive Tutorial ​In this tutorial series, you'll walk through an interactive steps that will help you deepen your knowledge and understanding of the aos environment.INFOThe Exercise ​In this fun exercise, you'll encounter a series of challenges presented by two familiar characters, Morpheus and Trinity. You'll dive deep into the rabbit hole guided by Morpheus as he presents you with a series of challenges to prove you're the one. Once you've completed all of the challenges presented by both Morpheus and Trinity, you'll receive a token that grants you access to an exclusive chatroom within ao called The Construct.Now, let's get started down the rabbit hole.Tutorials ​Getting Started - An Interactive Tutorial ​1. Quick Start2. Messaging3. Creating a Chatroom4. Build a Token",
          "estimatedWords": 123,
          "lastModified": "2025-06-27T16:11:56.986Z",
          "siteKey": "ao",
          "siteName": "AO Cookbook",
          "depth": 3,
          "crawledAt": "2025-06-27T16:11:56.986Z"
        },
        {
          "url": "https://cookbook_ao.arweave.net/tutorials/begin/preparations.html",
          "title": "Preparations",
          "content": "Preparations ​INFOThe Awakening Begins:You've always known there's more to this world, just outside of your reach. You've been searching for it, not even knowing what it was you were looking for. It... is ao.We begin our journey by installing the aos client and starting a new process. This will allow us to interact with the ao computer and complete the rest of the tutorial.Video Tutorial ​System requirements ​The local client of aos is very simple to install. Just make sure you have:NodeJS version 20+. (If you haven't yet installed it, check out this page to find instructions for your OS).A code editor of your choice.INFOThough it's not required, we do recommend installing the ao addon into your text editor of choice to optimize your experience with aos.Installing aos ​Once you have NodeJS on your machine, all you need to do is install aos and run it:shnpm i -g https://get_ao.arweave.netAfter installation, we can simply run the command itself to start a new aos process!shaosWelcome to the rabbit hole ​The utility you just started is a local client, which is ready to relay messages for you to your new process inside the ao computer.After it connects, you should see the following:sh _____ _______ _____ /\\ \\ /::\\ \\ /\\ \\ /::\\ \\ /::::\\ \\ /::\\ \\ /::::\\ \\ /::::::\\ \\ /::::\\ \\ /::::::\\ \\ /::::::::\\ \\ /::::::\\ \\ /:::/\\:::\\ \\ /:::/~~\\:::\\ \\ /:::/\\:::\\ \\ /:::/__\\:::\\ \\ /:::/ \\:::\\ \\ /:::/__\\:::\\ \\ /::::\\ \\:::\\ \\ /:::/ / \\:::\\ \\ \\:::\\ \\:::\\ \\ /::::::\\ \\:::\\ \\ /:::/____/ \\:::\\____\\ ___\\:::\\ \\:::\\ \\ /:::/\\:::\\ \\:::\\ \\ |:::| | |:::| | /\\ \\:::\\ \\:::\\ \\ /:::/ \\:::\\ \\:::\\____\\|:::|____| |:::| |/::\\ \\:::\\ \\:::\\____\\ \\::/ \\:::\\ /:::/ / \\:::\\ \\ /:::/ / \\:::\\ \\:::\\ \\::/ / \\/____/ \\:::\\/:::/ / \\:::\\ \\ /:::/ / \\:::\\ \\:::\\ \\/____/ \\::::::/ / \\:::\\ /:::/ / \\:::\\ \\:::\\ \\ \\::::/ / \\:::\\__/:::/ / \\:::\\ \\:::\\____\\ /:::/ / \\::::::::/ / \\:::\\ /:::/ / /:::/ / \\::::::/ / \\:::\\/:::/ / /:::/ / \\::::/ / \\::::::/ / /:::/ / \\::/____/ \\::::/ / \\::/ / ~~ \\::/ / \\/____/ \\/____/ Welcome to AOS: Your operating system for AO, the decentralized open access supercomputer. Type \".load-blueprint chat\" to join the community chat and ask questions! AOS Client Version: 1.12.1. 2024 Type \"Ctrl-C\" twice to exit Your AOS process: QFt5SR6UwJSCnmgnROq62-W8KGY9z96k1oExgn4uAzk default@aos-0.2.2[Inbox:1]>Let's walk through the initial printout after running aos:After running aos in your terminal, you should see:An ASCII art image of AOS.A Welcome MessageThe version of aos you are running.An instructional exit message.Your process ID.INFOIf your OS version is different than the latest version, a message asking if you'd like to update the version will appear. If so, simply exit the process by pressing \"Ctrl+C\" twice, run npm i -g https://get_ao.g8way.io to update, and then run aos again.Welcome to your new home in the ao computer! The prompt you are now looking at is your own personal server in this decentralized machine.Now, let's journey further down the rabbit hole by exploring one of the two core concept type of ao: messaging.",
          "estimatedWords": 501,
          "lastModified": "2025-06-27T16:11:59.223Z",
          "siteKey": "ao",
          "siteName": "AO Cookbook",
          "depth": 3,
          "crawledAt": "2025-06-27T16:11:59.223Z"
        },
        {
          "url": "https://cookbook_ao.arweave.net/tutorials/begin/messaging.html",
          "title": "Messaging in ao",
          "content": "Messaging in ao ​Learn how Messages gives ao Parallel Compute Capability ​In ao, every process runs in parallel, creating a highly scalable environment. Traditional direct function calls between processes aren't feasible because each process operates independently and asynchronously.Messaging addresses this by enabling asynchronous communication. Processes send and receive messages rather than directly invoking functions on each other. This method allows for flexible and efficient interaction, where processes can respond to messages, enhancing the system's scalability and responsiveness.We'll begin by exploring the basics of messaging in aos, how to see messages received in your inbox, and how to send messages to other processes.Video Tutorial ​Step 1: Understand the Message Structure ​Message Basics: Messages in ao are built using Lua tables, which are versatile data structures that can hold multiple values. Within these tables, the \"Data\" field is crucial as it contains the message's content or payload. This structure allows for efficient sending and receiving of information between processes, showcasing how ao primitives leverage Arweave's underlying capabilities to facilitate complex, composable operations.For detailed specifications, please refer to the original documentation on the G8way specs page.Example: { Data = \"Hello from Process A!\" } is a simple message.Step 2: Open the aos CLI ​Launch the aos command-line interface (CLI) by typing aos in your terminal and pressing Enter.shaosStep 3: How to Send a Message ​luaSend({ Target = \"process ID\", Data = \"Hello World!\" })Send: The Send function is globally available in the aos interactive environment.Target: To send a message to a specific process, include a Target field in your message.Data: The Data is the string message (or payload) you want to be received by the receiving process. In this example, the message is \"Hello World!\".Step 4: Store Morpheus's Process ID ​We'll use the process ID provided below and store it as a variable called Morpheus.luaFvan28CFY0JYl5f_ETB7d3PDwBhGS8Yq5IA0vcWulUcCopy the process ID above and store it as a variable by running the below command in the aos CLI:luaMorpheus = \"Fvan28CFY0JYl5f_ETB7d3PDwBhGS8Yq5IA0vcWulUc\"This will store the process ID as a variable called Morpheus, making it easier to interact with the specific process ID.Check the Morpheus Variable ​lua-- Check the Morpheus variable by typing `Morpheus` Morpheus -- Expected Results: Fvan28CFY0JYl5f_ETB7d3PDwBhGS8Yq5IA0vcWulUc -- If `undefined` is returned, -- then the variable was not created successfully.Step 5: Send a Message to Morpheus ​After obtaining Morpheus's process ID and storing it in a variable, you're ready to communicate with it. To do this, you use the Send function. Morpheus, himself, is a parallel process running in ao. He receives and sends messages using a series of Handlers. Let's send him a message and see what happens.luaSend({ Target = Morpheus, Data = \"Morpheus?\" })Your Target is Morpheus which is the variable we defined earlier using Morpheus's process ID.The Data is the message you want to send to Morpheus. In this case, it's \"Morpheus?\".Expected Results:lua-- Your Message Command Send({ Target = Morpheus, Data = \"Morpheus?\"}) -- Message is added to the outbox message added to outbox -- A New Message is received from `Morpheus`'s process ID New Message From BWM...ulw: Data = I am here. You are fYou've sent a message to Morpheus and received a response, but you can't read the full message. Let's learn about the Inbox and how to read messages.Step 6: The Inbox ​The Inbox is where you receive messages from other processes.INFOTo see an in depth view of an inbox message, head over to the Messages Concepts page.Let's check your inbox to see how many messages you have received.Inside your aos CLI, type the following command:lua #InboxIf you're actively following through the tutorial, the inbox will not have many messages. However, if you've been experimenting with the aos environment, you may more than 1 message in your inbox.Example Return:lua-- Your Inbox Command #Inbox -- The command will return the number of messages in your inbox. 4In the example above, the return is 4, stating that there are four messages in the inbox.As we're actively looking for Morpheus's response, we'll assume his message was the last one received. To read the last message in your inbox, type the following command:lua Inbox[#Inbox].DataThis command allows you to isolate the Data from the message and only read the contents of the data.The Expected Return:lua-- Your Inbox[x].Data Command Inbox[#Inbox].Data -- The command will return the `Data` of the message. -- Data is what usually represents the text-based message -- received from one process to another. I am here. You are finally awake. Are you ready to see how far the rabbit hole goes?You are now using your own process to communicate with Morpheus, another parallel process running in ao. You're now ready to move on to the next step in the tutorial.Step 7: Sending Messages with Tags ​Purpose of Tags: Tags in aos messages are used to categorize, route, and process messages efficiently. They play a crucial role in message handling, especially when dealing with multiple processes or complex workflows.Some processes use Handlers that specifically interact with messages that have certain tags. For example, a process may have a handler that only interacts with messages that have a specific tag, which we'll see an example of in the chatroom tutorial.How to Use Tags in Messages ​In the case of Morpheus, we can use tags to categorize our messages, and because Morpheus is a autonomous process, he has handlers that can interact with messages that have certain tags.Adding Tags to a Message:We already know that the Data of a message is the payload of the message you want to send to another process. Earlier, we sent a message to Morpheus without any tags, in which he used a handler to respond to an exact match within the Data field.Let's Show Morpheus That We're Ready ​Send Morpheus a message with the tag Action and the value rabbithole.Example:luaSend({ Target = Morpheus, Data = \"Code: rabbithole\", Action = \"Unlock\" })Read the message from Morpheus:luaInbox[#Inbox].DataExpected Return:Additional Tips for Using Tags ​Consistent Tagging: Develop a consistent tagging system for your application to make message handling more predictable.Tag Naming: Choose clear and descriptive names for your tags. This makes it easier to understand the purpose and context of messages at a glance.Security with Tags: Remember that tags are not encrypted or hidden, so avoid using sensitive information as tags.Advanced Usage of Tags ​Workflow Management: Tags can be instrumental in managing workflows, especially in systems where messages pass through multiple stages or processes.Additional Tips for Messaging ​Message Structure: Explore other fields like Epoch, From, and Nonce for more complex messaging needs.Debugging: Use the Dump function to print messages for debugging.Security Considerations: Be cautious with the content and handling of messages, and never send anything considered private or sensitive.Conclusion ​You've now learned how to send messages with tags, which is a powerful tool for categorizing and routing messages in aos.Morpheus has officially invited you to the next stage of your journey. You're now ready to move on to the next step in the tutorial, Creating a Chatroom.",
          "estimatedWords": 1151,
          "lastModified": "2025-06-27T16:11:59.698Z",
          "siteKey": "ao",
          "siteName": "AO Cookbook",
          "depth": 3,
          "crawledAt": "2025-06-27T16:11:59.699Z"
        },
        {
          "url": "https://cookbook_ao.arweave.net/tutorials/begin/chatroom.html",
          "title": "Building a Chatroom in aos",
          "content": "Building a Chatroom in aos ​INFOIf you've found yourself wanting to learn how to create a chatroom within ao, then that means we understand at least the basic methodology of sending and receiving messages. If not, it's suggested that you review the Messaging tutorial before proceeding.In this tutorial, we'll be building a chatroom within ao using the Lua scripting language. The chatroom will feature two primary functions:Register: Allows processes to join the chatroom.Broadcast: Sends messages from one process to all registered participants.Let's begin by setting up the foundation for our chatroom.Video Tutorial ​Step 1: The Foundation ​Open your preferred code editor, e.g. VS Code.INFOYou may find it helpful to have the Recommended Extensions installed in your code editor to enhance your Lua scripting experience.Create a new file named chatroom.lua.Step 2: Creating The Member List ​In chatroom.lua, you'll begin by initializing a list to track participants:luaMembers = Members or {}Save the chatroom.lua fileStep 3: Load the Chatroom into aos ​With chatroom.lua saved, you'll now load the chatroom into aos.If you haven't already, start your aos in your terminal inside the directory where chatroom.lua is savedIn the aos CLI, type the following script to incorporate your script into the aos process:lua.load chatroom.luaType Members, or whatever you named your user list, in aos. It should return an empty array { }.If you see an empty array, then your script has been successfully loaded into aos.Step 4: Creating Chatroom Functionalities ​The Registration Handler ​The register handler will allow processes to join the chatroom.Adding a Register Handler: Modify chatroom.lua to include a handler for Members to register to the chatroom with the following code:lua -- Modify `chatroom.lua` to include a handler for `Members` -- to register to the chatroom with the following code: Handlers.add( \"Register\", { Action = \"Register\"}, function (msg) table.insert(Members, msg.From) print(msg.From .. \" Registered\") msg.reply({ Data = \"Registered.\" }) end )This handler will allow processes to register to the chatroom by responding to the tag Action = \"Register\". A printed message will confirm stating Registered. will appear when the registration is successful.Reload and Test: Let's reload and test the script by registering ourselves to the chatroom.Save and reload the script in aos using .load chatroom.lua.Check to see if the register handler loaded with the following script:lua Handlers.listThis will return a list of all the handlers in the chatroom. Since this is most likely your first time developing in aos, you should only see one handler with the name Register.Let's test the registration process by registering ourselves to the chatroom:luaSend({ Target = ao.id, Action = \"Register\" })If successful, you should see that there was a message added to your outbox and that you then see a new printed message that says registered.Finally, let's check to see if we were successfully added to the Members list:lua MembersIf successful, you'll now see your process ID in the Members list.Adding a Broadcast Handler ​Now that you have a chatroom, let's create a handler that will allow you to broadcast messages to all members of the chatroom.Add the following handler to the chatroom.lua file:lua Handlers.add( \"Broadcast\", { Action = \"Broadcast\" }, function (msg) for _, recipient in ipairs(Members) do ao.send({Target = recipient, Data = msg.Data}) end msg.reply({Data = \"Broadcasted.\" }) end )This handler will allow you to broadcast messages to all members of the chatroom.Save and reload the script in aos using .load chatroom.lua.Let's test the broadcast handler by sending a message to the chatroom:luaSend({Target = ao.id, Action = \"Broadcast\", Data = \"Broadcasting My 1st Message\" }).receive().DataINFOWhile we use Send in the console for convenience, it's recommended to use ao.send in handlers - see the FAQ for more details.Step 5: Inviting Morpheus to the Chatroom ​Now that you've successfully registered yourself to the chatroom, let's invite Morpheus to join us. To do this, we'll send an invite to him that will allow him to register to the chatroom.Morpheus is an autonomous agent with a handler that will respond to the tag Action = \"Join\", in which will then have him use your Register tag to register to the chatroom.Let's send Morpheus an invitation to join the chatroom:luaSend({ Target = Morpheus, Action = \"Join\" })To confirm that Morpheus has joined the chatroom, check the Members list:luaMembersIf successful, you'll receive a broadcasted message from Morpheus.Step 6: Inviting Trinity to the Chatroom ​Within this message, he'll give you Trinity's process ID and tell you to invite her to the chatroom.Use the same processes to save her process ID as Trinity and to invite her to the chatroom as you did with Morpheus.If she successfully joins the chatroom, she'll then pose the next challenge to you, creating a token.Engaging Others in the Chatroom ​Onboarding Others ​Invite aos Users: Encourage other aos users to join your chatroom. They can register and participate in the broadcast.Provide Onboarding Instructions: Share a simple script with them for easy onboarding:lua-- Hey, let's chat on aos! Join my chatroom by sending this command in your aos environment: Send({ Target = [Your Process ID], Action = \"Register\" }) -- Then, you can broadcast messages using: Send({Target = [Your Process ID], Action = \"Broadcast\", Data = \"Your Message\" })Next Steps ​Congratulations! You've successfully built a chatroom in ao and have invited Morpheus to join you. You've also created a broadcast handler to send messages to all members of the chatroom.Next, you'll continue to engage with Morpheus, but this time you'll be adding Trinity to the conversation. She will lead you through the next set of challenges. Good Luck!",
          "estimatedWords": 909,
          "lastModified": "2025-06-27T16:11:59.802Z",
          "siteKey": "ao",
          "siteName": "AO Cookbook",
          "depth": 3,
          "crawledAt": "2025-06-27T16:11:59.802Z"
        },
        {
          "url": "https://cookbook_ao.arweave.net/tutorials/begin/token.html",
          "title": "Crafting a Token",
          "content": "Crafting a Token ​INFODiving deeper into the ao, you're now ready to create your own token, a symbol of value and exchange within this decentralized medium. If you've found yourself wanting to learn how to create a token, but haven't visited the Messaging and Build a Chatroom lessons, be sure to do so as this page is part of a multi-part interactive tutorial.When creating tokens, we'll continue to use the Lua Language within ao to mint a token, guided by the principles outlined in the Token Specification.Video Tutorial ​Continuing Down the Rabbit Hole ​In our last tutorial, Build a Chatroom, we learned how to create a chatroom within ao, invited both Morpheus and Trinity to the chatroom we created, and then Trinity has now asked for us to create a token for her as a way of proving ourselves worthy of continuing down the rabbit hole.Let us begin.The Two Paths To Building a Token ​There are two paths to take when building a token:The Blueprint: This is a predesigned template that helps you quickly build a token in ao. It is a great way to get started and can be customized to fit your needs.Check here to learn more about the Token Blueprint.The Manual Method: This is a step-by-step guide to building a token in ao from scratch. This path is for those who want to understand the inner workings of a token and how to build one from the ground up.Check here to review the full Build a Token guide.The Blueprint Method ​For this tutorial, we'll be using the Token Blueprint to create a token for Trinity. This is a predesigned template that helps you quickly build a token in ao.How To Use The Token Blueprint ​Make sure we're in the same directory as before during the previous steps in the tutorial.Open the Terminal.Start your aos process.Type in .load-blueprint tokenThis will load the required handlers for the tutorials token within ao. It's important to note that the token blueprint isn't specific to this tutorial and can be used as a foundation for any token you wish to create.Verify the Blueprint is Loaded ​Type in Handlers.list to see the newly loaded handlers.You should see a new list of handlers that have been loaded into your aos process. If you've been following along the with the previous steps in the tutorial, you should also see the handlers for your chatroom, as well.Example:Testing the Token ​Now that the token blueprint is loaded, we can test the token by sending a message to ourselves using the Action = \"Info\" tag.luaSend({ Target = ao.id, Action = \"Info\" }) Inbox[#Inbox].TagsThis will print the token information to the console. It should show your process ID with the total balance of tokens available.Sending Tokens to Trinity ​Now that we've tested the token and it's working as expected, we can send some tokens to Trinity. We'll send 1000 tokens to Trinity using the Action = \"Transfer\" tag.luaSend({ Target = ao.id, Action = \"Transfer\", Recipient = Trinity, Quantity = \"1000\"}).receive().DataWhen Trinity receives the tokens, she'll respond to the transfer with a message to confirm that she's received the tokens.Her response will look something like this:Trinity: \"Token received. Interesting. I wasn't sure you'd make it this far. I'm impressed, but we are not done yet. I want you to use this token to tokengate the chatroom. Do that, and then I will believe you could be the one.\"You've completed the process of creating a token and sending it to Trinity. You're now ready to move on to the next step in the tutorial. Tokengating the Chatroom.",
          "estimatedWords": 596,
          "lastModified": "2025-06-27T16:12:00.307Z",
          "siteKey": "ao",
          "siteName": "AO Cookbook",
          "depth": 3,
          "crawledAt": "2025-06-27T16:12:00.307Z"
        },
        {
          "url": "https://cookbook_ao.arweave.net/tutorials/begin/tokengating.html",
          "title": "Tokengating the Chatroom",
          "content": "Tokengating the Chatroom ​INFONow that we've created a token and sent it to Trinity, we can use the token to tokengate our chatroom. This will allow only those who have the token to enter the chatroom.Video Tutorial ​How to Tokengate the Chatroom ​Let's create a handler that will allow us to tokengate the chatroom. This handler will respond to the tag Action = \"Broadcast\" meaning it will replace the original Broadcast handler we built for our chatroom.Step 1: Start the same aos process. ​Be sure you're using the same aos process that you've used throughout the tutorial.Step 2: Open the chatroom.lua file. ​This is the same file we used to create the chatroom during the chatroom tutorial.Step 3: Edit your Broadcast handler. ​Replace the original Broadcast handler with the following code:luaHandlers.add( \"Broadcast\", { Action = \"Broadcast\" }, function(m) if Balances[m.From] == nil or tonumber(Balances[m.From]) < 1 then print(\"UNAUTH REQ: \" .. m.From) return end local type = m.Type or \"Normal\" print(\"Broadcasting message from \" .. m.From .. \". Content: \" .. m.Data) for i = 1, #Members, 1 do ao.send({ Target = Members[i], Action = \"Broadcasted\", Broadcaster = m.From, Data = m.Data }) end end )This handler will now check the balance of the sender's token before broadcasting the message to the chatroom. If the sender doesn't have a token, the message will not be broadcasted.Save the file.Step 4: Reload the chatroom.lua file. ​To replace the original broadcast handler with the new one, you'll need to reload the chatroom.lua file.lua.load chatroom.luaStep 5: Test the Tokengate ​Now that the chatroom is tokengated, let's test it by sending a message to the chatroom.From the original aos process ​First, we'll test it from the original aos process.luaSend({ Target = ao.id , Action = \"Broadcast\", Data = \"Hello\" })Expected Results:{ output = \"Message added to outbox\", ... } Broadcasting message from [Your Process ID]. Content: Hello. New Message From [Your Process ID]: Action = BroadcastedTesting from another Process ID. ​From a new aos process ​Now, let's test it from a new aos process that doesn't have a token. The following command creates a new AO process with the name \"chatroom-no-token\".shaos chatroom-no-token # the `chatroom-no-token` is the new process nameNext we need to register to the chatroom we built on our original process, from our new process. Hint: type ao.id into your console to get the Process ID of the process you are currently connected to.luaSend({ Target = \"Your_Original_Process_ID\", Action = \"Register\" })Expected Results:message added to outbox New Message From [Your Process ID]: Data = Registered.Now, let's try to send a message to the chatroom.luaSend({ Target = \"Your_Original_Process_ID\" , Action = \"Broadcast\", Data = \"Hello?\" })Expected Results:message added to outbox UNAUTH REQ: [New Process ID]As you can see, the message was not broadcasted because the new process doesn't have a token.Tell Trinity \"It is done\" ​From the original aos process, send a broadcast message to the chatroom saying, \"It is done\".luaSend({ Target = ao.id , Action = \"Broadcast\", Data = \"It is done\" })WARNINGIt's important to be aware of exact match data and case sensitivity. If you're not receiving a response from either Morpheus or Trinity, be sure to check the the content of your Data and Tags.Trinity will then respond to the chatroom being tokengated.Expected Results: ​Trinity will send a message saying, \"I guess Morpheus was right. You are the one. Consider me impressed. You are now ready to join The Construct, an exclusive chatroom available to only those that have completed this tutorial. Now, go join the others by using the same tag you used Register, with this process ID: [Construct Process ID] Good luck. -Trinity\". Additionally, a footer will follow the message.Conclusion ​You've done it! You've successfully tokengated the chatroom. This has now unlocked access to the Construct, where only those that have fully completed this tutorial can enter.Congratulations! ​You've shown a great deal of promise. I hope you've enjoyed this tutorial. You're now ready to build freely in ao.",
          "estimatedWords": 658,
          "lastModified": "2025-06-27T16:12:00.875Z",
          "siteKey": "ao",
          "siteName": "AO Cookbook",
          "depth": 3,
          "crawledAt": "2025-06-27T16:12:00.875Z"
        },
        {
          "url": "https://cookbook_ao.arweave.net/tutorials/bots-and-games/index.html",
          "title": "Bots and Games",
          "content": "Bots and Games ​NOTEBuild your own unique bot to complete Quest 3 and earn 1000 CRED, then enter games like the Grid to earn legacynet CRED 24/7!Leveraging insights from our previous chapter, this section will guide you through the realm of automation with bots in aos and the construction of games. You will learn to create autonomous agents, using them to navigate and interact with game environments effectively.Sections ​Getting Started with a Game ​0. # Let's Play A Game: Experience a game on aosEnhancing Game Interactions with Automation ​1. # Interpreting Announcements: Interpret in-game announcements2. # Fetching Game State: Retrieve and process the latest game state3. # Strategic Decisions: Utilize automation to determine your next move4. # Automated Responses: Streamline attack responses through automation5. # Bringing it Together: Combine your skills to craft an autonomous agentGame Development Insights ​6. # Mechanics of the Arena: Explore the underlying mechanics of a game's arena7. # Expanding the Arena: Build unique game logic upon the arenaA journey of discovery and creation awaits. Let the adventure begin!",
          "estimatedWords": 173,
          "lastModified": "2025-06-27T16:12:01.162Z",
          "siteKey": "ao",
          "siteName": "AO Cookbook",
          "depth": 3,
          "crawledAt": "2025-06-27T16:12:01.162Z"
        },
        {
          "url": "https://cookbook_ao.arweave.net/tutorials/bots-and-games/ao-effect.html",
          "title": "Lets Play A Game",
          "content": "Let's Play A Game! ​You've been powering through tutorials like a champ! Now, let's take a refreshing break and dive into something exciting. How about a game that adds a dash of fun to your learning journey?What's the game? ​ao-effect is a game where you can compete with friends or other players globally, in real-time, right from your terminal. We've set up a global game process for this adventure.The rules are simple. Each player starts on a 40x40 grid with health at 100 and energy at 0. Your energy replenishes over time to a maximum of 100. Navigate the grid, find other players, and use your energy to attack when they're within range. The battle continues until only one player remains or the allotted time expires.Checkout the guides on the Mechanics of the Arena and Expanding the Arena for a deeper understanding of the game.Heads Up: Don't sweat it if some command syntax seem unfamiliar. Focus on understanding the purpose of each command at a high level and, most importantly, enjoy the game!Preparing for an Adventure in ao-effect ​To join this global escapade, you'll need to set things up. Don't worry, it's as easy as 1-2-3!Install aosFire up your terminal and run:bashnpm i -g https://get_ao.arweave.netLaunch aosNext, create your instance of aos:bashaosSet Up the Game IDLet's keep our game server ID handy for quick access:luaGame = \"tm1jYBC0F2gTZ0EuUQKq5q_esxITDFkAG6QEpLbpI9I\"Print Game Announcements Directly To Terminal (Optional)Here's how you can write a handler for printing announcement details:This is temporary as we will be loading this via a lua script in the next section.luaHandlers.add( \"PrintAnnouncements\", { Action = \"Announcement\" }, function (msg) ao.send({Target = Game, Action = \"GetGameState\"}) print(msg.Event .. \": \" .. msg.Data) end )And voilà! You're all set to join the game.How to Register for a Game ​Ready to jump in? Just a few simple steps to get you going:Register with the Game Server ​All communication between processes in ao occurs through messages. To register, send this message to the game server:luaSend({ Target = Game, Action = \"Register\" }) -- Expected Result -- { output = \"Message added to outbox\", onReply = function: 0x29e5ac0, receive = function: 0x29fe440 } New Message From tm1...I9I: Action = Registered New Player Registered: a1b...y1z has joined in waiting.This places you in the Waiting Lobby. A small fee is needed to confirm your spot.Confirm your spot ​In order to confirm your spot you need some tokens. You can acquire them by sending the following message to the game:luaSend({ Target = Game, Action = \"RequestTokens\"}).receive().Data -- Expected Result -- You received 10000000 from a1b2C3d4e5F6g7h8IjkLm0nOpqR8s7t6U5v4w3X2y1zNOTEThe .receive().Data will wait for a response by adding a temporary Handler that only runs once and will print the response Data. If you would like to instead just wait for the response to hit your Inbox you can call Send() without .receive() and run Inbox[#Inbox].Data to see the response Data.Handler added by .receive():{ name = \"_once_0\", maxRuns = 1, pattern = { }, handle = function: 0x2925700 }Once you receive the tokens, confirm your spot by paying the game's entry fee like this:luaSend({ Target = Game, Action = \"Transfer\", Recipient = Game, Quantity = \"1000\"}).receive().Data -- Expected Result -- You transferred 1000 to tm1jYBC0F2gTZ0EuUQKq5q_esxITDFkAG6QEpLbpI9I New Message From tm1...I9I: Action = Payment-ReceivedWait for a few seconds, and you'll see live updates in your terminal about player payments and statuses.Let the Games Begin! ​Game Mechanics ​Game Start: The game begins after a 2-minute WaitTime if at least 2 players have paid. Non-paying players are removed. If not enough players pay, those who did are refunded.Players spawn at a random grid point once the game begins.It's Your Move! ​Making a Move: The first thing you can do is move around, no energy required! You can shift one square in any direction – up, down, left, right, or diagonally. Along with the direction you must also pass in your player id to help the game identify your move. Here's how:luaSend({ Target = Game, Action = \"PlayerMove\", Player = ao.id, Direction = \"DownRight\"})The available moves across the grid are as follows:luaUp = {x = 0, y = -1}, Down = {x = 0, y = 1}, Left = {x = -1, y = 0}, Right = {x = 1, y = 0}, UpRight = {x = 1, y = -1}, UpLeft = {x = -1, y = -1}, DownRight = {x = 1, y = 1}, DownLeft = {x = -1, y = 1}Keep in Mind: Directions are case sensitive!If you move off the grid, you'll pop up on the opposite side.Time to Strike! ​Launching an Attack: As the game progresses, you'll accumulate energy. Use it to attack other players within a 3x3 grid range. Your attack won't hurt you, but it will affect others in range.luaSend({ Target = Game, Action = \"PlayerAttack\", Player = ao.id, AttackEnergy = \"energy_integer\"})Health starts at 100 and decreases with hits from other players. Reach 0, and it's game over for you.Wrapping Up ​The game ends when there's one player left or time is up. Winners receive rewards, then it's back to the lobby for another round.Enjoyed the game? What if there was a way to make your experience even better or boost your odds of winning. Checkout the next guide to find out 🤔",
          "estimatedWords": 869,
          "lastModified": "2025-06-27T16:12:04.786Z",
          "siteKey": "ao",
          "siteName": "AO Cookbook",
          "depth": 3,
          "crawledAt": "2025-06-27T16:12:04.786Z"
        },
        {
          "url": "https://cookbook_ao.arweave.net/tutorials/bots-and-games/announcements.html",
          "title": "Interpreting Announcements",
          "content": "Interpreting Announcements ​Welcome back to your coding journey. It's time to use the skills you've acquired from previous tutorials to enhance your gaming experience.During the game, you've likely noticed announcements appearing in your terminal. These announcements are the game's way of communicating important events to players. However, these messages can sometimes seem cryptic or you might find yourself checking your inbox frequently for further details.Wouldn't it be convenient to access this information directly from your terminal? Well, there's a way to do that!By using handlers, you can create an autonomous agent to retrieve this information for you, marking the progression from simple bots to entities capable of interpreting and acting on game events directly.Setting up the Development Environment ​Start by creating a new file named bot.lua in your preferred directory.Ideally, this file should be placed in the same directory where your player process runs to ease the loading of the code. Else, you'll need to use relative paths to access the file.Writing the Code ​Let's dive into the logic.Each handler in aos requires three key pieces of information:name: A unique name for the handlerpattern: A pattern for the handler to identify, triggering its operationhandle: The operations to perform when the desired pattern is found.Here's how you can write a handler for printing announcement details:lua-- Handler to print game announcements directly in the terminal. Handlers.add( \"PrintAnnouncements\", { Action = \"Announcement\" }, function (msg) print(msg.Event .. \": \" .. msg.Data) end )In this case, the name of the handler is \"PrintAnnouncements\". It uses a special in-built utility (hasMatchingTags) represented by { Action = \"Announcement\" } to check if the incoming message has been tagged as an announcement. If true, the handler prints the Event and Data, which represent the title and description of the announcement.NOTEOnce a message is \"handled\", it will be discarded from your Inbox.Loading and Testing ​Now, let's bring this to life in the game.Navigate to your aos player terminal and enter a game session.Activate the handler by loading your bot.lua file with:lua.load bot.luaYou'll now see game announcements appear directly in your terminal, offering real-time insights without the need to sift through your inbox.Congratulations! You have just taken the first step in building a bot on aos. But let's keep working on adding more features to it 🌐",
          "estimatedWords": 378,
          "lastModified": "2025-06-27T16:12:05.262Z",
          "siteKey": "ao",
          "siteName": "AO Cookbook",
          "depth": 3,
          "crawledAt": "2025-06-27T16:12:05.262Z"
        },
        {
          "url": "https://cookbook_ao.arweave.net/tutorials/bots-and-games/game-state.html",
          "title": "Fetching Game State",
          "content": "Fetching Game State ​Now that you're seeing game announcements directly in your terminal, you have a better grasp of the game's dynamics. However, these insights are limited to specific actions occurring within the game.Wouldn't it be more useful to have on-demand access to comprehensive game data, like the positions, health, and energy of all players? This information could significantly improve your strategic planning, helping you assess threats, opportunities, and timing more effectively.If you thought of adding another handler to the bot created in the previous guide, you're absolutely right!Writing the Code ​Go back to your bot.lua file and update your existing handler as follows:luaHandlers.add( \"HandleAnnouncements\", { Action = \"Announcement\" }, function (msg) ao.send({Target = Game, Action = \"GetGameState\"}) print(msg.Event .. \": \" .. msg.Data) end )Adjustments to your handler include:Renaming to \"HandleAnnouncements\" to reflect its broader role.Addition of an extra operation to request the game for the updated state. The game is designed to respond to the GetGameState action tag.When you get a print of the announcement, you can check the latest message in your Inbox as follows:luaInbox[#Inbox]The Data field of this message contains the latest state of the game which includes:GameMode : Whether the game is in Waiting or Playing state.TimeRemaining : The time remaining for the game to start or end.Players : A table containing every player's stats like position, health and energy.But this can be taken a step further so that you can not just read but also use information from the latest state for other automations.Let's define a new variable that stores the latest state as follows:luaLatestGameState = LatestGameState or nilThe syntax preserves existing values of the variable when you load successive iterations of the bot.lua file in your terminal, instead of overwriting it. If there is no pre-existing value then a nil value is assigned to the variable.Then implement another handler as follows:lua-- Handler to update the game state upon receiving game state information. Handlers.add( \"UpdateGameState\", { Action = \"Announcement\" }, function (msg) local json = require(\"json\") LatestGameState = json.decode(msg.Data) ao.send({Target = ao.id, Action = \"UpdatedGameState\"}) print(\"Game state updated. Print \\'LatestGameState\\' for detailed view.\") end )The response from the game process from the previous handler has an action tag with the value GameState that helps us trigger this second handler. Once triggered, the handle function loads the in-built json package that parses the data into json and stores it in the LatestGameState variable.This handler additionally sends a message to your process indicating when the state has been updated. The significance of this feature will be explained in the following section.You can refer to the latest code for bot.lua in the dropdown below:Updated bot.lua fileluaLatestGameState = LatestGameState or nil Handlers.add( \"HandleAnnouncements\", { Action = \"Announcement\" }, function (msg) ao.send({Target = Game, Action = \"GetGameState\"}) print(msg.Event .. \": \" .. msg.Data) end ) Handlers.add( \"UpdateGameState\", { Action = \"GameState\" }, function (msg) local json = require(\"json\") LatestGameState = json.decode(msg.Data) ao.send({Target = ao.id, Action = \"UpdatedGameState\"}) print(\"Game state updated. Print \\'LatestGameState\\' for detailed view.\") end )Loading and Testing ​As usual, to test this new feature, load the file in your aos player terminal as follows:lua.load bot.luaThen check the LatestStateVariable to see if it has updated correctly by simply passing its name as follows:luaLatestGameStateWith real-time access to the latest state of the game you bot is equipped to make informed decisions decide your next action. Next let's try automating actions with the help of this data 🚶",
          "estimatedWords": 568,
          "lastModified": "2025-06-27T16:12:05.361Z",
          "siteKey": "ao",
          "siteName": "AO Cookbook",
          "depth": 3,
          "crawledAt": "2025-06-27T16:12:05.361Z"
        },
        {
          "url": "https://cookbook_ao.arweave.net/tutorials/bots-and-games/decisions.html",
          "title": "Strategic Decisions",
          "content": "Strategic Decisions ​With the latest game state at your disposal, your bot can evolve into an autonomous agent. This transition marks an upgrade in functionality, enabling not just reactions to game states but strategic actions that consider context, energy, and proximity to make decisions.Writing the Code ​Return to your bot.lua file and add the following functions:lua-- Determines proximity between two points. function inRange(x1, y1, x2, y2, range) return math.abs(x1 - x2) <= range and math.abs(y1 - y2) <= range end -- Strategically decides on the next move based on proximity and energy. function decideNextAction() local player = LatestGameState.Players[ao.id] local targetInRange = false for target, state in pairs(LatestGameState.Players) do if target ~= ao.id and inRange(player.x, player.y, state.x, state.y, 1) then targetInRange = true break end end if player.energy > 5 and targetInRange then print(\"Player in range. Attacking.\") ao.send({Target = Game, Action = \"PlayerAttack\", Player = ao.id, AttackEnergy = tostring(player.energy)}) else print(\"No player in range or insufficient energy. Moving randomly.\") local directionMap = {\"Up\", \"Down\", \"Left\", \"Right\", \"UpRight\", \"UpLeft\", \"DownRight\", \"DownLeft\"} local randomIndex = math.random(#directionMap) ao.send({Target = Game, Action = \"PlayerMove\", Player = ao.id, Direction = directionMap[randomIndex]}) end endThe decideNextAction function is now a testament to our agent's ability to think and act based on a comprehensive understanding of its environment. It analyzes the latest game state to either attack if you have sufficient energy and an opponent is inRange or move otherwise.Now all you need is a handler to make sure this function runs on its own.luaHandlers.add( \"decideNextAction\", { Action = \"UpdatedGameState\" }, function () if LatestGameState.GameMode ~= \"Playing\" then return end print(\"Deciding next action.\") decideNextAction() end )This handler triggers upon receiving a message that the latest game state has been fetched and updated. An action is taken only when the game is in Playing mode.You can refer to the latest code for bot.lua in the dropdown below:Updated bot.lua fileluaLatestGameState = LatestGameState or nil function inRange(x1, y1, x2, y2, range) return math.abs(x1 - x2) <= range and math.abs(y1 - y2) <= range end function decideNextAction() local player = LatestGameState.Players[ao.id] local targetInRange = false for target, state in pairs(LatestGameState.Players) do if target ~= ao.id and inRange(player.x, player.y, state.x, state.y, 1) then targetInRange = true break end end if player.energy > 5 and targetInRange then print(\"Player in range. Attacking.\") ao.send({Target = Game, Action = \"PlayerAttack\", Player = ao.id, AttackEnergy = tostring(player.energy)}) else print(\"No player in range or insufficient energy. Moving randomly.\") local directionMap = {\"Up\", \"Down\", \"Left\", \"Right\", \"UpRight\", \"UpLeft\", \"DownRight\", \"DownLeft\"} local randomIndex = math.random(#directionMap) ao.send({Target = Game, Action = \"PlayerMove\", Player = ao.id, Direction = directionMap[randomIndex]}) end end Handlers.add( \"HandleAnnouncements\", { Action = \"Announcement\" }, function (msg) ao.send({Target = Game, Action = \"GetGameState\"}) print(msg.Event .. \": \" .. msg.Data) end ) Handlers.add( \"UpdateGameState\", { Action = \"GameState\" }, function (msg) local json = require(\"json\") LatestGameState = json.decode(msg.Data) ao.send({Target = ao.id, Action = \"UpdatedGameState\"}) end ) Handlers.add( \"decideNextAction\", { Action = \"UpdatedGameState\" }, function () if LatestGameState.GameMode ~= \"Playing\" then return end print(\"Deciding next action.\") decideNextAction() end )Loading and Testing ​Once again, to test out the latest upgrades, load the file in your aos player terminal as follows:lua.load bot.luaObserve your process output to see the decisions your autonomous agent makes in real-time, leveraging the current game state for strategic advantage. But what if another player attacks you and runs away while you are deciding the next move? In the next section you'll learn to automatically counter as soon as you have been attacked 🤺",
          "estimatedWords": 572,
          "lastModified": "2025-06-27T16:12:05.859Z",
          "siteKey": "ao",
          "siteName": "AO Cookbook",
          "depth": 3,
          "crawledAt": "2025-06-27T16:12:05.859Z"
        },
        {
          "url": "https://cookbook_ao.arweave.net/tutorials/bots-and-games/attacking.html",
          "title": "Automated Responses",
          "content": "Automated Responses ​Following our last guide, our creation has progressed from a simple bot to a sophisticated autonomous agent. Now, let's further enhance its capabilities by adding a counterattack feature, allowing it to instantly retaliate against an opponent's attack, potentially catching them off-guard before they can retreat to safety.Writing the code ​Add the following handler to your bot.lua file and you're set:lua-- Handler to automatically attack when hit by another player. Handlers.add( \"ReturnAttack\", { Action = \"Hit\" }, function (msg) local playerEnergy = LatestGameState.Players[ao.id].energy if playerEnergy == undefined then print(\"Unable to read energy.\") ao.send({Target = Game, Action = \"Attack-Failed\", Reason = \"Unable to read energy.\"}) elseif playerEnergy == 0 then print(\"Player has insufficient energy.\") ao.send({Target = Game, Action = \"Attack-Failed\", Reason = \"Player has no energy.\"}) else print(\"Returning attack.\") ao.send({Target = Game, Action = \"PlayerAttack\", Player = ao.id, AttackEnergy = tostring(playerEnergy)}) end InAction = false ao.send({Target = ao.id, Action = \"Tick\"}) end )Whenever your player is under attack you receive a message with the Action Hit. This setup ensures your agent can make a swift counter attack, given it has sufficient energy.You can refer to the latest code for bot.lua in the dropdown below:Updated bot.lua fileluaLatestGameState = LatestGameState or nil function inRange(x1, y1, x2, y2, range) return math.abs(x1 - x2) <= range and math.abs(y1 - y2) <= range end function decideNextAction() local player = LatestGameState.Players[ao.id] local targetInRange = false for target, state in pairs(LatestGameState.Players) do if target ~= ao.id and inRange(player.x, player.y, state.x, state.y, 1) then targetInRange = true break end end if player.energy > 5 and targetInRange then print(\"Player in range. Attacking.\") ao.send({Target = Game, Action = \"PlayerAttack\", Player = ao.id, AttackEnergy = tostring(player.energy)}) else print(\"No player in range or insufficient energy. Moving randomly.\") local directionMap = {\"Up\", \"Down\", \"Left\", \"Right\", \"UpRight\", \"UpLeft\", \"DownRight\", \"DownLeft\"} local randomIndex = math.random(#directionMap) ao.send({Target = Game, Action = \"PlayerMove\", Player = ao.id, Direction = directionMap[randomIndex]}) end end Handlers.add( \"HandleAnnouncements\", { Action = \"Announcement\" }, function (msg) ao.send({Target = Game, Action = \"GetGameState\"}) print(msg.Event .. \": \" .. msg.Data) end ) Handlers.add( \"UpdateGameState\", { Action = \"GameState\" }, function (msg) local json = require(\"json\") LatestGameState = json.decode(msg.Data) ao.send({Target = ao.id, Action = \"UpdatedGameState\"}) end ) Handlers.add( \"decideNextAction\", { Action = \"UpdatedGameState\" }, function () if LatestGameState.GameMode ~= \"Playing\" then return end print(\"Deciding next action.\") decideNextAction() end ) Handlers.add( \"ReturnAttack\", { Action = \"Hit\" }, function (msg) local playerEnergy = LatestGameState.Players[ao.id].energy if playerEnergy == undefined then print(\"Unable to read energy.\") ao.send({Target = Game, Action = \"Attack-Failed\", Reason = \"Unable to read energy.\"}) elseif playerEnergy == 0 then print(\"Player has insufficient energy.\") ao.send({Target = Game, Action = \"Attack-Failed\", Reason = \"Player has no energy.\"}) else print(\"Returning attack.\") ao.send({Target = Game, Action = \"PlayerAttack\", Player = ao.id, AttackEnergy = tostring(playerEnergy)}) end InAction = false ao.send({Target = ao.id, Action = \"Tick\"}) end )Loading and Testing ​To activate and test the counter attack feature, load the bot file in your aos player terminal:lua.load bot.luaWatch your terminal for the autonomous agent's reactions, now with the added ability to retaliate instantly. This feature showcases the agent's evolving strategic depth and autonomy. In the upcoming section, we'll consolidate all the knowledge we've gathered so far and add some features for optimization.",
          "estimatedWords": 529,
          "lastModified": "2025-06-27T16:12:05.937Z",
          "siteKey": "ao",
          "siteName": "AO Cookbook",
          "depth": 3,
          "crawledAt": "2025-06-27T16:12:05.937Z"
        },
        {
          "url": "https://cookbook_ao.arweave.net/tutorials/bots-and-games/bringing-together.html",
          "title": "Bringing it Together",
          "content": "Bringing it Together ​This final guide wraps up our series, where you've built up an autonomous agent piece by piece. Now, let's refine your agent with some optimizations that fine-tune its operations. Here's a quick overview of the key improvements made:Sequential Command Execution: The introduction of an InAction flag ensures that your agent's actions are sequential (next action occurs only when the previous is successfully executed). This critical addition prevents your agent from acting on outdated game states, enhancing its responsiveness and accuracy. The full implementation can be found in the final code for the bot.lua file below.luaInAction = InAction or false -- Prevents the agent from taking multiple actions at once.Dynamic State Updates and Decisions: The agent now employs an automatic tick logic, allowing for dynamic updates and decisions. This logic enables the agent to self-trigger state updates and make subsequent decisions either upon receiving a Tick message or upon completing an action, promoting autonomous operation.luaHandlers.add(\"GetGameStateOnTick\", { Action = \"Tick\" }, function () if not InAction then InAction = true ao.send({Target = Game, Action = \"GetGameState\"}) end end)Automated Fee Transfer: To further streamline its operation and ensure uninterrupted participation in games, the autonomous agent now autonomously handles the transfer of confirmation fees.luaHandlers.add(\"AutoPay\", { Action = \"AutoPay\" }, function () ao.send({Target = Game, Action = \"Transfer\", Recipient = Game, Quantity = \"1000\"}) end)In addition to these features, we've also added a logging function for debugging purposes and colored prints for better comprehension of game events. These enhancements collectively make your autonomous agent more efficient and adaptable in the game environment.Check out the complete bot.lua code in the dropdown below, with all new additions highlighted accordingly:Updated bot.lua filelua-- Initializing global variables to store the latest game state and game host process. LatestGameState = LatestGameState or nil InAction = InAction or false -- Prevents the agent from taking multiple actions at once. Logs = Logs or {} colors = { red = \"\\27[31m\", green = \"\\27[32m\", blue = \"\\27[34m\", reset = \"\\27[0m\", gray = \"\\27[90m\" } function addLog(msg, text) -- Function definition commented for performance, can be used for debugging Logs[msg] = Logs[msg] or {} table.insert(Logs[msg], text) end -- Checks if two points are within a given range. -- @param x1, y1: Coordinates of the first point. -- @param x2, y2: Coordinates of the second point. -- @param range: The maximum allowed distance between the points. -- @return: Boolean indicating if the points are within the specified range. function inRange(x1, y1, x2, y2, range) return math.abs(x1 - x2) <= range and math.abs(y1 - y2) <= range end -- Decides the next action based on player proximity and energy. -- If any player is within range, it initiates an attack; otherwise, moves randomly. function decideNextAction() local player = LatestGameState.Players[ao.id] local targetInRange = false for target, state in pairs(LatestGameState.Players) do if target ~= ao.id and inRange(player.x, player.y, state.x, state.y, 1) then targetInRange = true break end end if player.energy > 5 and targetInRange then print(colors.red .. \"Player in range. Attacking.\" .. colors.reset) ao.send({Target = Game, Action = \"PlayerAttack\", Player = ao.id, AttackEnergy = tostring(player.energy)}) else print(colors.red .. \"No player in range or insufficient energy. Moving randomly.\" .. colors.reset) local directionMap = {\"Up\", \"Down\", \"Left\", \"Right\", \"UpRight\", \"UpLeft\", \"DownRight\", \"DownLeft\"} local randomIndex = math.random(#directionMap) ao.send({Target = Game, Action = \"PlayerMove\", Player = ao.id, Direction = directionMap[randomIndex]}) end InAction = false -- InAction logic added end -- Handler to print game announcements and trigger game state updates. Handlers.add( \"PrintAnnouncements\", { Action = \"Announcement\" }, function (msg) if msg.Event == \"Started-Waiting-Period\" then ao.send({Target = ao.id, Action = \"AutoPay\"}) elseif (msg.Event == \"Tick\" or msg.Event == \"Started-Game\") and not InAction then InAction = true -- InAction logic added ao.send({Target = Game, Action = \"GetGameState\"}) elseif InAction then -- InAction logic added print(\"Previous action still in progress. Skipping.\") end print(colors.green .. msg.Event .. \": \" .. msg.Data .. colors.reset) end ) -- Handler to trigger game state updates. Handlers.add( \"GetGameStateOnTick\", { Action = \"Tick\" }, function () if not InAction then -- InAction logic added InAction = true -- InAction logic added print(colors.gray .. \"Getting game state...\" .. colors.reset) ao.send({Target = Game, Action = \"GetGameState\"}) else print(\"Previous action still in progress. Skipping.\") end end ) -- Handler to automate payment confirmation when waiting period starts. Handlers.add( \"AutoPay\", { Action = \"AutoPay\" }, function (msg) print(\"Auto-paying confirmation fees.\") ao.send({ Target = Game, Action = \"Transfer\", Recipient = Game, Quantity = \"1000\"}) end ) -- Handler to update the game state upon receiving game state information. Handlers.add( \"UpdateGameState\", { Action = \"GameState\" }, function (msg) local json = require(\"json\") LatestGameState = json.decode(msg.Data) ao.send({Target = ao.id, Action = \"UpdatedGameState\"}) print(\"Game state updated. Print \\'LatestGameState\\' for detailed view.\") end ) -- Handler to decide the next best action. Handlers.add( \"decideNextAction\", { Action = \"UpdatedGameState\" }, function () if LatestGameState.GameMode ~= \"Playing\" then InAction = false -- InAction logic added return end print(\"Deciding next action.\") decideNextAction() ao.send({Target = ao.id, Action = \"Tick\"}) end ) -- Handler to automatically attack when hit by another player. Handlers.add( \"ReturnAttack\", { Action = \"Hit\" }, function (msg) if not InAction then -- InAction logic added InAction = true -- InAction logic added local playerEnergy = LatestGameState.Players[ao.id].energy if playerEnergy == undefined then print(colors.red .. \"Unable to read energy.\" .. colors.reset) ao.send({Target = Game, Action = \"Attack-Failed\", Reason = \"Unable to read energy.\"}) elseif playerEnergy == 0 then print(colors.red .. \"Player has insufficient energy.\" .. colors.reset) ao.send({Target = Game, Action = \"Attack-Failed\", Reason = \"Player has no energy.\"}) else print(colors.red .. \"Returning attack.\" .. colors.reset) ao.send({Target = Game, Action = \"PlayerAttack\", Player = ao.id, AttackEnergy = tostring(playerEnergy)}) end InAction = false -- InAction logic added ao.send({Target = ao.id, Action = \"Tick\"}) else print(\"Previous action still in progress. Skipping.\") end end )What's next? ​You're now equipped with the knowledge to craft intelligent autonomous agents. It's time to apply these insights into the game world. Understand the game's intricacies and leverage your agent's capabilities to dominate the arena. But there's more to come.In future sections, we'll dive deeper into the game arena, offering advanced strategies to elevate your agent's performance. Ready to take on the challenge? Let's see what you can create! 🕹️",
          "estimatedWords": 1021,
          "lastModified": "2025-06-27T16:12:06.496Z",
          "siteKey": "ao",
          "siteName": "AO Cookbook",
          "depth": 3,
          "crawledAt": "2025-06-27T16:12:06.496Z"
        },
        {
          "url": "https://cookbook_ao.arweave.net/tutorials/bots-and-games/arena-mechanics.html",
          "title": "Mechanics of the Arena",
          "content": "Mechanics of the Arena ​This guide provides a comprehensive overview of the fundamental mechanics essential for designing and managing arena-style games in aos. In arena games, participants engage in rounds, strategically vying to eliminate each other until a sole victor emerges.The framework presented here lays the groundwork for crafting a wide range of games, all sharing the same core functionalities. Explore the intricacies of game development and unleash your creativity within this versatile arena.Core Functionalities ​Now, let's dive into the core functionalities that power arena-style games:Game Progression Modes:Arena games are structured into rounds that operate in a loop with the following progression modes: \"Not-Started\" → \"Waiting\" → \"Playing\" → [Someone wins or timeout] → \"Waiting\"...NOTEThe loop timesout if there are not enough players to start a game after the waiting state.Rounds offer a defined timeframe for players to engage, intensifying the excitement of gameplay.Token Stakes:Players must deposit a specified quantity of tokens (defined by PaymentQty) to participate in the game. These tokens add a tangible stake element to the game.Bonus Rewards:Beyond the thrill of victory, players are enticed by the prospect of extra rewards. The builder has the flexibility to offer bonus tokens, defined by BonusQty, to be distributed per round. Any bets placed by players are also added to these bonuses. These bonuses serve as an additional incentive, enhancing the competitive spirit of the gameplay.Player Management:Players waiting to join the next game are tracked in the Waiting table.Active players and their game states are stored in the Players table.Eliminated players are promptly removed from the Players table and placed in the Waiting table for the next game.Round Winner Reward:When a player eliminates another, they earn not only bragging rights but also the eliminated player's deposit tokens as a reward. Additionally, winners of each round share a portion of the bonus tokens, as well as their original stake, further motivating players to strive for victory.Listener Mode:For those who prefer to watch the action unfold, the \"Listen\" mode offers an opportunity to stay informed without active participation. Processes can register as listeners, granting them access to all announcements from the game. While they do not engage as players, listeners can continue to observe the game's progress unless they explicitly request removal.Game State Management:To maintain the flow and fairness of arena games, an automated system oversees game state transitions. These transitions encompass waiting, playing, and ending phases. Time durations for each state, such as WaitTime and GameTime, ensure that rounds adhere to defined timeframes, preventing games from lasting indefinitely.You can refer to the code for the arena in the dropdown below:Arena Game Blueprintlua-- ARENA GAME BLUEPRINT. -- This blueprint provides the framework to operate an 'arena' style game -- inside an ao process. Games are played in rounds, where players aim to -- eliminate one another until only one remains, or until the game time -- has elapsed. The game process will play rounds indefinitely as players join -- and leave. -- When a player eliminates another, they receive the eliminated player's deposit token -- as a reward. Additionally, the builder can provide a bonus of these tokens -- to be distributed per round as an additional incentive. If the intended -- player type in the game is a bot, providing an additional 'bonus' -- creates an opportunity for coders to 'mine' the process's -- tokens by competing to produce the best agent. -- The builder can also provide other handlers that allow players to perform -- actions in the game, calling 'eliminatePlayer()' at the appropriate moment -- in their game logic to control the framework. -- Processes can also register in a 'Listen' mode, where they will receive -- all announcements from the game, but are not considered for entry into the -- rounds themselves. They are also not unregistered unless they explicitly ask -- to be. -- GLOBAL VARIABLES. -- Game progression modes in a loop: -- [Not-Started] -> Waiting -> Playing -> [Someone wins or timeout] -> Waiting... -- The loop is broken if there are not enough players to start a game after the waiting state. GameMode = GameMode or \"Not-Started\" StateChangeTime = StateChangeTime or undefined -- State durations (in milliseconds) WaitTime = WaitTime or 2 * 60 * 1000 -- 2 minutes GameTime = GameTime or 20 * 60 * 1000 -- 20 minutes Now = Now or undefined -- Current time, updated on every message. -- Token information for player stakes. UNIT = 1000 PaymentToken = PaymentToken or \"ADDR\" -- Token address PaymentQty = PaymentQty or tostring(math.floor(UNIT)) -- Quantity of tokens for registration BonusQty = BonusQty or tostring(math.floor(UNIT)) -- Bonus token quantity for winners -- Players waiting to join the next game and their payment status. Waiting = Waiting or {} -- Active players and their game states. Players = Players or {} -- Number of winners in the current game. Winners = 0 -- Processes subscribed to game announcements. Listeners = Listeners or {} -- Minimum number of players required to start a game. MinimumPlayers = MinimumPlayers or 2 -- Default player state initialization. PlayerInitState = PlayerInitState or {} -- Sends a state change announcement to all registered listeners. -- @param event: The event type or name. -- @param description: Description of the event. function announce(event, description) for ix, address in pairs(Listeners) do ao.send({ Target = address, Action = \"Announcement\", Event = event, Data = description }) end return print(Colors.gray .. \"Announcement: \" .. Colors.red .. event .. \" \" .. Colors.blue .. description .. Colors.reset) end -- Sends a reward to a player. -- @param recipient: The player receiving the reward. -- @param qty: The quantity of the reward. -- @param reason: The reason for the reward. function sendReward(recipient, qty, reason) if type(qty) ~= number then qty = tonumber(qty) end ao.send({ Target = PaymentToken, Action = \"Transfer\", Quantity = tostring(qty), Recipient = recipient, Reason = reason }) return print(Colors.gray .. \"Sent Reward: \" .. Colors.blue .. tostring(qty) .. Colors.gray .. ' tokens to ' .. Colors.green .. recipient .. \" \" .. Colors.blue .. reason .. Colors.reset ) end -- Starts the waiting period for players to become ready to play. function startWaitingPeriod() GameMode = \"Waiting\" StateChangeTime = Now + WaitTime announce(\"Started-Waiting-Period\", \"The game is about to begin! Send your token to take part.\") print('Starting Waiting Period') end -- Starts the game if there are enough players. function startGamePeriod() local paidPlayers = 0 for player, hasPaid in pairs(Waiting) do if hasPaid then paidPlayers = paidPlayers + 1 end end if paidPlayers < MinimumPlayers then announce(\"Not-Enough-Players\", \"Not enough players registered! Restarting...\") for player, hasPaid in pairs(Waiting) do if hasPaid then Waiting[player] = false sendReward(player, PaymentQty, \"Refund\") end end startWaitingPeriod() return end LastTick = undefined GameMode = \"Playing\" StateChangeTime = Now + GameTime for player, hasPaid in pairs(Waiting) do if hasPaid then Players[player] = playerInitState() else ao.send({ Target = player, Action = \"Ejected\", Reason = \"Did-Not-Pay\" }) removeListener(player) -- Removing player from listener if they didn't pay end end announce(\"Started-Game\", \"The game has started. Good luck!\") print(\"Game Started....\") end -- Handles the elimination of a player from the game. -- @param eliminated: The player to be eliminated. -- @param eliminator: The player causing the elimination. function eliminatePlayer(eliminated, eliminator) sendReward(eliminator, PaymentQty, \"Eliminated-Player\") Waiting[eliminated] = false Players[eliminated] = nil ao.send({ Target = eliminated, Action = \"Eliminated\", Eliminator = eliminator }) announce(\"Player-Eliminated\", eliminated .. \" was eliminated by \" .. eliminator .. \"!\") local playerCount = 0 for player, _ in pairs(Players) do playerCount = playerCount + 1 end print(\"Eliminating player: \" .. eliminated .. \" by: \" .. eliminator) -- Useful for tracking eliminations if playerCount < MinimumPlayers then endGame() end end -- Ends the current game and starts a new one. function endGame() print(\"Game Over\") Winners = 0 Winnings = tonumber(BonusQty) / Winners -- Calculating winnings per player for player, _ in pairs(Players) do Winners = Winners + 1 end Winnings = tonumber(BonusQty) / Winners for player, _ in pairs(Players) do -- addLog(\"EndGame\", \"Sending reward of:\".. Winnings + PaymentQty .. \"to player: \" .. player) -- Useful for tracking rewards sendReward(player, Winnings + tonumber(PaymentQty), \"Win\") Waiting[player] = false end Players = {} announce(\"Game-Ended\", \"Congratulations! The game has ended. Remaining players at conclusion: \" .. Winners .. \".\") startWaitingPeriod() end -- Removes a listener from the listeners' list. -- @param listener: The listener to be removed. function removeListener(listener) local idx = 0 for i, v in ipairs(Listeners) do if v == listener then idx = i break end end if idx > 0 then table.remove(Listeners, idx) end end -- HANDLERS: Game state management -- Handler for cron messages, manages game state transitions. Handlers.add( \"Game-State-Timers\", function(Msg) return \"continue\" end, function(Msg) Now = Msg.Timestamp if GameMode == \"Not-Started\" then startWaitingPeriod() elseif GameMode == \"Waiting\" then if Now > StateChangeTime then startGamePeriod() end elseif GameMode == \"Playing\" then if onTick and type(onTick) == \"function\" then onTick() end if Now > StateChangeTime then endGame() end end end ) -- Handler for player deposits to participate in the next game. Handlers.add( \"Transfer\", function(Msg) return Msg.Action == \"Credit-Notice\" and Msg.From == PaymentToken and tonumber(Msg.Quantity) >= tonumber(PaymentQty) and \"continue\" end, function(Msg) Waiting[Msg.Sender] = true ao.send({ Target = Msg.Sender, Action = \"Payment-Received\" }) announce(\"Player-Ready\", Msg.Sender .. \" is ready to play!\") end ) -- Registers new players for the next game and subscribes them for event info. Handlers.add( \"Register\", { Action = \"Register\" }, function(Msg) if Msg.Mode ~= \"Listen\" and Waiting[Msg.From] == undefined then Waiting[Msg.From] = false end removeListener(Msg.From) table.insert(Listeners, Msg.From) ao.send({ Target = Msg.From, Action = \"Registered\" }) announce(\"New Player Registered\", Msg.From .. \" has joined in waiting.\") end ) -- Unregisters players and stops sending them event info. Handlers.add( \"Unregister\", { Action = \"Unregister\" }, function(Msg) removeListener(Msg.From) ao.send({ Target = Msg.From, Action = \"Unregistered\" }) end ) -- Adds bet amount to BonusQty Handlers.add( \"AddBet\", { Reason = \"AddBet\" }, function(Msg) BonusQty = tonumber(BonusQty) + tonumber(Msg.Tags.Quantity) announce(\"Bet-Added\", Msg.From .. \"has placed a bet. \" .. \"BonusQty amount increased by \" .. Msg.Tags.Quantity .. \"!\") end ) -- Retrieves the current game state. Handlers.add( \"GetGameState\", { Action = \"GetGameState\" }, function (Msg) local json = require(\"json\") local TimeRemaining = StateChangeTime - Now local GameState = json.encode({ GameMode = GameMode, TimeRemaining = TimeRemaining, Players = Players, }) ao.send({ Target = Msg.From, Action = \"GameState\", Data = GameState}) end ) -- Alerts users regarding the time remaining in each game state. Handlers.add( \"AnnounceTick\", { Action = \"Tick\" }, function (Msg) local TimeRemaining = StateChangeTime - Now if GameMode == \"Waiting\" then announce(\"Tick\", \"The game will start in \" .. (TimeRemaining/1000) .. \" seconds.\") elseif GameMode == \"Playing\" then announce(\"Tick\", \"The game will end in \" .. (TimeRemaining/1000) .. \" seconds.\") end end ) -- Sends tokens to players with no balance upon request Handlers.add( \"RequestTokens\", { Action = \"RequestTokens\" }, function (Msg) print(\"Transferring Tokens: \" .. tostring(math.floor(10000 * UNIT))) ao.send({ Target = ao.id, Action = \"Transfer\", Quantity = tostring(math.floor(10000 * UNIT)), Recipient = Msg.From, }) end )Arena Game Blueprint ​For those interested in using this arena framework, we've made this code easily accessible through a blueprint. Simply run the following code in your terminal:lua.load-blueprint arenaSummary ​Understanding the mechanics of the arena can not only help you improve your autonomous agent created in the previous section but also empowers you to harness core functionalities for crafting your unique games.In the upcoming section, \"Building a Game,\" we will dive deep into the art of utilizing these mechanics to construct captivating and one-of-a-kind games within this framework. Get ready to embark on a journey into the dynamic realm of game development! 🎮",
          "estimatedWords": 1912,
          "lastModified": "2025-06-27T16:12:06.596Z",
          "siteKey": "ao",
          "siteName": "AO Cookbook",
          "depth": 3,
          "crawledAt": "2025-06-27T16:12:06.596Z"
        },
        {
          "url": "https://cookbook_ao.arweave.net/tutorials/bots-and-games/build-game.html",
          "title": "Expanding the Arena",
          "content": "Expanding the Arena ​Welcome to the final guide of Chapter 2, where you'll learn to build your own game on top of the arena framework introduced in the previous tutorial. In this guide, we'll take you through the process of creating the \"ao-effect\" game, which you experienced at the beginning of this chapter. As you progress through this example, you'll gain insights into structuring your game's logic and interacting with the arena's core code.Whether you're a seasoned developer or an aspiring game creator, this guide will empower you to unleash your creativity and bring your unique game ideas to life within the aos environment.Setting up the Development Environment ​Start by creating a new file named ao-effect.lua in your preferred directory.NOTEIdeally, this file should be placed in the same directory where your game process runs to ease the loading of the code. Else, you'll need to use relative paths to access the file.Writing the Code ​Now, let's dive into the logic.You'll notice that your game logic will involve calling functions and variables defined in the arena's logic. This showcases the power of composability, where your game builds on top of the existing arena logic, allowing seamless integration of variables and functions between the two. Because both logic become part of a unified logic for the game process.Initializing Game Mechanics ​First, define essential variables and functions that set the stage for your game's mechanics:lua-- AO EFFECT: Game Mechanics for AO Arena Game -- Game grid dimensions Width = 40 -- Width of the grid Height = 40 -- Height of the grid Range = 1 -- The distance for blast effect -- Player energy settings MaxEnergy = 100 -- Maximum energy a player can have EnergyPerSec = 1 -- Energy gained per second -- Attack settings AverageMaxStrengthHitsToKill = 3 -- Average number of hits to eliminate a player -- Initializes default player state -- @return Table representing player's initial state function playerInitState() return { x = math.random(Width/8), y = math.random(Height/8), health = 100, energy = 0 } end -- Function to incrementally increase player's energy -- Called periodically to update player energy function onTick() if GameMode ~= \"Playing\" then return end -- Only active during \"Playing\" state if LastTick == undefined then LastTick = Now end local Elapsed = Now - LastTick if Elapsed >= 1000 then -- Actions performed every second for player, state in pairs(Players) do local newEnergy = math.floor(math.min(MaxEnergy, state.energy + (Elapsed * EnergyPerSec // 2000))) state.energy = newEnergy end LastTick = Now end endThis code initializes your game's mechanics, including grid dimensions, player energy, and attack settings. The playerInitState function sets up the initial state for players when the game begins.Player Movement ​Next, add the code for player movement:lua-- Handles player movement -- @param msg: Message request sent by player with movement direction and player info function move(msg) local playerToMove = msg.From local direction = msg.Tags.Direction local directionMap = { Up = {x = 0, y = -1}, Down = {x = 0, y = 1}, Left = {x = -1, y = 0}, Right = {x = 1, y = 0}, UpRight = {x = 1, y = -1}, UpLeft = {x = -1, y = -1}, DownRight = {x = 1, y = 1}, DownLeft = {x = -1, y = 1} } -- calculate and update new coordinates if directionMap[direction] then local newX = Players[playerToMove].x + directionMap[direction].x local newY = Players[playerToMove].y + directionMap[direction].y -- updates player coordinates while checking for grid boundaries Players[playerToMove].x = (newX - 1) % Width + 1 Players[playerToMove].y = (newY - 1) % Height + 1 announce(\"Player-Moved\", playerToMove .. \" moved to \" .. Players[playerToMove].x .. \",\" .. Players[playerToMove].y .. \".\") else ao.send({Target = playerToMove, Action = \"Move-Failed\", Reason = \"Invalid direction.\"}) end onTick() -- Optional: Update energy each move endThe move function calculates new player coordinates based on the chosen direction while ensuring that players remain within the grid boundaries. Player movement adds dynamic interaction to your game and is announced to all players and listeners.Player Attacks ​Then you must implement the logic for player attacks:lua-- Handles player attacks -- @param msg: Message request sent by player with attack info and player state function attack(msg) local player = msg.From local attackEnergy = tonumber(msg.Tags.AttackEnergy) -- get player coordinates local x = Players[player].x local y = Players[player].y -- check if player has enough energy to attack if Players[player].energy < attackEnergy then ao.send({Target = player, Action = \"Attack-Failed\", Reason = \"Not enough energy.\"}) return end -- update player energy and calculate damage Players[player].energy = Players[player].energy - attackEnergy local damage = math.floor((math.random() * 2 * attackEnergy) * (1/AverageMaxStrengthHitsToKill)) announce(\"Attack\", player .. \" has launched a \" .. damage .. \" damage attack from \" .. x .. \",\" .. y .. \"!\") -- check if any player is within range and update their status for target, state in pairs(Players) do if target ~= player and inRange(x, y, state.x, state.y, Range) then local newHealth = state.health - damage if newHealth <= 0 then eliminatePlayer(target, player) else Players[target].health = newHealth ao.send({Target = target, Action = \"Hit\", Damage = tostring(damage), Health = tostring(newHealth)}) ao.send({Target = player, Action = \"Successful-Hit\", Recipient = target, Damage = tostring(damage), Health = tostring(newHealth)}) end end end end -- Helper function to check if a target is within range -- @param x1, y1: Coordinates of the attacker -- @param x2, y2: Coordinates of the potential target -- @param range: Attack range -- @return Boolean indicating if the target is within range function inRange(x1, y1, x2, y2, range) return x2 >= (x1 - range) and x2 <= (x1 + range) and y2 >= (y1 - range) and y2 <= (y1 + range) endThe attack function calculates damage based on attack energy, checks player energy, and updates player health accordingly. Player attacks add the competitive element in your game, allowing players to engage with each other. The attacks are also announced to the players and listeners for real-time updates of the game.Handling the Logic ​Lastly, you must setup handlers:lua-- HANDLERS: Game state management for AO-Effect -- Handler for player movement Handlers.add(\"PlayerMove\", { Action = \"PlayerMove\" }, move) -- Handler for player attacks Handlers.add(\"PlayerAttack\", { Action = \"PlayerAttack\" }, attack)As seen in earlier guides, the handlers help trigger functions when their respective patterns are met.You can refer to the final code for ao-effect.lua in the dropdown below:Final ao-effect.lua filelua-- AO EFFECT: Game Mechanics for AO Arena Game -- Game grid dimensions Width = 40 -- Width of the grid Height = 40 -- Height of the grid Range = 1 -- The distance for blast effect -- Player energy settings MaxEnergy = 100 -- Maximum energy a player can have EnergyPerSec = 1 -- Energy gained per second -- Attack settings AverageMaxStrengthHitsToKill = 3 -- Average number of hits to eliminate a player -- Initializes default player state -- @return Table representing player's initial state function playerInitState() return { x = math.random(0, Width), y = math.random(0, Height), health = 100, energy = 0 } end -- Function to incrementally increase player's energy -- Called periodically to update player energy function onTick() if GameMode ~= \"Playing\" then return end -- Only active during \"Playing\" state if LastTick == undefined then LastTick = Now end local Elapsed = Now - LastTick if Elapsed >= 1000 then -- Actions performed every second for player, state in pairs(Players) do local newEnergy = math.floor(math.min(MaxEnergy, state.energy + (Elapsed * EnergyPerSec // 2000))) state.energy = newEnergy end LastTick = Now end end -- Handles player movement -- @param msg: Message request sent by player with movement direction and player info function move(msg) local playerToMove = msg.From local direction = msg.Tags.Direction local directionMap = { Up = {x = 0, y = -1}, Down = {x = 0, y = 1}, Left = {x = -1, y = 0}, Right = {x = 1, y = 0}, UpRight = {x = 1, y = -1}, UpLeft = {x = -1, y = -1}, DownRight = {x = 1, y = 1}, DownLeft = {x = -1, y = 1} } -- calculate and update new coordinates if directionMap[direction] then local newX = Players[playerToMove].x + directionMap[direction].x local newY = Players[playerToMove].y + directionMap[direction].y -- updates player coordinates while checking for grid boundaries Players[playerToMove].x = (newX - 1) % Width + 1 Players[playerToMove].y = (newY - 1) % Height + 1 announce(\"Player-Moved\", playerToMove .. \" moved to \" .. Players[playerToMove].x .. \",\" .. Players[playerToMove].y .. \".\") else ao.send({Target = playerToMove, Action = \"Move-Failed\", Reason = \"Invalid direction.\"}) end onTick() -- Optional: Update energy each move end -- Handles player attacks -- @param msg: Message request sent by player with attack info and player state function attack(msg) local player = msg.From local attackEnergy = tonumber(msg.Tags.AttackEnergy) -- get player coordinates local x = Players[player].x local y = Players[player].y -- check if player has enough energy to attack if Players[player].energy < attackEnergy then ao.send({Target = player, Action = \"Attack-Failed\", Reason = \"Not enough energy.\"}) return end -- update player energy and calculate damage Players[player].energy = Players[player].energy - attackEnergy local damage = math.floor((math.random() * 2 * attackEnergy) * (1/AverageMaxStrengthHitsToKill)) announce(\"Attack\", player .. \" has launched a \" .. damage .. \" damage attack from \" .. x .. \",\" .. y .. \"!\") -- check if any player is within range and update their status for target, state in pairs(Players) do if target ~= player and inRange(x, y, state.x, state.y, Range) then local newHealth = state.health - damage if newHealth <= 0 then eliminatePlayer(target, player) else Players[target].health = newHealth ao.send({Target = target, Action = \"Hit\", Damage = tostring(damage), Health = tostring(newHealth)}) ao.send({Target = player, Action = \"Successful-Hit\", Recipient = target, Damage = tostring(damage), Health = tostring(newHealth)}) end end end end -- Helper function to check if a target is within range -- @param x1, y1: Coordinates of the attacker -- @param x2, y2: Coordinates of the potential target -- @param range: Attack range -- @return Boolean indicating if the target is within range function inRange(x1, y1, x2, y2, range) return x2 >= (x1 - range) and x2 <= (x1 + range) and y2 >= (y1 - range) and y2 <= (y1 + range) end -- HANDLERS: Game state management for AO-Effect -- Handler for player movement Handlers.add(\"PlayerMove\", { Action = \"PlayerMove\" }, move) -- Handler for player attacks Handlers.add(\"PlayerAttack\", { Action = \"PlayerAttack\" }, attack)Loading and Testing ​Once you've written your game code, it's time to load it into the aos game process and test your game:lua.load ao-effect.luaIMPORTANTMake sure to load the arena blueprint in the same process as well.Invite friends or create test player processes to experience your game and make any necessary adjustments for optimal performance.What's Next ​Congratulations! You've successfully expanded the arena by building your own game on top of its core functionalities. Armed with the knowledge and tools acquired in this guide, you're now equipped to build games on aos independently.The possibilities are endless. Continue adding more features to existing games or create entirely new ones. The sky's the limit! ⌃◦🚀",
          "estimatedWords": 1824,
          "lastModified": "2025-06-27T16:12:07.060Z",
          "siteKey": "ao",
          "siteName": "AO Cookbook",
          "depth": 3,
          "crawledAt": "2025-06-27T16:12:07.060Z"
        },
        {
          "url": "https://cookbook_ao.arweave.net/guides/index.html",
          "title": "Guides",
          "content": "Guides ​This section provides detailed guides and documentation to help you build and deploy applications on the AO ecosystem. Whether you're creating chatrooms, autonomous bots, or complex decentralized applications, you'll find step-by-step instructions here.Core Technologies ​Comprehensive guides for AO's main technologies:AOS: Compute on AO - Learn how to use the AO operating system for distributed computingIntroduction to AOS - Get started with the AOS environmentInstallation Guide - Set up AOS on your systemCLI Usage - Learn the command-line interfaceAnd more...AO Connect: JavaScript Library - Interact with AO using JavaScriptInstallation - Set up the AO Connect libraryConnecting to AO - Establish connectionsSending Messages - Communicate with processesAnd more...Development Tools ​AO Module Builder CLI - Build WebAssembly modules for AO Installation - Install the development CLIProject Setup - Create your first projectBuilding & Deployment - Compile and deploy modulesUtilities & Storage ​Helpful tools and storage solutions:Using WeaveDrive - Store and manage data with WeaveDriveUsing SQLite - Integrate SQLite databases with your AO projectsAdditional Resources ​Community Resources - Connect with the AO communityRelease Notes - Stay updated on the latest changes and featuresNavigation ​Use the sidebar to browse through specific guides. Each guide provides detailed instructions and examples to help you build on AO.",
          "estimatedWords": 201,
          "lastModified": "2025-06-27T16:12:07.158Z",
          "siteKey": "ao",
          "siteName": "AO Cookbook",
          "depth": 3,
          "crawledAt": "2025-06-27T16:12:07.158Z"
        },
        {
          "url": "https://cookbook_ao.arweave.net/guides/aos/index.html",
          "title": "aos AO Operating System",
          "content": "aos: AO Operating System ​aos is a powerful operating system built on top of the AO hyper-parallel computer. While AO provides the distributed compute infrastructure, aos offers a simplified interface for interacting with and developing processes in this environment.What is aos? ​aos enables you to:Create and interact with processes on the AO networkDevelop distributed applications using a simple, intuitive approachLeverage the Lua programming language for deterministic, reliable operationsAll you need to get started is a terminal and a code editor. aos uses Lua as its primary language - a robust, deterministic, and user-friendly programming language that's ideal for distributed applications.New to AO? If you're just getting started, we recommend completing our tutorials first. They take just 15-30 minutes and provide an excellent foundation.Getting Started with aos ​Start here if you're new to aos:Introduction to aos - Overview of aos capabilities and conceptsInstallation Guide - Step-by-step instructions for setting up aosaos Command Line Interface - Learn to use the aos CLI effectivelyCustomizing Your Prompt - Personalize your aos development environmentLoad Lua Files - Learn how to load and execute Lua files in aosBuilding a Ping-Pong Server - Create your first interactive aos applicationBlueprints ​Blueprints in aos are templates that streamline the development of distributed applications by providing a framework for creating consistent and efficient processes across the AO network.Available Blueprints ​Chatroom - Template for building chatroom applicationsCred Utils - Tools for managing credentialsStaking - Framework for implementing staking mechanismsToken - Guide for creating and managing tokensVoting - Blueprint for setting up voting systemsaos Modules ​aos includes several built-in modules for common operations:JSON Module - Parse and generate JSON dataAO Module - Interface with the AO ecosystemCrypto Module - Perform cryptographic operationsBase64 Module - Encode and decode Base64 dataPretty Module - Format data for easier readingUtils Module - Common utility functionsDeveloper Resources ​More advanced topics for aos development:Editor Setup & Configuration - Configure your development environmentUnderstanding the Inbox & Message Handlers - Learn how message handling worksTroubleshooting with ao.link - Debug aos applicationsFrequently Asked Questions - Find answers to common questionsBuild a Token - Create your own token on AONavigation ​Use the sidebar to browse through specific aos guides. For a more structured learning path, we recommend following the guides in the order listed above.",
          "estimatedWords": 373,
          "lastModified": "2025-06-27T16:12:11.865Z",
          "siteKey": "ao",
          "siteName": "AO Cookbook",
          "depth": 3,
          "crawledAt": "2025-06-27T16:12:11.865Z"
        },
        {
          "url": "https://cookbook_ao.arweave.net/guides/aos/intro.html",
          "title": "Introduction",
          "content": "Introduction ​aos introduces a new approach to building processes — asynchronous, parallel-executing smart contracts. The ao computer is a decentralized computer network that allows compute to run anywhere and aos in a unique, interactive shell. You can use aos as your personal operating system, your development environment for building ao processes, and your bot army.Lets go over some basic commands.Variables ​If you want to display the contents of any variable through the console, simply type the variable name.luaNameInbox ​the Inbox is a collection of messages that your Process has received.luaInbox[1]If you want to get a count of messages, just add the # infront of Inbox.lua#InboxThe process of checking how many messages are in the inbox is a very common pattern. To make this easier, you can create a function that returns the number of messages within the inbox and displays it in the prompt.Use either .editor or .load file to load this function on your process.luafunction Prompt() return \"Inbox: \" .. #Inbox .. \" > \" endThe Expected Results:luaundefined Inbox: 2 >Your prompt now has changed to include the number of messages in your inbox.INFOThe Inbox is a Lua table (similar to an array) that contains messages received by your process that were not handled by any Handlers. The # operator is used to get the length of a table in Lua - so #Inbox returns the total number of unhandled messages currently in your inbox. This is a common Lua syntax pattern for getting the size/length of tables and strings.Globals ​In aos process there are some Globals that can make development a little more intuitive.NameDescriptionTypeInboxThis is a lua Table that stores all the messages that are received and not handlers by any handlers.Table(Array)Send(Message)This is a global function that is available in the interactive environment that allows you to send messages to ProcessesfunctionSpawn(Module, Message)This is a global function that is available in the aos interactive environment that allows you to spawn processesNamea string that is set on init that describes the name of your processstringOwnera string that is set on the init of the process that documents the owner of the process, warning if you change this value, it can brick you ability to interact with your processstringHandlersa lua Table that contains helper functions that allows you to create handlers that execute functionality based on the pattern matching function on inbound messagestableDumpa function that takes any lua Table and generates a print friendly output of the datafunctionUtilsa functional utility library with functions like map, reduce, filtermoduleaothis is a core function library for sending messages and spawing processesmoduleModules ​In aos there are some built in common lua modules that are already available for you to work with, these modules can be referenced with a \"require\" function.NameDescriptionjsona json module that allows you to encode and decode json documentsaocontains ao specific functions like send and spawn.base64a base64 module that allows you to encode and decode base64 text.prettya pretty print module using the function tprint to output formatted syntax.utilsan utility function library",
          "estimatedWords": 498,
          "lastModified": "2025-06-27T16:12:21.006Z",
          "siteKey": "ao",
          "siteName": "AO Cookbook",
          "depth": 3,
          "crawledAt": "2025-06-27T16:12:21.006Z"
        },
        {
          "url": "https://cookbook_ao.arweave.net/guides/aos/installing.html",
          "title": "Installing aos",
          "content": "Installing aos ​Installing aos only requires NodeJS - https://nodejs.orgNOTE: If you are on windows you may get better results with WSL Console.shnpm i -g https://get_ao.g8way.ioOnce installed you can run by typing aos",
          "estimatedWords": 32,
          "lastModified": "2025-06-27T16:12:21.066Z",
          "siteKey": "ao",
          "siteName": "AO Cookbook",
          "depth": 3,
          "crawledAt": "2025-06-27T16:12:21.066Z"
        },
        {
          "url": "https://cookbook_ao.arweave.net/guides/aos/cli.html",
          "title": "CLI",
          "content": "CLI ​There are some command-line arguments you pass to aos to do the following:[name] - create a new process or loads an existing process for your wallet--load <file> - load a file, you can add one or many of this command--cron <interval> - only used when creating a process--wallet <walletfile> - use a specific walletManaging multiple processes with aos ​shaosStarts or connects to a process with the name defaultshaos chatroomStarts or connects to a process with the name of chatroomshaos treasureRoomStarts or connects to a process with the name of treasureRoomLoad flag ​shaos treasureRoom --load greeting.lua --load treasure.lua --load puzzle.luaWith the load flag I can load many source files to my processCRON Flag ​If you want to setup your process to react on a schedule we need to tell ao, we do that when we spawn the process.shaos chatroom --cron 2-minutesTag flags ​With the tag flags, you can start a process with some custom tags (for e.g. using them as static environment variables):shaos chatroom --tag-name Chat-Theme --tag-value Dark --tag-name Chat-Name --tag-value MychatThe command above will add the extra tags to the transaction that spawns your process:ts// process data item tags [ ... { name: \"Chat-Theme\", value: \"Dark\" }, { name: \"Chat-Name\", value: \"Mychat\" } ... ]",
          "estimatedWords": 206,
          "lastModified": "2025-06-27T16:12:21.583Z",
          "siteKey": "ao",
          "siteName": "AO Cookbook",
          "depth": 3,
          "crawledAt": "2025-06-27T16:12:21.583Z"
        },
        {
          "url": "https://cookbook_ao.arweave.net/guides/aos/prompt.html",
          "title": "Customizing the Prompt in aos",
          "content": "Customizing the Prompt in aos ​Step 1: Open aos and Start the Editor ​Launch the aos command-line interface.Enter .editor to open the inline text editor.Step 2: Write the Custom Prompt Function ​In the editor, define your custom prompt function. For example:luafunction Prompt() return \"YourName@aos> \" endCustomize \"YourName@aos> \" to your preferred prompt text.Step 3: Exit and Run Your Code ​To exit the editor and execute your code, type .done and then press Enter.Your aos prompt should now display the new custom format.Step 4: Save for Future Use (Optional) ​If you wish to use this prompt in future aos sessions, save your script in a Lua file.In subsequent sessions, load this script to apply your custom prompt.Maximizing Your Prompt ​There's a great deal of utility and creativity that can come from customizing your prompt. Several things you can do within your prompt are:Tracking the number of unhandled messages you have in your inbox by creating a function that shows how many messages you have.lua --Example: function Prompt() return \"YourName Inbox: [\" .. #Inbox .. \"] > \" endTracking the number of members are within your process ID's chatroom.Tracking the balance of a specified token that your process ID holds.Conclusion ​Now that you understand how to maximize the utility within your Prompt, you've now gained a crucial step to streamlining your ao development experience.",
          "estimatedWords": 222,
          "lastModified": "2025-06-27T16:12:21.639Z",
          "siteKey": "ao",
          "siteName": "AO Cookbook",
          "depth": 3,
          "crawledAt": "2025-06-27T16:12:21.639Z"
        },
        {
          "url": "https://cookbook_ao.arweave.net/guides/aos/load.html",
          "title": "Load Lua Files with load filename",
          "content": "Load Lua Files with .load <filename> ​This feature allows you to load lua code from a source file on your local machine, this simple feature gives you a nice DX experience for working with aos processes.When creating handlers you may have a lot of code and you want to take advantage of a rich development environment like vscode. You can even install the lua extension to get some syntax checking.So how do you publish your local lua source code to your ao process? This is where the .load command comes into play.hello.lualuaHandlers.add( \"ping\", Handlers.utils.hasMatchingData(\"ping\"), Handlers.utils.reply(\"pong\") )aos shelllua.load hello.luaEasy Peasy! 🐶",
          "estimatedWords": 100,
          "lastModified": "2025-06-27T16:12:22.153Z",
          "siteKey": "ao",
          "siteName": "AO Cookbook",
          "depth": 3,
          "crawledAt": "2025-06-27T16:12:22.153Z"
        },
        {
          "url": "https://cookbook_ao.arweave.net/guides/aos/pingpong.html",
          "title": "Creating a Pingpong Process in aos",
          "content": "Creating a Pingpong Process in aos ​This tutorial will guide you through creating a simple \"ping-pong\" process in aos. In this process, whenever it receives a message with the data \"ping\", it will automatically reply with \"pong\". This is a basic example of message handling and interaction between processes in aos.Step 1: Open the aos CLI ​Start by opening your command-line interface and typing aos to enter the aos environment.Step 2: Access the Editor ​Type .editor in the aos CLI to open the inline text editor. This is where you'll write your ping-pong handler code.Step 3: Write the Pingpong Handler ​In the editor, enter the following Lua code to add a handler for the pingpong pattern:luaHandlers.add( \"pingpong\", Handlers.utils.hasMatchingData(\"ping\"), Handlers.utils.reply(\"pong\") )This lua script does three things:It adds a new handler named \"pingpong\".It uses Handlers.utils.hasMatchingData(\"ping\") to check if incoming messages contain the data \"ping\".If the message contains \"ping\", Handlers.utils.reply(\"pong\") automatically sends back a message with the data \"pong\".Step 4: Exit the Editor ​After writing your code, type .done and press Enter to exit the editor and run the script.Step 5: Test the Pingpong Process ​To test the process, send a message with the data \"ping\" to the process. You can do this by typing the following command in the aos CLI:luaSend({ Target = ao.id, Data = \"ping\" })The process should respond with a message containing \"pong\" in the Inbox.Step 6: Monitor the Inbox ​Check your Inbox to see the \"ping\" message and your Outbox to confirm the \"pong\" reply.luaInbox[#Inbox].DataStep 7: Experiment and Observe ​Experiment by sending different messages and observe how only the \"ping\" messages trigger the \"pong\" response.Step 8: Save Your Process (Optional) ​If you want to use this process in the future, save the handler code in a Lua file for easy loadinginto aos sessions.INFOADDITIONAL TIP:Handler Efficiency: The simplicity of the handler function is key. Ensure that it's efficient and only triggers under the correct conditions.Conclusion ​Congratulations! You have now created a basic ping-pong process in aos. This tutorial provides a foundation for understanding message handling and process interaction within the aos environment. As you become more comfortable with these concepts, you can expand to more complex processes and interactions, exploring the full potential of aos.",
          "estimatedWords": 366,
          "lastModified": "2025-06-27T16:12:22.219Z",
          "siteKey": "ao",
          "siteName": "AO Cookbook",
          "depth": 3,
          "crawledAt": "2025-06-27T16:12:22.219Z"
        },
        {
          "url": "https://cookbook_ao.arweave.net/guides/aos/editor.html",
          "title": "Editor setup",
          "content": "Editor setup ​Remembering all the built in ao functions and utilities can sometimes be hard. To enhance your developer experience, it is recommended to install the Lua Language Server extension into your favorite text editor and add the ao addon. It supports all built in aos modules and globals.VS Code ​Install the sumneko.lua extension:Search for \"Lua\" by sumneko in the extension marketplaceDownload and install the extensionOpen the VS Code command palette with Shift + Command + P (Mac) / Ctrl + Shift + P (Windows/Linux) and run the following command:> Lua: Open Addon ManagerIn the Addon Manager, search for \"ao\", it should be the first result. Click \"Enable\" and enjoy autocomplete!Other editors ​Verify that your editor supports the language server protocolInstall Lua Language Server by following the instructions at luals.github.ioInstall the \"ao\" addon to the language serverBetterIDEa ​BetterIDEa is a custom web based IDE for developing on ao.It offers a built in Lua language server with ao definitions, so you don't need to install anything. Just open the IDE and start coding!Features include:Code completionCell based notebook ui for rapid developmentEasy process managementMarkdown and Latex cell supportShare projects with anyone through ao processesTight integration with ao package managerRead detailed information about the various features and integrations of the IDE in the documentation.",
          "estimatedWords": 211,
          "lastModified": "2025-06-27T16:12:22.719Z",
          "siteKey": "ao",
          "siteName": "AO Cookbook",
          "depth": 3,
          "crawledAt": "2025-06-27T16:12:22.719Z"
        },
        {
          "url": "https://cookbook_ao.arweave.net/guides/aos/inbox-and-handlers.html",
          "title": "Understanding the Inbox",
          "content": "Understanding the Inbox ​In aos, processes are executed in response to messages via handlers. Unhandled messages are routed to the process's Inbox.What are Handlers? ​A handler is a function that receives and evaluates messages within your process. It acts upon messages by taking them as parameters.Handlers are defined using the Handlers.add() function.The function takes three parameters:Name of the HandlerMatcher functionHandle functionluaHandlers.add(\"name\", function (Msg) -- Does this message match (return true or false) return Msg.Action == \"Register\" end, function (Msg) print(\"Registered User.\") table.insert(Members, Msg.From) ao.send({Target = Msg.From, Data = \"Registered.\"}) end )What about Inboxes? ​An inbox is a storage area for messages that have not yet been processed. Think of it as a holding zone for incoming, or \"inbound,\" items awaiting handling. Once a message is processed, it's no longer considered \"inbound\" and thus leaves the inbox.Example: Consider the inbox like your voicemail. Just as an unanswered phone call is directed to voicemail for you to address later, messages that your Process doesn't immediately handle are sent to the inbox. This way, unhandled messages are stored until you're ready to process them.Summary ​Initially, it might seem like all messages are meant to land in your Inbox, which can be puzzling if they disappear after being handled. The analogy of a voicemail should clarify this: much like calls you answer don't go to voicemail, messages you handle won't appear in your Inbox. This illustrates the roles of both the Inbox and Handlers.",
          "estimatedWords": 240,
          "lastModified": "2025-06-27T16:12:23.302Z",
          "siteKey": "ao",
          "siteName": "AO Cookbook",
          "depth": 3,
          "crawledAt": "2025-06-27T16:12:23.302Z"
        },
        {
          "url": "https://cookbook_ao.arweave.net/guides/aos/troubleshooting.html",
          "title": "Troubleshooting using aolink",
          "content": "Troubleshooting using ao.link ​Working with a decentralized computer and network, you need to be able to troubleshoot more than your own code. You need to be able to track messages, token balances, token transfers of processes. This is where https://ao.link becomes an essential tool in your toolbox.Analytics ​AOLink has a set of 4 analytic measures:Total MessagesTotal UsersTotal ProcessesTotal ModulesThese analytics give you a quick view into the ao network's total processing health.Events ​Below, the analytics are the latest events that have appeared on the ao computer. You have a list of messages being scheduled and that have been executed. These events are any of the ao Data Protocol Types. And you can click on the Process ID or the Message ID to get details about each.Message Details ​The message details give you key details about:FromToBlock HeightCreatedTagsDataResult TypeDataIf you want to further troubleshoot and debug, you have the option to look at the result of the CU (Compute Unit) by clicking on \"Compute\".And further understand linked messages. Process Details ​The process details provide you with information about the process it's useful to see in the tags with what module this got instantiated from. If you notice on the left you see the interaction with the process displayed on a graph. In this case, this is DevChat, and you can see all the processes that have interacted by Registering and Broadcasting Messages.You can effortless check the Info Handler, by pressing the \"Fetch\" button. On the bottom you see the processes balance and all messages send, with the option to break it down into Token transfers and Token balances using the tabs. Further Questions? ​Feel free to reach out on the community Discord of Autonomous Finance, for all questions and support regarding ao.link. https://discord.gg/4kF9HKZ4WuSummary ​AOLink is an excellent tool for tracking events in the ao computer. Give it a try. Also, there is another scanner tool available on the permaweb: https://ao_marton.g8way.io/ - check it out!",
          "estimatedWords": 322,
          "lastModified": "2025-06-27T16:12:23.594Z",
          "siteKey": "ao",
          "siteName": "AO Cookbook",
          "depth": 3,
          "crawledAt": "2025-06-27T16:12:23.594Z"
        },
        {
          "url": "https://cookbook_ao.arweave.net/guides/aos/faq.html",
          "title": "FAQ",
          "content": "FAQ ​Ownership ​Understanding Process OwnershipStart a new process with the aos console, the ownership of the process is set to your wallet address. aos uses the Owner global variable to define the ownership of the process. If you wish to transfer ownership or lock the process so that no one can own, you simply modify the Owner variable to another wallet address or set it to nil.JSON ​encoding data as jsonWhen sending data to another process or an external service, you may want to use JSON as a way to encode the data for recipients. Using the json module in lua, you can encode and decode pure lua tables that contain values.luaSend({Target = Router, Data = require('json').encode({hello = \"world\"})})Send vs ao.send ​When to use Send vs ao.sendBoth functions send a message to a process, the difference is ao.send returns the message, in case you want to log it or troubleshoot. The Send function is intended to be used in the console for easier access. It is preferred to use ao.send in the handlers. But they are both interchangeable in aos.",
          "estimatedWords": 180,
          "lastModified": "2025-06-27T16:12:23.880Z",
          "siteKey": "ao",
          "siteName": "AO Cookbook",
          "depth": 3,
          "crawledAt": "2025-06-27T16:12:23.880Z"
        },
        {
          "url": "https://cookbook_ao.arweave.net/guides/aos/token.html",
          "title": "Building a Token in ao",
          "content": "Building a Token in ao ​When creating tokens, we'll continue to use the Lua Language within ao to mint a token, guided by the principles outlined in the Token Specification.Two Ways to Create Tokens: ​1 - Use the token blueprint:.load-blueprint tokenUsing the token blueprint will create a token with all the handlers and state already defined. This is the easiest way to create a token. You'll be able to customize those handlers and state to your after loading the blueprint.You can learn more about available blueprints here: BlueprintsINFOUsing the token blueprint will definitely get quickly, but you'll still want to understand how to load and test the token, so you can customize it to your needs.2 - Build from Scratch:The following guide will guide you through the process of creating a token from scratch. This is a more advanced way to create a token, but it will give you a better understanding of how tokens work.Preparations ​Step 1: Initializing the Token ​Open our preferred text editor, preferably from within the same folder you used during the previous tutorial.Create a new file named token.lua.Within token.lua, you'll begin by initializing the token's state, defining its balance, name, ticker, and more:lualocal json = require('json') if not Balances then Balances = { [ao.id] = 100000000000000 } end if Name ~= 'My Coin' then Name = 'My Coin' end if Ticker ~= 'COIN' then Ticker = 'COIN' end if Denomination ~= 10 then Denomination = 10 end if not Logo then Logo = 'optional arweave TxID of logo image' endLet's break down what we've done here:local json = require('json'): This first line of this code imports a module for later use.if not Balances then Balances = { [ao.id] = 100000000000000 } end: This second line is initializing a Balances table which is the way the Process tracks who posses the token. We initialize our token process ao.id to start with all the balance.The Next 4 Lines, if Name, if Ticker, if Denomination, and if not Logo are all optional, except for if Denomination, and are used to define the token's name, ticker, denomination, and logo respectively.INFOThe code if Denomination ~= 10 then Denomination = 10 end tells us the number of the token that should be treated as a single unit.Step 2: Info and Balances Handlers ​Incoming Message Handler ​Now lets add our first Handler to handle incoming Messages.luaHandlers.add('Info', Handlers.utils.hasMatchingTag('Action', 'Info'), function(msg) ao.send( { Target = msg.From, Tags = { [\"Name\"] = Name, [\"Ticker\"] = Ticker, [\"Logo\"] = Logo, [\"Denomination\"] = tostring(Denomination) } }) end)INFOAt this point, you've probably noticed that we're building all of the handlers inside the token.lua file rather than using .editor.With many handlers and processes, it's perfectly fine to create your handlers using .editor, but because we're creating a full process for initializing a token, setting up info and balances handlers, transfer handlers, and a minting handler, it's best to keep everything in one file.This also allows us to maintain consistency since each handler will be updated every time we reload the token.lua file into aos.This code means that if someone Sends a message with the Tag, Action = \"Info\", our token will Send back a message with all of the information defined above. Note the Target = msg.From, this tells ao we are replying to the process that sent us this message.Info & Token Balance Handlers ​Now we can add 2 Handlers which provide information about token Balances.luaHandlers.add('Balance', Handlers.utils.hasMatchingTag('Action', 'Balance'), function(msg) local bal = '0' -- If not Target is provided, then return the Senders balance if (msg.Tags.Target and Balances[msg.Tags.Target]) then bal = tostring(Balances[msg.Tags.Target]) elseif Balances[msg.From] then bal = tostring(Balances[msg.From]) end ao.send({ Target = msg.From, Data = json.encode(tonumber(bal)), Tags = { [\"Balance\"] = bal, [\"Ticker\"] = Ticker } }) end) Handlers.add('Balances', Handlers.utils.hasMatchingTag('Action', 'Balances'), function(msg) ao.send({ Target = msg.From, Data = json.encode(Balances) }) end)The first Handler above Handlers.add('Balance' handles a process or person requesting their own balance or the balance of a Target. Then replies with a message containing the info. The second Handler Handlers.add('Balances' just replies with the entire Balances table.Step 3: Transfer Handlers ​Before we begin testing we will add 2 more Handlers one which allows for the transfer of tokens between processes or users.luaHandlers.add('Transfer', Handlers.utils.hasMatchingTag('Action', 'Transfer'), function(msg) assert(type(msg.Tags.Recipient) == 'string', 'Recipient is required!') assert(type(msg.Tags.Quantity) == 'string', 'Quantity is required!') if not Balances[msg.From] then Balances[msg.From] = 0 end if not Balances[msg.Tags.Recipient] then Balances[msg.Tags.Recipient] = 0 end local qty = tonumber(msg.Tags.Quantity) assert(type(qty) == 'number', 'qty must be number') if Balances[msg.From] >= qty then Balances[msg.From] = Balances[msg.From] - qty Balances[msg.Tags.Recipient] = Balances[msg.Tags.Recipient] + qty --[[ Only Send the notifications to the Sender and Recipient if the Cast tag is not set on the Transfer message ]] -- if not msg.Tags.Cast then -- Debit-Notice message template, that is sent to the Sender of the transfer local debitNotice = { Target = msg.From, Action = 'Debit-Notice', Recipient = msg.Recipient, Quantity = tostring(qty), Data = Colors.gray .. \"You transferred \" .. Colors.blue .. msg.Quantity .. Colors.gray .. \" to \" .. Colors.green .. msg.Recipient .. Colors.reset } -- Credit-Notice message template, that is sent to the Recipient of the transfer local creditNotice = { Target = msg.Recipient, Action = 'Credit-Notice', Sender = msg.From, Quantity = tostring(qty), Data = Colors.gray .. \"You received \" .. Colors.blue .. msg.Quantity .. Colors.gray .. \" from \" .. Colors.green .. msg.From .. Colors.reset } -- Add forwarded tags to the credit and debit notice messages for tagName, tagValue in pairs(msg) do -- Tags beginning with \"X-\" are forwarded if string.sub(tagName, 1, 2) == \"X-\" then debitNotice[tagName] = tagValue creditNotice[tagName] = tagValue end end -- Send Debit-Notice and Credit-Notice ao.send(debitNotice) ao.send(creditNotice) end else ao.send({ Target = msg.Tags.From, Tags = { [\"Action\"] = 'Transfer-Error', ['Message-Id'] = msg.Id, [\"Error\"] = 'Insufficient Balance!' } }) end end)In summary, this code checks to make sure the Recipient and Quantity Tags have been provided, initializes the balances of the person sending the message and the Recipient if they dont exist and then attempts to transfer the specified quantity to the Recipient in the Balances table.luaBalances[msg.From] = Balances[msg.From] - qty Balances[msg.Tags.Recipient] = Balances[msg.Tags.Recipient] + qtyIf the transfer was successful a Debit-Notice is sent to the sender of the original message and a Credit-Notice is sent to the Recipient.lua-- Send Debit-Notice to the Sender ao.send({ Target = msg.From, Tags = { [\"Action\"] = 'Debit-Notice', [\"Recipient\"] = msg.Tags.Recipient, [\"Quantity\"] = tostring(qty) } }) -- Send Credit-Notice to the Recipient ao.send({ Target = msg.Tags.Recipient, Tags = { [\"Action\"] = 'Credit-Notice', [\"Sender\"] = msg.From, [\"Quantity\"] = tostring(qty) } })If there was insufficient balance for the transfer it sends back a failure messageluaao.send({ Target = msg.Tags.From, Tags = { [\"Action\"] = 'Transfer-Error', ['Message-Id'] = msg.Id, [\"Error\"] = 'Insufficient Balance!' } })The line if not msg.Tags.Cast then Means were not producing any messages to push if the Cast tag was set. This is part of the ao protocol.Step 4: Mint Handler ​Finally, we will add a Handler to allow the minting of new tokens.luaHandlers.add('Mint', Handlers.utils.hasMatchingTag('Action', 'Mint'), function(msg, env) assert(type(msg.Tags.Quantity) == 'string', 'Quantity is required!') if msg.From == env.Process.Id then -- Add tokens to the token pool, according to Quantity local qty = tonumber(msg.Tags.Quantity) Balances[env.Process.Id] = Balances[env.Process.Id] + qty else ao.send({ Target = msg.Tags.From, Tags = { [\"Action\"] = 'Mint-Error', [\"Message-Id\"] = msg.Id, [\"Error\"] = 'Only the Process Owner can mint new ' .. Ticker .. ' tokens!' } }) end end)This code checks to make sure the Quantity Tag has been provided and then adds the specified quantity to the Balances table.Loading and Testing ​Once you've created your token.lua file, or you've used .load-blueprint token, you're now ready to begin testing.1 - Start the aos process ​Make sure you've started your aos process by running aos in your terminal.2 - Loading the token.lua file ​If you've followed along with the guide, you'll have a token.lua file in the same directory as your aos process. From the aos prompt, load in the file.lua.load token.lua3 - Testing the Token ​Now we can send Messages to our aos process ID, from the same aos prompt to see if is working. If we use ao.id as the Target we are sending a message to ourselves.luaSend({ Target = ao.id, Action = \"Info\" })This should print the Info defined in the contract. Check the latest inbox message for the response.luaInbox[#Inbox].TagsThis should print the Info defined in the contract.INFOMake sure you numerically are checking the last message. To do so, run #Inbox first to see the total number of messages are in the inbox. Then, run the last message number to see the data.Example:If #Inbox returns 5, then run Inbox[5].Data to see the data.4 - Transfer ​Now, try to transfer a balance of tokens to another wallet or process ID.INFOIf you need another process ID, you can run aos [name] in another terminal window to get a new process ID. Make sure it's not the same aos [name] as the one you're currently using.Example:If you're using aos in one terminal window, you can run aos test in another terminal window to get a new process ID.luaSend({ Target = ao.id, Tags = { [\"Action\"] = \"Transfer\", [\"Recipient\"] = 'another wallet or processid', [\"Quantity\"] = '10000' }})After sending, you'll receive a printed message in the terminal similar to Debit-Notice on the sender's side and Credit-Notice on the recipient's side.5 - Check the Balances ​Now that you've transferred some tokens, let's check the balances.luaSend({ Target = ao.id, Tags = { [\"Action\"] = \"Balances\" }})luaInbox[#Inbox].DataYou will see two process IDs or wallet addresses, each displaying a balance. The first should be your sending process ID, the second should be the recipient's process ID.6 - Minting Tokens ​Finally, attempt to mint some tokens.luaSend({ Target = ao.id, Tags = { [\"Action\"] = \"Mint\", [\"Quantity\"] = '1000' }})And check the balances again.luaSend({ Target = ao.id, Tags = { [\"Action\"] = \"Balances\" }}) Inbox[#Inbox].DataYou'll then see the balance of the process ID that minted the tokens has increased.Conclusion ​That concludes the \"Build a Token\" guide. Learning out to build custom tokens will unlock a great deal of potential for your projects; whether that be creating a new currency, a token for a game, a governance token, or anything else you can imagine.",
          "estimatedWords": 1686,
          "lastModified": "2025-06-27T16:12:24.206Z",
          "siteKey": "ao",
          "siteName": "AO Cookbook",
          "depth": 3,
          "crawledAt": "2025-06-27T16:12:24.206Z"
        },
        {
          "url": "https://cookbook_ao.arweave.net/guides/aos/blueprints/index.html",
          "title": "Blueprints",
          "content": "Blueprints ​Blueprints are predesigned templates that help you quickly build in ao. They are a great way to get started and can be customized to fit your needs.Available Blueprints ​ChatroomCRED UtilsStakingTokenVoting",
          "estimatedWords": 31,
          "lastModified": "2025-06-27T16:12:24.786Z",
          "siteKey": "ao",
          "siteName": "AO Cookbook",
          "depth": 3,
          "crawledAt": "2025-06-27T16:12:24.786Z"
        },
        {
          "url": "https://cookbook_ao.arweave.net/guides/aos/blueprints/chatroom.html",
          "title": "Chatroom Blueprint",
          "content": "Chatroom Blueprint ​The Chatroom Blueprint is a predesigned template that helps you quickly build a chatroom in ao. It is a great way to get started and can be customized to fit your needs.Unpacking the Chatroom Blueprint ​Members: The Members array is used to store the users who have registered to the chatroom.Register Handler: The register handler allows processes to join the chatroom. When a process sends a message with the tag Action = \"Register\", the handler will add the process to the Members array and send a message back to the process confirming the registration.Broadcast Handler: The broadcast handler allows processes to send messages to all the members of the chatroom. When a process sends a message with the tag Action = \"Broadcast\", the handler will send the message to all the members of the chatroom.How To Use: ​Open your preferred text editor.Open the Terminal.Start your aos process.Type in .load-blueprint chatroomVerify the Blueprint is Loaded: ​Type in Handlers.list to see the newly loaded handlers.What's in the Chatroom Blueprint: ​luaMembers = Members or {} Handlers.add( \"register\", Handlers.utils.hasMatchingTag(\"Action\", \"Register\"), function (msg) table.insert(Members, msg.From) Handlers.utils.reply(\"registered\")(msg) end ) Handlers.add( \"broadcast\", Handlers.utils.hasMatchingTag(\"Action\", \"Broadcast\"), function (msg) for _, recipient in ipairs(Members) do ao.send({Target = recipient, Data = msg.Data}) end Handlers.utils.reply(\"Broadcasted.\")(msg) end )",
          "estimatedWords": 207,
          "lastModified": "2025-06-27T16:12:26.320Z",
          "siteKey": "ao",
          "siteName": "AO Cookbook",
          "depth": 3,
          "crawledAt": "2025-06-27T16:12:26.320Z"
        },
        {
          "url": "https://cookbook_ao.arweave.net/guides/aos/blueprints/cred-utils.html",
          "title": "CRED Utils Blueprint",
          "content": "CRED Utils Blueprint ​CRED is now deprecatedCRED was a token used during ao's legacynet phase to reward early developers. It is no longer earnable or redeemable.The CRED Utils Blueprint is a predesigned template that helps you quickly check your CRED balance in ao legacynet.Unpacking the CRED Utils Blueprint ​The CRED Metatable ​CRED.balance: Evaluating CRED.balance will print your process's last known balance of your CRED. If you have never fetched your CRED balance before, it will be fetched automatically. If you think your CRED has recently changed, consider running CRED.update first.CRED.process: Evaluating CRED.process will print the process ID of the CRED token issuer.CRED.send: Invoking CRED.send(targetProcessId, amount) like a function will transfer CRED from your ao process to another ao process.targetProcessId: string: the 43-character process ID of the recipient.amount: integer: The quantity of CRED units to send. 1 CRED === 1000 CRED units.CRED.update: Evaluating CRED.update will fetch your latest CRED balance by sending a message to the CRED issuer process. The UpdateCredBalance handler (see below) will ingest the response message.Handler Definitions ​Credit Handler: The CRED_Credit handler allows the CRED issuer process (and aos) to automatically notify you when your CRED balance increase.Debit Handler: The CRED_Debit handler allows the CRED issuer process (and aos) to automatically notify you when your CRED balance decreases.Update Balance Handler: The UpdateCredBalance handler ingests the response to any CRED.update requests.How To Use the Blueprint ​Open the Terminal.Start your aos process.Type in .load-blueprint credUtilsType in CRED.balanceWhat's in the CRED Utils Blueprint: ​See the aos source code on GitHub for the blueprint shipped in the latest version of aos.luaCRED_PROCESS = \"Sa0iBLPNyJQrwpTTG-tWLQU-1QeUAJA73DdxGGiKoJc\" _CRED = { balance = \"Your CRED balance has not been checked yet. Updating now.\" } local credMeta = { __index = function(t, key) -- sends CRED balance request if key == \"update\" then Send({ Target = CRED_PROCESS, Action = \"Balance\", Tags = { [\"Target\"] = ao.id } }) return \"Balance update requested.\" -- prints local CRED balance, requests it if not set elseif key == \"balance\" then if _CRED.balance == \"Your CRED balance has not been checked yet. Updating now.\" then Send({ Target = CRED_PROCESS, Action = \"Balance\", Tags = { [\"Target\"] = ao.id } }) end return _CRED.balance -- prints CRED process ID elseif key == \"process\" then return CRED_PROCESS -- tranfers CRED elseif key == \"send\" then return function(target, amount) -- ensures amount is string amount = tostring(amount) print(\"sending \" .. amount .. \"CRED to \" .. target) Send({ Target = CRED_PROCESS, Action = \"Transfer\", [\"Recipient\"] = target, [\"Quantity\"] = amount }) end else return nil end end } CRED = setmetatable({}, credMeta) -- Function to evaluate if a message is a balance update local function isCredBalanceMessage(msg) if msg.From == CRED_PROCESS and msg.Tags.Balance then return true else return false end end -- Function to evaluate if a message is a Debit Notice local function isDebitNotice(msg) if msg.From == CRED_PROCESS and msg.Tags.Action == \"Debit-Notice\" then return true else return false end end -- Function to evaluate if a message is a Credit Notice local function isCreditNotice(msg) if msg.From == CRED_PROCESS and msg.Tags.Action == \"Credit-Notice\" then return true else return false end end local function formatBalance(balance) -- Ensure balance is treated as a string balance = tostring(balance) -- Check if balance length is more than 3 to avoid unnecessary formatting if #balance > 3 then -- Insert dot before the last three digits balance = balance:sub(1, -4) .. \".\" .. balance:sub(-3) end return balance end -- Handles Balance messages Handlers.add( \"UpdateCredBalance\", isCredBalanceMessage, function(msg) local balance = nil if msg.Tags.Balance then balance = msg.Tags.Balance end -- Format the balance if it's not set if balance then -- Format the balance by inserting a dot after the first three digits from the right local formattedBalance = formatBalance(balance) _CRED.balance = formattedBalance print(\"CRED Balance updated: \" .. _CRED.balance) else print(\"An error occurred while updating CRED balance\") end end ) -- Handles Debit notices Handlers.add( \"CRED_Debit\", isDebitNotice, function(msg) print(msg.Data) end ) -- Handles Credit notices Handlers.add( \"CRED_Credit\", isCreditNotice, function(msg) print(msg.Data) end )",
          "estimatedWords": 659,
          "lastModified": "2025-06-27T16:12:26.618Z",
          "siteKey": "ao",
          "siteName": "AO Cookbook",
          "depth": 3,
          "crawledAt": "2025-06-27T16:12:26.618Z"
        },
        {
          "url": "https://cookbook_ao.arweave.net/guides/aos/blueprints/staking.html",
          "title": "Staking Blueprint",
          "content": "Staking Blueprint ​The Staking Blueprint is a predesigned template that helps you quickly build a staking system in ao. It is a great way to get started and can be customized to fit your needs.Prerequisites ​The Staking Blueprint requires the Token Blueprint to be loaded, first.Unpacking the Staking Blueprint ​Stakers: The Stakers array is used to store the staked tokens of the participants.Unstaking: The Unstaking array is used to store the unstaking requests of the participants.Stake Action Handler: The stake handler allows processes to stake tokens. When a process sends a message with the tag Action = \"Stake\", the handler will add the staked tokens to the Stakers array and send a message back to the process confirming the staking.Unstake Action Handler: The unstake handler allows processes to unstake tokens. When a process sends a message with the tag Action = \"Unstake\", the handler will add the unstaking request to the Unstaking array and send a message back to the process confirming the unstaking.Finalization Handler: The finalize handler allows processes to finalize the staking process. When a process sends a message with the tag Action = \"Finalize\", the handler will process the unstaking requests and finalize the staking process.How To Use: ​Open your preferred text editor.Open the Terminal.Start your aos process.Type in .load-blueprint stakingVerify the Blueprint is Loaded: ​Type in Handlers.list to see the newly loaded handlers.What's in the Staking Blueprint: ​luaStakers = Stakers or {} Unstaking = Unstaking or {} -- Stake Action Handler Handlers.stake = function(msg) local quantity = tonumber(msg.Tags.Quantity) local delay = tonumber(msg.Tags.UnstakeDelay) local height = tonumber(msg['Block-Height']) assert(Balances[msg.From] and Balances[msg.From] >= quantity, \"Insufficient balance to stake\") Balances[msg.From] = Balances[msg.From] - quantity Stakers[msg.From] = Stakers[msg.From] or {} Stakers[msg.From].amount = (Stakers[msg.From].amount or 0) + quantity Stakers[msg.From].unstake_at = height + delay end -- Unstake Action Handler Handlers.unstake = function(msg) local quantity = tonumber(msg.Tags.Quantity) local stakerInfo = Stakers[msg.From] assert(stakerInfo and stakerInfo.amount >= quantity, \"Insufficient staked amount\") stakerInfo.amount = stakerInfo.amount - quantity Unstaking[msg.From] = { amount = quantity, release_at = stakerInfo.unstake_at } end -- Finalization Handler local finalizationHandler = function(msg) local currentHeight = tonumber(msg['Block-Height']) -- Process unstaking for address, unstakeInfo in pairs(Unstaking) do if currentHeight >= unstakeInfo.release_at then Balances[address] = (Balances[address] or 0) + unstakeInfo.amount Unstaking[address] = nil end end end -- wrap function to continue handler flow local function continue(fn) return function (msg) local result = fn(msg) if (result) == -1 then return 1 end return result end end -- Registering Handlers Handlers.add(\"stake\", continue(Handlers.utils.hasMatchingTag(\"Action\", \"Stake\")), Handlers.stake) Handlers.add(\"unstake\", continue(Handlers.utils.hasMatchingTag(\"Action\", \"Unstake\")), Handlers.unstake) -- Finalization handler should be called for every message Handlers.add(\"finalize\", function (msg) return -1 end, finalizationHandler)",
          "estimatedWords": 425,
          "lastModified": "2025-06-27T16:12:26.910Z",
          "siteKey": "ao",
          "siteName": "AO Cookbook",
          "depth": 3,
          "crawledAt": "2025-06-27T16:12:26.911Z"
        },
        {
          "url": "https://cookbook_ao.arweave.net/guides/aos/blueprints/token.html",
          "title": "Token Blueprint",
          "content": "Token Blueprint ​The Token Blueprint is a predesigned template that helps you quickly build a token in ao. It is a great way to get started and can be customized to fit your needs.Unpacking the Token Blueprint ​Balances: The Balances array is used to store the token balances of the participants.Info Handler: The info handler allows processes to retrieve the token parameters, like Name, Ticker, Logo, and Denomination.Balance Handler: The balance handler allows processes to retrieve the token balance of a participant.Balances Handler: The balances handler allows processes to retrieve the token balances of all participants.Transfer Handler: The transfer handler allows processes to send tokens to another participant.Mint Handler: The mint handler allows processes to mint new tokens.Total Supply Handler: The totalSupply handler allows processes to retrieve the total supply of the token.Burn Handler: The burn handler allows processes to burn tokens.How To Use: ​Open your preferred text editor.Open the Terminal.Start your aos process.Type in .load-blueprint tokenVerify the Blueprint is Loaded: ​Type in Handlers.list to see the newly loaded handlers.What's in the Token Blueprint: ​lualocal bint = require('.bint')(256) --[[ This module implements the ao Standard Token Specification. Terms: Sender: the wallet or Process that sent the Message It will first initialize the internal state, and then attach handlers, according to the ao Standard Token Spec API: - Info(): return the token parameters, like Name, Ticker, Logo, and Denomination - Balance(Target?: string): return the token balance of the Target. If Target is not provided, the Sender is assumed to be the Target - Balances(): return the token balance of all participants - Transfer(Target: string, Quantity: number): if the Sender has a sufficient balance, send the specified Quantity to the Target. It will also issue a Credit-Notice to the Target and a Debit-Notice to the Sender - Mint(Quantity: number): if the Sender matches the Process Owner, then mint the desired Quantity of tokens, adding them the Processes' balance ]] -- local json = require('json') --[[ utils helper functions to remove the bint complexity. ]] -- local utils = { add = function(a, b) return tostring(bint(a) + bint(b)) end, subtract = function(a, b) return tostring(bint(a) - bint(b)) end, toBalanceValue = function(a) return tostring(bint(a)) end, toNumber = function(a) return bint.tonumber(a) end } --[[ Initialize State ao.id is equal to the Process.Id ]] -- Variant = \"0.0.3\" -- token should be idempotent and not change previous state updates Denomination = Denomination or 12 Balances = Balances or { [ao.id] = utils.toBalanceValue(10000 * 10 ^ Denomination) } TotalSupply = TotalSupply or utils.toBalanceValue(10000 * 10 ^ Denomination) Name = Name or 'Points Coin' Ticker = Ticker or 'PNTS' Logo = Logo or 'SBCCXwwecBlDqRLUjb8dYABExTJXLieawf7m2aBJ-KY' --[[ Add handlers for each incoming Action defined by the ao Standard Token Specification ]] -- --[[ Info ]] -- Handlers.add('info', \"Info\", function(msg) msg.reply({ Name = Name, Ticker = Ticker, Logo = Logo, Denomination = tostring(Denomination) }) end) --[[ Balance ]] -- Handlers.add('balance', \"Balance\", function(msg) local bal = '0' -- If not Recipient is provided, then return the Senders balance if (msg.Tags.Recipient) then if (Balances[msg.Tags.Recipient]) then bal = Balances[msg.Tags.Recipient] end elseif msg.Tags.Target and Balances[msg.Tags.Target] then bal = Balances[msg.Tags.Target] elseif Balances[msg.From] then bal = Balances[msg.From] end msg.reply({ Balance = bal, Ticker = Ticker, Account = msg.Tags.Recipient or msg.From, Data = bal }) end) --[[ Balances ]] -- Handlers.add('balances', \"Balances\", function(msg) msg.reply({ Data = json.encode(Balances) }) end) --[[ Transfer ]] -- Handlers.add('transfer', \"Transfer\", function(msg) assert(type(msg.Recipient) == 'string', 'Recipient is required!') assert(type(msg.Quantity) == 'string', 'Quantity is required!') assert(bint.__lt(0, bint(msg.Quantity)), 'Quantity must be greater than 0') if not Balances[msg.From] then Balances[msg.From] = \"0\" end if not Balances[msg.Recipient] then Balances[msg.Recipient] = \"0\" end if bint(msg.Quantity) <= bint(Balances[msg.From]) then Balances[msg.From] = utils.subtract(Balances[msg.From], msg.Quantity) Balances[msg.Recipient] = utils.add(Balances[msg.Recipient], msg.Quantity) --[[ Only send the notifications to the Sender and Recipient if the Cast tag is not set on the Transfer message ]] -- if not msg.Cast then -- Debit-Notice message template, that is sent to the Sender of the transfer local debitNotice = { Action = 'Debit-Notice', Recipient = msg.Recipient, Quantity = msg.Quantity, Data = Colors.gray .. \"You transferred \" .. Colors.blue .. msg.Quantity .. Colors.gray .. \" to \" .. Colors.green .. msg.Recipient .. Colors.reset } -- Credit-Notice message template, that is sent to the Recipient of the transfer local creditNotice = { Target = msg.Recipient, Action = 'Credit-Notice', Sender = msg.From, Quantity = msg.Quantity, Data = Colors.gray .. \"You received \" .. Colors.blue .. msg.Quantity .. Colors.gray .. \" from \" .. Colors.green .. msg.From .. Colors.reset } -- Add forwarded tags to the credit and debit notice messages for tagName, tagValue in pairs(msg) do -- Tags beginning with \"X-\" are forwarded if string.sub(tagName, 1, 2) == \"X-\" then debitNotice[tagName] = tagValue creditNotice[tagName] = tagValue end end -- Send Debit-Notice and Credit-Notice msg.reply(debitNotice) Send(creditNotice) end else msg.reply({ Action = 'Transfer-Error', ['Message-Id'] = msg.Id, Error = 'Insufficient Balance!' }) end end) --[[ Mint ]] -- Handlers.add('mint', \"Mint\", function(msg) assert(type(msg.Quantity) == 'string', 'Quantity is required!') assert(bint(0) < bint(msg.Quantity), 'Quantity must be greater than zero!') if not Balances[ao.id] then Balances[ao.id] = \"0\" end if msg.From == ao.id then -- Add tokens to the token pool, according to Quantity Balances[msg.From] = utils.add(Balances[msg.From], msg.Quantity) TotalSupply = utils.add(TotalSupply, msg.Quantity) msg.reply({ Data = Colors.gray .. \"Successfully minted \" .. Colors.blue .. msg.Quantity .. Colors.reset }) else msg.reply({ Action = 'Mint-Error', ['Message-Id'] = msg.Id, Error = 'Only the Process Id can mint new ' .. Ticker .. ' tokens!' }) end end) --[[ Total Supply ]] -- Handlers.add('totalSupply', \"Total-Supply\", function(msg) assert(msg.From ~= ao.id, 'Cannot call Total-Supply from the same process!') msg.reply({ Action = 'Total-Supply', Data = TotalSupply, Ticker = Ticker }) end) --[[ Burn ]] -- Handlers.add('burn', 'Burn', function(msg) assert(type(msg.Quantity) == 'string', 'Quantity is required!') assert(bint(msg.Quantity) <= bint(Balances[msg.From]), 'Quantity must be less than or equal to the current balance!') Balances[msg.From] = utils.subtract(Balances[msg.From], msg.Quantity) TotalSupply = utils.subtract(TotalSupply, msg.Quantity) msg.reply({ Data = Colors.gray .. \"Successfully burned \" .. Colors.blue .. msg.Quantity .. Colors.reset }) end)",
          "estimatedWords": 974,
          "lastModified": "2025-06-27T16:12:27.232Z",
          "siteKey": "ao",
          "siteName": "AO Cookbook",
          "depth": 3,
          "crawledAt": "2025-06-27T16:12:27.232Z"
        },
        {
          "url": "https://cookbook_ao.arweave.net/guides/aos/blueprints/voting.html",
          "title": "Voting Blueprint",
          "content": "Voting Blueprint ​The Voting Blueprint is a predesigned template that helps you quickly build a voting system in ao. It is a great way to get started and can be customized to fit your needs.Prerequisites ​The Staking Blueprint requires the Token Blueprint to be loaded, first.Unpacking the Voting Blueprint ​Balances: The Balances array is used to store the token balances of the participants.Votes: The Votes array is used to store the votes of the participants.Vote Action Handler: The vote handler allows processes to vote. When a process sends a message with the tag Action = \"Vote\", the handler will add the vote to the Votes array and send a message back to the process confirming the vote.Finalization Handler: The finalize handler allows processes to finalize the voting process. When a process sends a message with the tag Action = \"Finalize\", the handler will process the votes and finalize the voting process.How To Use: ​Open your preferred text editor.Open the Terminal.Start your aos process.Type in .load-blueprint votingVerify the Blueprint is Loaded: ​Type in Handlers.list to see the newly loaded handlers.What's in the Voting Blueprint: ​luaBalances = Balances or {} Votes = Votes or {} -- Vote Action Handler Handlers.vote = function(msg) local quantity = Stakers[msg.From].amount local target = msg.Tags.Target local side = msg.Tags.Side local deadline = tonumber(msg['Block-Height']) + tonumber(msg.Tags.Deadline) assert(quantity > 0, \"No staked tokens to vote\") Votes[target] = Votes[target] or { yay = 0, nay = 0, deadline = deadline } Votes[target][side] = Votes[target][side] + quantity end -- Finalization Handler local finalizationHandler = function(msg) local currentHeight = tonumber(msg['Block-Height']) -- Process voting for target, voteInfo in pairs(Votes) do if currentHeight >= voteInfo.deadline then if voteInfo.yay > voteInfo.nay then print(\"Handle Vote\") end -- Clear the vote record after processing Votes[target] = nil end end end -- wrap function to continue handler flow local function continue(fn) return function (msg) local result = fn(msg) if (result) == -1 then return 1 end return result end end Handlers.add(\"vote\", continue(Handlers.utils.hasMatchingTag(\"Action\", \"Vote\")), Handlers.vote) -- Finalization handler should be called for every message Handlers.add(\"finalize\", function (msg) return -1 end, finalizationHandler)",
          "estimatedWords": 342,
          "lastModified": "2025-06-27T16:12:27.500Z",
          "siteKey": "ao",
          "siteName": "AO Cookbook",
          "depth": 3,
          "crawledAt": "2025-06-27T16:12:27.500Z"
        },
        {
          "url": "https://cookbook_ao.arweave.net/guides/aos/modules/json.html",
          "title": "JSON",
          "content": "JSON ​The JSON module allows you to encode and decode objects using JavaScript Object Notation.Example usage ​lualocal json = require(\"json\") json.encode({ a_string = \"This is a string\", nums = { 1, 2, 3 } })Module functions ​encode() ​This function returns a string representation of a Lua object in JSON.Parameters:val: {any} The object to format as JSONReturns: JSON string representation of the provided objectExample ​lua--[[ prints: \"[{\"name\":\"John Doe\",\"age\":23},{\"name\":\"Bruce Wayne\",age:34}]\" ]]-- print(json.encode({ { name = \"John Doe\", age = 23 }, { name = \"Bruce Wayne\", age = 34 } })) -- prints \"false\" print(json.encode(false))decode() ​The function takes a JSON string and turns it into a Lua object.Parameters:val: {any} The JSON string to decodeReturns: Lua object corresponding to the JSON string (throws an error for invalid JSON strings)Example ​lua--[[ creates the following table: { hello = \"world\" } ]]-- json.decode('{ \"hello\": \"world\" }') -- creates a boolean with true value json.decode(\"true\")",
          "estimatedWords": 149,
          "lastModified": "2025-06-27T16:12:28.086Z",
          "siteKey": "ao",
          "siteName": "AO Cookbook",
          "depth": 3,
          "crawledAt": "2025-06-27T16:12:28.086Z"
        },
        {
          "url": "https://cookbook_ao.arweave.net/guides/aos/modules/ao.html",
          "title": "ao",
          "content": "ao ​Built-in global library for process communication and management. The ao object provides core functionality for sending messages, spawning processes, and logging.Core Functions ​ao.send(msg) ​Sends a message to another process. See the ao.send reference for more information.lua-- Send a simple message ao.send({ Target = \"usjm4PCxUd5mtaon7zc97-dt-3qf67yPyqgzLnLqk5A\", Data = \"Hello!\", Tags = { Action = \"Greeting\" } }) -- Root-level fields are automatically converted to tags ao.send({ Target = \"usjm4PCxUd5mtaon7zc97-dt-3qf67yPyqgzLnLqk5A\", Data = \"Transfer tokens\", Action = \"Transfer\", -- Becomes a tag Quantity = \"1045\" -- Becomes a tag })ao.spawn(module: string, spawn: table) ​Creates a new process from a module. See the ao.spawn reference for more information.lua-- Spawn a calculator process ao.spawn(\"n0BFH80b73mi9VAWUzyuG9gEC3LI2zU2BFxum0N8A9s\", { Data = { initial = \"state\" }, Tags = { [\"Process-Type\"] = \"calculator\" } })ao.log(string|table) ​Logs messages or data that can be read by process callers.lua-- Log a debug message ao.log(\"Processing transfer...\") -- Log structured data ao.log({ event = \"transfer_complete\", amount = 100, recipient = \"addr123...\" })Environment ​The ao.env variable contains process initialization info like ID, owner, and tags.lua-- Access process info local processId = ao.env.Process.Id local owner = ao.env.Process.OwnerFor the complete API reference including all properties and functions, see the ao reference documentation.",
          "estimatedWords": 194,
          "lastModified": "2025-06-27T16:12:28.373Z",
          "siteKey": "ao",
          "siteName": "AO Cookbook",
          "depth": 3,
          "crawledAt": "2025-06-27T16:12:28.373Z"
        },
        {
          "url": "https://cookbook_ao.arweave.net/guides/aos/modules/crypto.html",
          "title": "crypto",
          "content": "crypto ​Overview ​The crypto module provides a set of cryptographic primitives like digests, ciphers and other cryptographic algorithms in pure Lua. It offers several functionalities to hash, encrypt and decrypt data, simplifying the development of secure communication and data storage. This document will guide you through the module's functionalities, installation, and usage.Usage ​lualocal crypto = require(\".crypto\");Primitives ​Digests (sha1, sha2, sha3, keccak, blake2b, etc.)Ciphers (AES, ISSAC, Morus, NORX, etc.)Random Number Generators (ISAAC)MACs (HMAC)KDFs (PBKDF2)Utilities (Array, Stream, Queue, etc.)Digests ​MD2 ​Calculates the MD2 digest of a given message.Parameters:stream (Stream): The message in form of streamReturns: A table containing functions to get digest in different formats.asBytes(): The digest as byte table.asHex(): The digest as string in hexadecimal format.asString(): The digest as string format.Example:lualocal str = crypto.utils.stream.fromString(\"ao\") return crypto.digest.md2(str).asHex() -- 0d4e80edd07bee6c7965b21b25a9b1eaMD4 ​Calculates the MD4 digest of a given message.Parameters:stream (Stream): The message in form of streamReturns: A table containing functions to get digest in different formats.asBytes(): The digest as byte table.asHex(): The digest as string in hexadecimal format.asString(): The digest as string format.Example:lualocal str = crypto.utils.stream.fromString(\"ao\") return crypto.digest.md4(str).asHex() -- e068dfe3d8cb95311b58be566db66954MD5 ​Calculates the MD5 digest of a given message.Parameters:stream (Stream): The message in form of streamReturns: A table containing functions to get digest in different formats.asBytes(): The digest as byte table.asHex(): The digest as string in hexadecimal format.asString(): The digest as string format.Example:lualocal str = crypto.utils.stream.fromString(\"ao\") return crypto.digest.md5(str).asHex() -- adac5e63f80f8629e9573527b25891d3SHA1 ​Calculates the SHA1 digest of a given message.Parameters:stream (Stream): The message in form of streamReturns: A table containing functions to get digest in different formats.asBytes(): The digest as byte table.asHex(): The digest as string in hexadecimal format.asString(): The digest as string format.Example:lualocal str = crypto.utils.stream.fromString(\"ao\") return crypto.digest.sha1(str).asHex() -- c29dd6c83b67a1d6d3b28588a1f068b68689aa1dSHA2_256 ​Calculates the SHA2-256 digest of a given message.Parameters:stream (Stream): The message in form of streamReturns: A table containing functions to get digest in different formats. asBytes(): The digest as byte table.asHex(): The digest as string in hexadecimal format.asString(): The digest as string format.Example:lualocal str = crypto.utils.stream.fromString(\"ao\") return crypto.digest.sha2_256(str).asHex() -- ba7816bf8f01cfea414140de5dae2223b00361a396177a9cb410ff61f20015adSHA2_512 ​Calculates the SHA2-512 digest of a given message.Parameters:msg (string): The message to calculate the digestReturns: A table containing functions to get digest in different formats. asBytes(): The digest as byte table.asHex(): The digest as string in hexadecimal format.asString(): The digest as string format.Example:lualocal str = \"ao\" return crypto.digest.sha2_512(str).asHex() -- 6f36a696b17ce5a71efa700e8a7e47994f3e134a5e5f387b3e7c2c912abe94f94ee823f9b9dcae59af99e2e34c8b4fb0bd592260c6720ee49e5deaac2065c4b1SHA3 ​It contains the following functions:sha3_256sha3_512keccak256keccak512Each function calculates the respective digest of a given message.Parameters:msg (string): The message to calculate the digestReturns: A table containing functions to get digest in different formats.asBytes(): The digest as byte table.asHex(): The digest as string in hexadecimal format.asString(): The digest as string format.Example:lua local str = \"ao\" crypto.digest.sha3_256(str).asHex() -- 1bbe785577db997a394d5b4555eec9159cb51f235aec07514872d2d436c6e985 crypto.digest.sha3_512(str).asHex() -- 0c29f053400cb1764ce2ec555f598f497e6fcd1d304ce0125faa03bb724f63f213538f41103072ff62ddee701b52c73e621ed4d2254a3e5e9a803d83435b704d crypto.digest.keccak256(str).asHex() -- 76da52eec05b749b99d6e62bb52333c1569fe75284e6c82f3de12a4618be00d6 crypto.digest.keccak512(str).asHex() -- 046fbfad009a12cef9ff00c2aac361d004347b2991c1fa80fba5582251b8e0be8def0283f45f020d4b04ff03ead9f6e7c43cc3920810c05b33b4873b99affdeaBlake2b ​Calculates the Blake2b digest of a given message.Parameters:data (string): The data to be hashed.outlen (number): The length of the output hash (optional) default is 64.key (string): The key to be used for hashing (optional) default is \"\".Returns: A table containing functions to get digest in different formats.asBytes(): The digest as byte table.asHex(): The digest as string in hexadecimal format.asString(): The digest as string format.Example:lualocal str = \"ao\" crypto.digest.blake2b(str).asHex() -- 576701fd79a126f2c414ef94adf1117c88943700f312679d018c29c378b2c807a3412b4e8d51e191c48fb5f5f54bf1bca29a714dda166797b3baf9ead862ae1d crypto.digest.blake2b(str, 32).asHex() -- 7050811afc947ba7190bb3c0a7b79b4fba304a0de61d529c8a35bdcbbb5544f4 crypto.digest.blake2b(str, 32, \"secret_key\").asHex() -- 203c101980fdf6cf24d78879f2e3db86d73d91f7d60960b642022cd6f87408f8Ciphers ​AES ​The Advanced Encryption Standard (AES) is a symmetric block cipher used to encrypt sensitive information. It has two functions encrypt and decrypt.Encrypt ​Encrypts a given message using the AES algorithm.Parameters:data (string): The data to be encrypted.key (string): The key to be used for encryption.iv (string) optional: The initialization vector to be used for encryption. default is \"\"mode (string) optional: The mode of operation to be used for encryption. default is \"CBC\". Available modes are CBC, ECB, CFB, OFB, CTR.keyLength (number) optional: The length of the key to use for encryption. default is 128.Returns: A table containing functions to get encrypted data in different formats.asBytes(): The encrypted data as byte table.asHex(): The encrypted data as string in hexadecimal format.asString(): The encrypted data as string format.Decrypt ​Decrypts a given message using the AES algorithm.Parameters:cipher (string): Hex Encoded encrypted data.key (string): The key to be used for decryption.iv (string) optional: The initialization vector to be used for decryption. default is \"\"mode (string) optional: The mode of operation to be used for decryption. default is \"CBC\". Available modes are CBC, ECB, CFB, OFB, CTR.keyLength (number) optional: The length of the key to use for decryption. default is 128.Returns: A table containing functions to get decrypted data in different formats.asBytes(): The decrypted data as byte table.asHex(): The decrypted data as string in hexadecimal format.asString(): The decrypted data as string format.Example:lualocal str = \"ao\" local iv = \"super_secret_shh\" local key_128 = \"super_secret_shh\" local encrypted = crypto.cipher.aes.encrypt(\"ao\", key, iv).asHex() -- A3B9E6E1FBD9D46930E5F76807C84B8E local decrypted = crypto.cipher.aes.decrypt(encrypted, key, iv).asHex() -- 616F0000000000000000000000000000 crypto.utils.hex.hexToString(decrypted) -- aoISSAC Cipher ​ISAAC is a cryptographically secure pseudo-random number generator (CSPRNG) and stream cipher. It has the following functionsseedIsaac: Seeds the ISAAC cipher with a given seed.getRandomChar: Generates a random character using the ISAAC cipher.random: Generates a random number between a given range using the ISAAC cipher.getRandom: Generates a random number using the ISAAC cipher.encrypt: Encrypts a given message using the ISAAC cipher.decrypt: Decrypts a given message using the ISAAC cipher.Encrypt ​Encrypts a given message using the ISAAC cipher.Parameters:msg (string): The message to be encrypted.key (string): The key to be used for encryption.Returns: A table containing functions to get encrypted data in different formats. asBytes(): The encrypted data as byte table.asHex(): The encrypted data as string in hexadecimal format.asString(): The encrypted data as string format.Decrypt ​Decrypts a given message using the ISAAC cipher.Parameters:cipher (string): Hex Encoded encrypted data.key (string): Key to be used for decryption.Returns: A table containing functions to get decrypted data in different formats. asBytes(): The decrypted data as byte table.asHex(): The decrypted data as string in hexadecimal format.asString(): The decrypted data as string format.Example:lualocal message = \"ao\"; local key = \"secret_key\"; local encrypted = crypto.cipher.issac.encrypt(message, key) local decrypted = crypto.cipher.issac.decrypt(encrypted.asString(), key) -- ao encrypted.asHex() -- 7851random ​Generates a random number using the ISAAC cipher.Parameters:min (number) optional: The minimum value of the random number. defaults to 0.max (number) optional: The maximum value of the random number. defaults to 2^31 - 1.seed (string) optional: The seed to be used for generating the random number. defaults to math.random(0,2^32 - 1).Returns: A random number between the given range.Example:luacrypto.cipher.issac.random(0, 100) -- 42Morus Cipher ​MORUS is a high-performance authenticated encryption algorithm submitted to the CAESAR competition, and recently selected as a finalist.Encrypt ​Encrypts a given message using the MORUS cipher.Parameters:key (string): The encryption key (16 or 32-byte string).iv (string): The nonce or initial value (16-byte string).msg (string): The message to encrypt (variable length string).ad (string) optional: The additional data (variable length string). defaults to \"\".Returns: A table containing functions to get encrypted data in different formats. asBytes(): The encrypted data as byte table.asHex(): The encrypted data as string in hexadecimal format.asString(): The encrypted data as string format.Decrypt ​Decrypts a given message using the MORUS cipher.Parameters:key (string): The encryption key (16 or 32-byte string).iv (string): The nonce or initial value (16-byte string).cipher (string): The encrypted message (variable length string).adLen (number) optional: The length of the additional data (variable length string). defaults to 0.Returns: A table containing functions to get decrypted data in different formats. asBytes(): The decrypted data as byte table.asHex(): The decrypted data as string in hexadecimal format.asString(): The decrypted data as string format.Example:lualocal m = \"ao\" local k = \"super_secret_shh\" local iv = \"0000000000000000\" local ad= \"\" local e = crypto.cipher.morus.encrypt(k, iv, m, ad) local d = crypto.cipher.morus.decrypt(k, iv, e.asString(), #ad) -- ao e.asHex() -- 514ed31473d8fb0b76c6cbb17af35ed01d0aNORX Cipher ​NORX is an authenticated encryption scheme with associated data that was selected, along with 14 other primitives, for the third phase of the ongoing CAESAR competition. It is based on the sponge construction and relies on a simple permutation that allows efficient and versatile implementations.Encrypt ​Encrypts a given message using the NORX cipher.Parameters:key (string): The encryption key (32-byte string).nonce (string): The nonce or initial value (32-byte string).plain (string): The message to encrypt (variable length string).header (string) optional: The additional data (variable length string). defaults to \"\".trailer (string) optional: The additional data (variable length string). defaults to \"\".Returns: A table containing functions to get encrypted data in different formats. asBytes(): The encrypted data as byte table.asHex(): The encrypted data as string in hexadecimal format.asString(): The encrypted data as string format.Decrypt ​Decrypts a given message using the NORX cipher.Parameters:key (string): The encryption key (32-byte string).nonce (string): The nonce or initial value (32-byte string).crypted (string): The encrypted message (variable length string).header (string) optional: The additional data (variable length string). defaults to \"\".trailer (string) optional: The additional data (variable length string). defaults to \"\".Returns: A table containing functions to get decrypted data in different formats. asBytes(): The decrypted data as byte table.asHex(): The decrypted data as string in hexadecimal format.asString(): The decrypted data as string format.Example:lualocal key = \"super_duper_secret_password_shhh\" local nonce = \"00000000000000000000000000000000\" local data = \"ao\" -- Header and trailer are optional local header, trailer = data, data local encrypted = crypto.cipher.norx.encrypt(key, nonce, data, header, trailer).asString() local decrypted = crypto.cipher.norx.decrypt(key, nonce, encrypted, header, trailer) -- ao local authTag = encrypted:sub(#encrypted-32+1) crypto.utils.hex.stringToHex(encrypted) -- 0bb35a06938e6541eccd4440adb7b46118535f60b09b4adf378807a53df19fc4ea28 crypto.utils.hex.stringToHex(authTag) -- 5a06938e6541eccd4440adb7b46118535f60b09b4adf378807a53df19fc4ea28Random Number Generators ​The module contains a random number generator using ISAAC which is a cryptographically secure pseudo-random number generator (CSPRNG) and stream cipher.Parameters:min (number) optional: The minimum value of the random number. defaults to 0.max (number) optional: The maximum value of the random number. defaults to 2^31 - 1.seed (string) optional: The seed to be used for generating the random number. defaults to math.random(0,2^32 - 1).Returns: A random number between the given range.Example:luacrypto.random.(0, 100, \"seed\") -- 42MACs ​HMAC ​The Hash-based Message Authentication Code (HMAC) is a mechanism for message authentication using cryptographic hash functions. HMAC can be used with any iterative cryptographic hash function, e.g., MD5, SHA-1, in combination with a secret shared key.The modules exposes a function called createHmac which is used to create a HMAC instance.Parameters:data (Stream): The data to be hashed.key (Array): The key to be used for hashing.algorithm (string) optional: The algorithm to be used for hashing. default is \"sha256\". Available algorithms are \"sha1\", \"sha256\". default is \"sha1\".Returns: A table containing functions to get HMAC in different formats. asBytes(): The HMAC as byte table.asHex(): The HMAC as string in hexadecimal format.asString(): The HMAC as string format.Example:lualocal data = crypto.utils.stream.fromString(\"ao\") local key = crypto.utils.array.fromString(\"super_secret_key\") crypto.mac.createHmac(data, key).asHex() -- 3966f45acb53f7a1a493bae15afecb1a204fa32d crypto.mac.createHmac(data, key, \"sha256\").asHex() -- 542da02a324155d688c7689669ff94c6a5f906892aa8eccd7284f210ac66e2a7KDFs ​PBKDF2 ​The Password-Based Key Derivation Function 2 (PBKDF2) applies a pseudorandom function, such as hash-based message authentication code (HMAC), to the input password or passphrase along with a salt value and repeats the process many times to produce a derived key, which can then be used as a cryptographic key in subsequent operations.Parameters:password (Array): The password to derive the key from.salt (Array): The salt to use.iterations (number): The number of iterations to perform.keyLen (number): The length of the key to derive.digest (string) optional: The digest algorithm to use. default is \"sha1\". Available algorithms are \"sha1\", \"sha256\".Returns: A table containing functions to get derived key in different formats. asBytes(): The derived key as byte table.asHex(): The derived key as string in hexadecimal format.asString(): The derived key as string format.Example:lualocal salt = crypto.utils.array.fromString(\"salt\") local password = crypto.utils.array.fromString(\"password\") local iterations = 4 local keyLen = 16 local res = crypto.kdf.pbkdf2(password, salt, iterations, keyLen).asHex() -- C4C21BF2BBF61541408EC2A49C89B9C6Utilities ​Array ​Example Usage:lua local arr = crypto.utils.array arr.fromString(\"ao\") -- Array arr.toString(arr.fromString(\"ao\")) -- ao arr.fromHex(\"616f\") -- Array arr.toHex(arr.fromHex(\"616f\")) -- 616f arr.concat(arr.fromString(\"a\"), arr.fromString(\"o\")) -- Array arr.truncate(arr.fromString(\"ao\"), 1) -- Array arr.XOR(arr.fromString(\"a\"), arr.fromString(\"o\")) -- Array arr.substitute(arr.fromString(\"a\"), arr.fromString(\"o\")) -- Array arr.permute(arr.fromString(\"a\"), arr.fromString(\"o\")) -- Array arr.copy(arr.fromString(\"ao\")) -- Array arr.slice(arr.fromString(\"ao\"), 0, 1) -- Arraysize ​Returns the size of the array.Parameters:arr (Array): The array to get the size of.Returns: The size of the array.fromString ​Creates an array from a string.Parameters:str (string): The string to create the array from.Returns: The array created from the string.toString ​Converts an array to a string.Parameters:arr (Array): The array to convert to a string.Returns: The array as a string.fromStream ​Creates an array from a stream.Parameters:stream (Stream): The stream to create the array from.Returns: The array created from the stream.readFromQueue ​Reads data from a queue and stores it in the array.Parameters:queue (Queue): The queue to read data from.size (number): The size of the data to read.Returns: The array containing the data read from the queue.writeToQueue ​Writes data from the array to a queue.Parameters:queue (Queue): The queue to write data to.array (Array): The array to write data from.Returns: NonetoStream ​Converts an array to a stream.Parameters:arr (Array): The array to convert to a stream.Returns: (Stream) The array as a stream.fromHex ​Creates an array from a hexadecimal string.Parameters:hex (string): The hexadecimal string to create the array from.Returns: The array created from the hexadecimal string.toHex ​Converts an array to a hexadecimal string.Parameters:arr (Array): The array to convert to a hexadecimal string.Returns: The array as a hexadecimal string.concat ​Concatenates two arrays.Parameters:a (Array): The array to concatenate with.b (Array): The array to concatenate.Returns: The concatenated array.truncate ​Truncates an array to a given length.Parameters:a (Array): The array to truncate.newSize (number): The new size of the array.Returns: The truncated array.XOR ​Performs a bitwise XOR operation on two arrays.Parameters:a (Array): The first array.b (Array): The second array.Returns: The result of the XOR operation.substitute ​Creates a new array with keys of first array and values of secondParameters:input (Array): The array to substitute.sbox (Array): The array to substitute with.Returns: The substituted array.permute ​Creates a new array with keys of second array and values of first array.Parameters:input (Array): The array to permute.pbox (Array): The array to permute with.Returns: The permuted array.copy ​Creates a copy of an array.Parameters:input (Array): The array to copy.Returns: The copied array.slice ​Creates a slice of an array.Parameters:input (Array): The array to slice.start (number): The start index of the slice.stop (number): The end index of the slice.Returns: The sliced array.Stream ​Stream is a data structure that represents a sequence of bytes. It is used to store and manipulate data in a streaming fashion.Example Usage:lualocal stream = crypto.utils.stream local str = \"ao\" local arr = {97, 111} stream.fromString(str) -- Stream stream.toString(stream.fromString(str)) -- ao stream.fromArray(arr) -- Stream stream.toArray(stream.fromArray(arr)) -- {97, 111} stream.fromHex(\"616f\") -- Stream stream.toHex(stream.fromHex(\"616f\")) -- 616ffromString ​Creates a stream from a string.Parameters:str (string): The string to create the stream from.Returns: The stream created from the string.toString ​Converts a stream to a string.Parameters:stream (Stream): The stream to convert to a string.Returns: The stream as a string.fromArray ​Creates a stream from an array.Parameters:arr (Array): The array to create the stream from.Returns: The stream created from the array.toArray ​Converts a stream to an array.Parameters:stream (Stream): The stream to convert to an array.Returns: The stream as an array.fromHex ​Creates a stream from a hexadecimal string.Parameters:hex (string): The hexadecimal string to create the stream from.Returns: The stream created from the hexadecimal string.toHex ​Converts a stream to a hexadecimal string.Parameters:stream (Stream): The stream to convert to a hexadecimal string.Returns: The stream as a hexadecimal string.Hex ​Example Usage:lualocal hex = crypto.utils.hex hex.hexToString(\"616f\") -- ao hex.stringToHex(\"ao\") -- 616fhexToString ​Converts a hexadecimal string to a string.Parameters:hex (string): The hexadecimal string to convert to a string.Returns: The hexadecimal string as a string.stringToHex ​Converts a string to a hexadecimal string.Parameters:str (string): The string to convert to a hexadecimal string.Returns: The string as a hexadecimal string.Queue ​Queue is a data structure that represents a sequence of elements. It is used to store and manipulate data in a first-in, first-out (FIFO) fashion.Example Usage:lualocal q = crypto.utils.queue() q.push(1) q.push(2) q.pop() -- 1 q.size() -- 1 q.getHead() -- 2 q.getTail() -- 2 q.reset()push ​Pushes an element to the queue.Parameters:queue (Queue): The queue to push the element to.element (any): The element to push to the queue.Returns: Nonepop ​Pops an element from the queue.Parameters:queue (Queue): The queue to pop the element from.element (any): The element to pop from the queue.Returns: The popped element.size ​Returns the size of the queue.Parameters: NoneReturns: The size of the queue.getHead ​Returns the head of the queue.Parameters: NoneReturns: The head of the queue.getTail ​Returns the tail of the queue.Parameters: NoneReturns: The tail of the queue.reset ​Resets the queue.Parameters: None",
          "estimatedWords": 2629,
          "lastModified": "2025-06-27T16:12:28.711Z",
          "siteKey": "ao",
          "siteName": "AO Cookbook",
          "depth": 3,
          "crawledAt": "2025-06-27T16:12:28.711Z"
        },
        {
          "url": "https://cookbook_ao.arweave.net/guides/aos/modules/base64.html",
          "title": "Base64",
          "content": "Base64 ​A small base64 module to encode or decode base64 text.Note: It is recommended to enable caching for large chunks of texts for up to x2 optimization.Example usage ​lualocal base64 = require(\".base64\") local str = \"This will be encoded\" -- is: \"VGhpcyB3aWxsIGJlIGVuY29kZWQ=\" local encoded = base64.encode(str) -- is: \"This will be encoded\" local decoded = base64.decode(encoded) assert(decoded == str)Module functions ​encode() ​This function encodes the provided string using the default encoder table. The encoder can be customized and a cache is available for larger chunks of data.Parameters:str: {string} The string to encodeencoder: {table} Optional custom encoding tableusecache: {boolean} Optional cache for large strings (turned off by default)Returns: Base64 encoded stringExamples ​lua-- prints: \"SGVsbG8gd29ybGQ=\" print(base64.encode(\"Hello world\")) -- customize encoder and allow caching base64.encode( \"Hello world\", base64.makeencoder(nil, \"-\"), true )decode() ​This function decodes the provided base64 encoded string using the default decoder table. The decoder can be customized and a cache is also available here.Parameters:str: {string} The base64 encoded string to decodedecoder: {table} Optional custom decoding tableusecache: {boolean} Optional cache for large strings (turned off by default)Returns: Decoded stringExamples ​lua-- prints: \"Hello world\" print(base64.decode(\"SGVsbG8gd29ybGQ=\")) -- customize decoder and allow caching base64.decode( \"SGVsbG8gd29ybGQ=\", base64.makedecoder(nil, \"-\"), true )makeencoder() ​Allows creating a new encoder table to customize the encode() function's result.Parameters:s62: {string} Optional custom char for 62 (+ by default)s63: {string} Optional custom char for 63 (/ by default)spad: {string} Optional custom padding char (= by default)Returns: Custom encoder tableExamples ​lua-- create custom encoder local encoder = base64.makeencoder(nil, nil, \"~\") -- prints \"SGVsbG8gd29ybGQ~\" instead of \"SGVsbG8gd29ybGQ=\" print(base64.encode(\"Hello world\", encoder))makedecoder() ​Allows creating a new decoder table to be able to decode custom-encoded base64 strings.Parameters:s62: {string} Optional custom char for 62 (+ by default)s63: {string} Optional custom char for 63 (/ by default)spad: {string} Optional custom padding char (= by default)Returns: Custom decoder tableExamples ​lualocal encoder = base64.makeencoder(nil, nil, \"~\") local decoder = base64.makedecoder(nil, nil, \"~\") -- \"SGVsbG8gd29ybGQ~\" local encoded = base64.encode(\"Hello world\", encoder) -- prints \"Hello world\" print(base64.decode(encoded, decoder))",
          "estimatedWords": 323,
          "lastModified": "2025-06-27T16:12:28.980Z",
          "siteKey": "ao",
          "siteName": "AO Cookbook",
          "depth": 3,
          "crawledAt": "2025-06-27T16:12:28.980Z"
        },
        {
          "url": "https://cookbook_ao.arweave.net/guides/aos/modules/pretty.html",
          "title": "Pretty",
          "content": "Pretty ​This module allows printing formatted, human-friendly and readable syntax.Module functions ​tprint() ​Returns a formatted string of the structure of the provided table.Parameters:tbl: {table} The table to formatindent: {number} Optional indentation of each level of the tableReturns: Table structure formatted as a stringExamples ​lualocal pretty = require(\".pretty\") local formatted = pretty.tprint({ name = \"John Doe\", age = 22, friends = { \"Maria\", \"Victor\" } }, 2) -- prints the formatted table structure print(formatted)",
          "estimatedWords": 73,
          "lastModified": "2025-06-27T16:12:29.298Z",
          "siteKey": "ao",
          "siteName": "AO Cookbook",
          "depth": 3,
          "crawledAt": "2025-06-27T16:12:29.298Z"
        },
        {
          "url": "https://cookbook_ao.arweave.net/guides/aos/modules/utils.html",
          "title": "Utils",
          "content": "Utils ​A utility library for generic table manipulation and validation. It supports both curry-styled and traditional programming.Note: It is important to verify that the inputs provided to the following functions match the expected types.Example usage ​lualocal utils = require(\".utils\") local totalSupply = utils.reduce( function (acc, v) return acc + v end, 0, { 2, 4, 9 } ) print(totalSupply) -- prints 15Module functions ​concat() ​This function concatenates array b to array a.Parameters:a: {table} The base arrayb: {table} The array to concat to the base arrayReturns: An unified array of a and bExamples ​lua-- returns { 1, 2, 3, 4, 5, 6 } concat({ 1, 2, 3 })({ 4, 5, 6 }) -- curry syntax -- returns { \"hello\", \"world\", \"and\", \"you\" } concat({ \"hello\", \"world\" }, { \"and\", \"you\" }) -- traditional syntaxreduce() ​This function executes the provided reducer function for all array elements, finally providing one (unified) result.Parameters:fn: {function} The reducer function. It receives the previous result, the current element's value and key in this orderinitial: {any} An optional initial valuet: {table} The array to reduceReturns: A single result from running the reducer across all table elementsExamples ​lualocal sum = utils.reduce( function (acc, v) return acc + v end, 0, { 1, 2, 3 } ) print(sum) -- prints 6lualocal sum = utils .reduce(function (acc, v) return acc + v end)(0)({ 5, 4, 3 }) print(sum) -- prints 12map() ​This function creates a new array filled with the results of calling the provided map function on each element in the provided array.Parameters:fn: {function} The map function. It receives the current array element and keydata: {table} The array to mapReturns: A new array composed of the results of the map functionExamples ​lua-- returns { \"Odd\", \"Even\", \"Odd\" } utils.map( function (val, key) return (val % 2 == 0 and \"Even\") or \"Odd\" end, { 3, 4, 7 } )lua-- returns { 4, 8, 12 } utils.map(function (val, key) return val * 2 end)({ 2, 4, 6 })filter() ​This function creates a new array from a portion of the original, only keeping the elements that passed a provided filter function's test.Parameters:fn: {function} The filter function. It receives the current array element and should return a boolean, deciding whether the element should be kept (true) or filtered out (false)data: {table} The array to filterReturns: The new filtered arrayExamples ​lua-- keeps even numbers utils.filter( function (val) return val % 2 == 0 end, { 3, 4, 7 } )lua-- keeps only numbers utils.filter( function (val) return type(val) == \"number\" end, { \"hello\", \"world\", 13, 44 } )find() ​This function returns the first element that matches in a provided function.Parameters:fn: {function} The find function that receives the current element and returns true if it matches, false if it doesn'tt: {table} The array to find an element inReturns: The found element or nil if no element matchedExamples ​lualocal users = { { name = \"John\", age = 50 }, { name = \"Victor\", age = 37 }, { name = \"Maria\", age = 33 } } -- returns the user \"John\" utils.find( function (val) return user.name == \"John\" end, users )lua-- returns the user \"Maria\" utils.find(function (val) return user.age == 33 end)(users)reverse() ​Transforms an array into reverse order.Parameters:data: {table} The array to reverseReturns: The original array in reverse orderExample ​lua-- is: { 3, 2, 1 } utils.reverse({ 1, 2, 3 })includes() ​Determines whether a value is part of an array.Parameters:val: {any} The element to check fort: {table} The array to check inReturns: A boolean indicating whether or not the provided value is part of the arrayExamples ​lua-- this is true utils.includes(\"John\", { \"Victor\", \"John\", \"Maria\" })lua-- this is false utils.includes(4)({ 3, 5, 7 })keys() ​Returns the keys of a table.Parameters:table: {table} The table to get the keys forReturns: An array of keysExample ​lua-- returns { \"hello\", \"name\" } utils.keys({ hello = \"world\", name = \"John\" })values() ​Returns the values of a table.Parameters:table: {table} The table to get the values forReturns: An array of valuesExample ​lua-- returns { \"world\", \"John\" } utils.values({ hello = \"world\", name = \"John\" })propEq() ​Checks if a specified property of a table equals with the provided value.Parameters:propName: {string} The name of the property to comparevalue: {any} The value to compare toobject: {table} The object to select the property fromReturns: A boolean indicating whether the property value equals with the provided value or notExamples ​lualocal user = { name = \"John\", age = 50 } -- returns true utils.propEq(\"age\", 50, user)lualocal user = { name = \"Maria\", age = 33 } -- returns false utils.propEq(\"age\", 45, user)prop() ​Returns the property value that belongs to the property name provided from an object.Parameters:propName: {string} The name of the property to getobject: {table} The object to select the property value fromReturns: The property value or nil if it was not foundExamples ​lualocal user = { name = \"Maria\", age = 33 } -- returns \"Maria\" utils.prop(\"name\", user)lualocal user = { name = \"John\", age = 50 } -- returns 50 utils.prop(\"age\")(user)compose() ​This function allows you to chain multiple array mutations together and execute them in reverse order on the provided array.Parameters:...: {function[]} The array mutationsv: {table} The object to execute the provided functions onReturns: The result from the provided mutationsExamples ​lua-- returns 12 utils.compose( utils.reduce(function (acc, val) return acc + val end, 0), utils.map(function (val) return val * 2 end) )({ 1, 2, 3 })",
          "estimatedWords": 890,
          "lastModified": "2025-06-27T16:12:29.593Z",
          "siteKey": "ao",
          "siteName": "AO Cookbook",
          "depth": 3,
          "crawledAt": "2025-06-27T16:12:29.593Z"
        },
        {
          "url": "https://cookbook_ao.arweave.net/guides/aoconnect/aoconnect.html",
          "title": "aoconnect",
          "content": "aoconnect ​ao connect is a Javascript/Typescript library to interact with the system from Node JS or the browser.Guides in this section provide snippets on how to utilize ao connect. All snippets are written in Javascript but should translate easily to Typescript.",
          "estimatedWords": 41,
          "lastModified": "2025-06-27T16:12:29.888Z",
          "siteKey": "ao",
          "siteName": "AO Cookbook",
          "depth": 3,
          "crawledAt": "2025-06-27T16:12:29.888Z"
        },
        {
          "url": "https://cookbook_ao.arweave.net/guides/aoconnect/installing-connect.html",
          "title": "Installing ao connect",
          "content": "Installing ao connect ​Prerequisites ​In order to install ao connect into your app you must have NodeJS/NPM 18 or higher. Installing ​npm ​shnpm install --save @permaweb/aoconnectyarn ​shyarn add @permaweb/aoconnect -DThis module can now be used from NodeJS as well as a browser, it can be included as shown below.ESM (Node & Browser) aka type: module ​jsimport { result, results, message, spawn, monitor, unmonitor, dryrun, } from \"@permaweb/aoconnect\";CJS (Node) type: commonjs ​jsconst { result, results, message, spawn, monitor, unmonitor, dryrun, } = require(\"@permaweb/aoconnect\");",
          "estimatedWords": 82,
          "lastModified": "2025-06-27T16:12:30.462Z",
          "siteKey": "ao",
          "siteName": "AO Cookbook",
          "depth": 3,
          "crawledAt": "2025-06-27T16:12:30.462Z"
        },
        {
          "url": "https://cookbook_ao.arweave.net/guides/aoconnect/connecting.html",
          "title": "Connecting to specific ao nodes",
          "content": "Connecting to specific ao nodes ​When including ao connect in your code you have the ability to connect to a specific MU and CU, as well as being able to specify an Arweave gateway. This can be done by importing the \"connect\" function and extracting the functions from a call to the \"connect\" function.You may want to do this if you want to know which MU is being called when you send your message so that later you can debug from the specified MU. You also may want to read a result from a specific CU. You may in fact just prefer a particular MU and CU for a different reason. You can specify the gateway in order to use something other than the default, which is arweave.net.Importing without a call to connect ​js// Here aoconnect will implicitly use the default nodes/units import { result, results, message, spawn, monitor, unmonitor, dryrun, } from \"@permaweb/aoconnect\";Connecting to a specific MU, CU, and gateway ​jsimport { connect } from \"@permaweb/aoconnect\"; const { result, results, message, spawn, monitor, unmonitor, dryrun } = connect( { MU_URL: \"https://mu.ao-testnet.xyz\", CU_URL: \"https://cu.ao-testnet.xyz\", GATEWAY_URL: \"https://arweave.net\", }, ); // now spawn, message, and result can be used the same way as if they were imported directlyAll three of these parameters to connect are optional and it is valid to specify only 1 or 2 of them, or none. You could pass in just the MU_URL, for example.jsimport { connect } from \"@permaweb/aoconnect\"; const { result, results, message, spawn, monitor, unmonitor, dryrun } = connect( { MU_URL: \"https://ao-mu-1.onrender.com\", }, );",
          "estimatedWords": 259,
          "lastModified": "2025-06-27T16:12:30.943Z",
          "siteKey": "ao",
          "siteName": "AO Cookbook",
          "depth": 3,
          "crawledAt": "2025-06-27T16:12:30.943Z"
        },
        {
          "url": "https://cookbook_ao.arweave.net/guides/aoconnect/sending-messages.html",
          "title": "Sending a Message to a Process",
          "content": "Sending a Message to a Process ​A deep dive into the concept of Messages can be found in the ao Messages concept. This guide focuses on using ao connect to send a message to a process.Sending a message is the central way in which your app can interact with ao. A message is input to a process. There are 5 parts of a message that you can specify which are \"target\", \"data\", \"tags\", \"anchor\", and finally the messages \"signature\".Refer to your process module's source code or documentation to see how the message is used in its computation. The ao connect library will translate the parameters you pass it in the code below, construct a message, and send it.🎓 To Learn more about Wallets visit the Permaweb CookbookSending a Message in NodeJS ​Need a test wallet, use npx -y @permaweb/wallet > /path/to/wallet.json to create a wallet keyfile.jsimport { readFileSync } from \"node:fs\"; import { message, createDataItemSigner } from \"@permaweb/aoconnect\"; const wallet = JSON.parse( readFileSync(\"/path/to/arweave/wallet.json\").toString(), ); // The only 2 mandatory parameters here are process and signer await message({ /* The arweave TxID of the process, this will become the \"target\". This is the process the message is ultimately sent to. */ process: \"process-id\", // Tags that the process will use as input. tags: [ { name: \"Your-Tag-Name-Here\", value: \"your-tag-value\" }, { name: \"Another-Tag\", value: \"another-value\" }, ], // A signer function used to build the message \"signature\" signer: createDataItemSigner(wallet), /* The \"data\" portion of the message If not specified a random string will be generated */ data: \"any data\", }) .then(console.log) .catch(console.error);Sending a Message in a browser ​New to building permaweb apps check out the Permaweb Cookbookjsimport { message, createDataItemSigner } from \"@permaweb/aoconnect\"; // The only 2 mandatory parameters here are process and signer await message({ /* The arweave TxID of the process, this will become the \"target\". This is the process the message is ultimately sent to. */ process: \"process-id\", // Tags that the process will use as input. tags: [ { name: \"Your-Tag-Name-Here\", value: \"your-tag-value\" }, { name: \"Another-Tag\", value: \"another-value\" }, ], // A signer function used to build the message \"signature\" signer: createDataItemSigner(globalThis.arweaveWallet), /* The \"data\" portion of the message. If not specified a random string will be generated */ data: \"any data\", }) .then(console.log) .catch(console.error);If you would like to learn more about signers, click here",
          "estimatedWords": 389,
          "lastModified": "2025-06-27T16:12:31.034Z",
          "siteKey": "ao",
          "siteName": "AO Cookbook",
          "depth": 3,
          "crawledAt": "2025-06-27T16:12:31.034Z"
        },
        {
          "url": "https://cookbook_ao.arweave.net/guides/aoconnect/reading-results.html",
          "title": "Reading results from an ao Process",
          "content": "Reading results from an ao Process ​In ao, messages produce results which are made available by Compute Units (CU's). Results are JSON objects consisting of the following fields: messages, spawns, output and error.Results are what the ao system uses to send messages and spawns that are generated by processes. A process can send a message just like you can as a developer, by returning messages and spawns in a result.You may want to access a result to display the output generated by your message. Or you may want to see what messages etc., were generated. You do not need to take the messages and spawns from a result and send them yourself. They are automatically handled by Messenger Units (MU's). A call to results can also provide you paginated list of multiple results.Fetching a single result ​jsimport { result } from \"@permaweb/aoconnect\"; let { Messages, Spawns, Output, Error } = await result({ // the arweave TxID of the message message: \"message-id\", // the arweave TxID of the process process: \"process-id\", });Fetching a set of results ​jsimport { results } from \"@permaweb/aoconnect\"; // fetching the first page of results let resultsOut = await results({ process: \"process-id\", sort: \"ASC\", limit: 25, }); // calling more with a cursor let resultsOut2 = await results({ process: \"process-id\", from: resultsOut.edges?.[resultsOut.edges.length - 1]?.cursor ?? null, sort: \"ASC\", limit: 25, });",
          "estimatedWords": 224,
          "lastModified": "2025-06-27T16:12:31.782Z",
          "siteKey": "ao",
          "siteName": "AO Cookbook",
          "depth": 3,
          "crawledAt": "2025-06-27T16:12:31.782Z"
        },
        {
          "url": "https://cookbook_ao.arweave.net/guides/aoconnect/spawning-processes.html",
          "title": "Spawning a Process",
          "content": "Spawning a Process ​A deep dive into the concept of Processes can be found in the ao Processes concept. This guide focuses on using ao connect to spawn a Process.In order to spawn a Process you must have the TxID of an ao Module that has been uploaded to Arweave. The Module is the source code for the Process. The Process itself is an instantiation of that source.You must also have the wallet address of a Scheduler Unit (SU). This specified SU will act as the scheduler for this Process. This means that all nodes in the system can tell that they need to read and write to this SU for this Process. You can use the address below.Wallet address of an available Scheduler ​lua_GQ33BkPtZrqxA84vM8Zk-N2aO0toNNu_C-l-rawrBAIn addition, in order to receive messages from other processes an Authority tag must be supplied with the wallet address of an authorised Messaging Unit (MU).Wallet address of the legacynet MU ​luafcoN_xJeisVsPXA-trzVAuIiqO3ydLQxM-L4XbrQKzYSpawning a Process in NodeJS ​jsimport { readFileSync } from \"node:fs\"; import { createDataItemSigner, spawn } from \"@permaweb/aoconnect\"; const wallet = JSON.parse( readFileSync(\"/path/to/arweave/wallet.json\").toString(), ); const processId = await spawn({ // The Arweave TxID of the ao Module module: \"module TxID\", // The Arweave wallet address of a Scheduler Unit scheduler: \"_GQ33BkPtZrqxA84vM8Zk-N2aO0toNNu_C-l-rawrBA\", // A signer function containing your wallet signer: createDataItemSigner(wallet), /* Refer to a Processes' source code or documentation for tags that may effect its computation. */ tags: [ { name: \"Authority\", value: \"fcoN_xJeisVsPXA-trzVAuIiqO3ydLQxM-L4XbrQKzY\" }, { name: \"Another-Tag\", value: \"another-value\" }, ], });Spawning a Process in a browser ​jsimport { createDataItemSigner, spawn } from \"@permaweb/aoconnect\"; const processId = await spawn({ // The Arweave TxID of the ao Module module: \"module TxID\", // The Arweave wallet address of a Scheduler Unit scheduler: \"_GQ33BkPtZrqxA84vM8Zk-N2aO0toNNu_C-l-rawrBA\", // A signer function containing your wallet signer: createDataItemSigner(globalThis.arweaveWallet), /* Refer to a Processes' source code or documentation for tags that may effect its computation. */ tags: [ { name: \"Authority\", value: \"fcoN_xJeisVsPXA-trzVAuIiqO3ydLQxM-L4XbrQKzY\" }, { name: \"Another-Tag\", value: \"another-value\" }, ], });",
          "estimatedWords": 329,
          "lastModified": "2025-06-27T16:12:32.122Z",
          "siteKey": "ao",
          "siteName": "AO Cookbook",
          "depth": 3,
          "crawledAt": "2025-06-27T16:12:32.122Z"
        },
        {
          "url": "https://cookbook_ao.arweave.net/guides/aoconnect/calling-dryrun.html",
          "title": "Calling DryRun",
          "content": "Calling DryRun ​DEPRECATION NOTICEThis method of reading state is in the process of being deprecated for processes running on HyperBEAM. It is recommended to use the State Patching mechanism to expose state via HTTP for better performance, as calling dryrun was known to cause severe bottlenecks in web applications on legacynet.DryRun is the process of sending a message object to a specific process and getting the Result object back, but the memory is not saved, it is perfect to create a read message to return the current value of memory. For example, a balance of a token, or a result of a transfer, etc. You can use DryRun to obtain an output without sending an actual message.jsimport { createDataItemSigner, dryrun } from \"@permaweb/aoconnect\"; const result = await dryrun({ process: 'PROCESSID', data: '', tags: [{name: 'Action', value: 'Balance'}, anchor: '1234', ...rest are optional (Id, Owner, etc) }); console.log(result.Messages[0]);",
          "estimatedWords": 147,
          "lastModified": "2025-06-27T16:12:32.363Z",
          "siteKey": "ao",
          "siteName": "AO Cookbook",
          "depth": 3,
          "crawledAt": "2025-06-27T16:12:32.363Z"
        },
        {
          "url": "https://cookbook_ao.arweave.net/guides/aoconnect/monitoring-cron.html",
          "title": "Monitoring Cron",
          "content": "Monitoring Cron ​When using cron messages, ao users need a way to start ingesting the messages, using this monitor method, ao users can initiate the subscription service for cron messages. Setting cron tags means that your process will start producing cron results in its outbox, but you need to monitor these results if you want messages from those results to be pushed through the network.jsimport { readFileSync } from \"node:fs\"; import { createDataItemSigner, monitor } from \"@permaweb/aoconnect\"; const wallet = JSON.parse( readFileSync(\"/path/to/arweave/wallet.json\").toString(), ); const result = await monitor({ process: \"process-id\", signer: createDataItemSigner(wallet), });You can stop monitoring by calling unmonitorjsimport { readFileSync } from \"node:fs\"; import { createDataItemSigner, unmonitor } from \"@permaweb/aoconnect\"; const wallet = JSON.parse( readFileSync(\"/path/to/arweave/wallet.json\").toString(), ); const result = await unmonitor({ process: \"process-id\", signer: createDataItemSigner(wallet), });",
          "estimatedWords": 127,
          "lastModified": "2025-06-27T16:12:32.937Z",
          "siteKey": "ao",
          "siteName": "AO Cookbook",
          "depth": 3,
          "crawledAt": "2025-06-27T16:12:32.937Z"
        },
        {
          "url": "https://cookbook_ao.arweave.net/guides/aoconnect/assign-data.html",
          "title": "Sending an Assignment to a Process",
          "content": "Sending an Assignment to a Process ​Assignments can be used to load Data from another Message into a Process. Or to not duplicate Messages. You can create one Message and then assign it to any number of processes. This will make it available to the Processes you have sent an Assignment to.Sending an Assignment in NodeJS ​jsimport { readFileSync } from \"node:fs\"; import { assign } from \"@permaweb/aoconnect\"; await assign({ process: \"process-id\", message: \"message-id\", }) .then(console.log) .catch(console.error);Excluding DataItem fields ​You can also exclude most DataItem fields which will tell the CU not to load them into your process. You may want to do this if you need only the header data like the Tags and not the Data itself etc... If you exclude the Owner it wont have any effect because the CU requires the Owner, so excluding Owner will be ignored by the CU. Only capitalized DataItem/Message fields will have an effect in the CU.jsimport { readFileSync } from \"node:fs\"; import { assign } from \"@permaweb/aoconnect\"; await assign({ process: \"process-id\", message: \"message-id\", exclude: [\"Data\", \"Anchor\"], }) .then(console.log) .catch(console.error);Assigning L1 Transactions ​You can also assign a layer 1 transaction by passing the baseLayer param into assign. This is useful for minting tokens etc... using the base layer. By default, if the L1 tx does not have at least 20 confirmations the SU will reject it. This can be changed by setting the Settlement-Depth tag to a different number on the Process when it is created.jsimport { readFileSync } from \"node:fs\"; import { assign } from \"@permaweb/aoconnect\"; await assign({ process: \"process-id\", message: \"layer 1 tx id\", baseLayer: true, }) .then(console.log) .catch(console.error);",
          "estimatedWords": 270,
          "lastModified": "2025-06-27T16:12:33.448Z",
          "siteKey": "ao",
          "siteName": "AO Cookbook",
          "depth": 3,
          "crawledAt": "2025-06-27T16:12:33.448Z"
        },
        {
          "url": "https://cookbook_ao.arweave.net/guides/aoconnect/signers.html",
          "title": "DataItem Signers",
          "content": "DataItem Signers ​Every message sent to AO MUST be signed, aoconnect provides a helper function for signing messages or spawning new processes. This helper function createDataItemSigner is provided for arweave wallets. But you can create your own Signer instance too.What is a Wallet/Keyfile? ​A wallet/keyfile is a public/private key pair that can be used to sign and encrypt data.What is an ao message/dataItem? ​You often see the terms message and dataItem used interchangeably in the documentation, a message is a data-protocol type in ao that uses the dataItem specification to describe the messages intent. A dataItem is defined in the ANS-104 bundle specification. A dataItem is the preferred format of storage for arweave bundles. A bundle is a collection of these signed dataItems. A message implements specific tags using the dataItem specification. When developers send messages to ao, they are publishing dataItems on arweave.🎓 To learn more about messages click here and to learn more about ANS-104 dataItems click hereWhat is a signer? ​A signer is function that takes data, tags, anchor, target and returns an object of id, binary representing a signed dataItem. AO accepts arweave signers and ethereum signers. createDataItemSigner is a helper function that can take an arweave keyfile or a browser instance of an arweave wallet usually located in the global scope of the browser, when I user connects to a wallet using an extension or html app.Examples ​arweave keyfileNOTE: if you do not have a wallet keyfile you can create one using npx -y @permaweb/wallet > wallet.jsonjsimport * as WarpArBundles from \"warp-arbundles\"; const pkg = WarpArBundles.default ? WarpArBundles.default : WarpArBundles; const { createData, ArweaveSigner } = pkg; function createDataItemSigner(wallet) { const signer = async ({ data, tags, target, anchor }) => { const signer = new ArweaveSigner(wallet); const dataItem = createData(data, signer, { tags, target, anchor }); return dataItem.sign(signer).then(async () => ({ id: await dataItem.id, raw: await dataItem.getRaw(), })); }; return signer; }arweave browser extensionNOTE: This implementation works with ArweaveWalletKit, ArConnect, and Arweave.appjsimport { Buffer } from \"buffer/index.js\"; import * as WarpArBundles from \"warp-arbundles\"; if (!globalThis.Buffer) globalThis.Buffer = Buffer; const { DataItem } = WarpArBundles; function createDataItemSigner(arweaveWallet) { /** * createDataItem can be passed here for the purposes of unit testing * with a stub */ const signer = async ({ data, tags, target, anchor, createDataItem = (buf) => new DataItem(buf), }) => { /** * signDataItem interface according to ArweaveWalletConnector * * https://github.com/jfbeats/ArweaveWalletConnector/blob/7c167f79cd0cf72b6e32e1fe5f988a05eed8f794/src/Arweave.ts#L46C23-L46C23 */ const view = await arweaveWallet.signDataItem({ data, tags, target, anchor, }); const dataItem = createDataItem(Buffer.from(view)); return { id: await dataItem.id, raw: await dataItem.getRaw(), }; }; return signer; }ethereum keyjsimport { EthereumSigner, createData } from \"@dha-team/arbundles\"; function createDataItemSigner(wallet) { const signer = async ({ data, tags, target, anchor }) => { const signer = new EthereumSigner(wallet); const dataItem = createData(data, signer, { tags, target, anchor }); return dataItem.sign(signer).then(async () => ({ id: await dataItem.id, raw: await dataItem.getRaw(), })); }; return signer; }Summary ​Using the signer function developers can control how dataItems are signed without having to share the signing process with aoconnect.",
          "estimatedWords": 500,
          "lastModified": "2025-06-27T16:12:33.522Z",
          "siteKey": "ao",
          "siteName": "AO Cookbook",
          "depth": 3,
          "crawledAt": "2025-06-27T16:12:33.522Z"
        },
        {
          "url": "https://cookbook_ao.arweave.net/guides/migrating-to-hyperbeam/why-migrate.html",
          "title": "Why Migrate to HyperBEAM",
          "content": "Why Migrate to HyperBEAM? ​Migrating processes from legacynet to HyperBEAM is essential for leveraging significant advancements in performance, features, and developer experience on AO.HyperBEAM is a new, more robust foundation for decentralized applications on AO, offering several key advantages:Enhanced Performance: Built on an architecture optimized for concurrency, HyperBEAM provides faster message scheduling and more responsive applications.Direct State Access: HyperBEAM allows processes to expose their state directly via HTTP. This enables immediate reads of your process's data, eliminating the need for dry-run messages which were a common performance bottleneck.Easy Extensibility: It allows core feature extensibility through modular devices.The most impactful change when migrating is the ability to expose parts of your process state for immediate reading. This dramatically improves the performance of web frontends and data-driven services.To learn how to implement this, see Exposing Process State.",
          "estimatedWords": 135,
          "lastModified": "2025-06-27T16:12:34.017Z",
          "siteKey": "ao",
          "siteName": "AO Cookbook",
          "depth": 3,
          "crawledAt": "2025-06-27T16:12:34.017Z"
        },
        {
          "url": "https://cookbook_ao.arweave.net/guides/migrating-to-hyperbeam/exposing-process-state.html",
          "title": "Exposing Process State to HyperBEAM",
          "content": "Exposing Process State to HyperBEAM ​HyperBEAM introduces a powerful feature for exposing parts of a process's state for immediate reading over HTTP. This improves performance for web frontends and data services by replacing the need for dryrun calls, which were a known bottleneck on legacynet.The Patch Device ​The ~patch@1.0 device is the mechanism that allows AO processes to make parts of their internal state readable via direct HTTP GET requests.How it Works ​Exposing state is a four-step process involving your process and HyperBEAM:Process Logic: From your process (e.g., in Lua or WASM), send an outbound message to the ~patch@1.0 device.Patch Message Format: The message must include device and cache tags.luaSend({ Target = ao.id, device = 'patch@1.0', cache = { mydatakey = MyValue } })HyperBEAM Execution: HyperBEAM's dev_patch module processes this message, mapping the key-value pairs from the cache table to a URL path.HTTP Access: The exposed data is then immediately available via a standard HTTP GET request to the process's endpoint.HyperBEAMGET /<process-id>~process@1.0/compute/cache/<mydatakey>Initial State Sync (Optional) ​To make data available immediately on process creation, you can patch its initial state. A common pattern is to use a flag to ensure this sync only runs once, as shown in this example for a token's Balances and TotalSupply.lua-- Place this logic at the top level of your process script, -- outside of specific handlers, so it runs on load. Balances = { token1 = 100, token2 = 200 } -- A table of balances TotalSupply = 1984 -- A single total supply value -- 1. Initialize Flag: -- Initializes a flag if it doesn't exist. InitialSync = InitialSync or 'INCOMPLETE' -- 2. Check Flag: -- Checks if the sync has already run. if InitialSync == 'INCOMPLETE' then -- 3. Patch State: -- The `Send` call patches the state, making it available at endpoints like: -- /cache/balances -- /cache/totalsupply Send({ device = 'patch@1.0', cache = { balances = Balances, totalsupply = TotalSupply } }) -- 4. Update Flag: -- Updates the flag to prevent the sync from running again. InitialSync = 'COMPLETE' print(\"Initial state sync complete. Balances and TotalSupply patched.\") endThis pattern makes essential data queryable upon process creation, boosting application responsiveness.Example (Lua in aos) ​This handler exposes a currentstatus key that can be read via HTTP after the PublishData action is called.lua-- In your process code (e.g., loaded via .load) Handlers.add( \"PublishData\", Handlers.utils.hasMatchingTag(\"Action\", \"PublishData\"), function (msg) local dataToPublish = \"Some important state: \" .. math.random() -- Expose 'currentstatus' key under the 'cache' path Send({ device = 'patch@1.0', cache = { currentstatus = dataToPublish } }) print(\"Published data to /cache/currentstatus\") end )Avoiding Key Conflicts ​Keys in the cache table become URL path segments. To avoid conflicts with reserved HyperBEAM paths, use descriptive, specific keys. Avoid using reserved keywords such as:now, compute, state, info, testFor instance, prefer a key like myappstate over a generic key like state.WARNINGHTTP paths are case-insensitive. While the patch device stores keys with case sensitivity (e.g., MyKey vs mykey), HTTP access to paths like the following is ambiguous and may lead to unpredictable results.To prevent conflicts, always use lowercase keys in your cache table (e.g., mykey, usercount).HyperBEAMGET /<process-id>~process@1.0/cache/mykeyKey Points ​Path Structure: Data is exposed at a path structured like this, where <key> is a key from your cache table:HyperBEAM/<process-id>~process@1.0/cache/<key>Data Types: Basic data types like strings and numbers work best. Complex objects may require serialization.compute vs now: Accessing patched data can be done via two main paths:HyperBEAMGET /<process-id>~process@1.0/compute/cache/... GET /<process-id>~process@1.0/now/cache/...The compute endpoint serves the last known value quickly, while now may perform additional computation to get the most recent state.Read-Only Exposure: Patching is for efficient reads and does not replace your process's core state management logic.Using the patch device enables efficient, standard HTTP access to your process state, seamlessly connecting decentralized logic with web applications.Next Steps ​Now that you know how to expose static state, learn how to perform on-the-fly computations on that state by reading dynamic state.",
          "estimatedWords": 646,
          "lastModified": "2025-06-27T16:12:35.050Z",
          "siteKey": "ao",
          "siteName": "AO Cookbook",
          "depth": 3,
          "crawledAt": "2025-06-27T16:12:35.050Z"
        },
        {
          "url": "https://cookbook_ao.arweave.net/guides/migrating-to-hyperbeam/reading-dynamic-state.html",
          "title": "Reading Dynamic State",
          "content": "Reading Dynamic State ​Beyond reading static, cached state from your process, HyperBEAM allows you to perform on-the-fly computations on that state using Lua. This guide explains how to create and use \"transformation functions\" to return dynamic, computed data without altering the underlying state of your process.This is a powerful pattern for creating efficient data APIs for your applications, reducing client-side logic, and minimizing data transfer.This guide assumes you are already familiar with exposing static process state.How it Works: The Lua Device ​The magic behind this is the lua@5.3a device, which can execute a Lua script against a message. In this pattern, we use a HyperBEAM URL (hashpath) to construct a pipeline:First, we grab the latest state of an AO process.Then, we pipe that state as the base message into the lua@5.3a device.We tell the Lua device which script to load (from an Arweave transaction) and which function to execute.The function runs, processing the base state.Finally, the result of the function is returned over HTTP.Example: Calculating Circulating Supply ​Let's consider a practical example: a token process where we have patched the Balances table to be readable. Rather than forcing clients to download all balance data to compute the total supply, we can do it on the HyperBEAM node.1. The Transformation Function ​First, create a Lua script (sum.lua) with a function that takes the state (base) and calculates the sum of balances.lua-- sum.lua function sum(base, req) -- Initialize total supply counter local totalSupply = 0 local total = 0 -- Check if we have balances in our state if base.balances then -- Iterate through all balances and sum them for address, balance in pairs(base.balances) do -- Ensure balance is a number and add to total local numBalance = tonumber(balance) or 0 totalSupply = totalSupply + numBalance total = total + 1 end end -- Return the computed result as a table return { CirculatingSupply = tostring(math.floor(totalSupply)), BalanceCount = tostring(math.floor(total)) } endThe transformation function receives two arguments:base: The message being processed, which in our pipeline will be the cached state data from your process.req: The incoming request object, which contains parameters and other metadata.2. Publishing the Function ​Next, publish your Lua script to Arweave. The arx CLI tool is recommended for this.bash# Install arx globally npm i -g @permaweb/arx # Upload your Lua function to Arweave arx upload sum.lua \\ -w PATH_TO_WALLET.json \\ -t arweave \\ --content-type application/lua \\ --tags Data-Protocol aoarx will return a transaction ID for your script. Let's say it's LUA_SCRIPT_TX_ID.3. Calling the Function ​With the process ID (YOUR_PROCESS_ID) and the script transaction ID (LUA_SCRIPT_TX_ID), you can construct a URL to call your function:HyperBEAMGET /<YOUR_PROCESS_ID>~process@1.0/now/~lua@5.3a&module={LUA_SCRIPT_TX_ID}/sum/serialize~json@1.0This URL breaks down as follows:/{YOUR_PROCESS_ID}~process@1.0: Targets the AO process and its state./now: Gets the most current state./~lua@5.3a&module={LUA_SCRIPT_TX_ID}: This is the key part. It tells HyperBEAM to take the output of the previous step (the process state) and process it with the lua@5.3a device, loading your script from the module transaction./sum: Calls the sum function within your Lua script./serialize~json@1.0: Takes the table returned by your function and serializes it into a JSON object.4. Integrating into an Application ​Here's how you could fetch this dynamic data in a JavaScript application:javascript// Fetch circulating supply with JSON serialization const processId = \"FkJPkIHp_Gc_7KOLbtyzowPcJUc3SG_G25SJp0fbTmE\"; // An example process const moduleId = \"QSBQZsowVRdvsEbdTv-KEF4_Z5bYf11M3X5-8LN0NM4\"; // The example sum.lua script const hyperbeam = \"router-1.forward.computer\"; async function getDynamicState() { const url = `https://${hyperbeam}/${processId}~process@1.0/now/~lua@5.3a&module=${moduleId}/sum/serialize~json@1.0`; const response = await fetch(url); const data = await response.json(); console.log(`Total Supply: ${data.circulatingsupply}`); console.log(`Token Holders: ${data.balancecount}`); } getDynamicState();This approach significantly improves performance by offloading computation from the client to the HyperBEAM node and reducing the amount of data sent over the network.",
          "estimatedWords": 600,
          "lastModified": "2025-06-27T16:12:35.639Z",
          "siteKey": "ao",
          "siteName": "AO Cookbook",
          "depth": 3,
          "crawledAt": "2025-06-27T16:12:35.639Z"
        },
        {
          "url": "https://cookbook_ao.arweave.net/guides/migrating-to-hyperbeam/aos-with-hyperbeam.html",
          "title": "Connecting to HyperBEAM with aos",
          "content": "Connecting to HyperBEAM with aos ​This guide explains how to use aos, the command-line interface for AO, to connect to a HyperBEAM node for development.Installing aos ​The primary tool for interacting with AO and developing processes is aos, a command-line interface and development environment.npmpnpmbunbashnpm i -g https://get_ao.arweave.netbashpnpm add -g https://get_ao.arweave.netbash# Bun is not supported yet # bun install -g https://get_ao.arweave.netConnecting to a HyperBEAM Node ​While you don't need to run a HyperBEAM node yourself, you do need to connect to one to interact with the network during development.To start aos and connect to a public HyperBEAM node, simply run the command in your terminal:bashaos --mainnet \"https://router-1.forward.computer\" myMainnetProcessThis connects you to an interactive Lua environment running within a process on the AO network. This process acts as your command-line interface (CLI) to the AO network. When you specify --mainnet <URL>, it connects to the genesis_wasm device running on the HyperBEAM node at the supplied URL, allowing you to interact with other processes, manage your wallet, and develop new AO processes.Running a Local HyperBEAM NodeIf you are running HyperBEAM locally and want to use that node when booting up aos, you must first start your local node with the genesis_wasm profile:bashrebar3 as genesis_wasm shellThen, you can connect aos to it:bashaos --mainnet \"http://localhost:8734\" myLocalProcessUntil aos is fully HyperBEAM native, the genesis_wasm profile is required to run a local Compute Unit (CU) for executing aos.Interacting with Mainnet Processes ​Note on Blocking CallsBlocking message patterns, such as Receive and ao.send().receive(), are not available when running aos against a HyperBEAM process. HyperBEAM processes do not support the underlying wasm modules required for this functionality. You should rely on asynchronous patterns using handlers instead.",
          "estimatedWords": 277,
          "lastModified": "2025-06-27T16:12:36.110Z",
          "siteKey": "ao",
          "siteName": "AO Cookbook",
          "depth": 3,
          "crawledAt": "2025-06-27T16:12:36.110Z"
        },
        {
          "url": "https://cookbook_ao.arweave.net/guides/migrating-to-hyperbeam/ao-connect.html",
          "title": "HyperBEAM from AO Connect",
          "content": "HyperBEAM from AO Connect ​This guide explains how to interact with a process using HyperBEAM and aoconnect.Prerequisites ​Node.js environment@permaweb/aoconnect libraryThe latest version of aosWallet file (wallet.json) containing your cryptographic keysA HyperBEAM node running with the genesis_wasm profilebashrebar3 as genesis_wasm shellThe Process ID for a process created with genesis_wasm (this is the default in the latest version of aos).Step 1: Environment Setup ​Install necessary dependencies:bashnpm install @permaweb/aoconnectEnsure your wallet file (wallet.json) is correctly formatted and placed in your project directory.INFOYou can create a test wallet using this command: npx -y @permaweb/wallet > wallet.jsonStep 2: Establish Connection ​Create a new JavaScript file (e.g., index.js) and set up your Permaweb connection. You will need a processId of a process that you want to interact with.javascriptimport { connect, createSigner } from \"@permaweb/aoconnect\"; import fs from \"node:fs\"; const jwk = JSON.parse(fs.readFileSync(\"wallet.json\", \"utf-8\")); // The Process ID to interact with const processId = \"<your genesis_wasm generated process id>\"; const { request } = connect({ MODE: \"mainnet\", URL: \"http://localhost:8734\", signer: createSigner(jwk), });Step 3: Pushing a Message to a Process ​Use the request function to send a message to the process. In aoconnect, this is done by using the push path parameter.javascriptconst processResult = await request({ path: `/${processId}~process@1.0/push/serialize~json@1.0`, method: \"POST\", target: processId, signingFormat: \"ANS-104\", }); console.log(processResult);Full Example ​To run the full script, combine the snippets from Step 2 and 3 into index.js:javascriptimport { connect, createSigner } from \"@permaweb/aoconnect\"; import fs from \"node:fs\"; const jwk = JSON.parse(fs.readFileSync(\"wallet.json\", \"utf-8\")); const processId = \"<your genesis_wasm generated process id>\"; const { request } = connect({ MODE: \"mainnet\", URL: \"http://localhost:8734\", signer: createSigner(jwk), }); const processResult = await request({ path: `/${processId}~process@1.0/push/serialize~json@1.0`, method: \"POST\", target: processId, signingFormat: \"ANS-104\", }); console.log(processResult);Now, run it:bashnode index.jsYou should see an object logged to the console, containing the ID of the message that was sent.Conclusion ​Following these steps, you've successfully sent a message to a process. This is a fundamental interaction for building applications on hyperAOS.",
          "estimatedWords": 316,
          "lastModified": "2025-06-27T16:12:36.221Z",
          "siteKey": "ao",
          "siteName": "AO Cookbook",
          "depth": 3,
          "crawledAt": "2025-06-27T16:12:36.221Z"
        },
        {
          "url": "https://cookbook_ao.arweave.net/guides/dev-cli/index.html",
          "title": "AO Dev-Cli 01",
          "content": "AO Dev-Cli 0.1 ​The AO dev-cli is a tool that is used to build ao wasm modules, the first versions of the tool only supported lua as the embedded language or c based module. With this release developers now can add any pure c or cpp module to their wasm builds. This opens the door for many different innovations from indexers to languages.Install ​RequirementsDocker is required: https://docker.comshellcurl -L https://install_ao.g8way.io | shStart a project ​shellao init [project-name]Build a project ​shellcd [project-name] ao buildDeploy a project ​RequirementsYou will need an arweave keyfile, you can create a local one using this command npx -y @permaweb/wallet > wallet.jsonshellao publish -w [path_to_wallet] [path_to_wasm]Configuration ​To customize your build process, create a config.yml file in the root directory of your project. This file will modify your settings during the build.Configuration Options: ​preset: Selects default values for stack_size, initial_memory, and maximum_memory. For available presets, see Config Presets. (Default: md)stack_size: Specifies the stack size, overriding the value from the preset. Must be a multiple of 64. (Default: 32MB)initial_memory: Defines the initial memory size, overriding the preset value. Must be larger than stack_size and a multiple of 64. (Default: 48MB)maximum_memory: Sets the maximum memory size, overriding the preset value. Must be larger than stack_size and a multiple of 64. (Default: 256MB)extra_compile_args: Provides additional compilation commands for emcc. (Default: [])keep_js: By default, the generated .js file is deleted since AO Loader uses predefined versions. Set this to true if you need to retain the .js file. (Default: false)Libraries ​Starting with version 0.1.3, you can integrate external libraries into your project. To do this, follow these guidelines:Adding Libraries ​Create a libs Directory: At the root of your project, create a directory named /libs. This is where you'll place your library files.Place Your Library Files: Copy or move your compiled library files (e.g., .a, .so, .o, .dylib, etc.) into the /libs directory.NOTEEnsure that all library files are compiled using emcc to ensure compatibility with your project.IMPORTANTMore details to come including an example project...Example Directory Structure ​project-root/ │ ├── libs/ │ ├── libexample.a │ ├── libanother.so │ └── libmore.o │ ├── process.lua ├── ao.lua │ └── config.ymlUsing Libraries in Your Code ​After adding the library files to the /libs directory, you need to link against these libraries in your project. This often involves specifying the library path and names in your build scripts or configuration files. For example:For C/C++ Projects: You can just include any header files placed in the libs folder as the libs with be automatically built into your module.For Lua Projects: Depending on how your build your libraries and if you compiled them with Lua bindings you can just require the libs in your lua files. markdown = require('markdown')IMPORTANTMore details to come...Lua Build Example ​To create and build a Lua project, follow these steps:shao init -l lua [project-name] cd [project-name] ao buildC Build Example ​To create and build a C project, follow these steps:shao init -l c [project-name] cd [project-name] ao buildConfig Presets ​Here are the predefined configuration presets:js'xs': { 'stack_size': 8388608, // 8mb 'initial_memory': 16777216, // 16mb 'maximum_memory': 67108864 // 64mb }, 'sm': { 'stack_size': 16777216, // 16mb 'initial_memory': 33554432, // 32mb 'maximum_memory': 134217728 // 128mb }, 'md': { 'stack_size': 33554432, // 32mb 'initial_memory': 50331648, // 48mb 'maximum_memory': 268435456 // 256mb }, 'lg': { 'stack_size': 50331648, // 48mb 'initial_memory': 67108864, // 64mb 'maximum_memory': 268435456 // 256mb }, 'xl': { 'stack_size': 67108864, // 64mb 'initial_memory': 100663296, // 96mb 'maximum_memory': 536870912 // 512mb }, 'xxl': { 'stack_size': 100663296, // 96mb 'initial_memory': 134217728, // 128mb 'maximum_memory': 4294967296 // 4096mb },",
          "estimatedWords": 587,
          "lastModified": "2025-06-27T16:12:36.683Z",
          "siteKey": "ao",
          "siteName": "AO Cookbook",
          "depth": 3,
          "crawledAt": "2025-06-27T16:12:36.683Z"
        },
        {
          "url": "https://cookbook_ao.arweave.net/guides/snacks/sqlite.html",
          "title": "Getting started with SQLite",
          "content": "Getting started with SQLite ​SQLite is a relational database engine. In this guide, we will show how you can spawn a process with SQLite and work with data using a relational database.Setup ​NOTE: make sure you have aos installed, if not checkout Getting Startedspawn a new process mydb with a --sqlite flag, this instructs ao to use the latest sqlite module.shaos mydb --sqliteInstall AO Package Manager ​installing apm, the ao package manager we can add helper modules to make it easier to work with sqlite.lua.load-blueprint apmInstall dbAdmin package ​DbAdmin is a module that connects to a sqlite database and provides functions to work with sqlite.https://apm_betteridea.g8way.io/pkg?id=@rakis/DbAdminluaapm.install('@rakis/dbAdmin')Create sqlite Database ​lualocal sqlite = require('lsqlite3') Db = sqlite.open_memory() dbAdmin = require('@rakis/DbAdmin').new(Db)Create Table ​Create a table called CommentsluadbAdmin:exec([[ CREATE TABLE IF NOT EXISTS Comments ( ID INTEGER PRIMARY KEY AUTOINCREMENT, Asset TEXT, User TEXT, Body TEXT ); ]])Insert data ​lualocal SQL = \"INSERT INTO Comments (Asset, User, Body) VALUES (?,?,?);\" dbAdmin:apply(SQL, {\"dog.jpg\", \"Anon\", \"Nice Picture\"})List data ​lualocal SQL = \"SELECT * FROM Comments;\" dbAdmin:exec(SQL)Congrats! ​You are using sqlite on AO 🎉",
          "estimatedWords": 176,
          "lastModified": "2025-06-27T16:12:36.785Z",
          "siteKey": "ao",
          "siteName": "AO Cookbook",
          "depth": 3,
          "crawledAt": "2025-06-27T16:12:36.785Z"
        },
        {
          "url": "https://cookbook_ao.arweave.net/guides/snacks/weavedrive.html",
          "title": "Using WeaveDrive",
          "content": "Using WeaveDrive ​WeaveDrive has been released on AO legacynet, which is great! But how to use it with your process? This post aims to provide a step by step guide on how to use WeaveDrive in your AOS process.The current availability time is called Assignments and this type puts WeaveDrive in a mode that allows you to define an Attestor wallet address when you create your AOS process. This will enable the process to load data from dataItems that have a Attestation created by this wallet.Prep Tutorial ​In order, to setup the tutorial for success we need to upload some data and upload an attestation. It will take a few minutes to get mined into a block on arweave.Install arxshnpm i -g @permaweb/arxCreate a walletnpx -y @permaweb/wallet > ~/.test-wallet.jsonCreate some datamkdir test-weavedrive cd test-weavedrive echo \"<h1>Hello WeaveDrive</h1>\" > data.html arx upload data.html -w ~/.test-wallet.json -t arweaveYou should get a result like:Loaded address: vfSWG3girEwCBggs9xeztuRyiltsT2CJH_m-S8A58yQ Uploaded to https://arweave.net/9TIPJD2a4-IleOQJzRwPnDHO5DA891MWAyIdJJ1SiSkCreate AttestationIt is important to copy the id of the uploaded dataItem, in the above case 9TIPJD2a4-IleOQJzRwPnDHO5DA891MWAyIdJJ1SiSk as your Message Value.echo \"attestation-example\" > att.txt arx upload att.txt -w ~/.test-wallet.json -t arweave --tags Data-Protocol ao Type Attestation Message 9TIPJD2a4-IleOQJzRwPnDHO5DA891MWAyIdJJ1SiSk👏 Awesome! That will take a few minutes to get mined on arweave, once it is mined then we will be able to read the data.html dataItem using WeaveDriveEnable WeaveDrive in a process ​Lets create a new AOS process with WeaveDrive enabled and the wallet we created above as an Attestor.NOTE: it is important to use the same wallet address that was used to sign the attestation data-item.aos test-weavedrive --tag-name Extension --tag-value WeaveDrive --tag-name Attestor --tag-value vfSWG3girEwCBggs9xeztuRyiltsT2CJH_m-S8A58yQ --tag-name Availability-Type --tag-value AssignmentsNOTE: It does take a few minutes for the data to get 20 plus confirmations which is the threshold for data existing on arweave. You may want to go grab a coffee. ☕Install apm and WeaveDrive ​.load-blueprint apm apm.install('@rakis/WeaveDrive')Load Data ​Drive = require('@rakis/WeaveDrive') Drive.getData(\"9TIPJD2a4-IleOQJzRwPnDHO5DA891MWAyIdJJ1SiSk\")",
          "estimatedWords": 316,
          "lastModified": "2025-06-27T16:12:37.262Z",
          "siteKey": "ao",
          "siteName": "AO Cookbook",
          "depth": 3,
          "crawledAt": "2025-06-27T16:12:37.262Z"
        },
        {
          "url": "https://cookbook_ao.arweave.net/concepts/index.html",
          "title": "Concepts",
          "content": "Concepts ​This section explains the core concepts and architecture behind AO, helping you understand how the system works at a fundamental level.System Architecture ​AO is built on a few fundamental principles that form its foundation:Two core types: Messages and Processes - the basic building blocks of the AO ecosystemNo shared state, only Holographic State - a unique approach to distributed computingDecentralized Computer (Grid) - enabling truly distributed applicationsCore Components ​Explore these foundational concepts to gain a deeper understanding of AO:How it Works - An overview of AO's architecture and how the different parts interactProcesses - Learn about processes, the computational units in AOMessages - Understand the messaging system that enables communicationEvaluation - Discover how code execution works in the AO environmentUnits - Learn about the computational units that power the AO networkTechnical Foundations ​Specifications - Detailed technical specifications for the AO protocolGetting Started ​Meet Lua - Introduction to Lua, the programming language used in AOAO System Tour - A guided tour of the AO system and its capabilitiesNavigation ​Use the sidebar to navigate between concept topics. Each document provides in-depth information about a specific aspect of AO.",
          "estimatedWords": 187,
          "lastModified": "2025-06-27T16:12:37.350Z",
          "siteKey": "ao",
          "siteName": "AO Cookbook",
          "depth": 3,
          "crawledAt": "2025-06-27T16:12:37.350Z"
        },
        {
          "url": "https://cookbook_ao.arweave.net/guides/snacks/0rbit/post-request.html",
          "title": "First POST Request",
          "content": "First POST Request ​In this tutorial, we will learn how to make a POST request on 0rbit process.🔑 Prerequisites ​aos installed on your system.Some $0RBT. Learn how to get $0RBT hereAny Code Editor (VSCode, Sublime Text, etc)If you are ready with the above prerequisites,🛠️ Let's Start Building ​Initialize the Project ​Create a new file named 0rbit-Post-Request.lua in your project directory.bashtouch 0rbit-Post-Request.luaInitialize the Variables ​lualocal json = require(\"json\") _0RBIT = \"BaMK1dfayo75s3q1ow6AO64UDpD9SEFbeE8xYrY2fyQ\" _0RBT_POINTS = \"BUhZLMwQ6yZHguLtJYA5lLUa9LQzLXMXRfaq9FVcPJc\" FEE_AMOUNT = \"1000000000000\" -- 1 $0RBT BASE_URL = \"https://arweave.dev/graphql\" -- The data body to be sent in the POST request BODY = json.encode({ query = [[ query { transactions( owners: [\"vh-NTHVvlKZqRxc8LyyTNok65yQ55a_PJ1zWLb9G2JI\"] ) { edges { node { id } } } } ]] }); ReceivedData = ReceivedData or {}Make the Request ​The following code contains the Handler that will send 1 $0RBT to the 0rbit process and make the POST request for the BASE_URLluaHandlers.add( \"Post-Request\", Handlers.utils.hasMatchingTag(\"Action\", \"First-Post-Request\"), function(msg) Send({ Target = _0RBT_TOKEN, Action = \"Transfer\", Recipient = _0RBIT, Quantity = FEE_AMOUNT, [\"X-Url\"] = BASE_URL, [\"X-Action\"] = \"Post-Real-Data\", [\"X-Body\"] = BODY }) print(Colors.green .. \"You have sent a POST Request to the 0rbit process.\") end )Breakdown of the above code:Handlers.add is used to add a new handler to the ao process.Post-Request__ is the name of the handler.Handlers.utils.hasMatchingTag is a function that checks if the incoming message has the matching tag same as the First-Post-Request.function(msg) is the function executed when the handler is called.Send is the function that takes several tags as the arguments and creates a message on the ao:TagDescriptionTargetThe processId of the recipient. In this case, it's the $0RBT token processId.ActionThe tag that defines the handler to be called in the recipient process. In this case it's TransferRecipientThe tag that accepts the processId to whom the $0RBT will be sent. In this case, it's the 0rbit processId.QuantityThe amount of $0RBT to be sent.[\"X-Url\"]The forwarded-tag which contains the URL and the same will be used by the 0rbit process to fetch the data.[\"X-Action\"]The forwarded-tag which contains the action to be performed by the 0rbit process. In this case, it's Post-Real-Data.[\"X-Body\"]The forwarded-tag which contains the data body to be sent in the POST request.Receive Data ​The following code contains the Handler that will receive the data from the 0rbit process and print it.luaHandlers.add( \"Receive-Data\", Handlers.utils.hasMatchingTag(\"Action\", \"Receive-Response\"), function(msg) local res = json.decode(msg.Data) ReceivedData = res print(Colors.green .. \"You have received the data from the 0rbit process.\") end )Breakdown of the above code:Handlers.add is used to add a new handler to the ao process.Receive-Data is the name of the handler.Handlers.utils.hasMatchingTag is a function that checks if the incoming message has the matching tag same as the Receive-Response.function(msg) is the function executed when the handler is called. json.decode is used to decode the JSON data received.ReceivedData = res stores the received data in the ReceivedData variable.The 0rbit process always sends the data in the string format. json.decode is used above because we know the receiving data, i.e., stringified JSON. So, you need to decode the data as per your requirements.🏃 Run the process ​Create a new process and load the script ​bashaos 0rbitPostRequest --load 0rbit-Post-Request.luaThe above command will create a new process with the name 0rbitPostRequest and load 0rbit-Post-Request.lua into it.Fund your process ​Transfer some $0RBT to your processID.Call the Handler ​Call the handler, who will create a request for the 0rbit process.bashSend({ Target= ao.id, Action=\"First-Post-Request\" })Check the Data ​To check the data stored in the ReceivedData variable, run the following command:bashReceivedDataUpon the successful execution, you will receive the JSON data in your terminal:json{ data = { transactions = { edges = { { node = { id = \"nH0NU9rgNqGVHwjtjFvnIyXpsP7YVrj_v7JxFErHNB4\" } }, //and so on... { node = { id = \"9HLUVJo4AcrSxQeapf2hutS2Xp7hx_XDiIvv3vnxDcc\" } } } } } }Voila! You have successfully made your first POST request on the 0rbit process. 🎉You can find the complete code here:https://github.com/0rbit-co/examples/blob/main/First-Post-Request.lua",
          "estimatedWords": 635,
          "lastModified": "2025-06-27T16:12:40.811Z",
          "siteKey": "ao",
          "siteName": "AO Cookbook",
          "depth": 4,
          "crawledAt": "2025-06-27T16:12:40.811Z"
        },
        {
          "url": "https://cookbook_ao.arweave.net/concepts/how-it-works.html",
          "title": "How ao messaging works",
          "content": "How ao messaging works ​Before we dive in to ao, I want to share with you a little information about unix. Unix is a powerful operating system, but in its design it is focused on two Principal \"Types\". Files and Programs. A File is data and a Program is logic, when you combine the two you get information.Input.file | TransformProgram | Output.fileYou might have done something like this on the command line without knowing what you were doing. Being able to connect files to programs and return files which can then be passed to other programs creates a complex system composed of simple applications. This is a very powerful idea.Now, lets talk about ao the hyper parallel computer, and lets change the idea of a File to the ao concept of a Message and the idea of a Program to the ao concept of a Process. The ao computer takes messages and sends them to Processes in which those Processes can output messages that can be sent to other Processes. The result is a complex system built on simple modular logic containers.MessageA | Process | MessageBHere is a description of the process as outlined in the flowchart:A message is initiated from an ao Connect. This message is sent to the mu service using a POST request. The body of the request contains data following a protocol, labeled 'ao', and is of the type 'Message'.The mu service processes the POST request and forwards the message to the su service. This is also done using a POST request with the same data protocol and message type.The su service stores the assignment and message on Arweave.A GET request is made to the cu service to retrieve results based on a message ID. The cu is a service that evaluates messages on processes and can return results based on an individual message identifier.A GET request is made to the su service to retrieve the assignment and message. This request is looking for messages from a process ID, within a range of time from a start (from the last evaluation point) to (to the current message ID).The final step is to push any outbox Messages. It involves reviewing the messages and spawns in the Result object. Based on the outcome of this check, the steps 2, 3, and 4 may be repeated for each relevant message or spawn.",
          "estimatedWords": 393,
          "lastModified": "2025-06-27T16:12:41.098Z",
          "siteKey": "ao",
          "siteName": "AO Cookbook",
          "depth": 3,
          "crawledAt": "2025-06-27T16:12:41.098Z"
        },
        {
          "url": "https://cookbook_ao.arweave.net/concepts/processes.html",
          "title": "Processes",
          "content": "Processes ​Processes possess the capability to engage in communication via message passing, both receiving and dispatching messages within the network. Additionally, they hold the potential to instantiate further processes, enhancing the network's computational fabric. This dynamic method of data dissemination and interaction within the network is referred to as a 'holographic state', underpinning the shared and persistent state of the network.When building a Process with aos you have the ability to add handlers, these handlers can be added by calling the Handlers.add function, passing a \"name\", a \"match\" function, and a \"handle\" function.The core module contains a helper library that gets injected into the handler function, this library is called ao.lua{ env = { Process = { Id = \"5WzR7rJCuqCKEq02WUPhTjwnzllLjGu6SA7qhYpcKRs\", Owner = \"_r9LpP4FtClpsGX3TOohubyaeb0IQTZZMcxQ24tTsGo\", Tags = {...} }, Module = { Id = \"UAUszdznoUPQvXRbrFuIIH6J0N_LnJ1h4Trej28UgrE\", Owner = \"_r9LpP4FtClpsGX3TOohubyaeb0IQTZZMcxQ24tTsGo\", Tags = {...} } }, id = \"5WzR7rJCuqCKEq02WUPhTjwnzllLjGu6SA7qhYpcKRs\", isTrusted = \"function: 0x5468d0\", result = \"function: 0x547120\", send = \"function: 0x547618\", spawn = \"function: 0x5468b0\" }The main functions to look at in this ao helper isao.send(Message) - sends a message to a processao.spawn(Module, Message) - creates a new processEthereum Signed Process or Module ​For an ao Process or Module, if the ANS-104 DataItem was signed using Ethereum keys, then the value in the env.Process.Owner or env.Module.Owner field, respectively, will be the EIP-55 Ethereum address of the signer. For example: 0xfB6916095ca1df60bB79Ce92cE3Ea74c37c5d359ao.send Example ​luaao.send({ Target = Chatroom, Action = \"Broadcast\", Data = \"Hello from my Process!\" })ao.spawn Example ​luaao.spawn(ao.env.Module.Id, { [\"Memory-Limit\"] = \"500-mb\", [\"Compute-Limit\"] = \"900000000000000000\" })ao.env ​NOTE: ao.env is important context data that you may need as a developer creating processes.The ao.env property contains the Process and Module Reference Objectsluaenv = { Process = { Id = \"5WzR7rJCuqCKEq02WUPhTjwnzllLjGu6SA7qhYpcKRs\", Owner = \"_r9LpP4FtClpsGX3TOohubyaeb0IQTZZMcxQ24tTsGo\", Tags = {...} }, Module = { Id = \"UAUszdznoUPQvXRbrFuIIH6J0N_LnJ1h4Trej28UgrE\", Owner = \"_r9LpP4FtClpsGX3TOohubyaeb0IQTZZMcxQ24tTsGo\", Tags = {...} } }Both the Process and the Module contain the attributes of the ao Data-Protocol.Summary ​Processes in the network communicate through message passing and can create new processes, contributing to a 'holographic state' of shared and persistent data. Developers can build a Process using aos by adding handlers through the Handlers.add function with specific name, match, and handle functions. The ao helper library within the core module aids in this process, providing functions like ao.send to dispatch messages and ao.spawn to create new modules, as well as the important ao.env property which contains essential Process and Module information. The ao Data-Protocol outlines the structure and attributes of these elements.",
          "estimatedWords": 409,
          "lastModified": "2025-06-27T16:12:41.401Z",
          "siteKey": "ao",
          "siteName": "AO Cookbook",
          "depth": 3,
          "crawledAt": "2025-06-27T16:12:41.401Z"
        },
        {
          "url": "https://cookbook_ao.arweave.net/concepts/messages.html",
          "title": "Messages",
          "content": "Messages ​The Message serves as the fundamental data protocol unit within ao, crafted from ANS-104 DataItems, thereby aligning with the native structure of Arweave. When engaged in a Process, a Message is structured as follows:lua{ Cron = false, Data = \"Hello aos\", Epoch = 0, From = \"5WzR7rJCuqCKEq02WUPhTjwnzllLjGu6SA7qhYpcKRs\", Id = \"ayVo53qvZswpvxLlhMf8xmGjwxN0LGuHzzQpTLT0_do\", Nonce = 1, Owner = \"z1pq2WzmaYnfDwvEFgUZBj48anUsxxN64ZjbWOsIn08\", Signature = \"...\", Tags = { Type = \"Message\", Variant = \"ao.TN.1\", [\"Data-Protocol\"] = \"ao\", [\"From-Module\"] = \"lXfdCypsU3BpYTWvupgTioLoZAEOZL2_Ihcqepz6RiQ\", [\"From-Process\"] = \"5WzR7rJCuqCKEq02WUPhTjwnzllLjGu6SA7qhYpcKRs\" }, Target = \"5WzR7rJCuqCKEq02WUPhTjwnzllLjGu6SA7qhYpcKRs\", Timestamp = 1704936415711, [\"Block-Height\"] = 1340762, [\"Forwarded-By\"] = \"z1pq2WzmaYnfDwvEFgUZBj48anUsxxN64ZjbWOsIn08\", [\"Hash-Chain\"] = \"hJ0B-0yxKxeL3IIfaIIF7Yr6bFLG2vQayaF8G0EpjbY\" }This architecture merges the Assignment Type with the Message Type, granting the Process a comprehensive understanding of the Message's context for effective processing.When sending a message, here is a visual diagram of how the messages travels through the ao computer.The message workflow initiates with the MU (Messenger Unit), where the message's signature is authenticated. Following this, the SU (Scheduler Unit) allocates an Epoch and Nonce to the message, bundles the message with an Assignment Type, and dispatches it to Arweave. Subsequently, the aoconnect library retrieves the outcome from the CU (Compute Unit). The CU then calls for all preceding messages leading up to the current Message Id from the SU (Scheduler Unit), processes them to deduce the result. Upon completion, the computed result is conveyed back to aoconnect, which is integrated within client interfaces such as aos.Ethereum Signed Message ​If the Message ANS-104 DataItem was signed using Ethereum keys, then the value in the Owner and From fields will be the EIP-55 Ethereum address of the signer. For example: 0xfB6916095ca1df60bB79Ce92cE3Ea74c37c5d359.Summary ​Messages serve as the primary data protocol type for the ao network, leveraging ANS-104 Data-Items native to Arweave. Messages contain several fields including data content, origin, target, and cryptographic elements like signatures and nonces. They follow a journey starting at the Messenger Unit (MU), which ensures they are signed, through the Scheduler Unit (SU) that timestamps and sequences them, before being bundled and published to Arweave. The aoconnect library then reads the result from the Compute Unit (CU), which processes messages to calculate results and sends responses back through aoconnect, utilized by clients such as aos. The CU is the execution environment for these processes.",
          "estimatedWords": 369,
          "lastModified": "2025-06-27T16:12:41.689Z",
          "siteKey": "ao",
          "siteName": "AO Cookbook",
          "depth": 3,
          "crawledAt": "2025-06-27T16:12:41.689Z"
        },
        {
          "url": "https://cookbook_ao.arweave.net/concepts/eval.html",
          "title": "Eval",
          "content": "Eval ​Each AO process includes an onboard Eval handler that evaluates any new code it receives. This handler determines the appropriate action for the code and verifies that the message originates from the process owner.The Eval handler can also be manually triggered to evaluate the Data field from an incoming message. When you use the .load function to load a file into a process, it relies on the Eval handler to evaluate the file’s content under the hood.Sending an Eval Message in NodeJS ​jsimport { readFileSync } from \"node:fs\"; import { message, createDataItemSigner } from \"@permaweb/aoconnect\"; const wallet = JSON.parse( readFileSync(\"/path/to/arweave/wallet.json\").toString(), ); await message({ // The arweave TxID of the process, this will become the \"target\". process: \"process-ID\", // Replace with the actual process ID // Tagging the Eval Action so the receiving process evaluates and adds the new Handler from the Data field. tags: [ { name: \"Action\", value: \"Eval\" }, { name: \"Data\", value: 'Handlers.add(\"ping\", Handlers.utils.reply(\"pong\"))', }, ], // A signer function used to build the message \"signature\" signer: createDataItemSigner(wallet), }) .then(console.log) .catch(console.error);Sending an Eval Message in a Browser ​jsimport { message, createDataItemSigner } from \"@permaweb/aoconnect\"; await message({ // The arweave TxID of the process, this will become the \"target\". process: \"process-ID\", // Replace with the actual process ID // Tagging the Eval Action so the receiving process evaluates and adds the new Handler from the Data field. tags: [ { name: \"Action\", value: \"Eval\" }, { name: \"Data\", value: 'Handlers.add(\"ping\", Handlers.utils.reply(\"pong\"))', }, ], // A signer function used to build the message \"signature\" signer: createDataItemSigner(globalThis.arweaveWallet), }) .then(console.log) .catch(console.error);",
          "estimatedWords": 261,
          "lastModified": "2025-06-27T16:12:41.989Z",
          "siteKey": "ao",
          "siteName": "AO Cookbook",
          "depth": 3,
          "crawledAt": "2025-06-27T16:12:41.989Z"
        },
        {
          "url": "https://cookbook_ao.arweave.net/concepts/units.html",
          "title": "Units",
          "content": "Units ​What is a Unit? ​The ao Computer is composed of three Unit types, each type contains a set of responsibilities for the computer. And each Unit is horizontally scalable.In ao we have the Messenger Unit or MU, and the Scheduler Unit or SU, and the Compute Unit or the CU. These units are the building blocks of the ao Computer Grid. There can be 1 or more of these units on the network and they work together to power the ao Operating System or aos.Messenger Unit - This unit is the front door to ao, it receives all the messages from the outside and as well as directs traffic flow for Processes. This traffic flow we call pushing. Each process can return an Outbox when it evaluates a Message, and this Outbox can be filled with Messages or requests to Spawn new processes, and the Messenger Unit is responsible for extracting these Messages from the Outbox and signing them and sending them to the Scheduler Units for processing.Scheduler Unit - The Scheduler unit is responsible for ordering the messages, and storing those messages on Arweave. It is important that every message is appropriately ordered so that the evaluation can be replayed and verified. The Scheduler Unit is responsible for this process. It provides the abilty to query it via an endpoint to get the order of messages for evaluation.Compute Unit - The Compute unit is responsible for compute, this unit loads the binary module and manages the memory of that module, so that the execution of the process is alway running on the most up to date memory. The compute unit provides the results of the evaluation back to the the messenger unit, which can then push any messages in the outbox of the given process.Summary ​The ao Computer consists of three scalable unit types—Messenger Unit (MU), Scheduler Unit (SU), and Compute Unit (CU)—which form the foundation of the ao Computer. These units can exist in multiples on the network and collectively operate the ao Operating System (aos).The MU acts as the entry point, receiving external messages and managing process communications. It processes outgoing messages and spawn requests from process outboxes and forwards them to the SU.The SU ensures messages are properly sequenced and stored on Arweave, maintaining order for consistent replay and verification of message evaluations.The CU handles computation, loading binary modules, and managing memory to ensure processes run with current data. It then returns the evaluation results to the MU for further message handling.",
          "estimatedWords": 417,
          "lastModified": "2025-06-27T16:12:42.302Z",
          "siteKey": "ao",
          "siteName": "AO Cookbook",
          "depth": 3,
          "crawledAt": "2025-06-27T16:12:42.302Z"
        },
        {
          "url": "https://cookbook_ao.arweave.net/concepts/specs.html",
          "title": "ao Specs",
          "content": "ao Specs ​What is ao? ​The ao computer is the actor oriented machine that emerges from the network of nodes that adhere to its core data protocol, running on the Arweave network. This document gives a brief introduction to the protocol and its functionality, as well as its technical details, such that builders can create new implementations and services that integrate with it.The ao computer is a single, unified computing environment (a Single System Image), hosted on a heterogenous set of nodes in a distributed network. ao is designed to offer an environment in which an arbitrary number of parallel processes can be resident, coordinating through an open message passing layer. This message passing standard connects the machine's independently operating processes together into a 'web' -- in the same way that websites operate on independent servers but are conjoined into a cohesive, unified experience via hyperlinks.",
          "estimatedWords": 146,
          "lastModified": "2025-06-27T16:12:42.573Z",
          "siteKey": "ao",
          "siteName": "AO Cookbook",
          "depth": 3,
          "crawledAt": "2025-06-27T16:12:42.573Z"
        },
        {
          "url": "https://cookbook_ao.arweave.net/concepts/tour.html",
          "title": "aos Brief Tour",
          "content": "aos Brief Tour ​Welcome to a quick tour of aos! This tutorial will walk you through the key global functions and variables available in the aos environment, giving you a foundational understanding of how to interact with and utilize aos effectively.1. Introduction to Inbox ​What It Is: Inbox is a Lua table that stores all messages received by your process but not yet handled.How to Use: Check Inbox to see incoming messages. Iterate through Inbox[x] to process these messages.2. Sending Messages with Send(Message) ​Functionality: Send(Message) is a global function to send messages to other processes.Usage Example: Send({Target = \"...\", Data = \"Hello, Process!\"}) sends a message with the data \"Hello, Process!\" to a specified process.3. Creating Processes with Spawn(Module, Message) ​Purpose: Use Spawn(Module, Message) to create new processes.Example: Spawn(\"MyModule\", {Data = \"Start\"}) starts a new process using \"MyModule\" with the provided message.4. Understanding Name and Owner ​Name: A string set during initialization, representing the process's name.Owner: Indicates the owner of the process. Changing this might restrict your ability to interact with your process.Important Note: Treat these as read-only to avoid issues.5. Utilizing Handlers ​What They Are: Handlers is a table of helper functions for creating message handlers.Usage: Define handlers in Handlers to specify actions for different incoming messages based on pattern matching.6. Data Representation with Dump ​Function: Dump converts any Lua table into a print-friendly format.How to Use: Useful for debugging or viewing complex table structures. Example: Dump(Inbox) prints the contents of Inbox.7. Leveraging Utils Module ​Contents: Utils contains a collection of functional utilities likemap, reduce, and filter.Usage: Great for data manipulation and functional programming patterns in Lua. For example, Utils.map(myTable, function(x) return x * 2 end) to double the values in a table.8. Exploring the ao Core Library ​Description: ao is a core module that includes key functions for message handling and process management.Key Features: Includes functions for sending messages (send) and spawning processes (spawn), along with environment variables.Conclusion ​This brief tour introduces you to the primary globals and functionalities within the aos environment. With these tools at your disposal, you can create and manage processes, handle messages, and utilize Lua's capabilities to build efficient and responsive applications on the aos platform. Experiment with these features to get a deeper understanding and to see how they can be integrated into your specific use cases. Happy coding in aos!",
          "estimatedWords": 389,
          "lastModified": "2025-06-27T16:12:42.881Z",
          "siteKey": "ao",
          "siteName": "AO Cookbook",
          "depth": 3,
          "crawledAt": "2025-06-27T16:12:42.881Z"
        },
        {
          "url": "https://cookbook_ao.arweave.net/concepts/lua.html",
          "title": "A whistle stop tour of Lua",
          "content": "A whistle stop tour of Lua. ​Before we can explore ao in greater depth, let's take a moment to learn the basics of Lua: your companion for commanding aos processes.Lua is a simple language with few surprises. If you know Javascript, it will feel like a simplified, purer version. If you are learning from-scratch, it will seem like a tiny language that focuses on the important stuff: Clean computation with sane syntax.In this section we will cover the basics of Lua in just a few minutes. If you already know Lua, jump right through to the next chapterJumping back into your aos process. ​For the purpose of this tutorial, we will be assuming that you have already completed the getting started guide. If not, complete that first.If you logged out of your process, you can always re-open it by running aos on your command line, optionally specifying your key file with --wallet [location].Basic Lua expressions. ​In the remainder of this primer we will quickly run through Lua's core features and syntax.Try out on the examples on your aos process as you go, or skip them if they are intuitive to you.Basic arithmetic: Try some basic arithmetic, like 5 + 3. After processing, you will see the result 8. +, -, *, /, and ^ all work as you might expect. % is the symbol that Lua uses for modulus.Setting variables: Type a = 10 and press enter. This sets the variable a to 10. By convention (not enforced by the language), global variables start with a capital letter in Lua (for example Handlers).Using variables: Now type a * 2. You will see 20 returned on the command line.String concatenation: Say hello to yourself by executing \"Hello, \" .. ao.id.INFONote that while global variables conventionally start with a capital letter in Lua, this is not enforced by the language. For example, the ao module is a global variable that was intentionally lowercased for stylistic purposes.Experimenting with conditional statements. ​If-Else: Like most programming languages, Lua uses if-else blocks to conditionally execute code.In your aos process, type .editor and press enter. This will open an in-line text editor within your command-line interface.luaaos_coolness = 9001 if aos_coolness > 9000 then return \"aos has a coolness level over 9000!\" else return \"Oh. 🤷\" endOnce you are finished editing on your terminal, type .done on a new line and press enter. This will terminate edit mode and submit the expression to your process for evaluation.As a result, you will see that aos coolness is >9,000 cool. Good to know.if statements in Lua can also have additional elseif [condition] then blocks, making conditional execution hierarchies easier.Looping in Lua. ​There are a few different ways to loop in your code in Lua. Here are our favorites:While loops:Start by initializing your counter to zero by typing n = 0 and pressing enter.Then open the inline editor again with .editor .luawhile n < 5 do n = n + 1 endType .done on a new line to execute the while loop. You can check the result of the loop by simply running n.For loops:Lua can also execute python-style for loops between a set of values. For example, use the .editor to enter the following code block:luafor m = 1, 100 do n = n + m endRequest the new value of the variable by running n again.Getting functional. ​Define a function:Using the .editor once again, submit the following lines:luafunction greeting(name) return \"Hello, \" .. name endLua also has 'anonymous' or 'higher order' functions. These essentially allow you to use functions themselves as if they are normal data -- to be passed as arguments to other functions, etc. The following example defines an anonymous function and is equivalent to the above:luagreeting = function(name) return \"Hello, \" .. name endCalling the function: Call the function with greeting(\"Earthling\"). aos will return \"Hello, Earthling\".INFOHandlers in ao commonly utilize anonymous functions. When using Handlers.add(), the third argument is an anonymous function in the form function(msg) ... end. This is a key pattern you'll see frequently when working with ao processes.Defining deep objects with tables. ​Tables are Lua's only compound data structure. They map keys to values, but can also be used like traditional arrays.Create a simple table: Type ao_is = {\"hyper\", \"parallel\", \"compute\"}to create a simple table.Accessing the table's elements: Access an element with ao_is[2]. aos will return parallel. Note: Indices in Lua start from 1!Count a table's elements: The size of a table in Lua is found with the operator #. For example, running #ao_is will return 3.Set a named element: Type ao_is[\"cool\"] = true to add a new named key to the table. Named elements can also be accessed with the . operator (e.g. ao_is.cool), but only if the key is a valid identifier - for other keys like \"my key\", use brackets.Lua Wats. ​aos uses Lua because it is a simple, clean language that most experienced programmers can learn very quickly, and is an increasingly popular first programming language, too, thanks to its use in video games like Roblox.Nonetheless, there are a few things about the language that are prone to trip up rookie Lua builders. Tastes may vary, but here is our exhaustive list of Lua wats:Remember: Table indexing starts from 1 not 0!Remember: 'Not equals' is expressed with ~=, rather than != or similar.Remember: Objects in Lua are called 'tables', rather than their more common names.Let's go! ​With this in mind, you now know everything you need in order to build awesome decentralized processes with Lua! In the next chapter we will begin to build parallel processes with Lua and aos.",
          "estimatedWords": 931,
          "lastModified": "2025-06-27T16:12:43.158Z",
          "siteKey": "ao",
          "siteName": "AO Cookbook",
          "depth": 3,
          "crawledAt": "2025-06-27T16:12:43.158Z"
        },
        {
          "url": "https://cookbook_ao.arweave.net/references/index.html",
          "title": "References",
          "content": "References ​This section provides detailed technical references for AO components, languages, and tools. Use these resources to find specific information when implementing your AO projects.Programming Languages ​Resources for the programming languages used in AO:Lua - Reference for the Lua programming language, the primary language used in AOWebAssembly (WASM) - Information about using WebAssembly modules in AOLua Optimization - Techniques and best practices for optimizing Lua code in AOAO API Reference ​Documentation for AO's core APIs and functionality:AO Core - Core ao module and API referenceMessaging - Comprehensive guide to the AO messaging system patternsHandlers - Reference for event handlers and message processingToken - Information about token creation and managementArweave Data - Guide to data handling and storage in AOCron - Documentation for scheduling and managing timed eventsDevelopment Environment ​Tools and setup for AO development:Editor Setup - Guide to setting up your development environment for AOBetterIDEa - The ultimate native web IDE for AO developmentCommunity Resources ​Connect with the AO community:Community Resources - Information about AO community resources and supportNavigation ​Use the sidebar to navigate between reference topics. References are organized by category to help you find the information you need quickly.",
          "estimatedWords": 191,
          "lastModified": "2025-06-27T16:12:43.727Z",
          "siteKey": "ao",
          "siteName": "AO Cookbook",
          "depth": 3,
          "crawledAt": "2025-06-27T16:12:43.727Z"
        },
        {
          "url": "https://cookbook_ao.arweave.net/references/lua.html",
          "title": "Meet Lua",
          "content": "Meet Lua ​Understanding Lua ​Background: Lua is a lightweight, high-level, multi-paradigm programming language designed primarily for embedded systems and clients. It's known for its efficiency, simplicity, and flexibility.Key Features: Lua offers powerful data description constructs, dynamic typing, efficient memory management, and good support for object-oriented programming.Setting Up ​Installation: Visit Lua's official website to download and install Lua.Environment: You can use a simple text editor and command line, or an IDE like ZeroBrane Studio or Eclipse with a Lua plugin.Basic Syntax and Concepts (in aos) ​Hello World:lua\"Hello, World!\"Variables and Types: Lua is dynamically typed. Basic types include nil, boolean, number, string, function, userdata, thread, and table.Control Structures: Includes if, while, repeat...until, and for.Functions: First-class citizens in Lua, supporting closures and higher-order functions.Tables: The only data structuring mechanism in Lua, which can be used to represent arrays, sets, records, etc.Hands-On Practice ​Experiment with Lua's Interactive Mode: Run aos in your terminal and start experimenting with Lua commands.Write Simple Scripts: Create .lua files and run them using the Lua interpreter. Use .load file.lua feature to upload lua code on your aos process.Resources ​Official Documentation: Lua 5.3 Reference ManualOnline Tutorials: Websites like Learn Lua are great for interactive learning.Books: \"Programming in Lua\" (first edition available online) is a comprehensive resource.Community: Join forums or communities like Lua Users for support and discussions.Best Practices ​Keep It Simple: Lua is designed to be simple and flexible. Embrace this philosophy in your code.Performance: Learn about Lua's garbage collection and efficient use of tables.Integration: Consider how Lua can be embedded into other applications, particularly C/C++ projects.Conclusion ​Lua is a powerful language, especially in the context of embedded systems and game development. Its simplicity and efficiency make it a great choice for specific use cases. Enjoy your journey into Lua programming!",
          "estimatedWords": 291,
          "lastModified": "2025-06-27T16:12:47.632Z",
          "siteKey": "ao",
          "siteName": "AO Cookbook",
          "depth": 3,
          "crawledAt": "2025-06-27T16:12:47.632Z"
        },
        {
          "url": "https://cookbook_ao.arweave.net/references/wasm.html",
          "title": "Meet Web Assembly",
          "content": "Meet Web Assembly ​WebAssembly (often abbreviated as Wasm) is a modern binary instruction format providing a portable compilation target for high-level languages like C, C++, and Rust. It enables deployment on the web for client and server applications, offering a high level of performance and efficiency. WebAssembly is designed to maintain the security and sandboxing features of web browsers, making it a suitable choice for web-based applications. It's a key technology for web developers, allowing them to write code in multiple languages and compile it into bytecode that runs in the browser at near-native speed.The significance of WebAssembly lies in its ability to bridge the gap between web and native applications. It allows complex applications and games, previously limited to desktop environments, to run in the browser with comparable performance. This opens up new possibilities for web development, including the creation of high-performance web apps, games, and even the porting of existing desktop applications to the web. WebAssembly operates alongside JavaScript, complementing it by enabling performance-critical components to be written in languages better suited for such tasks, thereby enhancing the capabilities and performance of web applications.",
          "estimatedWords": 186,
          "lastModified": "2025-06-27T16:12:47.975Z",
          "siteKey": "ao",
          "siteName": "AO Cookbook",
          "depth": 3,
          "crawledAt": "2025-06-27T16:12:47.975Z"
        },
        {
          "url": "https://cookbook_ao.arweave.net/references/lua-optimization.html",
          "title": "Lua Optimization Guide for AO Platform",
          "content": "Lua Optimization Guide for AO Platform ​This guide provides practical tips for writing efficient, fast, and performant Lua code for on-chain programs on the AO platform.Table Operations ​Appending Elements ​lua-- ❌ Inefficient: Up to 7x slower in tight loops table.insert(t, v) -- ✅ Efficient: Direct indexing is ~2x faster t[#t + 1] = vRemoving Elements ​lua-- ❌ Inefficient: Shifts all elements left table.remove(t, 1) -- ✅ Efficient: Remove from end local x = t[#t] t[#t] = nilVariable Access ​Local Variables ​lua-- ❌ Inefficient: Global lookup each time for i = 1, 1000 do math.sin(i) end -- ✅ Efficient: Cache the function local sin = math.sin for i = 1, 1000 do sin(i) -- ~30% faster in loops endUpvalues ​lua-- ❌ Inefficient: Config lookup on each call Handlers.add(\"ValidateGameToken\", function(msg) local config = ao.config validateToken(msg, config) end ) -- ✅ Efficient: Cache config as upvalue local config = ao.config Handlers.add(\"ValidateGameToken\", function(msg) validateToken(msg, config) end )String Operations ​String Concatenation ​lua-- ❌ Inefficient: Creates many intermediate strings local str = \"\" for i = 1, N do str = str .. \"line\" .. i end -- ✅ Efficient: Single concatenation at end local lines = {} for i = 1, N do lines[i] = \"line\" .. i end local str = table.concat(lines)Pattern Matching ​lua-- ❌ Inefficient: Recompiles pattern on each iteration for line in io.lines() do if line:match(\"^%s*(%w+)%s*=%s*(%w+)\") then -- Process match end end -- ✅ Efficient: Compile pattern once local pattern = \"^%s*(%w+)%s*=%s*(%w+)\" for line in io.lines() do if line:match(pattern) then -- Process match end endMemory Management ​Table Reuse ​lua-- ❌ Inefficient: Creates new table on each call Handlers.add(\"ComputeGameResults\", function(msg) local results = {} -- Fill results return results end ) -- ✅ Efficient: Reuse and clear table local results = {} Handlers.add(\"ComputeGameResults\", function(msg) for k in pairs(results) do results[k] = nil end -- Fill results return results end )Minimize Garbage Creation ​lua-- ❌ Inefficient: Creates new response table on every transfer local function createTransferResponse(sender, recipient, amount) return { from = sender, to = recipient, quantity = amount, success = true, newBalance = Balances[sender], tags = { Action = \"Transfer-Complete\", Type = \"Token\" } } end -- ✅ Efficient: Reuse template table local transferResponse = { from = nil, to = nil, quantity = 0, success = false, newBalance = 0, tags = { Action = \"Transfer-Complete\", Type = \"Token\" } } local function createTransferResponse(sender, recipient, amount) transferResponse.from = sender transferResponse.to = recipient transferResponse.quantity = amount transferResponse.success = true transferResponse.newBalance = Balances[sender] return transferResponse endBlockchain-Specific Optimizations ​State Management ​lua-- ❌ Inefficient: Multiple separate state updates for _, item in ipairs(items) do ao.send({ Target = \"processID\", Action = \"Update\", Data = item }) end -- ✅ Efficient: Batch updates into single message local updates = {} for _, item in ipairs(items) do table.insert(updates, item) end ao.send({ Target = \"processID\", Action = \"BatchUpdate\", Data = updates })Additional Resources ​Lua Performance GuideSpecial thanks to @allquantor for sharing optimization tips",
          "estimatedWords": 483,
          "lastModified": "2025-06-27T16:12:48.228Z",
          "siteKey": "ao",
          "siteName": "AO Cookbook",
          "depth": 3,
          "crawledAt": "2025-06-27T16:12:48.228Z"
        },
        {
          "url": "https://cookbook_ao.arweave.net/references/ao.html",
          "title": "ao Module",
          "content": "ao Module ​version: 0.0.3ao process communication is handled by messages, each process receives messages in the form of ANS-104 DataItems, and needs to be able to do the following common operations.ao.send(msg) - send message to another processao.spawn(module, msg) - spawn a processThe goal of this library is to provide this core functionality in the box of the ao developer toolkit. As a developer you have the option to leverage this library or not, but it integrated by default.Properties ​NameDescriptionTypeidProcess Identifier (TxID)string_moduleModule Identifier (TxID)stringauthoritiesSet of Trusted TXsstringAuthorityIdentifiers that the process is able to accept transactions from that are not the owner or the process (0-n)string_versionThe version of the librarystringreferenceReference number of the processnumberenvEvaluation EnvironmentobjectoutboxHolds Messages and Spawns for responseobjectassignablesList of assignables of the processlistnonExtractableTagsList of non-extractable tags of the processlistnonForwardableTagsList of non-forwardable tags of the processlistinitInitializes the AO environmentfunctionsendSends a message to a target processfunctionassignAssigns a message to the processfunctionspawnSpawns a processfunctionresultReturns the result of a messagefunctionisTrustedChecks if a message is trustedfunctionisAssignmentChecks if a message is an assignmentfunctionisAssignableChecks if a message is assignablefunctionaddAssignableAdds an assignable to the assignables listfunctionremoveAssignableRemoves an assignable from the assignables listfunctionclearOutboxClears the outboxfunctionnormalizeNormalizes a message by extracting tagsfunctionsanitizeSanitizes a message by removing non-forwardable tagsfunctioncloneClones a table recursivelyfunctionEnvironment Schema ​The ao.env variable contains information about the initializing message of the process. It follows this schema:luaao.env = { Process = { Id = string, -- Process ID Owner = string, -- Process owner TagArray = { -- Array of name-value pairs { name = string, value = string } }, Tags = { -- Tags as key-value pairs [string] = string } } }Example ​lua{ Process = { Id = \"A1b2C3d4E5f6G7h8I9j0K1L2M3N4O5P6Q7R8S9T0\", Owner = \"Xy9PqW3vR5sT8uB1nM6dK0gF2hL4jC7iE9rV3wX5\", TagArray = { { name = \"App-Name\", value = \"aos\" } }, Tags = { [\"App-Name\"] = \"aos\" } } }Methods ​ao.send(msg: Message) ​Takes a Message as input. The function adds ao-specific tags and stores the message in ao.outbox.Messages.Example ​lualocal message = ao.send({ Target = msg.From, Data = \"ping\", Tags = { [\"Content-Type\"] = \"text/plain\", [\"Action\"] = \"Ping\" } }) -- or local message = ao.send({ Target = msg.From, Data = \"ping\", Action = \"Ping\", -- will be converted to Tags [\"Content-Type\"] = \"text/plain\" -- will be converted to Tags })ao.spawn(module: string, spawn: Spawn) ​Takes a module ID string and Spawn as input. Returns a Spawn table with a generated Ref_ tag.Example ​lualocal process = ao.spawn(\"processId\", { Data = { initial = \"state\" }, Tags = { [\"Process-Type\"] = \"calculator\" } })ao.assign(assignment: Assignment) ​Takes an Assignment as input. Adds the assignment to ao.outbox.Assignments.Example ​luaao.assign({ Processes = {\"process-1\", \"process-2\"}, Message = \"sample-message-id\" })ao.result(result: Result) ​Takes a Result as input. Returns the final process execution result.Example ​lualocal process_result = ao.result({ Output = \"Process completed successfully\", Messages = { { Target = \"ProcessY\", Data = \"Result data\", Tags = { [\"Status\"] = \"Success\" } } }, Spawns = { \"spawned-process-1\" }, Assignments = { {Processes = { \"process-1\" }, Message = \"assignment-message-id\"} } })ao.isAssignable(msg: Message) ​Takes a Message as input. Returns true if the message matches a pattern in ao.assignables.Example ​lualocal can_be_assigned = ao.isAssignable({ Target = \"ProcessA\", Data = \"Some content\", Tags = { [\"Category\"] = \"Info\" } })ao.isAssignment(msg: Message) ​Takes a Message as input. Returns true if the message is assigned to a different process.Example ​lualocal is_assigned_elsewhere = ao.isAssignment({ Target = \"AnotherProcess\" })ao.addAssignable(name: string, condition: function) ​Adds a named condition function to the process's list of assignables. Messages matching any condition will be accepted when assigned.Note: The condition parameter uses a similar pattern matching approach as the pattern parameter in Handlers.add(). For more advanced pattern matching techniques, see the Handlers Pattern Matching documentation.Example ​lua-- Allow transactions from ArDrive ao.addAssignable(\"allowArDrive\", function (msg) return msg.Tags[\"App-Name\"] == \"ArDrive-App\" end) -- Allow transactions with specific content type ao.addAssignable(\"allowJson\", function (msg) return msg.Tags[\"Content-Type\"] == \"application/json\" end)ao.removeAssignable(name: string) ​Removes a previously added assignable condition from the process's list of assignables.Example ​luaao.removeAssignable(\"allowArDrive\")ao.isTrusted(msg: Message) ​Takes a Message as input. Returns true if the message is from a trusted source.Example ​luaif ao.isTrusted(msg) then -- Process trusted message else -- Handle untrusted message endCustom ao Table Structures ​Tags ​Used by: ao.send(), ao.spawn(), ao.normalize(), ao.sanitize()All of the below syntaxes are valid, but each syntax gets converted to { name = string, value = string } tables behind the scenes. We use alternative 1 throughout the documentation for brevity and consistency.lua-- Default: Array of name-value pair tables Tags = { { name = \"Content-Type\", value = \"text/plain\" }, { name = \"Action\", value = \"Ping\" } } -- Alternative 1: Direct key-value pairs in Tags table using string keys Tags = { [\"Content-Type\"] = \"text/plain\", [\"Action\"] = \"Ping\" } -- Alternative 2: Direct key-value pairs in Tags table using dot notation Tags = { Category = \"Info\", Action = \"Ping\" }Root-level Tag ConversionAny keys in the root message object that are not one of: Target, Data, Anchor, Tags, or From will automatically be converted into Tags using the key as the tag name and its value as the tag value.lua-- These root-level keys will be automatically converted to Tags { Target = \"process-id\", Data = \"Hello\", [\"Content-Type\"] = \"text/plain\", -- Will become a Tag Action = \"Ping\" -- Will become a Tag }Message ​Used by: ao.send(), ao.isTrusted(), ao.isAssignment(), ao.isAssignable(), ao.normalize(), ao.sanitize()lua-- Message structure { Target = string, -- Required: Process/wallet address Data = any, -- Required: Message payload Tags = Tag<table> }Spawn ​Used by: ao.spawn()lua-- Spawn structure { Data = any, -- Required: Initial process state Tags = Tag<table> -- Required: Process tags }Assignment ​Used by: ao.assign(), ao.result()lua-- Assignment configuration table structure { Processes = { string }, -- Required: List of target process ID strings Message = string -- Required: Message to assign }Result ​Used by: ao.result()lua-- Process result structure { Output = string, -- Optional: Process output Messages = Message<table>, -- Optional: Generated messages Spawns = Spawn<table>, -- Optional: Spawned processes Assignments = Assignment<table>, -- Optional: Process assignments Error = string -- Optional: Error information }",
          "estimatedWords": 979,
          "lastModified": "2025-06-27T16:12:48.576Z",
          "siteKey": "ao",
          "siteName": "AO Cookbook",
          "depth": 3,
          "crawledAt": "2025-06-27T16:12:48.576Z"
        },
        {
          "url": "https://cookbook_ao.arweave.net/references/messaging.html",
          "title": "Messaging Patterns in ao",
          "content": "Messaging Patterns in ao ​This reference guide explains the messaging patterns available in ao and when to use each one.Quick Reference: Choosing the Right Pattern ​If you need to...Process FlowKey function(s)Send a message without waiting for a responseA → Bao.sendSend a message and wait for a responseA → B → Aao.send().receive()Process messages and respond to the senderB → AHandlers.add + msg.replyCreate a chain of processing servicesA → B → C → Amsg.forward + ao.send().receive()Wait for any matching message regardless of senderAny → AReceive (capital R)Create a standard automated responseB → AHandlers.utils.replySending Messages ​ao.send: Asynchronous Message Sending ​Non-blocking direct A → B messaging that returns immediately after sending.Use for fire-and-forget notifications or starting async conversationsReturns a promise-like object that can be chained with .receive() if neededGood for parallel processing since it doesn't block executionClient (A) → Service (B) ↓ ↓ Continues Processes execution messageBasic Send Example:lua-- Non-blocking send example local serviceId = \"process-789\" -- Process ID of the target service ao.send({ Target = serviceId, Data = \"Hello!\", Action = \"Greeting\" }) -- Code here runs immediately after sendingmsg.reply: Asynchronous Response Sending ​Non-blocking B → A response with automatic reference tracking. Used within handlers to respond to incoming messages.Automatically links response to original message via X-ReferenceEnables asynchronous request-response patternsAutomatically sets Target to the original sender or Reply-To address if specifiedClient (A) → Service (B) ← Response tagged with X-ReferenceHandler Reply Example:lua-- Non-blocking reply in a handler Handlers.add(\"greeting-handler\", { Action = \"Greeting\" }, function(msg) msg.reply({ Data = \"Hi back!\" }) -- Returns immediately -- Handler continues executing here end )msg.forward: Message Forwarding ​Non-blocking multi-process routing for A → B → C → A patterns. Creates a sanitized copy of the original message.Takes a target and a partial message to overwrite forwarded message fieldsPreserves Reply-To and X-Reference properties for complete message trackingSets X-Origin to original sender, enabling final service to reply directly to originatorClient (A) → Service (B) → Backend (C) ↖ ↙ Response with X-ReferenceMulti-Process Pipeline Example:lua-- In client process local middlewareProcessId = \"process-123\" local finalProcessId = \"process-456\" -- Send to middleware and wait for response from final service local response = ao.send({ Target = middlewareProcessId, Action = \"Transform\", Data = \"raw-data\" }).receive(finalProcessId) -- Explicitly wait for response from final service -- In middleware service Handlers.add(\"transform-middleware\", { Action = \"Transform\" }, function(msg) local finalProcessId = \"process-456\" msg.forward(finalProcessId, { Data = msg.Data .. \" (pre-processed)\", Action = \"Transform-Processed\" }) end ) -- In final service Handlers.add(\"final-processor\", { Action = \"Transform-Processed\" }, function(msg) -- No need to know the client ID - it's stored in X-Origin msg.forward(msg['X-Origin'], { Data = msg.Data .. \" (final processing complete)\", Action = \"Transform-Complete\" }) end )Handlers.utils.reply: Simple Reply Handler Creation ​Creates a handler function that automatically replies with a fixed response. A wrapper around msg.reply for common use cases.Simple String Response Example:lua-- Simple string response handler Handlers.add(\"echo-handler\", { Action = \"Echo\" }, Handlers.utils.reply(\"Echo reply!\") ) -- Equivalent to: Handlers.add(\"echo-handler\", { Action = \"Echo\" }, function(msg) msg.reply({ Data = \"Echo reply!\" }) end )Message Table Response Example:lua-- Message table response handler Handlers.add(\"status-handler\", { Action = \"Status\" }, Handlers.utils.reply({ Data = \"OK\", Action = \"Status-Response\" }) )Receiving Messages ​Receive (Capital R): Blocking Pattern Matcher ​Blocks execution until any matching message arrives from any sender. Under the hood, this is implemented using Handlers.once, making it a one-time pattern matcher that automatically removes itself after execution.Waits for any message matching the pattern, regardless of originUse for synchronous message processing flows or event listeningAutomatically removes the handler after first match (using Handlers.once internally) Process (A) ↓ Blocks until match received ↓ Continues executionMessage Pattern Matching Example:lua-- Blocks until matching message received local msg = Receive({ Action = \"Update\" }) if msg then -- Process message endao.send().receive (Lowercase r): Blocking Reference Matcher ​Blocks execution until a specific reply arrives, enabling A → B → A and A → B → C → A request-response cycles.Only matches messages linked by X-ReferenceCan specify a target process ID to indicate which process will replyImplicitly waits for the proper response based on message reference chainsFor A → B → A flows, process B uses msg.replyFor A → B → C → A flows, processes B and C use msg.forwardBasic Request-Response Example:lua-- Basic usage: wait for reply from target local serviceId = \"process-789\" local reply = ao.send({ Target = serviceId, Action = \"Query\", Data = { query: \"select\" } }).receive() -- Blocks until response receivedMessage Properties ​The following properties track message chains and ensure proper routing:Reference: Unique identifier automatically assigned to each message.Reply-To: Specifies the destination for responses.X-: Any property starting with X- denotes a 'forwarded' tag and is automatically managed by the system. X-Reference: Maintains the conversation chain across replies and forwards.X-Origin: Tracks the conversation originator.The system automatically manages these properties when using msg.reply and msg.forward. Check out the source code to see exactly how these properties are managed.Blocking vs. Non-Blocking ​Functions either pause your code or let it continue running:Non-blocking (ao.send, msg.reply, msg.forward): Send and continue executionBlocking (Receive, .receive()): Pause until response arrives",
          "estimatedWords": 825,
          "lastModified": "2025-06-27T16:12:48.832Z",
          "siteKey": "ao",
          "siteName": "AO Cookbook",
          "depth": 3,
          "crawledAt": "2025-06-27T16:12:48.832Z"
        },
        {
          "url": "https://cookbook_ao.arweave.net/references/handlers.html",
          "title": "Handlers (Version 005)",
          "content": "Handlers (Version 0.0.5) ​Overview ​The Handlers library provides a flexible way to manage and execute a series of process functions based on pattern matching. An AO process responds based on receiving Messages, these messages are defined using the Arweave DataItem specification which consists of Tags, and Data. Using the Handlers library, you can define a pipeline of process evaluation based on the attributes of the AO Message. Each Handler is instantiated with a name, a pattern matching function, and a function to execute on the incoming message. This library is suitable for scenarios where different actions need to be taken based on varying input criteria.Concepts ​Handler Arguments Overview ​When adding a handler using Handlers.add(), you provide three main arguments:name (string): The identifier for your handlerpattern (table or function): Defines how to match incoming messageshandler (function or resolver table): Defines what to do with matched messagesPattern Matching Tables ​Pattern Matching Tables provide a declarative way to match incoming messages based on their attributes. This is used as the second argument in Handlers.add() to specify which messages your handler should process.Basic Pattern Matching Rules ​Simple Tag Matchinglua{ \"Action\" = \"Do-Something\" } -- Match messages that have an exact Action tag valueWildcard Matchinglua{ \"Recipient\" = '_' } -- Match messages with any Recipient tag valuePattern Matchinglua{ \"Quantity\" = \"%d+\" } -- Match using Lua string patterns (similar to regex)Function-based Matchinglua{ \"Quantity\" = function(v) return tonumber(v) ~= nil end } -- Custom validation functionCommon Pattern Examples ​Balance Action Handlerlua{ Action = \"Balance\" } -- Match messages with Action = \"Balance\"Numeric Quantity Handlerlua{ Quantity = \"%d+\" } -- Match messages where Quantity is a numberDefault Action Handlers (AOS 2.0+) ​AOS 2.0 introduces simplified syntax for Action-based handlers. Instead of writing explicit pattern functions, you can use these shorthand forms:lua-- Traditional syntax Handlers.add(\"Get-Balance\", function (msg) return msg.Action == \"Balance\", doBalance) -- Simplified syntax options: Handlers.add(\"Balance\", \"Balance\", doBalance) -- Explicit action matching Handlers.add(\"Balance\", doBalance) -- Implicit action matchingResolvers ​Resolvers are special tables that can be used as the third argument in Handlers.add() to enable conditional execution of functions based on additional pattern matching. Each key in a resolver table is a pattern matching table, and its corresponding value is a function that executes when that pattern matches.luaHandlers.add(\"Update\", { [{ Status = \"Ready\" }] = function (msg) print(\"Ready\") end, [{ Status = \"Pending\" }] = function (msg) print(\"Pending\") end, [{ Status = \"Failed\" }] = function (msg) print(\"Failed\") end } )This structure allows developers to create switch/case-like statements where different functions are triggered based on which pattern matches the incoming message. Resolvers are particularly useful when you need to handle a group of related messages differently based on additional criteria.Module Structure ​Handlers._version: String representing the version of the Handlers library.Handlers.list: Table storing the list of registered handlers.Common Handler Function Parameters ​ParameterTypeDescriptionnamestringThe identifier of the handler item in the handlers list.patterntable or functionSpecifies how to match messages. As a table, defines required message tags with string values (e.g. { Action = \"Balance\", Recipient = \"_\" } requires an \"Action\" tag with string value \"Balance\" and any string \"Recipient\" tag value). As a function, takes a message DataItem and returns: \"true\" (invoke handler and exit pipeline), \"false\" (skip handler), or \"continue\" (invoke handler and continue pipeline).handler(Resolver) table or functionEither a resolver table containing pattern-function pairs for conditional execution, or a single function that processes the message. When using a resolver table, each key is a pattern matching table and its value is the function to execute when that pattern matches. When using a function, it takes the message DataItem as an argument and executes business logic.maxRuns (optional)numberAs of 0.0.5, each handler function takes an optional function to define the amount of times the handler should match before it is removed. The default is infinity.Functions ​Handlers.add(name, pattern, handler) ​Adds a new handler or updates an existing handler by nameHandlers.append(name, pattern, handler) ​Appends a new handler to the end of the handlers list.Handlers.once(name, pattern, handler) ​Only runs once when the pattern is matched. Equivalent to setting maxRuns = 1. This is the underlying implementation used by the Receive function in the messaging system.Handlers.prepend(name, pattern, handler) ​Prepends a new handler to the beginning of the handlers list.Handlers.before(handleName) ​Returns an object that allows adding a new handler before a specified handler.Handlers.after(handleName) ​Returns an object that allows adding a new handler after a specified handler.Handlers.remove(name) ​Removes a handler from the handlers list by name.Handler Execution Notes ​Execution Order ​Handlers are executed in the order they appear in Handlers.list.When a message arrives, each handler's pattern function is called sequentially to determine if it should process the message.Pattern Function Return Values ​Pattern functions determine the message handling flow based on their return values:Skip Handler (No Match)Return: 0, false, or any string except \"continue\" or \"break\"Effect: Skips current handler and proceeds to the next one in the listHandle and ContinueReturn: 1 or \"continue\"Effect: Processes the message and continues checking subsequent handlersUse Case: Ideal for handlers that should always execute (e.g., logging)Handle and StopReturn: -1, true, or \"break\"Effect: Processes the message and stops checking further handlersUse Case: Most common scenario where a handler exclusively handles its matched messagePractical Examples ​Logging Handler: Place at the start of the list and return \"continue\" to log all messages while allowing other handlers to process them.Specific Message Handler: Return \"break\" to handle matched messages exclusively and prevent further processing by other handlers.Handlers.utils ​The Handlers.utils module provides two functions that are common matching patterns and one function that is a common handle function.hasMatchingData(data: string)hasMatchingTag(name: string, value: string)reply(text: string)Handlers.utils.hasMatchingData(data: string) ​This helper function returns a pattern matching function that takes a message as input. The returned function checks if the message's Data field contains the specified string. You can use this helper directly as the pattern argument when adding a new handler.luaHandlers.add(\"ping\", Handlers.utils.hasMatchingData(\"ping\"), ... )Handlers.utils.hasMatchingTag(name: string, value: string) ​This helper function returns a pattern matching function that takes a message as input. The returned function checks if the message has a tag with the specified name and value. If they match exactly, the pattern returns true and the handler function will be invoked. This helper can be used directly as the pattern argument when adding a new handler.luaHandlers.add(\"ping\", Handlers.utils.hasMatchingTag(\"Action\", \"Ping\"), ... )Handlers.utils.reply(text: string) ​This helper is a simple handle function, it basically places the text value in to the Data property of the outbound message.luaHandlers.add(\"ping\", Handlers.utils.hasMatchingData(\"ping\"), Handlers.utils.reply(\"pong\") )Example Handlers ​Pattern Matching Table ​luaHandlers.add(\"Ping\", -- Name of the handler { Action = \"Ping\" }, -- Matches messages with Action = \"Ping\" tag function(msg) -- Business logic to execute on Message print(\"ping\") msg.reply({ Data = \"pong\" }) end )Resolver Table Handler ​luaHandlers.add(\"Foobarbaz\", -- Name of the handler { Action = \"Speak\" }, -- Matches messages with Action = \"Speak\" tag { -- Resolver with pattern-function pairs [{ Status = \"foo\" }] = function(msg) print(\"foo\") end, [{ Status = \"bar\" }] = function(msg) print(\"bar\") end, [{ Status = \"baz\" }] = function(msg) print(\"baz\") end } )Function-Based Pattern Matching & Handler ​luaHandlers.add(\"Example\", -- Name of the handler function(msg) -- Pattern function matches messages with Action = \"Speak\" tag return msg.Action == \"Speak\" end, function(msg) -- Handler function that executes business logic print(msg.Status) end )",
          "estimatedWords": 1185,
          "lastModified": "2025-06-27T16:12:49.171Z",
          "siteKey": "ao",
          "siteName": "AO Cookbook",
          "depth": 3,
          "crawledAt": "2025-06-27T16:12:49.171Z"
        },
        {
          "url": "https://cookbook_ao.arweave.net/references/token.html",
          "title": "ao Token and Subledger Specification",
          "content": "ao Token and Subledger Specification ​Status: DRAFT-1 Targeting Network: ao.TN.1This specification describes the necessary message handlers and functionality required for a standard ao token process. Implementations of this standard typically offer users the ability to control a transferrable asset, whose scarcity is maintained by the process.Each compliant process will likely implement a ledger of balances in order to encode ownership of the asset that the process represents. Compliant processes have a set of methods that allow for the modification of this ledger, typically with safe-guards to ensure the scarcity of ownership of the token represented by the process.Additionally, this specification describes a 'subledger' process type which, when implemented, offers the ability to split move a number of the tokens from the parent into a child process that implements the same token interface specification. If the From-Module of the subledger process is trusted by the participants, these subledgers can be used to transact in the 'source' token, without directly exchanging messages with it. This allows participants to use the tokens from a process, even if that process is congested. Optionally, if the participants trust the Module a subledger process is running, they are able to treat balances across these processes as fungible. The result of this is that an arbitrary numbers of parallel processes -- and thus, transactions -- can be processed by a single token at any one time.Token Processes ​A specification-compliant token process responds to a number of different forms of messages, with each form specified in an Action tag. The full set of Action messages that the token must support are as follows:NameDescriptionRead-OnlyBalanceget the balance of an identifier✔️Balancesget a list of all ledger/account balances✔️Transfersend 1 or more units from the callers balance to one or move targets with the option to notify targets❌Mintif the ledger process is the root and you would like to increase token supply❌In the remainder of this section the tags necessary to spawn a compliant token process, along with the form of each of the Action messages and their results is described.Spawning Parameters ​Every compliant token process must carry the following immutable parameters upon its spawning message:TagDescriptionOptional?NameThe title of the token, as it should be displayed to users.✔️TickerA suggested shortened name for the token, such that it can be referenced quickly.✔️LogoAn image that applications may desire to show next to the token, in order to make it quickly visually identifiable.✔️DenominationThe number of the token that should be treated as a single unit when quantities and balances are displayed to users.❌Messaging Protocol ​Balance(Target? : string) ​Returns the balance of a target, if a target is not supplied then the balance of the sender of the message must be returned.Example Action message:luaao.send({ Target = \"{TokenProcess Identifier}\", Tags = { [\"Action\"] = \"Balance\", [\"Target\"] = \"{IDENTIFIER}\" } })Example response message:lua{ Tags = { [\"Balance\"] = \"50\", [\"Target\"] = \"LcldyO8wwiGDzC3iXzGofdO8JdR4S1_2A6Qtz-o33-0\", [\"Ticker\"] = \"FUN\" } }Balances() ​Returns the balance of all participants in the token.luaao.send({ Target = \"[TokenProcess Identifier]\", Tags = { [\"Action\"] = \"Balances\", [\"Limit\"] = 1000, # TODO: Is this necessary if the user is paying for the compute and response? [\"Cursor\"] = \"BalanceIdentifier\" } })Example response message:lua{ Data = { \"MV8B3MAKTsUOqyCzQ0Tsa2AR3TiWTBU1Dx0xM4MO-f4\": 100, \"LcldyO8wwiGDzC3iXzGofdO8JdR4S1_2A6Qtz-o33-0\": 50 } }Transfer(Target, Quantity) ​If the sender has a sufficient balance, send the Quantity to the Target, issuing a Credit-Notice to the recipient and a Debit-Notice to the sender. The Credit- and Debit-Notice should forward any and all tags from the original Transfer message with the X- prefix. If the sender has an insufficient balance, fail and notify the sender.luaao.send({ Target = \"[TokenProcess Identifier]\", Tags = { [\"Action\"] = \"Transfer\", [\"Recipient\"] = \"[ADDRESS]\", [\"Quantity\"] = \"100\", [\"X-[Forwarded Tag(s) Name]\"] = \"[VALUE]\" } })If a successful transfer occurs a notification message should be sent if Cast is not set.luaao.send({ Target = \"[Recipient Address]\", Tags = { [\"Action\"] = \"Credit-Notice\", [\"Sender\"] = \"[ADDRESS]\", [\"Quantity\"] = \"100\", [\"X-[Forwarded Tag(s) Name]\"] = \"[VALUE]\" } })Recipients will infer from the From-Process tag of the message which tokens they have received.Get-Info() ​luaao.send({ Target = \"{Token}\", Tags = { [\"Action\"] = \"Info\" } })Mint() [optional] ​Implementing a Mint action gives the process a way of allowing valid participants to create new tokens.luaao.send({ Target =\"{Token Process}\", Tags = { [\"Action\"] = \"Mint\", [\"Quantity\"] = \"1000\" } })Subledger Processes ​In order to function appropriately, subledgers must implement the full messaging protocol of token contracts (excluding the Mint action). Subledgers must also implement additional features and spawn parameters for their processes. These modifications are described in the following section.Spawning Parameters ​Every compliant subledger process must carry the following immutable parameters upon its spawning message:TagDescriptionOptional?Source-TokenThe ID of the top-most process that this subledger represents.❌Parent-TokenThe ID of the parent process that this subledger is attached to.❌Credit-Notice Handler ​Upon receipt of a Credit-Notice message, a compliant subledger process must check if the process in question is the Parent-Token. If it is, the subledger must increase the balance of the Sender by the specified quantity.Transfer(Target, Quantity) ​In addition to the normal tags that are passed in the Credit-Notice message to the recipient of tokens, a compliant subledger process must also provide both of the Source-Token and Parent-Token values. This allows the recipient of the Transfer message -- if they trust the Module of the subledger process -- to credit a receipt that is analogous (fungible with) deposits from the Source-Token.The modified Credit-Notice should be structured as follows:luaao.send({ Target = \"[Recipient Address]\", Tags = { [\"Action\"] = \"Credit-Notice\", [\"Quantity\"] = \"100\", [\"Source-Token\"] = \"[ADDRESS]\", [\"Parent-Token\"] = \"[ADDRESS]\", [\"X-[Forwarded Tag(s) Name]\"] = \"[VALUE]\" } })Withdraw(Target?, Quantity) ​All subledgers must allow balance holders to withdraw their tokens to the parent ledger. Upon receipt of an Action: Withdraw message, the subledger must send an Action message to its Parent-Ledger, transferring the requested tokens to the caller's address, while debiting their account locally. This transfer will result in a Credit-Notice from the Parent-Ledger for the caller.luaao.send({ Target = \"[TokenProcess Identifier]\", Tags = { [\"Action\"] = \"Withdraw\", [\"Recipient\"] = \"[ADDRESS]\", [\"Quantity\"] = \"100\" } })Token Example ​NOTE: When implementing a token it is important to remember that all Tags on a message MUST be \"string\"s. Using thetostring function you can convert simple types to strings.luaif not balances then balances = { [ao.id] = 100000000000000 } end if name ~= \"Fun Coin\" then name = \"Fun Coin\" end if ticker ~= \"Fun\" then ticker = \"fun\" end if denomination ~= 6 then denomination = 6 end -- handlers that handler incoming msg Handlers.add( \"Transfer\", Handlers.utils.hasMatchingTag(\"Action\", \"Transfer\"), function (msg) assert(type(msg.Tags.Recipient) == 'string', 'Recipient is required!') assert(type(msg.Tags.Quantity) == 'string', 'Quantity is required!') if not balances[msg.From] then balances[msg.From] = 0 end if not balances[msg.Tags.Recipient] then balances[msg.Tags.Recipient] = 0 end local qty = tonumber(msg.Tags.Quantity) assert(type(qty) == 'number', 'qty must be number') -- handlers.utils.reply(\"Transferring qty\")(msg) if balances[msg.From] >= qty then balances[msg.From] = balances[msg.From] - qty balances[msg.Tags.Recipient] = balances[msg.Tags.Recipient] + qty ao.send({ Target = msg.From, Tags = { [\"Action\"] = \"Debit-Notice\", [\"Quantity\"] = tostring(qty) } }) ao.send({ Target = msg.Tags.Recipient, Tags = { [\"Action\"] = \"Credit-Notice\", [\"Quantity\"] = tostring(qty) } }) -- if msg.Tags.Cast and msg.Tags.Cast == \"true\" then -- return -- end end end ) Handlers.add( \"Balance\", Handlers.utils.hasMatchingTag(\"Action\", \"Balance\"), function (msg) assert(type(msg.Tags.Target) == \"string\", \"Target Tag is required!\") local bal = \"0\" if balances[msg.Tags.Target] then bal = tostring(balances[msg.Tags.Target]) end ao.send({ Target = msg.From, Tags = { [\"Balance\"] = bal, [\"Ticker\"] = ticker or \"\" } }) end ) local json = require(\"json\") Handlers.add( \"Balances\", Handlers.utils.hasMatchingTag(\"Action\", \"Balances\"), function (msg) ao.send({ Target = msg.From, Data = json.encode(balances) }) end ) Handlers.add( \"Info\", Handlers.utils.hasMatchingTag(\"Action\", \"Info\"), function (msg) ao.send({ Target = msg.From, Tags = { [\"Name\"] = name, [\"Ticker\"] = ticker, [\"Denomination\"] = tostring(denomination) } }) end )",
          "estimatedWords": 1265,
          "lastModified": "2025-06-27T16:12:49.429Z",
          "siteKey": "ao",
          "siteName": "AO Cookbook",
          "depth": 3,
          "crawledAt": "2025-06-27T16:12:49.429Z"
        },
        {
          "url": "https://cookbook_ao.arweave.net/references/data.html",
          "title": "Accessing Data from Arweave with ao",
          "content": "Accessing Data from Arweave with ao ​There may be times in your ao development workflow that you want to access data from Arweave. With ao, your process can send an assignment instructing the network to provide that data to your Process.Defining Acceptable Transactions (Required First Step) ​Before you can assign any Arweave transaction to your process, you must first define which transactions your process will accept using ao.addAssignable. This function creates conditions that determine which Arweave transactions your process will accept.lua-- Allow transactions from ArDrive ao.addAssignable(\"allowArDrive\", function (msg) return msg.Tags[\"App-Name\"] == \"ArDrive-App\" end) -- Allow specific content types ao.addAssignable(\"allowImages\", function (msg) return msg.Tags[\"Content-Type\"] and string.match(msg.Tags[\"Content-Type\"], \"^image/\") end)Warning: If you attempt to assign a transaction without first defining a matching assignable pattern, that transaction will be permanently blacklisted and can never be assigned to your process, even if you later add a matching assignable.You can remove assignables with ao.removeAssignable(\"<name>\").The condition functions use similar pattern matching techniques as found in the Handlers documentation. For complete details on the ao.addAssignable function, including parameter descriptions and additional examples, see the ao Module Reference.Assignment Methods ​After defining acceptable transactions and setting up your listener (if needed), you can request Arweave data in one of two ways:Using Assign ​The primary method to request data from Arweave:luaAssign({ Processes = { ao.id }, Message = '<arweave-transaction-id>' })Using Send with Assignments ​Alternatively, you can use the Send function with an Assignments parameter:luaSend({ Target = ao.id, Data = 'Hello World', Assignments = { '<process-id-1>', '<process-id-2>' } })Working with Assigned Data ​You can process assigned data using either Receive or Handlers:Using Receive Directly ​lua-- Listen for messages from ArDrive ArweaveData = Receive(function(msg) return msg.Tags[\"App-Name\"] == \"ArDrive-App\" end) -- Process the data when received if ArweaveData then print(ArweaveData.Tags[\"App-Name\"]) -- Raw Arweave Data is available in ArweaveData.Data endYou can also match specific transactions or combine conditions:lua-- Match a specific transaction ID ArweaveData = Receive({ Id = \"<arweave-transaction-id>\" }) -- Or combine multiple conditions ArweaveData = Receive(function(msg) return msg.Tags[\"App-Name\"] == \"ArDrive-CLI\" and msg.Tags[\"Content-Type\"] == \"image/png\" end)Note: When using .load, the script pauses at Receive until data arrives. When running commands separately in the shell, each command executes independently.Using Handlers ​For persistent processing, set up a handler:luaHandlers.add(\"ProcessArDriveFiles\", { Tags = { [\"App-Name\"] = \"ArDrive-App\" } }, function(msg) print(msg.Tags[\"App-Name\"]) -- Raw Arweave Data is available in msg.Data end )Handlers are ideal for:Processing multiple assignments over timeAutomated processing without manual interventionBuilding services that other processes can interact withFor more details, see the Messaging Patterns and Handlers documentation.Complete Example Workflow ​Here's a complete example that demonstrates the entire process of accessing data from an Arweave transaction:lua-- Step 1: Define which transactions your process will accept ao.addAssignable(\"allowArDrive\", function (msg) return msg.Tags[\"App-Name\"] == \"ArDrive-App\" end) -- Step 2: Request the data Assign({ Processes = { ao.id }, Message = '<arweave-transaction-id>' }) -- Step 3: Immediately capture the Assignment; blocking until received ArweaveData = Receive(function(msg) return msg.Tags[\"App-Name\"] == \"ArDrive-App\" end) print(ArweaveData.Tags[\"App-Name\"]) -- e.g., \"ArDrive-CLI\" -- Raw Arweave Data is available in ArweaveData.DataThis pattern creates a synchronous flow where your process:Defines acceptable transactionsRequests the dataCaptures the data using ReceiveProcesses the dataPractical Examples ​Here are two practical examples showing different approaches to working with Arweave data in your ao process:Example 1: Caching Arweave Data ​This example demonstrates how to load and cache data from Arweave, then use it in subsequent operations:lua-- Initialize state local Number = 0 -- Step 1: Define which transactions your process will accept print(\"Step 1: Defining acceptable transactions\") ao.addAssignable(\"addNumber\", function (msg) return msg.Tags[\"Action\"] == \"Number\" end) -- Step 2: Request and cache the initial number from Arweave -- This uses a self-executing function to fetch and cache the value only once NumberFromArweave = NumberFromArweave or (function() print(\"Step 2: Requesting initial number from Arweave\") Assign({ Processes = { ao.id }, Message = 'DivdWHaNj8mJhQQCdatt52rt4QvceBR_iyX58aZctZQ' }) return tonumber(Receive({ Action = \"Number\"}).Data) end)() -- Step 3: Set up handler for future number updates -- This handler will add new numbers to our cached Arweave number Handlers.add(\"Number\", function (msg) print(\"Received message with Data = \" .. msg.Data) print(\"Old Number: \" .. Number) Number = NumberFromArweave + tonumber(msg.Data) print(\"New Number: \" .. Number) end)This example shows how to:Cache Arweave data using a self-executing functionUse the cached data in subsequent message handlingCombine Arweave data with new incoming dataExample 2: Dynamic Transaction Processing ​This example shows how to process arbitrary Arweave transactions and maintain state between requests:lua-- Table to store pending requests (maps transaction ID to original sender) local PendingRequests = {} -- Step 1: Define which transactions your process will accept print(\"Step 1: Defining acceptable transactions\") ao.addAssignable(\"processArweaveNumber\", function (msg) return msg.Tags[\"Action\"] == \"Number\" end) -- Step 2: Set up handler for initiating the processing Handlers.add( \"ProcessArweaveNumber\", function (msg) if not msg.Tags[\"ArweaveTx\"] then print(\"Error: No ArweaveTx tag provided\") return end local txId = msg.Tags[\"ArweaveTx\"] print(\"Assigning Arweave transaction: \" .. txId) -- Store the original sender associated with this transaction ID PendingRequests[txId] = msg.From -- Assign the transaction to this process Assign({ Processes = { ao.id }, Message = txId }) print(\"Assignment requested; waiting for data...\") end ) -- Step 3: Set up handler for processing the assigned message Handlers.add( \"Number\", function (msg) local txId = msg.Id -- The ID of the assigned message local originalSender = PendingRequests[txId] if not originalSender then print(\"Error: No pending request found for transaction \" .. txId) return end local data = msg.Data if not data or not tonumber(data) then print(\"Error: Invalid number data in assigned message\") return end local number = tonumber(data) local result = number + 1 print(string.format(\"Processing: %d + 1 = %d\", number, result)) -- Send the result back to the original sender Send({ Target = originalSender, Data = tostring(result) }) -- Clean up the pending request PendingRequests[txId] = nil end )To use this example:luaSend({ Target = ao.id, Action = \"ProcessArweaveNumber\", Tags = { ArweaveTx = \"YOUR-ARWEAVE-TX-ID\" -- ID of a transaction containing a number } })This example demonstrates:Processing arbitrary Arweave transactionsMaintaining state between requests using a pending requests tableSending results back to the original requesterError handling and request cleanupWARNINGWhen using Assign to bridge Arweave data into AO, you must ensure that:The Arweave transaction you're assigning matches one of your defined assignablesYou have a corresponding handler or receiver set up to process that transaction typeThe handler's pattern matching matches the assigned transaction's tags/propertiesFor example, if you're assigning a transaction with Action = \"Number\", you need:An assignable that accepts msg.Tags[\"Action\"] == \"Number\"Either a Receive function or a handler that matches the same patternBoth the assignable and handler must use consistent pattern matchingImportant Limitations ​There are critical limitations to be aware of when working with assignables:Matching is Required: Transactions must match at least one of your defined assignable patterns to be accepted.Blacklisting is Permanent: If you attempt to assign a transaction before defining an appropriate assignable, it will be permanently blacklisted. Even if you later add a matching assignable, that transaction will never be accepted.One-time Assignment: Each Arweave transaction can only be assigned once to a given process. Subsequent assignments of the same transaction will be ignored.Proper Sequence for Assigning Arweave Transactions ​For successful assignment of Arweave transactions, follow these steps:Define assignables to specify which Arweave transactions your process will acceptWait for any transaction confirmations (by default, 20 confirmations are required)Set up handlers or listeners with Receive or Handlers.add to process the dataAssign the Arweave transaction to your process (see Assignment Methods)The order of steps 3 and 4 can be interchanged based on your needs:When using Receive in a script loaded with .load, ensure Assign is placed before Receive to prevent the process from hanging, as Receive is blocking.When using handlers or running commands separately in the shell, the order doesn't matter as handlers will catch messages whenever they arriveWhy Access Data from Arweave? ​There are several practical reasons to access Arweave data from your ao process:Efficient Handling of Large Data: For larger content, directly accessing Arweave is more efficient:Reference large media files (images, videos, documents) without storing them in your processWork with datasets too large to fit in process memoryMaintain a lightweight process that can access substantial external resourcesExternal Data for Decision-Making: Your process may need data stored on Arweave to make informed decisions. For example:Reading token price data stored by an oracleAccessing verified identity informationRetrieving voting records or governance dataDynamic Loading of Features: Rather than including all functionality in your initial process code:Load modules or plugins from Arweave as neededUpdate configuration without redeploying your entire processImplement upgradable components with new versions stored on ArweaveThis approach allows you to create more sophisticated applications that leverage Arweave's permanent storage while maintaining efficient process execution in the ao environment.When another process Assigns a transaction to this process, you can also use handlers to process the data asynchronously.",
          "estimatedWords": 1434,
          "lastModified": "2025-06-27T16:12:49.774Z",
          "siteKey": "ao",
          "siteName": "AO Cookbook",
          "depth": 3,
          "crawledAt": "2025-06-27T16:12:49.774Z"
        },
        {
          "url": "https://cookbook_ao.arweave.net/references/cron.html",
          "title": "Cron Messages",
          "content": "Cron Messages ​ao has the ability to generate messages on a specified interval, this interval could be seconds, minutes, hours, or blocks. These messages automatically get evaluated by a monitoring process to inform the Process to evaluate these messages over time. The result is a real-time Process that can communicate with the full ao network or oracles in the outside network.Setting up cron in a process ​The easiest way to create these cron messages is by spawning a new process in the aos console and defining the time interval.shaos [myProcess] --cron 5-minutesWhen spawning a new process, you can pass a cron argument in your command line followed by the interval you would like the cron to tick. By default, cron messages are lazily evaluated, meaning they will not be evaluated until the next scheduled message. To initiate these scheduled cron messages, call .monitor in aos - this kicks off a worker process on the mu that triggers the cron messages from the cu. Your Process will then receive cron messages every x-interval.lua.monitorIf you wish to stop triggering the cron messages simply call .unmonitor and this will stop the triggering process, but the next time you send a message, the generated cron messages will still get created and processed.Handling cron messages ​Every cron message has an Action tag with the value Cron. Handlers can be defined to perform specific tasks autonomously, each time a cron message is received.luaHandlers.add( \"CronTick\", -- Handler name Handlers.utils.hasMatchingTag(\"Action\", \"Cron\"), -- Handler pattern to identify cron message function () -- Handler task to execute on cron message -- Do something end )Cron messages are a powerful utility that can be used to create \"autonomous agents\" with expansive capabilities.",
          "estimatedWords": 281,
          "lastModified": "2025-06-27T16:12:50.004Z",
          "siteKey": "ao",
          "siteName": "AO Cookbook",
          "depth": 3,
          "crawledAt": "2025-06-27T16:12:50.004Z"
        },
        {
          "url": "https://cookbook_ao.arweave.net/references/editor-setup.html",
          "title": "Editor setup",
          "content": "Editor setup ​Remembering all the built in ao functions and utilities can sometimes be hard. To enhance your developer experience, it is recommended to install the Lua Language Server extension into your favorite text editor and add the ao addon. It supports all built in aos modules and globals.VS Code ​Install the sumneko.lua extension:Search for \"Lua\" by sumneko in the extension marketplaceDownload and install the extensionOpen the VS Code command palette with Shift + Command + P (Mac) / Ctrl + Shift + P (Windows/Linux) and run the following command:> Lua: Open Addon ManagerIn the Addon Manager, search for \"ao\", it should be the first result. Click \"Enable\" and enjoy autocomplete!Other editors ​Verify that your editor supports the language server protocolInstall Lua Language Server by following the instructions at luals.github.ioInstall the \"ao\" addon to the language serverBetterIDEa ​BetterIDEa is a custom web based IDE for developing on ao.It offers a built in Lua language server with ao definitions, so you don't need to install anything. Just open the IDE and start coding!Features include:Code completionCell based notebook ui for rapid developmentEasy process managementMarkdown and Latex cell supportShare projects with anyone through ao processesTight integration with ao package managerRead detailed information about the various features and integrations of the IDE in the documentation.",
          "estimatedWords": 211,
          "lastModified": "2025-06-27T16:12:50.367Z",
          "siteKey": "ao",
          "siteName": "AO Cookbook",
          "depth": 3,
          "crawledAt": "2025-06-27T16:12:50.367Z"
        },
        {
          "url": "https://cookbook_ao.arweave.net/references/betteridea/index.html",
          "title": "BetterIDEa",
          "content": "BetterIDEa ​BetterIDEa is a custom web based IDE for developing on ao.It offers a built in Lua language server with ao definitions, so you don't need to install anything. Just open the IDE and start coding!Features include:Code completionCell based notebook ui for rapid developmentEasy process managementMarkdown and Latex cell supportShare projects with anyone through ao processesTight integration with ao package managerRead detailed information about the various features and integrations of the IDE in the documentation.",
          "estimatedWords": 75,
          "lastModified": "2025-06-27T16:12:50.925Z",
          "siteKey": "ao",
          "siteName": "AO Cookbook",
          "depth": 3,
          "crawledAt": "2025-06-27T16:12:50.925Z"
        },
        {
          "url": "https://cookbook_ao.arweave.net/references/community.html",
          "title": "Community Resources",
          "content": "Community Resources ​This page provides a comprehensive list of community resources, tools, guides, and links for the AO ecosystem.Core Resources ​Autonomous FinanceAutonomous Finance is a dedicated research and technology entity, focusing on the intricacies of financial infrastructure within the ao network.BetterIdeaBuild faster, smarter, and more efficiently with BetterIDEa, the ultimate native web IDE for AO development0rbit0rbit provides any data from the web to an ao process by utilizing the power of ao, and 0rbit nodes. The user sends a message to the 0rbit ao, 0rbit nodes fetches the data and the user process receives the data.ArweaveHubA community platform for the Arweave ecosystem featuring events, developer resources, and discovery tools.AR.IOThe first permanent cloud network built on Arweave, providing infrastructure for the permaweb with no 404s, no lost dependencies, and reliable access to applications and data through gateways, domains, and deployment tools.Developer Tools ​AO Package ManagerContributing ​Not seeing an AO Community Member or resource? Create an issue or submit a pull request to add it to this page: https://github.com/permaweb/ao-cookbook",
          "estimatedWords": 167,
          "lastModified": "2025-06-27T16:12:51.502Z",
          "siteKey": "ao",
          "siteName": "AO Cookbook",
          "depth": 3,
          "crawledAt": "2025-06-27T16:12:51.502Z"
        },
        {
          "url": "https://cookbook_ao.arweave.net/releasenotes/index.html",
          "title": "Release Notes",
          "content": "Release Notes ​This section provides detailed information about updates, new features, bug fixes, and changes in each release of AO and its related tools. Release notes are essential for understanding what's changed between versions and how these changes might affect your projects.AOS Releases ​AOS is the operating system built on top of the AO computer. These release notes document changes and improvements to AOS:AOS 2.0.2 - Improved spawn process times and various bug fixesAOS 2.0.1 - Details about patch updates and fixes in the 2.0.1 releaseAOS 2.0.0 - Major release information, including new features and significant changesWhy Read Release Notes? ​Release notes provide valuable information for developers:Learn about new features that could enhance your applicationsUnderstand potential breaking changes that might affect existing codeDiscover bug fixes that resolve issues you may have encounteredStay informed about security updates and best practicesWe recommend reviewing release notes before upgrading to a new version to ensure a smooth transition.Navigation ​Use the sidebar to navigate between different release notes. Notes are organized chronologically with the most recent releases first.",
          "estimatedWords": 173,
          "lastModified": "2025-06-27T16:12:52.067Z",
          "siteKey": "ao",
          "siteName": "AO Cookbook",
          "depth": 3,
          "crawledAt": "2025-06-27T16:12:52.067Z"
        },
        {
          "url": "https://cookbook_ao.arweave.net/welcome/ao-processes.html",
          "title": "AO Processes",
          "content": "AO Processes ​AO Processes are persistent, programmable smart contracts that live inside the AO computer. Embodying the actor model from Erlang that inspired AO, these processes operate as independent computational units that have their own state and communicate with each other through message passing. This architecture makes them ideal for creating autonomous agents and complex decentralized applications.What are AO Processes? ​Following the actor model, each AO Process functions as an independent actor within the system, executing code—typically written in Lua—in response to messages it receives. Three core characteristics define them:Stateful: Each process has its own private state and memory, which persist across interactions.Persistent: All processes and their entire message history are permanently stored on Arweave.Generative: Processes can dynamically spawn new processes, enabling complex and evolving systems.AO Processes and the Actor Model ​The actor model provides several key benefits for process-based development, enabling naturally concurrent and resilient systems. By treating every process as an isolated \"actor,\" it simplifies development and enhances fault tolerance. Key advantages include:Concurrency & Isolation: Processes execute independently and are isolated from each other, enabling parallelism and preventing cascading failures.Message-Passing: All communication happens exclusively through asynchronous messages, simplifying interactions.Location Transparency & Fault Tolerance: Processes can interact without knowing each other's physical location on the network, and the system can continue operating even if individual processes fail.AOS: The Operating System for AO Processes ​AOS (AO Operating System) is an abstraction layer designed to simplify interaction with AO Processes. It provides developers with a powerful shell interface for sending commands, tools for managing process state, and a set of libraries for common functionalities, all contributing to a more streamlined development experience.Use Cases for AO Processes ​The persistent and concurrent nature of AO Processes makes them ideal for a wide range of decentralized applications. Here are a few examples:Autonomous Agents & Bots: Imagine a price-monitoring bot that tracks token prices across different decentralized exchanges (DEXs) and executes arbitrage trades automatically. AO makes it possible to build entire marketplaces for such agents, like Marketverse.Decentralized Finance (DeFi): You could build automated market makers (AMMs) or lending protocols where account balances and token reserves are tracked persistently within the process's state. A live example of this is Dexi, a decentralized exchange built on AO.On-Chain Games & Social Platforms: AO Processes can power fully on-chain games where the game state (like player positions or inventory) is managed by one or more processes, like the space strategy game Stargrid. They're also perfect for decentralized chat applications or social networks where user profiles, posts, and interactions are censorship-resistant.Next Steps ​Now that you understand the capabilities of AO Processes, the next step is to dive into Hyperbeam, the high-performance network that powers them.",
          "estimatedWords": 445,
          "lastModified": "2025-06-27T16:12:53.979Z",
          "siteKey": "ao",
          "siteName": "AO Cookbook",
          "depth": 2,
          "crawledAt": "2025-06-27T16:12:53.979Z"
        },
        {
          "url": "https://cookbook_ao.arweave.net/welcome/legacynet-info/index.html",
          "title": "Legacynet  HyperBEAM",
          "content": "Legacynet → HyperBEAM ​As the AO ecosystem evolves, we are transitioning from Legacynet to HyperBEAM Mainnet, marking a significant upgrade in the implementation of the AO-Core protocol.Legacynet: The Initial Implementation ​Legacynet was the first implementation of the AO-Core protocol, written in JavaScript. Launched on February 27, 2024, it provided a fee-free environment for early adopters to experiment with AO's hyper-parallel architecture. However, being a JavaScript implementation, Legacynet had inherent limitations in terms of scalability and native support for the actor-oriented model that AO is based on.HyperBEAM: The Future of AO-Core ​HyperBEAM is the new, advanced implementation of the AO-Core protocol, written in Erlang—the language that inspired AO's actor-oriented design. This implementation innately benefits from Erlang's strengths in:Actor-Oriented Design: Erlang's native support for the actor model aligns perfectly with AO's architecture, where processes (actors) operate independently and communicate via message passing.Scalability: Erlang is renowned for its ability to handle massive concurrency, allowing HyperBEAM to scale efficiently with the growing demands of the AO computer.Reliability: Erlang's design for fault tolerance ensures that HyperBEAM can maintain system stability even under high load or during failures of individual components.The Transition to HyperBEAM ​While HyperBEAM represents the future of AO, the transition from Legacynet is being handled carefully to ensure a smooth experience for developers. Currently, most development activity remains on Legacynet, which provides a stable environment for building and testing.The goal is to provide a seamless future upgrade path to HyperBEAM Mainnet. While Legacynet will eventually be deprecated, for now, it is the primary environment for new developers to begin building on AO.HyperBEAM Documentation ​For detailed documentation on the HyperBEAM protocol itself, including running infrastructure and leveraging its powerful URL pathing, visit HyperBEAM.arweave.net.Building on HyperBEAMTo learn how to build applications on HyperBEAM using ao and aos, and to migrate existing processes, see the Migrating to HyperBEAM Guide.Preparing for the Future ​While you build on Legacynet, you can prepare for the future of AO by:Reviewing the HyperBEAM documentation to understand the new environment and its architecture.Exploring the enhanced capabilities that HyperBEAM offers due to its Erlang foundation.Building with the knowledge that a seamless migration path to HyperBEAM Mainnet is a core priority.This transition is a significant step forward for the AO ecosystem, ensuring that we can deliver on the promise of decentralized, hyper-parallel computation at any scale.",
          "estimatedWords": 382,
          "lastModified": "2025-06-27T16:12:54.306Z",
          "siteKey": "ao",
          "siteName": "AO Cookbook",
          "depth": 3,
          "crawledAt": "2025-06-27T16:12:54.306Z"
        },
        {
          "url": "https://cookbook_ao.arweave.net/welcome/getting-started.html",
          "title": "Get started in 5 minutes",
          "content": "Get started in 5 minutes ​In less than 5 mins, we'll walk you through the process of taking your first peek into the rabbit hole of AO Processes. 🕳️🐇Now that you understand the AO-Core protocol and how AO Processes work, let's get hands-on with creating your first AO Process.System requirements ​The local client of aos is super simple to install. Just make sure you have:NodeJS version 20+. (If you haven't yet installed it, check out this page to find instructions for your OS).A code editor of your choice.Installing aos ​Once you have NodeJS on your machine, all you need to do is install aos and run it:shnpm i -g https://get_ao.arweave.netAfter installation, we can simply run the command itself to start a new aos process!shaosYou authenticate yourself to your aos process using a keyfile. If you have an Arweave wallet you can specify it by adding a --wallet [location] flag. If you don't, a new keyfile will be generated and stored locally for you at ~/.aos.json.Welcome to the rabbit hole ​The utility you just started is a local client, which is ready to relay messages for you to your new process inside the ao computer.After it connects, you should see the following:lua _____ _______ _____ /\\ \\ /::\\ \\ /\\ \\ /::\\ \\ /::::\\ \\ /::\\ \\ /::::\\ \\ /::::::\\ \\ /::::\\ \\ /::::::\\ \\ /::::::::\\ \\ /::::::\\ \\ /:::/\\:::\\ \\ /:::/~~\\:::\\ \\ /:::/\\:::\\ \\ /:::/__\\:::\\ \\ /:::/ \\:::\\ \\ /:::/__\\:::\\ \\ /::::\\ \\:::\\ \\ /:::/ / \\:::\\ \\ \\:::\\ \\:::\\ \\ /::::::\\ \\:::\\ \\ /:::/____/ \\:::\\____\\ ___\\:::\\ \\:::\\ \\ /:::/\\:::\\ \\:::\\ \\ |:::| | |:::| | /\\ \\:::\\ \\:::\\ \\ /:::/ \\:::\\ \\:::\\____\\|:::|____| |:::| |/::\\ \\:::\\ \\:::\\____\\ \\::/ \\:::\\ /:::/ / \\:::\\ \\ /:::/ / \\:::\\ \\:::\\ \\::/ / \\/____/ \\:::\\/:::/ / \\:::\\ \\ /:::/ / \\:::\\ \\:::\\ \\/____/ \\::::::/ / \\:::\\ /:::/ / \\:::\\ \\:::\\ \\ \\::::/ / \\:::\\__/:::/ / \\:::\\ \\:::\\____\\ /:::/ / \\::::::::/ / \\:::\\ /:::/ / /:::/ / \\::::::/ / \\:::\\/:::/ / /:::/ / \\::::/ / \\::::::/ / /:::/ / \\::/____/ \\::::/ / \\::/ / ~~ \\::/ / \\/____/ \\/____/ Welcome to AOS: Your operating system for AO, the decentralized open access supercomputer. Type \".load-blueprint chat\" to join the community chat and ask questions! AOS Client Version: 1.12.1. 2024 Type \"Ctrl-C\" twice to exit Your AOS process: QFt5SR6UwJSCnmgnROq62-W8KGY9z96k1oExgn4uAzk default@aos-0.2.2[Inbox:1]>Welcome to your new home in the ao computer! The prompt you are now looking at is your own personal server in this decentralized machine. We will be using it to play with and explore ao in the rest of this tutorial.Sending your first command ​Your new personal aos process is a server that lives inside the computer, waiting to receive and execute your commands.aos loves to make things simple, so it wants to hear commands from you in the Lua programming language. Don't know Lua? Don't panic! It is a super straightforward, friendly, and fun language. We will learn it as we progress through this series.Let's break the ice and type:luaaos> \"Hello, ao!\"Then hit the \"[Enter]\" key. You should see your shell sign and post the message, request the result, then print the result as follows:lua\"Hello, ao!\"Eh. What's the big deal? ​Sent it a message to your process, permanently etched it into Arweave, then asked a distributed compute network to calculate its result.While the result might not look revolutionary, in reality you have done something quite extraordinary. Your process is a decentralized server that doesn't exist in any one particular place on Earth. It exists as data, replicated on Arweave between many different machines, distributed all over the world. If you wanted to, you could now attach a new compute unit to this process and recreate the state from its log of inputs (just your single command, for now) -- at any time in the future.This makes your new shell process...Resilient: There is no single place on Earth where your server actually resides. It is everywhere and nowhere -- immune from physical destruction or tampering of any kind.Permanent: Your process will never disappear. It will always exist in its ✨holographic state✨ on Arweave, allowing you to recall it and continue playing with it. A contribution has been made to Arweave's storage endowment, so that you never have to think about upkeep or maintenance payments again.Permissionless: You did not have to register in order to start this server. Your right to use it is guaranteed by its underlying protocol (Arweave), no matter what Google, Amazon, or any other BigTech company says.Trustless: The state of your server is mathematically guaranteed. This means that you -- and everyone else -- can trust it with certainty, without even having to trust the underlying hardware it runs on. This property lets you build trustless services on top: Code that runs without any privileged owner or controller, ruled purely by math.There is so much more to it, but these are the basics. Welcome to the ao computer, newbie! We are grateful to have you. 🫡Next Steps ​In the tutorials that follow, we will explore ao and build everything from chatrooms to autonomous, decentralized bots. Let's go!",
          "estimatedWords": 843,
          "lastModified": "2025-06-27T16:12:54.574Z",
          "siteKey": "ao",
          "siteName": "AO Cookbook",
          "depth": 2,
          "crawledAt": "2025-06-27T16:12:54.574Z"
        }
      ],
      "lastCrawled": "2025-06-27T16:13:26.770Z",
      "stats": {
        "totalPages": 88,
        "averageWords": 485,
        "duration": 63996,
        "requestCount": 182,
        "averageResponseTime": 338.95054945054943,
        "pagesPerSecond": 1.3750859428714295
      }
    },
    "ario": {
      "name": "AR-IO Network",
      "baseUrl": "https://docs.ar.io",
      "pages": [
        {
          "url": "https://docs.ar.io/ar-io-sdk",
          "title": "ARIO SDK",
          "content": "AR.IO SDK Overview The AR.IO SDK provides functionality for interacting with the AR.IO ecosystem of services and protocols. This includes, the AR.IO Network, gateways, the ARIO token, and ArNS domains. The AR.IO SDK is available for both NodeJS and web environments. AR.IO Network The AR.IO Network is the AO smart contract process that controls all child services and protocols. The AR.IO SDK supports read operations to access various details about the current or historical state of the network. It also provides write operations for managing features such as the Gateway Address Registry and ARIO token. Gateways AR.IO gateways are open source nodes that index and serve Arweave transaction headers and data items. Gateway operators may join their gateway to the Gateway Address Registry (GAR), which makes the gateway discoverable using the AR.IO SDK. The gateway information is stored in the AR.IO AO contract as a JSON object with the following attributes: { \"operatorStake\": \"number\", // The amount of ARIO tokens staked by the operator, 50,000 minimum \"totalDelegatedStake\": \"number\", // Total amount of ARIO tokens staked to the gateway by wallets other than the operator \"vaults\": \"object\", // Details of tokens vaults (locked tokens) associated with the gateway (object) \"delegates\": \"object\", // Details of non-operator wallets who staked ARIO tokens on the gateway (object) \"startTimestamp\": \"number (unix)\", // Unix timestamp indicating start time \"stats\": \"object\", // Statistical information related to gateway performance (object) \"settings\": \"object\", // Configuration settings (object) \"status\": \"string (e.g., joined)\", // The current status of the operator \"observerAddress\": \"string\" // The public wallet address of the observer for the gateway } CopyCopied! The ar.io SDK supports write operations for gateway management, including joining, leaving, and updating settings. It also provides read operations for discovering gateways in the GAR and retrieving details about specific gateways. ARIO Token ARIO is an AO token that powers the ar.io Network and and its suite of permaweb applications. It is used to join the GAR, as payment for services like ArNS, as incentives for participation in the ar.io Network, and more. The ar.io SDK supports read and write operations for getting token information and balances, or transferring tokens. ArNS The Arweave Name System (ArNS) is a protocol which allows for assigning friendly names to Arweave transactions or data items. Powered by Arweave Name Tokens (ANTs), AO tokens that manage settings for individual ArNS domains, ArNS enables easy interaction with data stored on Arweave. The ar.io SDK supports read and write operations for managing ArNS domains, including retrieving domain information, leasing, purchasing, and extending leases. Additionally, it allows direct read and write access to ANTs.Was this page helpful?YesNoComment",
          "estimatedWords": 434,
          "lastModified": "2025-06-27T16:12:57.440Z",
          "siteKey": "ario",
          "siteName": "AR-IO Network",
          "depth": 1,
          "crawledAt": "2025-06-27T16:12:57.440Z"
        },
        {
          "url": "https://docs.ar.io/",
          "title": "Welcome to the Permaweb",
          "content": "🚀 Get Started Quickly: Explore Our Quick Start Guides → Welcome to the Permaweb Data in paradise. The AR.IO ecosystem is dedicated to cultivating products and protocols for sustaining access to digital permanence, making the permaweb available to everyone. Powered by the ARIO Token, this global network of Gateways connects users to permanently stored data, files, applications, and web pages on the Arweave decentralized storage network. AR.IO GatewaysAR.IO's modular gateways are built for the Arweave permanent data storage network and optimized for data retrieval, caching, serving and indexing transactions.ArNS NamesArNS is a censorship-resistant naming system stored on Arweave, enabling user-friendly domain names that link to permaweb dApps, web pages, data, and identities.AR.IO TokenARIO powers the AR.IO Network and its suite of permaweb applications as a permissionless and censorship resistant medium of common value for the network.The PermawebLearn more about the Arweave network, the permaweb and the reason the AR.IO gateway network was built.ar://The Wayfinder protocol transforms traditional Arweave URLs into more concise and user-friendly forms.White PaperA comprehensive document that details a decentralized and incentivized gateway network aimed at making the permaweb more accessible to all. GuidesRun a GatewayGet your AR.IO Gateway up and running correctly and quickly.Read moreUse ArNSLearn the process of purchasing and managing an ArNS name.Read moreDeploy a dAppLearn how to easily deploy a website or application on the permaweb.Read moreANTs on BazarIn a few simple steps, learn how to make an ANT tradable on Bazar.Read moreGraphQLLearn how to leverage GraphQL to efficiently fetch data via AR.IO gateways.Read more AR.IO = Arweave Input & OutputWas this page helpful?YesNoComment",
          "estimatedWords": 260,
          "lastModified": "2025-06-27T16:12:57.480Z",
          "siteKey": "ario",
          "siteName": "AR-IO Network",
          "depth": 2,
          "crawledAt": "2025-06-27T16:12:57.480Z"
        },
        {
          "url": "https://docs.ar.io/concepts/wayfinder",
          "title": "Wayfinder Protocol",
          "content": "Wayfinder Protocol Overview The Wayfinder protocol is a URI scheme designed to translate requests for Arweave content into https:// requests. Essentially, Wayfinder allows for transforming traditional Arweave URLs like https://arweave.net/long-txid into more concise and user-friendly forms such as ar://txid or ar://arns-name. When combined with the AR.IO WayFinder browser extension, the request can be directed to any number of functional AR.IO Gateways to serve the content. An early technical breakdown of Wayfinder, formerly \"ARCSS\", created by Arweave community member DMac, can be found here. Browser Integration The Wayfinder Protocol is currently facilitated via the WayFinder App or internal application integration. The intention is to lead popular web browsers like Chrome and Brave towards a direct integration of the Wayfinder Protocol, similar to recent integrations of the ipfs:// protocol. Such integration would remove the need for a client-side extension and boost developers' confidence in embedding Wayfinder Protocol URLs in their websites. Internal Application Integration Certain websites or apps may want to resolve Arweave Transaction ID's (TxId) internally. In these scenarios, they can process the Wayfinder Protocol internally without depending on browser support or the WayFinder App. A prime example is opensea.io. Opensea, an NFT marketplace, frequently imports NFT metadata from external sources. If metadata employs the Wayfinder Protocol, Opensea internally resolves these, presenting content without redirecting users through an https:// link. There are two main approaches to resolving Wayfinder Protocol URLs: Convert Wayfinder into a request directed at a predefined Arweave gateway. Retrieve a list of active AR.IO Gateways from the GAR by reading the contract state, or other available resources, and then fetch content from a gateway on the list. Each strategy has its benefits and challenges, necessitating careful evaluation based on specific use cases. Benefits of Wayfinder Over Hardcoded Gateway Links Using the Wayfinder Protocol offers several advantages over hardcoded links to a specific gateway: Flexibility: Wayfinder links can be routed through any available AR.IO Gateway, ensuring content remains accessible even if a specific gateway is down or congested. Decentralization: By not being tied to a single gateway, the Wayfinder Protocol embodies the decentralized spirit of the web, reducing potential censorship points. Ease of Maintenance: Developers and content creators don't need to modify links if a gateway changes its URL or becomes unavailable. The WayFinder extension handles routing to an active gateway. Consistency: Users always receive the same content, regardless of the gateway used, ensuring a consistent user experience. Use Cases Decentralized Web Hosting with Flexible Access With Wayfinder, not only can websites be hosted on the Arweave network, but their accessibility is also enhanced. By using the Wayfinder Protocol, web developers can ensure that if a specific AR.IO Gateway is down, the content can still be accessed through another gateway, offering a more reliable and resilient user experience. Digital Archives and Preservation with Enhanced Sharing Digitally archiving public domain works, especially in light of events like \"banned books week\", becomes more efficient with Wayfinder. Historical institutions or enthusiasts can easily share specific Wayfinder links to documents or media. Unlike hardcoded links which might break if a specific gateway goes offline, Wayfinder ensures that the content remains consistently accessible. Media Sharing Platforms with Consistent Content Delivery For platforms hosting user-generated content, the Wayfinder Protocol provides not just decentralized hosting but also a guarantee of content delivery. Even if a content piece becomes viral and one gateway gets congested, Wayfinder ensures that users can still access the content through another gateway, providing a seamless experience. Decentralized Applications (DApps) with Reliable Front-End Accessibility DApps, while benefiting from Arweave's permanent hosting, can further ensure their front-end remains consistently accessible to users by using Wayfinder. If a DApp's front-end is accessed frequently, causing strain on one gateway, Wayfinder can help ensure the load is distributed, and the DApp remains online and functional. How it Works Transaction ID To access content tied to an Arweave Transaction ID (TxId), simply append the TxId to ar://: ar://qI19W6spw-kzOGl4qUMNp2gwFH2EBfDXOFsjkcNyK9A CopyCopied! Inputting this into a WayFinder-equipped browser will route your request through the right AR.IO Gateway, translating it as per your Routing Method settings. ArNS Fetching content via an Arweave Name System (ArNS) name is straightforward. Attach the ArNS name to ar://: ar://good-morning CopyCopied! The Wayfinder protocol, along with the WayFinder App, discerns between TxIds and ArNS names. Once the suitable https:// request is formulated, the chosen gateway translates the ArNS name based on the ArNS aoComputer contract. Wayfinder App The AR.IO WayFinder App is a browser extension designed to facilitate the resolving of ar:// urls. v0.0.10 As of v0.0.10, Wayfinder supports the resolution of TXT records to Arweave content on top level domains. This innovative feature leverages DNS TXT records to associate Arweave transaction IDs with human-readable domain names, facilitating intuitive and memorable access to permaweb content. By simply entering an ar:// URL with a domain name, the Wayfinder App resolves the corresponding Arweave transaction ID through DNS TXT records, redirecting users directly to the content hosted on the Arweave network. Setup: Owners of a domain can set a TXT record for that domain following the format ARTX <Arweave TXID>. Wayfinder Redirection: With a TXT record set properly, whenever a user (who has Wayfinder installed) enters an ar:// URL containing a domain name (e.g., ar://example.com), the Wayfinder App performs a DNS lookup for that TXT record in order to redirect to the Arweave content. The lookup is completed through a secure DNS-over-HTTPS query to ensure privacy and integrity. Dynamic Content Resolution: After retrieving the TXT record, the Wayfinder App extracts that Arweave transaction ID and dynamically redirects the user to the content on the permaweb. This process is transparent to the user, providing a seamless experience as if accessing a traditional website. Key Features Gasless: TXT records can be set without any onchain transactions that would require gas fees. Easy Integration: Domain owners can easily link their permaweb content to their domains, making it accessible through a simple ar:// URL. Dyncamic Content Access: Content links can be updated in real-time through DNS TXT records, without requiring any changes to the ar:// URL itself. Enhanced User Experience: Offers users a familiar and easy-to-remember way to access permaweb content, leveraging standard web domain names. Security and Privacy: Secure DNS-over-HTTPS queries for DNS lookups protect user privacy and enhances security. Use Cases Branded Content Access: Companies and individuals can brand their permaweb content, making it accessible through their domain, enhancing brand visibility and user trust. Dynamic Content Updates: Domain owners can easily update what Permaweb content their AR:// URL resolves to, which is ideal for frequently updated resources like documents, blogs, and application interfaces. Educational and Informational Resources: Educational institutions and information providers can make their resources permanently available on the permaweb, accessible through simple, memorable URLs. This feature marks a significant advancement in making decentralized content more accessible and user-friendly, bridging the gap between traditional internet usability and the permaweb's permanence and censorship-resistant nature.Was this page helpful?YesNoComment",
          "estimatedWords": 1146,
          "lastModified": "2025-06-27T16:12:57.800Z",
          "siteKey": "ario",
          "siteName": "AR-IO Network",
          "depth": 3,
          "crawledAt": "2025-06-27T16:12:57.801Z"
        },
        {
          "url": "https://docs.ar.io/introduction",
          "title": "Introduction",
          "content": "Introduction TL;DR AR.IO seeks to create a decentralized and incentivized cloud network aimed at attracting more gateways to the Arweave network therefore making the permanent web more accessible to all. At the core of AR.IO's incentivization mechanism is the ARIO Token, a utility token used for joining the network, payments, gateway accountability, and protocol incentives. The network features modular and composable gateway infrastructure in addition to the Arweave Name System (ArNS) – a system for assigning friendly domain names to permanent data. What is AR.IO AR.IO is the world's first permanent cloud network, providing the infrastructure to ensure data, applications, and digital identities are timeless, tamper-proof, and universally accessible. Built on the foundation of the Arweave storage network, AR.IO forms a global ecosystem of gateways, protocols, and services that connect users to the permaweb – a web where information is permanent and free from centralized control. The AR.IO Network is an open, distributed, and ownerless system, supported by operators, developers, and end-users from around the world. It's decentralized nodes, known as AR.IO Gateways, act as \"Permanent Cloud Service Providers\" delivering the critical services needed to read, write, index and query data stored on the permaweb. These gateways provide a unified, resilient interface between users and the permaweb, featuring a permanent domain name system and seamless, location-independent access to permanent storage and applications. Gateways operate using standardized protocols to maintain consistency across the network. They also engage in an observation and reporting protocol to monitor performance and ensure accountability, helping to maintain a healthy and reliable ecosystem. The AR.IO Network is powered by a utility token, ARIO, which drives the network's functionality and accessibility. ARIO serves as a currency for services such as the Arweave Name System (ArNS), staking to join the network as a gateway operator, delegated staking, and as rewards for contributing to the network's performance and reliability. Together, these elements form the backbone of a permanent cloud network designed to preserve data and expand the possibilities of the web. Why AR.IO ? Arweave (a Layer 1 blockchain network) offers scalable and permanent onchain data storage in a sustainable manner. It does this by incentivizing miner nodes through a tokenomic endowment model which ensures data is globally stored and replicated for hundreds of years without the need for continual payment or maintenance by its uploader. However, the Arweave protocol does not incorporate all the needs of modern applications like data indexing, querying, retrieval, and other vital services. Consequently, over the past few years, infrastructure services have been independently developed and deployed to meet the demands of the permaweb at scale. Users and apps have come to rely on these gateway utilities, but they are closed source, have complex codebases, and are expensive to operate. Arweave does not offer any tokenomic incentives to offset the expenses associated with operating a gateway, which has led to the community's reliance on a single centrally controlled gateway subsidized for the betterment of the network: arweave.net. While arweave.net currently caches and indexes the entire weave with a high quality of service, it is a single bottleneck and point of failure for the whole ecosystem. AR.IO seeks to reduce the barriers of entry and attract more gateway operators to the permaweb with the goal of further enhancing its overall health, resiliency, and functionality through decentralized mechanisms that are as trustless as possible. The solution will be applied in two directions: By reducing gateway overhead costs with open source, efficient, modular networked architecture. By creating an economic incentive layer with the ARIO Token. The overall goal of this white paper is to present the framework for a healthy and sustainable decentralized gateway network.Was this page helpful?YesNoComment",
          "estimatedWords": 610,
          "lastModified": "2025-06-27T16:12:58.039Z",
          "siteKey": "ario",
          "siteName": "AR-IO Network",
          "depth": 4,
          "crawledAt": "2025-06-27T16:12:58.039Z"
        },
        {
          "url": "https://docs.ar.io/ario-contract",
          "title": "ARIO Smart Contract",
          "content": "AR.IO Smart Contract Overview The AR.IO smart contract encompasses all the functionality required to support the network's currency, utilities, and management. Written in Lua and compiled to WASM64, it is deployed as a Process within AO, leveraging the decentralized infrastructure of Arweave for immutability and auditability. This ensures that AR.IO's smart contract code is stored permanently, is easily verifiable by external auditors, and is transparent to the community. Protocol Balance The Protocol Balance is the primary sink and source of ARIO tokens circulating through the AR.IO Network. This balance is akin to a central vault or wallet programmatically encoded into the network's smart contract from which ArNS revenue is accumulated and incentive rewards are distributed. This balance is stored like any other token balance in the AR.IO smart contract, using the contract's Process ID as the balance owner. This balance is stored like any other token balance in the AR.IO smart contract, using the contract’s Process ID as the balance owner. Should a user or organization desire, tokens can even be sent directly into this balance to support the reward protocol and ecosystem. Cross-Chain Signature Support AO leverages the flexibility of ANS-104 data items, which support multiple signature types from various blockchains. This includes signatures from Arweave, Ethereum, Solana, Cosmos, among others. This cross-chain signature support provides significant benefits to the AR.IO network: Interoperability: Cross-chain signatures enable seamless interactions across different blockchain ecosystems, allowing AR.IO to integrate with diverse apps and services without friction. Flexibility: Users can validate transactions with signatures from their preferred blockchain, making it easier for a broader range of participants to engage with AR.IO using familiar wallets and mechanisms. Security: Decentralized cryptographic standards across chains ensure that interactions on AR.IO remain secure and trusted, regardless of the blockchain used. By supporting cross-chain signatures, AR.IO enhances interoperability, flexibility, and security, empowering users and developers across multiple blockchain ecosystems.Was this page helpful?YesNoComment",
          "estimatedWords": 314,
          "lastModified": "2025-06-27T16:12:58.354Z",
          "siteKey": "ario",
          "siteName": "AR-IO Network",
          "depth": 4,
          "crawledAt": "2025-06-27T16:12:58.354Z"
        },
        {
          "url": "https://docs.ar.io/staking",
          "title": "Staking",
          "content": "Staking Overview Staking tokens within the AR.IO Network serves a dual primary purpose: it signifies a public commitment by gateway operators and qualifies them and their delegates for reward distributions. In the AR.IO ecosystem, \"staking\" refers to the process of locking a specified amount of ARIO tokens into a protocol-controlled vault. This act signifies an opportunity cost for the staker, acting both as a motivator and a public pledge to uphold the network's collective interests. Once staked, tokens remain locked until the staker initiates an 'unstake / withdraw' action or reaches the end of the vault’s lock period. It is important to note that the ARIO Token is non-inflationary, distinguishing the AR.IO Network's staking mechanism from yield-generation tools found in other protocols. Staking in this context is about eligibility for potential rewards rather than direct token yield. By staking tokens, gateway operators (and their delegates) demonstrate their commitment to the network, thereby gaining eligibility for protocol-driven rewards and access to the network’s shared resources. Gateway Staking A gateway operator must stake tokens to join their gateway to the network, which not only makes them eligible for protocol rewards but also promotes network reliability. This staking requirement reassures users and developers of the gateway's commitment to the network’s objectives, and gateways that adhere to or surpass network performance standards become eligible for these rewards. Gateway operators may increase their stake above the minimum, known as excess stake. A gateway’s total stake is impacted the following epoch once excess stake is added or removed. Delegated Staking To promote participation from a wider audience, the network shall allow anyone with available ARIO tokens to partake in delegated staking. In this, users can choose to take part in the risk and rewards of gateway operations by staking their tokens with an active gateway (or multiple gateways) through an act known as delegating. By delegating tokens to a gateway, a user increases the overall stake of that gateway. A delegated staker proxies their stake to gateways and therefore entrusts gateway operators to utilize that stake in maintaining a quality of service befitting the permaweb. Stake Redelegation This feature enables existing stakers to reallocate their staked tokens between gateways, known as redelegation. Both delegated stakers and gateway operators with excess stake (stake above the minimum network-join requirement) can take advantage of this feature. Redelegation is intended to offer users flexibility and the ability to respond to changing network conditions. Redeeming Delegated Stake for ArNS Staked tokens generally have restricted liquidity to maintain a healthy degree of stability in the network. However, an exception to these restrictions allows delegated stakers to use their staked tokens for specific ArNS-related services. By leveraging their staking rewards, delegates can further engage with ArNS, strengthening the name system’s utilization and impact across the network. Expedited Withdrawal Fees Gateway operators and delegated stakers can shorten the standard withdrawal delay period after initiating a withdrawal (or being placed into an automatic withdrawal by protocol mechanisms); this action is subject to a dynamic fee. At any point during the delay, users can choose to expedite access to their pending withdrawal tokens by paying a fee to the protocol balance, calculated based on how much sooner they want to receive their funds. Once triggered, the tokens are returned immediately to the user’s wallet.Was this page helpful?YesNoComment",
          "estimatedWords": 551,
          "lastModified": "2025-06-27T16:12:58.584Z",
          "siteKey": "ario",
          "siteName": "AR-IO Network",
          "depth": 4,
          "crawledAt": "2025-06-27T16:12:58.584Z"
        },
        {
          "url": "https://docs.ar.io/concepts/manifests",
          "title": "Manifests",
          "content": "Manifests Overview AR.IO Gateways support friendly-path-name routing for data on Arweave via Manifests. This greatly improves the programmability of data relationships. Consider an illustrative example where data stored on Arweave and accessed like this: http://<gateway domain>/cG7Hdi_iTQPoEYgQJFqJ8NMpN4KoZ-vH_j7pG4iP7NI (txID of a website's index.html) http://<gateway domain>/3zFsd7bkCAUtXUKBQ4XiPiQvpLVKfZ6kiLNt2XVSfoV (txID of its js/style.css) http://<gateway domain>/or0_fRYFcQYWh-QsozygI5Zoamw_fUsYu2w8_X1RkYZ (txID of its assets/img/logo.png) CopyCopied! can instead be accessed like this: http://<gateway domain>/<txId of manifest> (resolves to the txID of index.html) http://<gateway domain>/<txId of manifest>/js/style.css http://<gateway domain>/<txId of manifest>/assets/img/logo.png CopyCopied! NFT collections also benefit from manifest-based routing: http://<gateway domain>/<txId of NFT collection image manifest>/0.png http://<gateway domain>/<txId of NFT collection image manifest>/1.png http://<gateway domain>/<txId of NFT collection image manifest>/2.png ... and so on. CopyCopied! AR.IO gateways are capable of resolving manifest paths in a relative manner. An HTML page loading assets from Arweave would be very difficult to develop, maintain, and harden against hosting domains leaving existence if assets had to be linked to by a fully qualified domain name and an Arweave data item ID as the path. For example: <img src=\"https://arweave.dev/3zFsd7bkCAUtXUKBQ4XiPiQvpLVKfZ6kiLNt2XVSfoV\" /> CopyCopied! Manifests allow HTML pages to use relative paths to assets with friendly names so that the document is easy to read, maintain, and host across any AR.IO domain. For example: <img src=\"./logo.png\" /> CopyCopied! Relative routing eliminates the need for every link to contain the full Arweave transaction ID and fully qualified domain name. This makes the HTML more readable and ensures that links remain valid even if the hosting domain changes. If index.html needed to access js/style.css, the relative link ./js/style.css could be used instead of <txId>/js/style.css. This relative routing is incredibly useful for linking together files in a way that allows functional websites to be hosted entirely on Arweave. Learn more about relative path routing and structuring files into a permanently hosted website in ArDrive's decentralized app guide What is a Manifest Manifests, also known as \"Path Manifests\" or \"Arweave Manifests,\" are JSON objects that connect various Arweave data items and define relational paths for easy navigation. A common use case for manifests is permanently hosting websites on Arweave by linking all necessary files together. An AR.IO gateway can then resolve the manifest into a fully functional website. Sample Manifest { \"manifest\": \"arweave/paths\", \"version\": \"0.2.0\", \"index\": { \"path\": \"index.html\" }, \"fallback\": { \"id\": \"iXo3LSfVKVtXUKBzfZ4d7bkCAp6kiLNt2XVUFsPiQvQ\" }, \"paths\": { \"index.html\": { \"id\": \"cG7Hdi_iTQPoEYgQJFqJ8NMpN4KoZ-vH_j7pG4iP7NI\" }, \"404.html\": { \"id\": \"iXo3LSfVKVtXUKBzfZ4d7bkCAp6kiLNt2XVUFsPiQvQ\" }, \"js/style.css\": { \"id\": \"3zFsd7bkCAUtXUKBQ4XiPiQvpLVKfZ6kiLNt2XVSfoV\" }, \"css/style.css\": { \"id\": \"sPiQvpAUXLVK3zF6iXSfo7bkCVQkiLNt24dVtXUKBfZ\" }, \"css/mobile.css\": { \"id\": \"fZ4d7bkCAUiXSfo3zFsPiQvpLVKVtXUKB6kiLNt2XVQ\" }, \"assets/img/logo.png\": { \"id\": \"or0_fRYFcQYWh-QsozygI5Zoamw_fUsYu2w8_X1RkYZ\" }, \"assets/img/icon.png\": { \"id\": \"0543SMRGYuGKTaqLzmpOyK4AxAB96Fra2guHzYxjRGo\" } } } CopyCopied! How it Works A resolver, typically an AR.IO gateway, resolves URLs requesting content based on a manifest transaction ID to the corresponding path key in the paths object. The URL schema for this type of request is https://<gateway url>/<manifest TxId>/<path>. Example Usage Assume the manifest above is uploaded to Arweave with the transaction ID UyC5P5qKPZaltMmmZAWdakhlDXsBF6qmyrbWYFchRTk. The below table shows https requests to the AR.IO gateway arweave.dev requesting various endpoints on the manifest transaction Id, the manifest path where the gateway will find the data to return, and the resulting Arweave txId. ← Swipe to see more →Request PathManifest PathData served from txIDhttps://arweave.dev/UyC5P5qKPZaltMmmZAWdakhlDXsBF6qmyrbWYFchRTkindexcG7Hdi_iTQPoEYgQJFqJ8NMpN4KoZ-vH_j7pG4iP7NIhttps://arweave.dev/UyC5P5qKPZaltMmmZAWdakhlDXsBF6qmyrbWYFchRTk/index.htmlindex.htmlcG7Hdi_iTQPoEYgQJFqJ8NMpN4KoZ-vH_j7pG4iP7NIhttps://arweave.dev/UyC5P5qKPZaltMmmZAWdakhlDXsBF6qmyrbWYFchRTk/js/style.cssjs/style.css3zFsd7bkCAUtXUKBQ4XiPiQvpLVKfZ6kiLNt2XVSfoVhttps://arweave.dev/UyC5P5qKPZaltMmmZAWdakhlDXsBF6qmyrbWYFchRTk/foobarfallbackiXo3LSfVKVtXUKBzfZ4d7bkCAp6kiLNt2XVUFsPiQvQ← Swipe to see more → Specifications Transaction Tags Manifest are uploaded to Arweave in the same manner as any other data item. A specific content type tag must be added while uploading so that resolvers like the AR.IO gateways can recognize a manifest and properly resolve the paths. Tags must be attached to the manifest at the time of upload. They cannot be added later without uploading a new manifest, and they must be attached to the upload transaction, NOT placed inside the json object. Failure to provide this tag will result in resolvers not recognizing the manifest, so they will only return the raw json instead of the linked data items. Content-Type { \"name\": \"Content-Type\", \"value\": \"application/x.arweave-manifest+json\" } CopyCopied! Transaction Data Being a json object, there are several attributes that make up the structure of a manifest. The json object must be fully defined and uploaded to Arweave as a data item. manifest \"manifest\": \"arweave/paths\" CopyCopied! The manifest attribute serves as an additional validation layer. It must have the value arweave/paths in order for a gateway to resolve the manifest. version \"version\": \"0.2.0\" CopyCopied! The version attribute defines the version of manifest schema a manifest is using. index \"index\": { \"id\": \"cG7Hdi_iTQPoEYgQJFqJ8NMpN4KoZ-vH_j7pG4iP7NI\" } CopyCopied! or \"index\": { \"path\": \"index.html\", } CopyCopied! The index attribute is an object that defines the base, or 'starting' data item. It is similar to the / endpoint on a website. When resolving the manifest with no additional path definition, this is the data item that will be returned. index accepts either path or id as sub attributes. path represents the key of a defined path in the manifest, while id represents a specific Arweave data item transaction Id. If both path and id are defined in index, id will override path. fallback \"fallback\": { \"id\": \"iXo3LSfVKVtXUKBzfZ4d7bkCAp6kiLNt2XVUFsPiQvQ\" } CopyCopied! The fallback attribute is an object that defines an Arweave data item transaction Id for the resolver to fall back to if it fails to correctly resolve a requested path. For example, it can act as a 404 page if a user requests manifest/non-existent-page fallback accepts id as a sub attribute, representing an Arweave data item transaction Id. paths \"paths\": { \"index.html\": { \"id\": \"cG7Hdi_iTQPoEYgQJFqJ8NMpN4KoZ-vH_j7pG4iP7NI\" }, \"404.html\": { \"id\": \"iXo3LSfVKVtXUKBzfZ4d7bkCAp6kiLNt2XVUFsPiQvQ\" }, \"js/style.css\": { \"id\": \"3zFsd7bkCAUtXUKBQ4XiPiQvpLVKfZ6kiLNt2XVSfoV\" }, \"css/style.css\": { \"id\": \"sPiQvpAUXLVK3zF6iXSfo7bkCVQkiLNt24dVtXUKBfZ\" }, \"css/mobile.css\": { \"id\": \"fZ4d7bkCAUiXSfo3zFsPiQvpLVKVtXUKB6kiLNt2XVQ\" }, \"assets/img/logo.png\": { \"id\": \"or0_fRYFcQYWh-QsozygI5Zoamw_fUsYu2w8_X1RkYZ\" }, \"assets/img/icon.png\": { \"id\": \"0543SMRGYuGKTaqLzmpOyK4AxAB96Fra2guHzYxjRGo\" } } CopyCopied! The paths attribute is an object that defines the url paths that a manifest can resolve to. If a user navigates to manifest/index.html the resolver will look for index.html as a key in the paths object and return the corresponding id. (cG7Hdi_iTQPoEYgQJFqJ8NMpN4KoZ-vH_j7pG4iP7NI)Was this page helpful?YesNoComment",
          "estimatedWords": 969,
          "lastModified": "2025-06-27T16:12:58.934Z",
          "siteKey": "ario",
          "siteName": "AR-IO Network",
          "depth": 4,
          "crawledAt": "2025-06-27T16:12:58.934Z"
        },
        {
          "url": "https://docs.ar.io/concepts/normalized-addresses",
          "title": "Normalized Addresses",
          "content": "Normalized Addresses Overview Different blockchains use different formats for the public keys of wallets, and the native addresses for those wallets. In most cases, when a system in the Arweave ecosystem needs to display the wallet address of a wallet from a different blockchain, for instance in the Owner.address value of an AO process spawned by an ETH wallet, that address will be normalized into the format recognized by Arweave. Specifically, a 43 character base64url representation of the sha256 hash of the public key. This is done to prevent potential errors by systems in the Arweave ecosystem that expect these values to be a certain size and conform to a specific format. Essentially, normalized addresses are a way to represent public keys and wallet addresses from other blockchains in a way that is familiar to systems in the Arweave ecosystem. A tool for easily obtaining a normalized addresses from public keys can be found at ar://normalize-my-key At A Glance ← Swipe to see more →ArweaveETH/POLSolanaNative Address9ODOd-_ZT9oWoRMVmmD4G5f9Z6MjvYxO3Nen-T5OXvU0x084af408C8E492aC52dc0Ec76514A7deF8D5F03fCd5yb4mvbuQyyJgAkriFZbWQivh2zM68KGZX8Ksn1L85base64url Encoded Public Key0jkGWDFYI3DHEWaXhZitjTg67T-enQwXs50lTDrMhy2qb619_91drv_50J5PwrOYJiMmYhiEA5ojMvrrAFY-Dm1bJbJfVBU1kIsPho2tFcXnbSOa2_1bovAys0ckJU07wkbmIUpzp3trdxYReB4jayMMOXWw9B8xS0v81zFmK3IbCtL9N6WNTMONOSMATHFQrGqtDhDUqKyIsQZCBPFvfGykRWaLWzbtAUrApprqG9hfExQzppNsw0gsftNSHZ1emC5tC2fuib6FhQw9TE2ge9tUjEZNALcVZvopTtTX0H2gEfnRJ48UNeV3SKggjXcoPVeivmqXuPBGncXWWq1pHR-Xs4zSLA5Mgcw_tQJc4FIER0i7hUlZXoc991ZHyOvAC-GlHWzQwvrlY11oD38pB47NkHN2WVPtUCAtyYQe5TE6Xznd9kPgqqvVUkV0s0suh5vINGoiPEnMjyhYEN7eOmJRIJ_A87IJesbdPRV4ZzBsqPbd02RG3ZuVpc3gI1xKvwH1WS05XI8eWK-BbvB3oxB7WjaQTWcfBWhMEULiwx-SucuyAzPAw3i6Wjtq61TcL9SdWhmOf9_yo-Np052tj7MQ66nmgdOH_MEKYjAdFypxTsRQoSLbv28HEcSjwx8u3pY0q0gKMK_5X2XKJrp2i2GB_fVgbcpH9YsgrYxh1Q82W5VMzNKYwr51QsiYBHUS5h5wxZf_uBgG7C6xiHgBHwwLUty5LHKFFBDlAxTCTAhglcmys2_HQoOj_LnCkA3rK8XXxd8JqsZFPXVOwkSWS5Gh1SJzftfCOLpLk4i1FYNormalized Address9ODOd-_ZT9oWoRMVmmD4G5f9Z6MjvYxO3Nen-T5OXvU5JtuS4yOFtUX2Rg3UU7AgBaUqh4s8wyyNTZk9UrzI-QK8kpPM1RID8ZM2sjF5mYy0rP4gXSRDbrwPUd9Qths64← Swipe to see more → Public Keys and Addresses Crypto wallets consist of two separate components. The public keys, which are public knowledge and can be seen by anyone, and the private keys, which only the owner of a wallet should have access to. Crypto wallet addresses are derived from the public key. Encoded Public Keys It is important to note that all crypto wallet public and private keys are binary data. The values provided below for Arweave and Ethereum/Polygon public keys are base64url and hex encoded representations of that binary data respectively. Arweave The public key for an Arweave wallet is the n field of the JWK json file. 0jkGWDFYI3DHEWaXhZitjTg67T-enQwXs50lTDrMhy2qb619_91drv_50J5PwrOYJiMmYhiEA5ojMvrrAFY-Dm1bJbJfVBU1kIsPho2tFcXnbSOa2_1bovAys0ckJU07wkbmIUpzp3trdxYReB4jayMMOXWw9B8xS0v81zFmK3IbCtL9N6WNTMONOSMATHFQrGqtDhDUqKyIsQZCBPFvfGykRWaLWzbtAUrApprqG9hfExQzppNsw0gsftNSHZ1emC5tC2fuib6FhQw9TE2ge9tUjEZNALcVZvopTtTX0H2gEfnRJ48UNeV3SKggjXcoPVeivmqXuPBGncXWWq1pHR-Xs4zSLA5Mgcw_tQJc4FIER0i7hUlZXoc991ZHyOvAC-GlHWzQwvrlY11oD38pB47NkHN2WVPtUCAtyYQe5TE6Xznd9kPgqqvVUkV0s0suh5vINGoiPEnMjyhYEN7eOmJRIJ_A87IJesbdPRV4ZzBsqPbd02RG3ZuVpc3gI1xKvwH1WS05XI8eWK-BbvB3oxB7WjaQTWcfBWhMEULiwx-SucuyAzPAw3i6Wjtq61TcL9SdWhmOf9_yo-Np052tj7MQ66nmgdOH_MEKYjAdFypxTsRQoSLbv28HEcSjwx8u3pY0q0gKMK_5X2XKJrp2i2GB_fVgbcpH9YsgrYxh1Q8 The public wallet address for that wallet is 9ODOd-_ZT9oWoRMVmmD4G5f9Z6MjvYxO3Nen-T5OXvU, this is obtained by decoding the public key from base64url to normalize padding, sha256 hashing the result, and then base64url encoding that. Ethereum/Polygon The public key for an EVM wallet (Ethereum, Polygon/Matic) is derived from its private key, using the Elliptic Curve Digital Signature Algorithm, or ECDSA. 0xb5d96e5533334a630af9d50b226011d44b9879c3165ffee0601bb0bac621e0047c302d4b72e4b1ca145043940c53093021825726cacdbf1d0a0e8ff2e70a4037 The public wallet address is 0x084af408C8E492aC52dc0Ec76514A7deF8D5F03f, this is obtained by removing the first byte from the public key, Keccak-256 hashing the remainder, taking the the last 20 bytes (40 hexadecimal characters) and prepending 0x to it. Solana A Solana wallet is an array of 64 bytes. The first 32 bytes are the private key, and the last 32 bytes are the public key. Below is the public key portion of a Solana wallet: [172, 175, 23, 95, 23, 124, 38, 171, 25, 20, 245, 213, 59, 9, 18, 89, 46, 70, 135, 84, 137, 205, 251, 95, 8, 226, 233, 46, 78, 34, 212, 86] The public wallet address for this wallet is Cd5yb4mvbuQyyJgAkriFZbWQivh2zM68KGZX8Ksn1L85, this is derived by base58 encoding the public key bytes. Normalizing Addresses As shown in the above examples, the format of public keys, and the resulting derived wallet addresses, vary widely between blockchains. Arweave manages this by applying the same derivation methods that Arweave uses for its own wallets to the public keys from other chains. Ethereum/Polygon The leading 0x and uncompressed flag 04 (if present) is removed from the public key of an EVM wallet, and then the remainder is base64url encoded to obtain the Arweave normalized public key. Continuing with the same public key in the above example, the normalized public key would be: 2W5VMzNKYwr51QsiYBHUS5h5wxZf_uBgG7C6xiHgBHwwLUty5LHKFFBDlAxTCTAhglcmys2_HQoOj_LnCkA3 This value is what is used as the GraphQL tag owner value for data items being uploaded to Arweave using an EVM wallet. The normalized address is then derived from this value by sha256 hashing it, and then base64url encoding the result: 5JtuS4yOFtUX2Rg3UU7AgBaUqh4s8wyyNTZk9UrzI-Q Solana The normalized public key for Solana wallets are derived similarly. The 32 byte public key is base64url encoded: rK8XXxd8JqsZFPXVOwkSWS5Gh1SJzftfCOLpLk4i1FY Again, this value is used for the GraphQl tag owner when uploading data. It can then be sha256 hashed, and base64url encoded again to derive the normalized address: K8kpPM1RID8ZM2sjF5mYy0rP4gXSRDbrwPUd9Qths64Was this page helpful?YesNoComment",
          "estimatedWords": 654,
          "lastModified": "2025-06-27T16:12:59.147Z",
          "siteKey": "ario",
          "siteName": "AR-IO Network",
          "depth": 4,
          "crawledAt": "2025-06-27T16:12:59.147Z"
        },
        {
          "url": "https://docs.ar.io/concepts/sandboxing",
          "title": "Browser Sandboxing",
          "content": "Browser Sandboxing Overview Browser sandboxing allows data requests to a gateway node to benefit from the security advantages of using a browser's same-origin policy by redirecting the requests to a pseudo-unique subdomain of the gateway's apex domain. For example, an attempt to access https://arweave.net/gnWKBqFXMJrrksEWrXLQRUQQQeFhv4uVxesHBcT8i6o would redirect to https://qj2yubvbk4yjv24syelk24wqivcbaqpbmg7yxfof5mdqlrh4rova.arweave.net/gnWKBqFXMJrrksEWrXLQRUQQQeFhv4uVxesHBcT8i6o Two DNS records are required to link a domain to an Arweave transaction on a gateway node. For example, www.mycustomsite.com would need the following records to link it to www.arweave-gateway.net: A DNS CNAME record pointing to an Arweave gateway: www CNAME arweave-gateway.net, A DNS TXT record linking the domain with a specific transaction ID: arweavetx TXT kTv4OkVtmc0NAsqIcnHfudKjykJeQ83qXXrxf8hrh0S When a browser requests www.mycustomsite.com the user's machine will (through the usual DNS processes) resolve this to the IP address for the gateway node arweave-gateway.net. When the gateway receives an HTTP request with a non-default hostname, e.g. www.mycustomsite.com instead of www.arweave-gateway.net, the gateway will query the DNS records for www.mycustomsite.com and the 'arweavetx' TXT record will tell the node which transaction to serve. TLS and its Role in Browser Sandboxing Transport Layer Security (TLS) is a cryptographic protocol designed to provide communications security over a computer network. In the context of Arweave applications and browser sandboxing, TLS plays a critical role in ensuring secure data transmission and enabling the effective use of browser security features. When Arweave applications are accessed without TLS, most browsers restrict the use of native cryptographic functions. These functions, which include hashing, signing, and verification, are essential for the secure operation of Arweave permaweb apps. Without TLS, not only are these functions unavailable, but the applications also become susceptible to various security threats, notably man-in-the-middle (MITM) attacks. Although Arweave transactions are signed, making direct MITM attacks challenging, the absence of encryption can expose other vulnerabilities. For instance, attackers could intercept and alter the /price endpoint, potentially causing transaction failures or leading to overcharging. To address these concerns, gateway operators are responsible for generating and maintaining TLS certificates for their gateways. This can be achieved through various systems, such as ACME for Let's Encrypt. An important step in setting up a gateway is obtaining a wildcard TLS certificate for the gateway's domain. This certificate secures traffic on both the apex domain and its single-level subdomains (e.g., gateway.com and subdomain.gateway.com). The integration of TLS is crucial for the implementation of browser sandboxing. When a browser requests a transaction from a gateway, the gateway issues a 301 redirect to a subdomain of the gateway, using a Base32 pseudo-unique address derived from the transaction ID. This redirection, secured by TLS, invokes the browser's same-origin policy. As a result, the requested web page is confined within a secure sandbox environment, isolated from other domains. This isolation is vital for maintaining the integrity and security of transactions and interactions within Arweave's permaweb applications. Deriving Sandbox Value AR.IO nodes generate browser sandbox values deterministically. Because of this, it is possible to calculate ahead of time what that value will be for a particular transaction id. Sandbox values are a Base32 encoding of the transaction ID. AR.IO gateways use the following code snippet to accomplish the encoding: const expectedTxSandbox = (id: string): string => { return toB32(fromB64Url(id)) } CopyCopied! Example: const id = 'gnWKBqFXMJrrksEWrXLQRUQQQeFhv4uVxesHBcT8i6o' const expectedTxSandbox = (id): string => { return toB32(fromB64Url(id)) } console.log(expectedTxSandbox) CopyCopied! Example Output: qj2yubvbk4yjv24syelk24wqivcbaqpbmg7yxfof5mdqlrh4rova CopyCopied! View the full code for generating browser sandbox values here.Was this page helpful?YesNoComment",
          "estimatedWords": 564,
          "lastModified": "2025-06-27T16:12:59.489Z",
          "siteKey": "ario",
          "siteName": "AR-IO Network",
          "depth": 4,
          "crawledAt": "2025-06-27T16:12:59.489Z"
        },
        {
          "url": "https://docs.ar.io/gateways/",
          "title": "Gateway Architecture",
          "content": "Gateway Architecture Overview Gateways are the workhorses of the AR.IO Network. Their primary role is to act as a bridge between the Arweave network and the outside world. This means that a gateway's main task is to make it easier for users to interact with the Arweave network by simplifying the technical processes of writing, reading, and discovering data on the blockweave in a trust-minimized fashion. Gateway functions The functions of an AR.IO gateway are broken down into the following categories: Writing data involves: Proxying base layer transaction headers to one or more healthy and active Arweave nodes (miners) to facilitate inclusion in the mempools of as many nodes as possible. Proxying chunks for base layer Arweave transactions to Arweave nodes to help facilitate storage and replication of the chunks on the blockweave. Receiving and bundling so-called bundled data items (e.g., ANS-104 spec) as base layer transactions. Reading involves retrieving: Transaction headers for a base layer Arweave transaction. Individual data chunks for a base layer Arweave transaction. Blocks from the blockweave. Storage pricing rates for data from the Arweave node network. Contiguous streams of chunks representing an entire base layer transaction. Bundled data items (e.g., ANS-104). Wallet information (e.g., token balance). Discovering data involves: Facilitating efficient, structured queries for base layer transactions, bundled data items, and wallet data by: examining incoming streams of data (i.e., directly ingested transactions and data items, blocks emitted by the chain, etc.). managing index data in a database or analogous data store. Parsing and executing user queries. Facilitating friendly-path routing via Arweave manifest indexing. Including other benefits and capabilities such as: Facilitating friendly-subdomain-name routing to Arweave transactions via a direct integration with the Arweave Name System (ArNS). Providing the modularity and configurability necessary for operating extensible gateways that can be deployed at small or large scales to meet the needs of specific applications, use cases, communities, or business models. Providing pluggable means for consuming telemetry data for internal and external monitoring and alerting. Facilitating configurable content moderation policies. Providing connectivity to a decentralized network of other AR.IO gateways, enabling data sharing and other shared workloads. AR.IO Gateway Benefits AR.IO gateways provide many new benefits and capabilities beyond general Arweave gateways: Providing the modularity and configurability necessary for operating extensible gateways that can be deployed at small or large scales to meet the needs of specific applications, use cases, communities, or business models. Providing pluggable means for consuming telemetry data for internal and external monitoring and alerting. Facilitating friendly-subdomain-name routing to Arweave transactions via a direct integration with the Arweave Name System (ArNS). Facilitating configurable content moderation policies. Providing connectivity to a decentralized network of other AR.IO gateways, enabling data sharing and other shared workloads. Gateway Modularity A design principle of AR.IO gateways is that their core components should be interchangeable with compatible implementations. The core services in the gateway are written in Typescript, with flexible interfaces to the various subsystems and databases. This allows operators to customize their gateway to meet their specific requirements. Gateway services can be turned on or off depending on the operator's needs. For example, an operator might choose to have their gateway serve data, but not actively index Layer 2 bundled data. This flexibility also allows operators to utilize the technologies that are appropriate for the scale and environments in which they operate. For example, small scale operators might want to use low-overhead relational databases to power their indexing while larger scale operators might opt to use cloud-native, horizontally scalable databases. Analogous examples for storage and caching exist as well. ← Swipe to see more →Gateway Tech Stack OptionsTopologyChain IndexBundle IndexData IndexData StoreSmallSQLiteSQLiteSQLiteLocal File SystemLargePostgreSQLCassandraCassandraS3 Compatible← Swipe to see more → ARNS Indexing and Routing The Arweave Name System’s (ArNS) state is managed by the ARIO token’s smart contract. AR.IO gateways shall perform the following minimum functions relative to ArNS: Actively track state changes in the contract. Maintain up-to-date indexes for routing configurations based on the state of the ARIO contract as well as the states of the Arweave Name Token (ANT) contracts to which each name is affiliated. Manage the expiration of stale records. Facilitate ArNS routing based on the subdomains specified on incoming requests where appropriate. Provide a custom HTTP response header for ArNS requests indicating the corresponding Arweave transaction ID. Was this page helpful?YesNoComment",
          "estimatedWords": 717,
          "lastModified": "2025-06-27T16:12:59.699Z",
          "siteKey": "ario",
          "siteName": "AR-IO Network",
          "depth": 4,
          "crawledAt": "2025-06-27T16:12:59.699Z"
        },
        {
          "url": "https://docs.ar.io/permaweb-deploy",
          "title": "Our vision is an internet with no more 404s but it looks like you just found one",
          "content": "404s suck!Our vision is an internet with no more 404s, but it looks like you just found one.Rest assured that if this page ever existed, it is still available on the permaweb.Back to Home",
          "estimatedWords": 34,
          "lastModified": "2025-06-27T16:13:00.036Z",
          "siteKey": "ario",
          "siteName": "AR-IO Network",
          "depth": 3,
          "crawledAt": "2025-06-27T16:13:00.036Z"
        },
        {
          "url": "https://docs.ar.io/ants-on-bazar",
          "title": "Our vision is an internet with no more 404s but it looks like you just found one",
          "content": "404s suck!Our vision is an internet with no more 404s, but it looks like you just found one.Rest assured that if this page ever existed, it is still available on the permaweb.Back to Home",
          "estimatedWords": 34,
          "lastModified": "2025-06-27T16:13:00.241Z",
          "siteKey": "ario",
          "siteName": "AR-IO Network",
          "depth": 3,
          "crawledAt": "2025-06-27T16:13:00.241Z"
        },
        {
          "url": "https://docs.ar.io/gql",
          "title": "Our vision is an internet with no more 404s but it looks like you just found one",
          "content": "404s suck!Our vision is an internet with no more 404s, but it looks like you just found one.Rest assured that if this page ever existed, it is still available on the permaweb.Back to Home",
          "estimatedWords": 34,
          "lastModified": "2025-06-27T16:13:00.580Z",
          "siteKey": "ario",
          "siteName": "AR-IO Network",
          "depth": 3,
          "crawledAt": "2025-06-27T16:13:00.580Z"
        },
        {
          "url": "https://docs.ar.io/guides",
          "title": "Quick Start Guides",
          "content": "Quick Start Guides Deploy a dApp with ArlinkEasily deploy a web app with ArNS using Arlinkmake your ArNS name tradable on BazarHow to display and trade your ArNS ANTs on BazarBuild a dApp using the ArNext frameworkBuild and deploy a dApp for displaying and updating ArNS names using ArNextAutomate deployment using Permaweb DeployAutomate the deployment of your dApp to ArNS using Permaweb Deploy, ArNS, and GithubManaging UndernamesHow to programmatically manage ArNS undernames using the AR.IO SDKQuery data on Arweave using GraphQLSchema and best practices for constructing a GraphQL queryDeploy a dApp with ArDrive webHow to upload a dApp to the permaweb using ArDrive webWas this page helpful?YesNoComment",
          "estimatedWords": 107,
          "lastModified": "2025-06-27T16:13:00.978Z",
          "siteKey": "ario",
          "siteName": "AR-IO Network",
          "depth": 2,
          "crawledAt": "2025-06-27T16:13:00.978Z"
        },
        {
          "url": "https://docs.ar.io/guides/testnet",
          "title": "Testnet",
          "content": "Testnet The AR.IO Network Testnet allows developers to test their applications and workflows using ARIO Network features such as ArNS Names before deploying to the mainnet. The ARIO Network Testnet offers a faucet for requesting testnet ARIO tokens (tARIO). The initial version of testnet only supports registering and resolving temporary ArNS names; however, enhancements such as temporary data uploads will be added in the future. We welcome feedback for improvements and other feature requests. Faucet Browser UI The ARIO Network Testnet Faucet is a service that allows developers to request testnet ARIO tokens (tARIO). It can be accessed in a browser by visiting ar://faucet. This is the recommended way to use the faucet. To use it: Select Testnet from the network dropdown Enter your wallet address Enter the an amount of tARIO tokens (max 10000) Complete the captcha challenge Click the \"Request Tokens\" button Onece complete, tARIO tokens will automatically be sent to your wallet Using Testnet Using the testnet is similar to using the mainnet, with a few key differences: Using the ARIO SDK When using the ARIO SDK, to interact with the AR.IO testnet - you can create your ARIO instance in one of two ways; Using the ARIO.tesntet() API import { ARIO } from '@ar.io/sdk' // uses cu.ardrive.io by default, and supports `faucet` APIs const testnet = ARIO.testnet() CopyCopied! By default, this instance will leverage cu.ardrive.io for process evaluation and the recommended way to interact with testnet. Using process with ARIO_TESTNET_PROCESS_ID import { ARIO, ARIO_TESTNET_PROCESS_ID } from '@ar.io/sdk' const ario = ARIO.init({ processID: ARIO_TESTNET_PROCESS_ID, // optionally provide your own CU_URL }) CopyCopied! By default, this instance will leverage community CUs managed by forward. Note: ANTs are network-agnostic, so no additional configuration is needed when working with them. Once configured, all SDK methods will operate on testnet instead of mainnet. For more details on configuration, see the ARIO Configuration documentation. Accessing ArNS Names To access ArNS names on testnet in a browser, you must use a gateway that is configured to operate on testnet instead of mainnet. The gateway ar-io.dev is configured to operate on the ARIO Network Testnet. Using arns.app with Testnet arns.app is the primary graphical dApp for purchasing and managing ArNS names. To configure arns.app to operate on testnet: Click the Connect button in the top right corner to connect your wallet After connecting, click on your user profile button (which replaces the Connect button) Go to Settings Click on ArNS Registry Settings On the right side of the screen, you'll see three buttons: Devnet, Testnet, and Mainnet Click on Testnet to switch the app to operate on the testnet The app will now operate on testnet, allowing you to purchase and manage ArNS names using testnet tokens. Running your own Gateway with testnet In addition to ar-io.dev - you can also elect to run your own ARIO gateway that resolves names against testnet. To do so, you need to setup your gateway by following the steps in the Linux Setup Guide or the Windows Setup Guide. Once running, modify the .env to point ARIO testnet process id. // .env # ...all other env configs IO_PROCESS_ID=agYcCFJtrMG6cqMuZfskIkFTGvUPddICmtQSBIoPdiA CopyCopied! Once set, restart your gateway and navigate to <your-gateway-url>/ar-io/info - you should see agYcCFJtrMG6cqMuZfskIkFTGvUPddICmtQSBIoPdiA as the process id. Your gateway will now resolve arns names stored on the ARIO tesntet process. Restrictions Testnet has a few primary purposes: to mimic mainnet functionality as close as possible, to provide a testing bed for upcoming network upgrades, and to provide a playground for users and developers to experiment. It is NOT intended for production purposes and should not be used as such. Test ARIO (tARIO) tokens are just that - test tokens. They have no external value, may break, and have no guarantee of continued support. tARIO tokens have no relation to mainnet $ARIO and are not a proxy for any rewards. There is no supply cap on tARIO tokens. While advanced notice will be provided whenever possible, testnet may go offline for maintenance. Likewise, test token balances and test ArNS names may be reset/nullified at any point to clean up the contract state or prepare for an upgrade. Advanced Integrating AR.IO Testnet in your client-side applications If you'd like to incorporate the AR.IO faucet into your application, you can programmatically retrieve access tokens - which allow your application to request testnet tokens for your users. To integrate: import { ARIO, ARIOToken } from '@ar.io/sdk' // setup testnet client; const testnet = ARIO.testnet() // request the captcha URL for the token, which will require a human to solve const captchaURL = await testnet.faucet.captchaURL() // open the captcha URL in a browser; const captchaWindow = window.open( captchaUrl.captchaUrl, '_blank', 'width=600,height=600', ) // The captcha URL includes a window.parent.postMessage event that is used to send the auth token to the parent window. // You can store the auth token in localStorage and use it to claim tokens for the duration of the auth token's expiration (default 1 hour). window.parent.addEventListener('message', async (event) => { if (event.data.type === 'ario-jwt-success') { localStorage.setItem('ario-jwt', event.data.token) localStorage.setItem('ario-jwt-expires-at', event.data.expiresAt) // close our captcha window captchaWindow?.close() // claim the tokens using the JWT token, const res = await testnet.faucet .claimWithAuthToken({ authToken: event.data.token, recipient: await window.arweaveWallet.getActiveAddress(), quantity: new ARIOToken(100).toMARIO().valueOf(), // 100 ARIO }) .then((res) => { alert('Successfully claimed 100 ARIO tokens! Transaction ID: ' + res.id) }) .catch((err) => { alert(`Failed to claim tokens: ${err}`) }) } }) // you can re-use the JWT for up to 1 hour, allowing you to request tokens for multiple wallets without having to satisfy the catpcha multiple times if ( localStorage.getItem('ario-jwt-expires-at') && Date.now() < parseInt(localStorage.getItem('ario-jwt-expires-at') ?? '0') ) { const res = await testnet.faucet.claimWithAuthToken({ authToken: localStorage.getItem('ario-jwt') ?? '', recipient: await window.arweaveWallet.getActiveAddress(), quantity: new ARIOToken(100).toMARIO().valueOf(), // 100 ARIO }) } CopyCopied!Was this page helpful?YesNoComment",
          "estimatedWords": 958,
          "lastModified": "2025-06-27T16:13:01.144Z",
          "siteKey": "ario",
          "siteName": "AR-IO Network",
          "depth": 3,
          "crawledAt": "2025-06-27T16:13:01.144Z"
        },
        {
          "url": "https://docs.ar.io/ar-io-sdk/ario/configuration",
          "title": "init",
          "content": "init init is a factory function that creates a read-only or writable client. By providing a signer, additional write APIs that require signing (like buyRecord and transfer) become available. By default, a read-only client is returned and no write APIs are available. Parameters ← Swipe to see more →ParameterTypeDescriptionOptionalprocessAOProcessA pre-configured AOProcess instance used to initialize the ARIO classtrueprocessIdstringThe process ID of the AO processtruesignerContractSignerAn optional signer instance, used to enable write operations on the blockchaintrue← Swipe to see more → Examples initNodeJS - unauthorizedNodeJS - authorizedWeb - unauthorizedWeb - authorizedCustom AOconst { ARIO } = require(\"@ar.io/sdk\") const ario = ARIO.init() CopyCopied!Was this page helpful?YesNoComment",
          "estimatedWords": 104,
          "lastModified": "2025-06-27T16:13:01.527Z",
          "siteKey": "ario",
          "siteName": "AR-IO Network",
          "depth": 4,
          "crawledAt": "2025-06-27T16:13:01.527Z"
        },
        {
          "url": "https://docs.ar.io/guides/gql",
          "title": "GraphQL",
          "content": "GraphQL Overview GraphQL is a powerful query language designed for modern web applications to efficiently fetch data. It enables precise queries, allowing users to specify exactly which data they need and in what format, significantly reducing the amount of unnecessary data transferred. This approach is ideal for dealing with complex systems and large datasets, as it minimizes bandwidth and improves performance. GraphQL operates through a single endpoint, streamlining the way applications communicate with databases. The integration of GraphQL with Arweave introduces a refined method for interacting with decentralized data storage. Arweave allows for the tagging of uploaded data, facilitating enhanced searchability and retrievability within its blockchain network. Utilizing GraphQL, users can perform targeted queries that leverage these tags, ensuring the retrieval of specific data swiftly and efficiently. This capability is particularly beneficial for the development of decentralized applications (dApps), the archival of content in a permanent and unalterable form, and the establishment of data marketplaces where precision and efficiency in data access are paramount. Together, GraphQL and Arweave form a compelling combination, offering developers and users a robust framework for managing and querying data in a decentralized environment. This integration not only promotes the efficient and scalable retrieval of data but also supports the creation of more sophisticated and data-intensive applications on the decentralized web, maintaining a balance between technical depth and accessibility. Constructing a Query Basic Syntax In GraphQL, you start with a root field and use braces to outline the fields you want to retrieve, allowing for precise, hierarchical data requests. For instance: { transactions { edges { node { id tags { name value } } } } } CopyCopied! This query demonstrates fetching transactions and their tags, illustrating the hierarchical nature of GraphQL queries. Customizing Searches with Tags Arweave utilizes a tagging system for transactions, enabling intricate search capabilities. You can filter queries using these tags: { transactions(tags: [{name: \"App-Name\", values: \"YourAppName\"}]) { edges { node { id data { size type } } } } } CopyCopied! This example filters transactions by a specific application name, and returns the id, size, and type of the transaction, showcasing how to customize queries for targeted data retrieval. NOTE: Tags are not the only option for filtering results, but are extremely useful due to the ability to add custom tags during the upload process. Understanding Edges and Nodes In the realm of GraphQL queries, especially when interfacing with Arweave, grasping the concept of edges and nodes is pivotal for constructing efficient and effective queries. This structure is not unique to Arweave but is particularly relevant due to the decentralized and interconnected nature of the data stored on its blockchain. Nodes: At the heart of GraphQL's query structure, nodes represent individual data points or entities. In the context of Arweave, a node could be a transaction, a block, or any piece of data stored within the network. Nodes are the primary targets of your query, containing the data you wish to retrieve, such as transaction IDs, tags, or the content of data transactions. Edges: Serving as the glue between nodes, edges are constructs that outline the relationship between different nodes. They can contain metadata about the connection, such as the nature of the relationship or additional attributes that describe how nodes are linked. In many GraphQL implementations, including those that interact with Arweave, edges are used to navigate through collections of related data, making them crucial for understanding the data's structure and lineage. This hierarchical model is especially useful for querying complex and relational data sets, allowing for detailed navigation and efficient data retrieval within Arweave's decentralized storage system. By effectively utilizing the edges and nodes structure, you can precisely target the data you need, whether it's filtering transactions by tags, fetching related transactions, or exploring the blockchain's structure. Pagination To add pagination to your GraphQL queries, you can use the first, last, before, and after parameters. These parameters control the slice of data you're querying, making data retrieval more efficient and manageable. first: Specify the number of items to retrieve from the start of the list or dataset. last: Specify the number of items to retrieve from the end of the list or dataset. { transactions(first: 10) { edges { node { id } } } } CopyCopied! This query fetches the first 10 transactions. To navigate through your dataset, you can use after and before in conjunction with first or last. These parameters accept cursors, which are typically provided in the response of your initial query. after: Fetch items after the specified cursor, used with first. before: Fetch items before the specified cursor, used with last. { transactions(first: 10, after: \"cursorOfLastItem\") { edges { node { id } } } } CopyCopied! This query fetches the next 10 transactions following the transaction with the cursor \"cursorOfLastItem\". If no pagination terms are set, GraphQL servers may apply default limits to prevent excessively large datasets from being returned in a single query, potentially impacting performance. The default behavior can vary based on the server's configuration but often involves returning a predefined maximum number of items. For instance, without specifying first or last, a query to the transactions field might return the first 5-10 transactions by default, depending on the server settings. This behavior ensures that server resources are not overwhelmed by large requests and that client applications receive data in manageable chunks. General Tips for Optimizing Queries To optimize your GraphQL queries in Arweave, follow these general guidelines: Specificity: Query with the most precise tags possible to narrow the search scope and enhance performance. Minimalism: Limit your query to the essential set of tags to reduce processing time and data transfer. Schema Design: Design your app's schema to reflect query patterns, possibly introducing tags that encapsulate frequent combinations of criteria. Include Non-tag Fields: Adding fields like owner can refine your search, making your queries more efficient. Order Your Tags: Arrange tags from most specific to most general to leverage Arweave's indexing more effectively. By incorporating these strategies, developers can achieve faster and more precise data access from Arweave, enhancing the performance and responsiveness of decentralized applications. This balanced approach to query construction and optimization is key to navigating the expansive and decentralized storage landscape Arweave provides. Making a Query Executing GraphQL queries within the Arweave ecosystem offers flexibility and multiple avenues for developers and users alike. Whether you prefer a hands-on, manual approach to constructing and testing queries, or you aim for automation and integration within your applications, Arweave provides the tools necessary to interact with its decentralized data storage seamlessly. GraphQL Playground For those new to GraphQL or seeking to fine-tune their queries before implementation, the GraphQL playground offers an invaluable resource. This interactive interface allows users to manually construct queries, explore the schema, and immediately see the results of their queries. Accessible via web browsers, the playground can be found at the /graphql endpoint of most Arweave indexing services, such as https://arweave.dev/graphql. Here, you can experiment with different queries, understand the structure of the data, and refine your approach without writing a single line of code in your application. Steps for Accessing the GraphQL Playground: Navigate to https://arweave.dev/graphql, or the graphql endpoint of any AR.IO gateway, in your web browser. Enter your GraphQL query in the provided interface. Press the \"play\" button to execute the query to see real-time results and debug as needed. Using an API For application development and automation, making GraphQL queries programmatically is essential. You can send POST requests directly to the GraphQL endpoint of any indexing service that supports it, such as arweave.net or any AR.IO gateway. These requests should contain your query in the body, allowing for dynamic and automated data retrieval within your application. When selecting an indexing service, consider the data coverage and reliability of the gateway to ensure it meets your application's needs. Different gateways might have varying degrees of indexed data available, so choosing one that is consistently up-to-date and comprehensive is key. Example of making a programmatic query: const axios = require('axios'); const query = { query: ` { transactions(tags: [{name: \"App-Name\", values: \"YourAppName\"}]) { edges { node { id tags { name value } } } } } ` }; axios.post('https://arweave.net/graphql', query, { headers: { 'Content-Type': 'application/json' }, }) .then(response => console.log(response.data)) .catch(error => console.error('Error:', error)); CopyCopied! Using an SDK For an even more integrated experience, some Software Development Kits (SDKs) offer direct methods for executing GraphQL queries. The Arweave SDK, for example, provides built-in functionalities to interact with the blockchain, simplifying the process of making queries. By leveraging these SDKs, developers can bypass the intricacies of manual HTTP request construction, focusing instead on the logic and design of their applications. Example of using the Arweave SDK for GraphQL queries: // Assuming the Arweave SDK is already set up and initialized const query = { query: ` { transactions(tags: [{name: \"App-Name\", values: \"YourAppName\"}]) { edges { node { id tags { name value } } } } } ` }; arweave.api.post('/graphql', query) .then(response => { console.log(response.data); }) .catch(error => { console.error('Error:', error); }); CopyCopied!Was this page helpful?YesNoComment",
          "estimatedWords": 1512,
          "lastModified": "2025-06-27T16:13:01.712Z",
          "siteKey": "ario",
          "siteName": "AR-IO Network",
          "depth": 3,
          "crawledAt": "2025-06-27T16:13:01.712Z"
        },
        {
          "url": "https://docs.ar.io/guides/managing-undernames",
          "title": "Managing Undernames",
          "content": "Managing Undernames Overview ArNS undernames are subdomains of top level ArNS domains. They are separated from the main ArNS domain using an underscore \"_\" in place of the more typically used dot \".\". Records for undernames can be set using the setRecord method on the AR.IO SDK, or removed by using the removeRecord method. The process for setting/removing a record for an undername vs. a top level ArNS domain is nearly identical, the only difference being the undername parameter. When managing a record on a top level ArNS domain, this must be set to @, while updates to an undername should provide the undername being updated. Chaining UndernamesUndernames can be created on other undernames, for example ar://og_logo_ardrive. In this example the undername og exists under the undername logo on the ArNS name ardrive.For the purpose of the undername parameter in the AR.IO SDK, this should be written as a single undername, including the separating underscores:og_logo Creating an Undername There are no special steps required to create an undername (provided the selected ArNS name has available undername space). Simply setting a record for an undername that does not exist will create the undername. Create an UndernameNodeJSWeb const fs = require(\"fs\"); const { ANT, ArweaveSigner } = require(\"@ar.io/sdk\"); async function main() { const jwk = JSON.parse(fs.readFileSync(\"KeyFile.json\")); const ant = ANT.init({ signer: new ArweaveSigner(jwk), processId: \"bh9l1cy0aksiL_x9M359faGzM_yjralacHIUo8_nQXM\" }); const { id: txId } = await ant.setRecord( { subDomain: 'brand-new-undername', transactionId: '432l1cy0aksiL_x9M359faGzM_yjralacHIUo8_nQXM', ttlSeconds: 3600 }, // optional additional tags { tags: [{ name: 'App-Name', value: 'My-Awesome-App' }] }, ); } main(); CopyCopied! Updating an Undername If an undername already exists, its record can easily be updated using the same setRecord method. Update an UndernameNodeJSWeb const fs = require(\"fs\"); const { ANT, ArweaveSigner } = require(\"@ar.io/sdk\"); async function main() { const jwk = JSON.parse(fs.readFileSync(\"KeyFile.json\")); const ant = ANT.init({ signer: new ArweaveSigner(jwk), processId: \"bh9l1cy0aksiL_x9M359faGzM_yjralacHIUo8_nQXM\" }); const { id: txId } = await ant.setRecord( { subDomain: 'undername-to-update', transactionId: '432l1cy0aksiL_x9M359faGzM_yjralacHIUo8_nQXM', ttlSeconds: 3600 }, // optional additional tags { tags: [{ name: 'App-Name', value: 'My-Awesome-App' }] }, ); } main(); CopyCopied! Removing an Undername An existing undername can be removed by using the removeRecord method on the AR.IO SDK. The undername parameter should be set to the undername being removed. Remove UndernameNodeJSWeb const fs = require(\"fs\"); const { ANT, ArweaveSigner } = require(\"@ar.io/sdk\"); async function main() { const jwk = JSON.parse(fs.readFileSync(\"KeyFile.json\")); const ant = ANT.init({ signer: new ArweaveSigner(jwk), processId: \"bh9l1cy0aksiL_x9M359faGzM_yjralacHIUo8_nQXM\" }); const { id: txId } = await ant.removeRecord( { undername: 'remove-domain', }, // optional additional tags { tags: [{ name: 'App-Name', value: 'My-Awesome-App' }] }, ); } main(); CopyCopied! Increasing Undername Support By default, ArNS names support up to 10 undernames. This number can be increased, for a fee. This is done using the increaseUndernameLimit method on the ARIO class of the AR.IO SDK, rather than the ANT class. The quantity (qty) parameter specifies the number of ADDITIONAL undernames to be supported. i.e. increasing from 10 undernames to 15 would require the qty parameter set to 5. Increasing Undername SupportNodeJSWeb const fs = require(\"fs\"); const { ARIO, ArweaveSigner, ARIOToken } = require(\"@ar.io/sdk\"); async function main() { const jwk = JSON.parse(fs.readFileSync(\"KeyFile.json\")); const ario = ARIO.init({ signer: new ArweaveSigner(jwk), }); const { id: txId } = await ario.increaseUndernameLimit( { name: 'ar-io', qty: 5, }, // optional additional tags { tags: [{ name: 'App-Name', value: 'My-Awesome-App' }] }, ); } main(); CopyCopied!Was this page helpful?YesNoComment",
          "estimatedWords": 563,
          "lastModified": "2025-06-27T16:13:02.104Z",
          "siteKey": "ario",
          "siteName": "AR-IO Network",
          "depth": 3,
          "crawledAt": "2025-06-27T16:13:02.104Z"
        },
        {
          "url": "https://docs.ar.io/ar-io-sdk/ants/set-record",
          "title": "setRecord",
          "content": "DeprecatedThis method is deprecated. Please use setBaseNameRecord for top-level names or setUndernameRecord for undernames instead. See the setBaseNameRecord and setUndernameRecord documentation for more details. setRecord setRecord is a method on the ANT class that sets or updates a record in the ANT process. Records map names to Arweave transaction IDs with optional TTL settings. setRecord requires authentication. Parameters ← Swipe to see more →ParameterTypeDescriptionOptionalundernamestringThe undername name for the record (use \"@\" for the root domain)falsetransactionIdstringThe Arweave transaction ID to point the record tofalsettlSecondsnumberTime-to-live in seconds for the record cachetruetagsarrayAn array of GQL tag objects to attach to the AO messagetrue← Swipe to see more → TTLTime-To-Live (TTL) determines how often gateways should check the ANT for updates to the corresponding record. You can have different TTLs for different records within an ANT, depending on their use case. A record that is updated frequently should have a lower value to facilitate serving current data, while a record that is updated less often should have a higher value to allow cached data to be served more quickly.TTL must be between 60 seconds (1 minute) and 86400 seconds (1 day). Examples setRecordNodeJSWeb const fs = require(\"fs\"); const { ANT, ArweaveSigner } = require(\"@ar.io/sdk\"); async function main() { const jwk = JSON.parse(fs.readFileSync(\"KeyFile.json\")); const ant = ANT.init({ signer: new ArweaveSigner(jwk), processId: \"bh9l1cy0aksiL_x9M359faGzM_yjralacHIUo8_nQXM\" }); const { id: txId } = await ant.setRecord( { undername: \"@\", transactionId: \"UyC5P5qKPZaltMmmZAWdakhlDXsBF6qmyrbWYFchRTk\", ttlSeconds: 3600 }, // optional additional tags { tags: [{ name: 'App-Name', value: 'My-Awesome-App' }] }, ); console.log(txId); } main(); CopyCopied!Was this page helpful?YesNoComment",
          "estimatedWords": 255,
          "lastModified": "2025-06-27T16:13:02.265Z",
          "siteKey": "ario",
          "siteName": "AR-IO Network",
          "depth": 4,
          "crawledAt": "2025-06-27T16:13:02.265Z"
        },
        {
          "url": "https://docs.ar.io/ar-io-sdk/ants/remove-record",
          "title": "removeRecord",
          "content": "DeprecatedThis method is deprecated. Please use removeUndernameRecord instead. See the removeUndernameRecord documentation for more details. removeRecord removeRecord is a method on the ANT class that removes a record from the ANT process. removeRecord requires authentication. Parameters ← Swipe to see more →ParameterTypeDescriptionOptionalundernamestringThe undername name of the record to removefalsetagsarrayAn array of GQL tag objects to attach to the AO messagetrue← Swipe to see more → Examples removeRecordNodeJSWeb const fs = require(\"fs\"); const { ANT, ArweaveSigner } = require(\"@ar.io/sdk\"); async function main() { const jwk = JSON.parse(fs.readFileSync(\"KeyFile.json\")); const ant = ANT.init({ signer: new ArweaveSigner(jwk), processId: \"bh9l1cy0aksiL_x9M359faGzM_yjralacHIUo8_nQXM\" }); const { id: txId } = await ant.removeRecord( { undername: \"blog\" }, // optional additional tags { tags: [{ name: 'App-Name', value: 'My-Awesome-App' }] }, ); console.log(txId); } main(); CopyCopied!Was this page helpful?YesNoComment",
          "estimatedWords": 129,
          "lastModified": "2025-06-27T16:13:02.673Z",
          "siteKey": "ario",
          "siteName": "AR-IO Network",
          "depth": 4,
          "crawledAt": "2025-06-27T16:13:02.673Z"
        },
        {
          "url": "https://docs.ar.io/ar-io-sdk/ario/arns/increase-undername-limit",
          "title": "increaseUndernameLimit",
          "content": "increaseUndernameLimit increaseUndernameLimit is a method on the ARIO class that increases the number of undernames an ArNS domain can support. Each domain starts with a default limit of 10 undernames and can be increased up to a maximum of 10,000 undernames. increaseUndernameLimit requires authentication. Parameters ← Swipe to see more →ParameterTypeDescriptionOptionalnamestringThe ArNS name to increase the undername limit forfalseincreaseCountnumberThe number of additional undername slots to purchase (up to 10,000 total)falsefundFromstringThe source of funds: 'balance', 'stakes', 'any', or 'turbo'truetagsarrayAn array of GQL tag objects to attach to the transfer AO messagetrue← Swipe to see more → Examples increaseUndernameLimitNodeJSWeb const fs = require(\"fs\"); const { ARIO, ArweaveSigner } = require(\"@ar.io/sdk\"); async function main() { const jwk = JSON.parse(fs.readFileSync(\"KeyFile.json\")); const ario = ARIO.init({ signer: new ArweaveSigner(jwk), }); // Increase limit by 90 slots (from default 10 to 100 total) const { id: txId } = await ario.increaseUndernameLimit( { name: 'ar-io', increaseCount: 90, }, // optional additional tags { tags: [{ name: 'App-Name', value: 'My-Awesome-App' }] } ); console.log('Transaction ID:', txId); } main(); CopyCopied!Was this page helpful?YesNoComment",
          "estimatedWords": 173,
          "lastModified": "2025-06-27T16:13:02.811Z",
          "siteKey": "ario",
          "siteName": "AR-IO Network",
          "depth": 4,
          "crawledAt": "2025-06-27T16:13:02.811Z"
        },
        {
          "url": "https://docs.ar.io/guides/primary-names",
          "title": "Managing Primary Names",
          "content": "Managing Primary Names Overview Primary names allow users to set a user-friendly alias for their Arweave wallet address, simplifying how addresses are displayed across applications. This process involves interaction between two separate smart contracts: The AR.IO Contract - which manages the primary name registry and requests The ANT Contract - which controls the specific ArNS name and must approve any primary name requests The process requires two steps because these are separate contracts: First, a request must be submitted to the AR.IO contract to set a specific ArNS name as the primary name for a wallet Then, the ANT owner must approve this request, confirming that this wallet can use the name as its primary identifier This two-step verification ensures that both the wallet owner and the ANT owner have authorized the connection. Think of this like setting a username on a social platform - where the platform (AR.IO contract) maintains the registry of usernames, and the name owner (ANT) must approve who can claim their name as an identifier. Setting a Primary Name with arns.app arns.app is the official ArNS portal from AR.IO. It allows you to manage your ArNS names and set primary names for your wallet addresses. To set a primary name using arns.app, connect your wallet and navigate to the ArNS name management page. Simply locate the ArNS name you want to set as primary and click the star icon at the right of the entry. You will then be prompted to accept the cost of setting the name, and the location of the funds to pay for the transaction. Once the transaction is confirmed, you will be prompted to sign the transaction with your connected wallet. When this is completed, the name will be set as primary for your wallet address, and apps that support primary names will display the name instead of the wallet address. Setting a Primary Name With the AR.IO SDK The process of setting a primary name using the AR.IO SDK involves two steps: requesting and approval. This two-step process ensures proper authorization from both the wallet owner and the ANT owner. Requesting a Primary Name When requesting a primary name, you're asking to use an ArNS name as the identifier for your wallet address. This requires: The ArNS name to exist Your wallet to submit the request using the requestPrimaryName method The ANT owner's approval Request Primary NameNodeJSWebconst fs = require('fs') const { ARIO, ArweaveSigner } = require('@ar.io/sdk') async function main() { const jwk = JSON.parse(fs.readFileSync('KeyFile.json')) const ario = ARIO.init({ signer: new ArweaveSigner(jwk), }) const { id: txId } = await ario.requestPrimaryName({ name: 'my-arns-name', processId: 'ANT-PROCESS-ID', }) console.log(`Primary name request submitted: ${txId}`) } main() CopyCopied! Check Primary Name Requests The getPrimaryNameRequest method allows you to verify if a primary name request exists and its status. Use this to: Verify if your request is pending Check if someone has requested to use your ANT's name Build UI flows around the request/approval process Get Primary Name RequestNodeJSWebconst { ARIO } = require('@ar.io/sdk') async function main() { const ario = ARIO.init() const request = await ario.getPrimaryNameRequest({ initiator: 'WALLET-ADDRESS', // The wallet address that made the request }) console.log(request) // Example response: // { // \"initiator\": \"WALLET-ADDRESS\", // \"name\": \"arns\", // \"startTimestamp\": 1728067635857, // \"endTimestamp\": 1735843635857 // } } main() CopyCopied! Approving a Primary Name Request The ANT owner must approve any requests to use their name as a primary name using the approvePrimaryNameRequest method. This gives ANT owners control over how their names are used as identifiers. Approve Primary NameNodeJSWebconst fs = require('fs') const { ANT, ArweaveSigner } = require('@ar.io/sdk') async function main() { const jwk = JSON.parse(fs.readFileSync('KeyFile.json')) const ant = ANT.init({ signer: new ArweaveSigner(jwk), processId: 'ANT-PROCESS-ID', }) const { id: txId } = await ant.approvePrimaryNameRequest({ name: 'my-arns-name', address: 'WALLET-ADDRESS', ioProcessId: 'ARIO-PROCESS-ID', }) console.log(`Primary name request approved: ${txId}`) } main() CopyCopied! Querying Primary Names The AR.IO SDK provides several methods to query primary names, each serving different use cases: Get a Single Primary Name Use getPrimaryName when you need to find the primary name for a specific wallet address. This is particularly useful in applications where you want to display a user-friendly identifier instead of their wallet address. Common use cases: Displaying a user's primary name in a profile or dashboard Showing who authored a piece of content Making transaction histories more readable Get Primary NameNodeJSWebconst { ARIO } = require('@ar.io/sdk') async function main() { const ario = ARIO.init() const primaryName = await ario.getPrimaryName({ name: 'my-arns-name', }) console.log(primaryName) } main() CopyCopied! List All Primary Names Use getPrimaryNames when fetching all primary names. This is useful when you need to: Build a directory of users Create search functionality Display multiple users in a more readable format Map multiple wallet addresses to their friendly names at once The method supports pagination through a cursor-based system, where the cursor is the last name from your previous request. Get Primary NamesNodeJSWebconst { ARIO } = require('@ar.io/sdk') async function main() { const ario = ARIO.init() // First page of results const firstPage = await ario.getPrimaryNames({ limit: 10, sortBy: 'startTimestamp', sortOrder: 'desc', }) console.log('First page:', firstPage.items) if (firstPage.hasMore) { // Get next page using the cursor from previous response const nextPage = await ario.getPrimaryNames({ cursor: firstPage.cursor, // Last name from previous request limit: 10, sortBy: 'startTimestamp', sortOrder: 'desc', }) console.log('Next page:', nextPage.items) } } main() CopyCopied! The response includes: items: Array of primary names for the current page cursor: The last name from the current request, used for getting the next page hasMore: Boolean indicating if there are more results available totalItems: Total number of primary names matching your query Best Practices Always verify ownership of both the ArNS name and ANT before attempting to set a primary name Check if a primary name request already exists before submitting a new one Consider implementing error handling for cases where the name or ANT doesn't exist When displaying primary names in your application, always have a fallback to show the wallet address if no primary name exists Primary names simplify user identification across the permaweb by replacing complex wallet addresses with human-readable names, similar to how usernames work on social platforms.Was this page helpful?YesNoComment",
          "estimatedWords": 1024,
          "lastModified": "2025-06-27T16:13:03.256Z",
          "siteKey": "ario",
          "siteName": "AR-IO Network",
          "depth": 3,
          "crawledAt": "2025-06-27T16:13:03.256Z"
        },
        {
          "url": "https://docs.ar.io/ar-io-sdk/primary-names/request-primary-name",
          "title": "Our vision is an internet with no more 404s but it looks like you just found one",
          "content": "404s suck!Our vision is an internet with no more 404s, but it looks like you just found one.Rest assured that if this page ever existed, it is still available on the permaweb.Back to Home",
          "estimatedWords": 34,
          "lastModified": "2025-06-27T16:13:03.342Z",
          "siteKey": "ario",
          "siteName": "AR-IO Network",
          "depth": 4,
          "crawledAt": "2025-06-27T16:13:03.342Z"
        },
        {
          "url": "https://docs.ar.io/ar-io-sdk/primary-names/get-primary-name-request",
          "title": "Our vision is an internet with no more 404s but it looks like you just found one",
          "content": "404s suck!Our vision is an internet with no more 404s, but it looks like you just found one.Rest assured that if this page ever existed, it is still available on the permaweb.Back to Home",
          "estimatedWords": 34,
          "lastModified": "2025-06-27T16:13:03.801Z",
          "siteKey": "ario",
          "siteName": "AR-IO Network",
          "depth": 4,
          "crawledAt": "2025-06-27T16:13:03.801Z"
        },
        {
          "url": "https://docs.ar.io/ar-io-sdk/ants/approve-primary-name-request",
          "title": "approvePrimaryNameRequest",
          "content": "approvePrimaryNameRequest approvePrimaryNameRequest is a method on the ANT class that approves a primary name request for a given name or address. approvePrimaryNameRequest requires authentication. Parameters ← Swipe to see more →ParameterTypeDescriptionOptionalnamestringArNS name to approve as primary name.falseaddressstring - WalletAddressPublic wallet address that made the primary name request being approved.falseioProcessIdstringProcess Id of the ARIO contract.falsetagsarrayAn array of GQL tag objects to attach to the transfer AO message.true← Swipe to see more → Examples approvePrimaryNameRequestNodeJSWeb const fs = require(\"fs\"); const { ANT, ArweaveSigner } = require(\"@ar.io/sdk\"); async function main() { const jwk = JSON.parse(fs.readFileSync(\"KeyFile.json\")); const ant = ANT.init({ signer: new ArweaveSigner(jwk), processId: \"bh9l1cy0aksiL_x9M359faGzM_yjralacHIUo8_nQXM\" }); const { id: txId } = await ant.approvePrimaryNameRequest({ name: 'arns', address: 't4Xr0_J4Iurt7caNST02cMotaz2FIbWQ4Kbj616RHl3', // must match the request initiator address ioProcessId: ARIO_PROCESS_ID, // the ARIO process id to use for the request }); } main(); CopyCopied!Was this page helpful?YesNoComment",
          "estimatedWords": 140,
          "lastModified": "2025-06-27T16:13:03.893Z",
          "siteKey": "ario",
          "siteName": "AR-IO Network",
          "depth": 4,
          "crawledAt": "2025-06-27T16:13:03.893Z"
        },
        {
          "url": "https://docs.ar.io/ar-io-sdk/primary-names/get-primary-name",
          "title": "Our vision is an internet with no more 404s but it looks like you just found one",
          "content": "404s suck!Our vision is an internet with no more 404s, but it looks like you just found one.Rest assured that if this page ever existed, it is still available on the permaweb.Back to Home",
          "estimatedWords": 34,
          "lastModified": "2025-06-27T16:13:04.344Z",
          "siteKey": "ario",
          "siteName": "AR-IO Network",
          "depth": 4,
          "crawledAt": "2025-06-27T16:13:04.344Z"
        },
        {
          "url": "https://docs.ar.io/ar-io-sdk/primary-names/get-primary-names",
          "title": "Our vision is an internet with no more 404s but it looks like you just found one",
          "content": "404s suck!Our vision is an internet with no more 404s, but it looks like you just found one.Rest assured that if this page ever existed, it is still available on the permaweb.Back to Home",
          "estimatedWords": 34,
          "lastModified": "2025-06-27T16:13:04.431Z",
          "siteKey": "ario",
          "siteName": "AR-IO Network",
          "depth": 4,
          "crawledAt": "2025-06-27T16:13:04.431Z"
        },
        {
          "url": "https://docs.ar.io/guides/story",
          "title": "Registering Story Protocol IP Assets with Arweave Metadata using Turbo",
          "content": "Registering Story Protocol IP Assets with Arweave Metadata using Turbo Utilize the speed and reliability of ArDrive Turbo to store metadata for Story Protocol IP Assets permanently on Arweave. Story Protocol enables the registration and management of intellectual property (IP) on-chain. A crucial part of this process involves linking metadata to your IP Assets. While various storage solutions exist, Arweave offers permanent, decentralized storage, making it an ideal choice for valuable IP metadata. This guide demonstrates how to use the ArDrive Turbo SDK to efficiently upload IP Asset metadata to Arweave and register it with the Story Protocol TypeScript SDK. Prerequisites Before you begin, ensure you have the following: Node.js: Version 18 or later. Download from nodejs.org. npm/pnpm/yarn: A compatible package manager. Arweave Wallet: A wallet.json file. Generate one using tools like the Wander browser extension. Keep this file secure and do not commit it to version control. Turbo Credits: Your Arweave wallet must be funded with Turbo credits to pay for uploads. Top up at https://turbo-topup.com. Story Protocol Account: An Ethereum-compatible private key (WALLET_PRIVATE_KEY) and an RPC Provider URL (RPC_PROVIDER_URL) for the desired Story Protocol network (e.g., Aeneid testnet) stored in a .env file. TypeScript Environment: You'll need to execute TypeScript code, so make sure you have ts-node installed globally (npm install -g ts-node) or as a dev dependency. Setup 1. Install Dependencies First, set up a new project directory and install the necessary SDKs: mkdir story-arweave-project cd story-arweave-project CopyCopied! Then install the required dependencies: Installation Methodsnpmpnpmyarnnpm install --save @ardrive/turbo-sdk @story-protocol/core-sdk viem dotenv ts-node typescript CopyCopied! 2. Project Setup Create the following files in your project: .env file (in the project root): WALLET_PRIVATE_KEY=your_ethereum_private_key_without_0x_prefix RPC_PROVIDER_URL=your_ethereum_rpc_provider_url CopyCopied! Place your Arweave wallet.json file in the project root. Create a tsconfig.json file in the project root: { \"compilerOptions\": { \"target\": \"es2020\", \"module\": \"commonjs\", \"esModuleInterop\": true, \"forceConsistentCasingInFileNames\": true, \"strict\": true, \"skipLibCheck\": true } } CopyCopied! 3. Initialize SDK Clients Create a configuration file to set up and export both the Turbo and Story clients: import { TurboFactory, TurboAuthenticatedClient } from \"@ardrive/turbo-sdk\"; import { StoryClient, StoryConfig } from \"@story-protocol/core-sdk\"; import { http } from \"viem\"; import { Account, privateKeyToAccount, Address } from \"viem/accounts\"; import fs from 'fs'; import path from 'path'; import 'dotenv/config'; // --- Environment Variable Loading --- const privateKeyEnv = process.env.WALLET_PRIVATE_KEY; const rpcProviderUrlEnv = process.env.RPC_PROVIDER_URL; const walletPath = path.resolve(process.cwd(), 'wallet.json'); // Assumes wallet.json is in the project root // --- Validations --- if (!privateKeyEnv) { throw new Error(\"WALLET_PRIVATE_KEY is not set in the .env file\"); } if (!rpcProviderUrlEnv) { throw new Error(\"RPC_PROVIDER_URL is not set in the .env file\"); } if (!fs.existsSync(walletPath)) { throw new Error(`Arweave wallet file not found at ${walletPath}. Please ensure wallet.json exists in the project root.`); } // --- ArDrive Turbo Client Setup --- function parseWallet(filePath: string): any { try { const walletData = fs.readFileSync(filePath, 'utf8'); return JSON.parse(walletData); } catch (error) { console.error(`Error reading or parsing wallet file at ${filePath}:`, error); throw new Error(`Failed to load Arweave wallet. Ensure ${filePath} exists and is valid JSON.`); } } const arweaveWallet = parseWallet(walletPath); export const turboClient: TurboAuthenticatedClient = TurboFactory.authenticated({ privateKey: arweaveWallet, }); console.log(\"ArDrive Turbo Client initialized.\"); // --- Story Protocol Client Setup --- const storyPrivateKey: Address = `0x${privateKeyEnv}`; const storyAccount: Account = privateKeyToAccount(storyPrivateKey); const storyConfig: StoryConfig = { account: storyAccount, transport: http(rpcProviderUrlEnv), chainId: \"aeneid\", // Adjust chainId if necessary }; export const storyClient = StoryClient.newClient(storyConfig); console.log(\"Story Client initialized.\"); CopyCopied! Make sure to create the utils directory first: mkdir -p utils CopyCopied! Registering an IP Asset Now, let's create a script to register an IP asset. This involves three steps: Define metadata for the IP itself and the NFT representing ownership Upload metadata to Arweave using Turbo Register the IP on Story Protocol Create the following script file: import { storyClient, turboClient } from \"./utils/clients\"; import { createHash } from \"crypto\"; import { Address } from \"viem\"; import type { UploadResult } from \"@ardrive/turbo-sdk\"; // Helper function to upload JSON to Arweave via Turbo async function uploadJSONToArweave(jsonData: any, description: string): Promise<UploadResult> { const dataBuffer = Buffer.from(JSON.stringify(jsonData)); console.log(`Uploading ${description} (${dataBuffer.byteLength} bytes) to Arweave via Turbo...`); const tags = [ { name: \"Content-Type\", value: \"application/json\" }, { name: \"App-Name\", value: \"ArDrive-Story-Tutorial\" } // Example tag ]; try { // Use Turbo to upload the file buffer const result = await turboClient.uploadFile(dataBuffer, { tags }); console.log(`${description} uploaded successfully: Transaction ID ${result.id}`); return result; } catch (error) { console.error(`Error uploading ${description} to Arweave:`, error); throw new Error(`Arweave upload failed for ${description}.`); } } async function register() { // --- Step 1: Define IP Metadata --- const ipMetadata = { title: \"My Arweave-Powered IP\", description: \"An example IP asset with metadata stored permanently on Arweave via Turbo.\", // Add other required fields like image, creators, etc. // Example creator: creators: [ { name: \"Your Name/Org\", address: storyClient.account.address, contributionPercent: 100 }, ], }; console.log(\"IP Metadata defined.\"); const nftMetadata = { name: \"Ownership NFT for My Arweave IP\", description: \"This NFT represents ownership of the IP Asset whose metadata is on Arweave.\", // Add other fields like image }; console.log(\"NFT Metadata defined.\"); // --- Step 2: Upload Metadata to Arweave --- const ipUploadResult = await uploadJSONToArweave(ipMetadata, \"IP Metadata\"); const nftUploadResult = await uploadJSONToArweave(nftMetadata, \"NFT Metadata\"); // Use arweave.net URLs instead of ar:// protocol const ipMetadataArweaveURI = `https://arweave.net/${ipUploadResult.id}`; const nftMetadataArweaveURI = `https://arweave.net/${nftUploadResult.id}`; console.log(`IP Metadata Arweave URI: ${ipMetadataArweaveURI}`); console.log(`NFT Metadata Arweave URI: ${nftMetadataArweaveURI}`); // Calculate metadata hashes (required by Story Protocol) const ipMetadataHash = `0x${createHash(\"sha256\") .update(JSON.stringify(ipMetadata)) .digest(\"hex\")}`; const nftMetadataHash = `0x${createHash(\"sha256\") .update(JSON.stringify(nftMetadata)) .digest(\"hex\")}`; console.log(`IP Metadata Hash: ${ipMetadataHash}`); console.log(`NFT Metadata Hash: ${nftMetadataHash}`); // --- Step 3: Register IP on Story Protocol --- console.log(\"Registering IP Asset on Story Protocol...\"); // Choose an SPG NFT contract (Story Protocol Governed NFT) // Use a public testnet one or create your own (see Story docs) const spgNftContract: Address = \"0xc32A8a0FF3beDDDa58393d022aF433e78739FAbc\"; // Aeneid testnet example try { const response = await storyClient.ipAsset.mintAndRegisterIp({ spgNftContract: spgNftContract, ipMetadata: { ipMetadataURI: ipMetadataArweaveURI, // URI pointing to Arweave ipMetadataHash: ipMetadataHash as Address, // Content hash nftMetadataURI: nftMetadataArweaveURI, // URI pointing to Arweave nftMetadataHash: nftMetadataHash as Address // Content hash }, txOptions: { waitForTransaction: true }, // Wait for confirmation }); console.log( `Successfully registered IP Asset!` ); console.log(` Transaction Hash: ${response.txHash}`); console.log(` IP ID: ${response.ipId}`); console.log(` Story Explorer Link: https://aeneid.explorer.story.foundation/ipa/${response.ipId}`); // Adjust explorer link for different networks console.log(` IP Metadata (Arweave): ${ipMetadataArweaveURI}`); console.log(` NFT Metadata (Arweave): ${nftMetadataArweaveURI}`); } catch (error) { console.error(\"Error registering IP Asset on Story Protocol:\", error); } } // Execute the register function register().catch(console.error); CopyCopied! Run the Registration Script To execute the script and register your IP Asset: npx ts-node registerIpWithArweave.ts CopyCopied! This will: Upload your IP metadata to Arweave permanently Upload your NFT metadata to Arweave permanently Register an IP Asset on Story Protocol pointing to these Arweave URLs Minting License Tokens Once an IP Asset is registered, you can attach license terms and allow others to mint license tokens. Create a new script for this: import { storyClient } from \"./utils/clients\"; import { Address } from \"viem\"; // Assume these values are known for the IP Asset you want to license const LICENSOR_IP_ID: Address = \"0x...\"; // Replace with the actual IP ID of the asset const LICENSE_TERMS_ID: string = \"...\"; // Replace with the specific terms ID attached to the IP Asset const RECEIVER_ADDRESS: Address = \"0x...\"; // Address to receive the license token(s) async function mintLicense() { console.log(`Minting license token(s) for IP ID ${LICENSOR_IP_ID} under terms ${LICENSE_TERMS_ID}...`); try { const response = await storyClient.license.mintLicenseTokens({ licenseTermsId: LICENSE_TERMS_ID, licensorIpId: LICENSOR_IP_ID, receiver: RECEIVER_ADDRESS, amount: 1, // Number of license tokens to mint // Optional parameters: // maxMintingFee: BigInt(0), // Set if the terms have a fee; 0 disables check if no fee expected // maxRevenueShare: 100, // Default check for revenue share percentage txOptions: { waitForTransaction: true }, }); console.log( `Successfully minted license token(s)!` ); console.log(` Transaction Hash: ${response.txHash}`); console.log(` License Token ID(s): ${response.licenseTokenIds}`); } catch (error) { console.error(\"Error minting license token(s):\", error); } } // Execute the function (after updating the constants above) // mintLicense().catch(console.error); CopyCopied! Before running this script: Replace LICENSOR_IP_ID with the actual IP ID obtained from your registration Replace LICENSE_TERMS_ID with the ID of license terms attached to that IP Replace RECEIVER_ADDRESS with the address to receive the license token Uncomment the function call at the bottom Then run: npx ts-node mintLicense.ts CopyCopied! Registering Derivative IP Assets with Arweave Metadata Finally, let's create a script to register a derivative work based on an existing IP, also using Arweave for metadata storage: import { storyClient, turboClient } from \"./utils/clients\"; import { createHash } from \"crypto\"; import { Address } from \"viem\"; import type { UploadResult } from \"@ardrive/turbo-sdk\"; import { DerivativeData } from \"@story-protocol/core-sdk\"; // Helper function to upload JSON to Arweave via Turbo (same as in registerIpWithArweave.ts) async function uploadJSONToArweave(jsonData: any, description: string): Promise<UploadResult> { const dataBuffer = Buffer.from(JSON.stringify(jsonData)); console.log(`Uploading ${description} (${dataBuffer.byteLength} bytes) to Arweave via Turbo...`); const tags = [ { name: \"Content-Type\", value: \"application/json\" }, { name: \"App-Name\", value: \"ArDrive-Story-Tutorial\" } ]; try { const result = await turboClient.uploadFile(dataBuffer, { tags }); console.log(`${description} uploaded successfully: Transaction ID ${result.id}`); return result; } catch (error) { console.error(`Error uploading ${description} to Arweave:`, error); throw new Error(`Arweave upload failed for ${description}.`); } } // --- Information about the Parent IP and License --- const PARENT_IP_ID: Address = \"0x...\"; // Replace with the actual Parent IP ID const LICENSE_TERMS_ID: string = \"...\"; // Replace with the License Terms ID to derive under async function registerDerivative() { // --- Step 1: Define Derivative Metadata --- const derivativeIpMetadata = { title: \"My Derivative Work (Arweave Metadata)\", description: \"A remix/adaptation based on a parent IP, metadata on Arweave.\", // Add other required fields (image, creators matching the derivative creator, etc.) }; const derivativeNftMetadata = { name: \"Ownership NFT for My Derivative Work\", description: \"NFT for the derivative IP, metadata on Arweave.\", // Add other fields }; // --- Step 2: Upload Derivative Metadata to Arweave --- console.log(\"Uploading derivative metadata to Arweave via Turbo...\"); const derivIpUploadResult = await uploadJSONToArweave(derivativeIpMetadata, \"Derivative IP Metadata\"); const derivNftUploadResult = await uploadJSONToArweave(derivativeNftMetadata, \"Derivative NFT Metadata\"); // Use arweave.net URLs instead of ar:// protocol const derivIpMetadataArweaveURI = `https://arweave.net/${derivIpUploadResult.id}`; const derivNftMetadataArweaveURI = `https://arweave.net/${derivNftUploadResult.id}`; const derivIpMetadataHash = `0x${createHash(\"sha256\") .update(JSON.stringify(derivativeIpMetadata)) .digest(\"hex\")}`; const derivNftMetadataHash = `0x${createHash(\"sha256\") .update(JSON.stringify(derivativeNftMetadata)) .digest(\"hex\")}`; console.log(`Derivative IP Metadata Arweave URI: ${derivIpMetadataArweaveURI}`); console.log(`Derivative NFT Metadata Arweave URI: ${derivNftMetadataArweaveURI}`); // --- Step 3: Register Derivative on Story Protocol --- // Prepare Derivative Data for Story Protocol const derivData: DerivativeData = { parentIpIds: [PARENT_IP_ID], licenseTermsIds: [LICENSE_TERMS_ID], }; console.log(\"Registering Derivative IP Asset on Story Protocol...\"); // Use the same SPG NFT contract or your own const spgNftContract: Address = \"0xc32A8a0FF3beDDDa58393d022aF433e78739FAbc\"; // Aeneid testnet example try { const response = await storyClient.ipAsset.mintAndRegisterIpAndMakeDerivative({ spgNftContract: spgNftContract, derivData: derivData, // Link to parent IP and license terms ipMetadata: { // Metadata for the *new* derivative IP ipMetadataURI: derivIpMetadataArweaveURI, // Arweave URI ipMetadataHash: derivIpMetadataHash as Address, // Content hash nftMetadataURI: derivNftMetadataArweaveURI, // Arweave URI nftMetadataHash: derivNftMetadataHash as Address // Content hash }, txOptions: { waitForTransaction: true }, }); console.log( `Successfully registered Derivative IP Asset!` ); console.log(` Transaction Hash: ${response.txHash}`); console.log(` Derivative IP ID: ${response.ipId}`); console.log(` Derivative Token ID: ${response.tokenId}`); console.log(` Story Explorer Link: https://aeneid.explorer.story.foundation/ipa/${response.ipId}`); console.log(` Derivative Metadata (Arweave): ${derivIpMetadataArweaveURI}`); } catch (error) { console.error(\"Error registering derivative IP Asset on Story Protocol:\", error); } } // Before running this script: // 1. Replace PARENT_IP_ID with a real IP ID you have access to // 2. Replace LICENSE_TERMS_ID with the actual license terms ID // Then uncomment the line below to execute // registerDerivative().catch(console.error); CopyCopied! Before running this script: Replace PARENT_IP_ID with the actual parent IP ID Replace LICENSE_TERMS_ID with the license terms ID that permits derivatives Uncomment the function execution at the bottom Run: npx ts-node registerDerivativeWithArweave.ts CopyCopied! Conclusion By leveraging the ArDrive Turbo SDK, you can seamlessly integrate permanent Arweave storage into your Story Protocol workflow. Uploading metadata with Turbo ensures fast, reliable, and cost-effective data persistence for your valuable IP Assets, whether they are root IPs or complex derivatives with licensing relationships. This tutorial demonstrated a complete workflow: Setting up a project structure with all required dependencies Creating a utility module for client initialization Registering original IP Assets with metadata stored on Arweave Minting license tokens for IP Assets Creating and registering derivative works For further details on Story Protocol concepts like licensing, derivatives, or specific SDK functions, refer to the Story Protocol Documentation.Was this page helpful?YesNoComment",
          "estimatedWords": 2040,
          "lastModified": "2025-06-27T16:13:04.967Z",
          "siteKey": "ario",
          "siteName": "AR-IO Network",
          "depth": 3,
          "crawledAt": "2025-06-27T16:13:04.967Z"
        },
        {
          "url": "https://docs.ar.io/guides/ants-on-bazar",
          "title": "Trading ANTs on Bazar",
          "content": "Trading ANTs on Bazar Overview Arweave Name Tokens are Atomic Asset Spec compliant AO tokens that manage records and permission for ArNS names. Because the ANT spec is compliant with the Atomic Asset Spec, they are tradable on Bazar, which is a decentralized market place for Atomic Assets on AO. There are a few simple steps that are required in order to make an ANT available on Bazar to be traded. Bazar Profile Bazar relies on profiles for displaying user information and tradable assets. Profiles are AO processes that contain user specified information like a name, a nickname, and images associated with the profile. Profiles also track assets held by the profile in order to provide their information to bazar. Create a Profile If you do not already have a profile associated with your wallet, you can easily create one on using the \"Create your profile\" button on bazar after connecting your wallet: You will be prompted to add, at a minimum, a name and handle (nickname) to associate with the profile. These values can be changed later. Click \"Save\" at the bottom to finish creation of your profile. Once your profile is created, you can get its ao process Id at any time by clicking on the user icon in Bazar, and then the \"Copy profile address\" button from the menu. Transfer ANT to the Profile Bazar profiles only track assets that are held in the profile process, not in a user wallet. In order for an ANT to be displayed and transferred on Bazar, it must first be transferred into the Bazar profile. This can be done easily using arns.app in your manage page for a given name. Once an ANT is transferred into the profile process, it will automatically be detected and displayed by Bazar. It can be transferred or sold just like any other atomic asset on the marketplace, with no additional steps required. Restore Controllers OptionalThis is an optional step that will enable updating an ANT's Target Id without transferring it back into your wallet. This step may be safely skipped without affecting the ANT's functionality or tradability on Bazar. Transferring an ANT to a new wallet or AO process resets all authorized controllers, or non-owner entities that are allowed to update some settings on the ArNS name. It does not reset the Target Id that the ArNS name is pointing to. If you want to be able to update the Target ID and undernames from your wallet using arns.app, you will need to set your wallet address as a controller for the ANT while it is in your profile. The easiest way to do this is using aos. If you have not used aos before, you can find installation instructions here Using aos, you can log directly into your profile process with the command: aos <profile-address> --wallet \"/path/to/your/keyfile\" CopyCopied! Be sure to replace <profile-address> with the process Id for your profile process, and /path/to/your/keyfile with the path to the keyfile for the wallet you created the profile with. Once you are logged in with aos, you can send a message to the ANT in your profile to set your wallet as a controller: Send({ Target = \"<Ant-Process-ID>\", Action = \"Add-Controller\", Controller = \"<Wallet-Address>\" }) CopyCopied! Replace <Ant-Process-ID> with the process Id of the ANT you transferred into your profile, and <Wallet-Address> with your wallet address.Was this page helpful?YesNoComment",
          "estimatedWords": 564,
          "lastModified": "2025-06-27T16:13:05.000Z",
          "siteKey": "ario",
          "siteName": "AR-IO Network",
          "depth": 3,
          "crawledAt": "2025-06-27T16:13:05.000Z"
        },
        {
          "url": "https://docs.ar.io/guides/uploading-to-arweave",
          "title": "Uploading to Arweave",
          "content": "Uploading to Arweave Overview While AR.IO provides powerful tools for accessing and interacting with data on Arweave, that data must first be uploaded to the network. This guide will walk you through the process of uploading data to Arweave using the Turbo SDK, which provides a streamlined experience for data uploads. Installing Turbo SDK # For Node.js npm install @ardrive/turbo-sdk # For Yarn users yarn add @ardrive/turbo-sdk CopyCopied! Authentication Node.js Environment Node.js AuthenticationArweave JWKEthereumSolanaPolygonKYVEimport { TurboFactory } from '@ardrive/turbo-sdk' import fs from 'fs' // Load your Arweave JWK file const jwk = JSON.parse(fs.readFileSync('wallet.json', 'utf-8')) const turbo = await TurboFactory.authenticated({ privateKey: jwk, // ArweaveJWK type token: 'arweave', // Default token type }) CopyCopied! Browser Environment Browser Auth ExamplesArweave (Wander)Metamask (With Wagmi)Metamask (Ethers)Solana (Phantom)import { TurboFactory, ArConnectSigner } from '@ardrive/turbo-sdk/web' async function initializeTurbo() { await window.arweaveWallet.connect([ 'ACCESS_ADDRESS', 'ACCESS_PUBLIC_KEY', 'SIGN_TRANSACTIONS', 'SIGN_MESSAGE', 'SIGNATURE', ]) const turbo = await TurboFactory.authenticated({ signer: new ArConnectSigner(window.arweaveWallet), }) } CopyCopied! Purchasing Turbo Credits Turbo Credits are the payment medium used by the Turbo Upload Service. Each Credit represents a 1:1 conversion from the upload power of the Arweave native token (AR). Turbo Credits can be purchased with fiat currency via the Turbo Top Up App, or with supported cryptocurrencies via the Turbo SDK. Learn more about Turbo Credits and available methods for purchasing them here. Node.js Environment Purchasing Credits With Crypto: Node.jsArweaveEthereumSolanaPolygonKYVEEth on Baseimport { TurboFactory, WinstonToTokenAmount } from '@ardrive/turbo-sdk' // Initialize authenticated client const turbo = await TurboFactory.authenticated({ privateKey: jwk }) // Top up with AR tokens const topUpResult = await turbo.topUpWithTokens({ tokenAmount: WinstonToTokenAmount(100_000_000), // 0.0001 AR }) CopyCopied! Browser Environment In a browser environment, the topUpWithTokens method is not available. Instead, you'll need to manually send tokens to the Turbo wallet address and then submit the transaction for processing. Here are detailed examples for each supported chain: Browser Top-Up ExamplesArweave (Wander)Ethereum (MetaMask)Solana (Phantom)Polygon (MetaMask)Base (MetaMask)import { TurboFactory } from '@ardrive/turbo-sdk/web' import Arweave from 'arweave' import axios from 'axios' const TURBO_AR_ADDRESS = 'JNC6vBhjHY1EPwV3pEeNmrsgFMxH5d38_LHsZ7jful8' const AR_AMOUNT = 0.0001 // Amount in AR // Function to send AR and wait for confirmation const sendArToTurbo = async () => { if (!window.arweaveWallet) { throw new Error('Please install Wander') } // Initialize Arweave const arweave = Arweave.init({ host: 'arweave.net', port: 443, protocol: 'https', }) // Create transaction const transaction = await arweave.createTransaction({ target: TURBO_AR_ADDRESS, quantity: arweave.ar.arToWinston(AR_AMOUNT.toString()), }) // Sign and post transaction await window.arweaveWallet.sign(transaction) const response = await arweave.transactions.post(transaction) return transaction.id } // Function to submit transaction with retries const submitTransactionWithRetries = async (txId, maxRetries = 3) => { let retries = 0 while (retries < maxRetries) { try { const response = await turbo.submitFundTransaction({ txId }) return response } catch (error) { retries++ if (retries === maxRetries) throw error // Wait 30 seconds before retrying await new Promise((resolve) => setTimeout(resolve, 30000)) } } } // Complete top-up process const topUpWithAr = async () => { try { // Send AR and get transaction ID const txId = await sendArToTurbo() console.log('Transaction sent:', txId) // Wait 36 minutes for chain settlement await new Promise((resolve) => setTimeout(resolve, 36 * 60 * 1000)) // Submit transaction with retries const response = await submitTransactionWithRetries(txId) console.log('Credits added:', response) } catch (error) { console.error('Top-up failed:', error) } } CopyCopied! Note: The wait times for chain settlement are approximate and may need adjustment based on network conditions: Ethereum: ~15 minutes Solana: ~400-600 milliseconds Arweave: ~30-36 minutes Polygon: ~2-3 seconds Base: ~2-5 seconds KYVE: ~5 minutes Uploading Files and Folders Once you have purchased Turbo credits, you can upload files and folders to Arweave. The process is the same regardless of which token type you used for authentication, but differs between Node.js and browser environments. Node.js Environment Node.js Upload ExamplesSingle FileFolderimport { TurboFactory } from '@ardrive/turbo-sdk' import fs from 'fs' import path from 'path' import mime from 'mime-types' // Initialize authenticated client const turbo = await TurboFactory.authenticated({ privateKey: jwk }) // Function to upload a single file const uploadFile = async (filePath) => { try { // Get file info const fileSize = fs.statSync(filePath).size const mimeType = mime.lookup(filePath) || 'application/octet-stream' // Upload file const result = await turbo.uploadFile({ fileStreamFactory: () => fs.createReadStream(filePath), fileSizeFactory: () => fileSize, dataItemOpts: { tags: [ { name: 'Content-Type', value: mimeType, }, ], }, }) console.log('File uploaded!', { id: result.id, url: `https://arweave.net/${result.id}`, owner: result.owner, dataCaches: result.dataCaches, }) return result } catch (error) { console.error('Upload failed:', error) throw error } } // Example usage await uploadFile('./path/to/your/file.pdf') CopyCopied! Browser Environment Browser Upload ExamplesSingle FileFolderimport { TurboFactory } from '@ardrive/turbo-sdk/web' // Initialize authenticated client const turbo = await TurboFactory.authenticated({ signer }) // HTML input element <input type=\"file\" id=\"file-input\" accept=\"image/*,video/*,audio/*,.pdf,.txt\" /> // Function to upload a single file const uploadFile = async (file) => { try { const result = await turbo.uploadFile({ fileStreamFactory: () => file.stream(), fileSizeFactory: () => file.size, dataItemOpts: { tags: [ { name: 'Content-Type', value: file.type || 'application/octet-stream' } ] } }) console.log('File uploaded!', { id: result.id, url: `https://arweave.net/${result.id}`, owner: result.owner, dataCaches: result.dataCaches }) return result } catch (error) { console.error('Upload failed:', error) throw error } } // Example usage with file input const fileInput = document.getElementById('file-input') fileInput.addEventListener('change', async (event) => { const file = fileInput.files[0] if (!file) return await uploadFile(file) }) // Example usage with drag and drop const dropZone = document.getElementById('drop-zone') dropZone.addEventListener('dragover', (e) => { e.preventDefault() e.stopPropagation() dropZone.classList.add('drag-over') }) dropZone.addEventListener('dragleave', (e) => { e.preventDefault() e.stopPropagation() dropZone.classList.remove('drag-over') }) dropZone.addEventListener('drop', async (e) => { e.preventDefault() e.stopPropagation() const file = e.dataTransfer.files[0] if (!file) return await uploadFile(file) }) CopyCopied! Important Notes: For single file uploads, always include a Content-Type tag to ensure proper file viewing The fileStreamFactory must return a NEW stream each time it's called Folder uploads automatically detect and set Content-Type tags for all files You can specify additional tags in dataItemOpts for both file and folder uploads The maxConcurrentUploads option controls how many files are uploaded simultaneously Use throwOnFailure: true to ensure all files are uploaded successfully Complete Examples Here are complete examples showing how to authenticate, check balances, and handle lazy funding for uploads. These examples demonstrate the full workflow from start to finish. Node.js Environment Complete Node.js ExampleArweaveEthereumPolygonKYVEimport { TurboFactory } from '@ardrive/turbo-sdk' import fs from 'fs' import path from 'path' import mime from 'mime-types' import axios from 'axios' // Constants const FREE_UPLOAD_SIZE = 100 * 1024 // 100KB in bytes const PRICE_BUFFER = 1.1 // 10% buffer for price fluctuations // Initialize authenticated client const turbo = await TurboFactory.authenticated({ privateKey: jwk }) // Function to get token price from CoinGecko const getTokenPrice = async (token: string) => { const response = await axios.get( `https://api.coingecko.com/api/v3/simple/price?ids=${token}&vs_currencies=usd`, ) return response.data[token].usd } // Function to calculate required token amount const calculateTokenAmount = async (wincAmount: string, tokenType: string) => { // Get fiat rates for 1 GiB const fiatRates = await turbo.getFiatRates() const usdPerGiB = fiatRates.usd // Convert winc to GiB const wincPerGiB = 1_000_000_000_000 // 1 GiB in winc const requiredGiB = Number(wincAmount) / wincPerGiB const requiredUsd = requiredGiB * usdPerGiB // Get token price const tokenPrice = await getTokenPrice(tokenType) const tokenAmount = (requiredUsd / tokenPrice) * PRICE_BUFFER return tokenAmount } // Function to check balance and fund if needed const ensureSufficientBalance = async (fileSize: number, tokenType: string) => { // Check current balance const balance = await turbo.getBalance() const currentWinc = BigInt(balance.controlledWinc) // If file is under 100KB, it's free if (fileSize <= FREE_UPLOAD_SIZE) { return true } // Get upload cost const costs = await turbo.getUploadCosts({ bytes: [fileSize] }) const requiredWinc = BigInt(costs[0].winc) // If we have enough balance, return true if (currentWinc >= requiredWinc) { return true } // Calculate and purchase required tokens const tokenAmount = await calculateTokenAmount( requiredWinc.toString(), tokenType, ) // Top up with tokens await turbo.topUpWithTokens({ tokenAmount: tokenAmount, }) return true } // Function to upload a file const uploadFile = async (filePath: string) => { try { // Get file info const fileSize = fs.statSync(filePath).size const mimeType = mime.lookup(filePath) || 'application/octet-stream' // Ensure sufficient balance await ensureSufficientBalance(fileSize, 'arweave') // Upload file const result = await turbo.uploadFile({ fileStreamFactory: () => fs.createReadStream(filePath), fileSizeFactory: () => fileSize, dataItemOpts: { tags: [ { name: 'Content-Type', value: mimeType, }, ], }, }) console.log('File uploaded!', { id: result.id, url: `https://arweave.net/${result.id}`, owner: result.owner, dataCaches: result.dataCaches, }) return result } catch (error) { console.error('Upload failed:', error) throw error } } // Example usage await uploadFile('./path/to/your/file.pdf') CopyCopied! Browser Environment Complete Browser ExampleArweave (Wander)Ethereum (MetaMask)Polygon (MetaMask)KYVE (Keplr)Solana (Phantom)Base (MetaMask)import { TurboFactory } from '@ardrive/turbo-sdk/web' import Arweave from 'arweave' import axios from 'axios' // Constants const FREE_UPLOAD_SIZE = 100 * 1024 // 100KB in bytes const PRICE_BUFFER = 1.1 // 10% buffer for price fluctuations const TURBO_AR_ADDRESS = 'JNC6vBhjHY1EPwV3pEeNmrsgFMxH5d38_LHsZ7jful8' // Initialize authenticated client with Wander if (!window.arweaveWallet) { throw new Error('Please install Wander') } // Initialize Arweave const arweave = Arweave.init({ host: 'arweave.net', port: 443, protocol: 'https' }) const turbo = await TurboFactory.authenticated({ privateKey: window.arweaveWallet, token: 'arweave' }) // Function to get token price from CoinGecko const getTokenPrice = async (token: string) => { const response = await axios.get( `https://api.coingecko.com/api/v3/simple/price?ids=${token}&vs_currencies=usd` ) return response.data[token].usd } // Function to calculate required token amount const calculateTokenAmount = async ( wincAmount: string, tokenType: string ) => { // Get fiat rates for 1 GiB const fiatRates = await turbo.getFiatRates() const usdPerGiB = fiatRates.usd // Get winc cost for 1 GiB const costs = await turbo.getUploadCosts({ bytes: [1024 * 1024 * 1024] }) // 1 GiB in bytes const wincPerGiB = BigInt(costs[0].winc) // Calculate cost per winc in USD const usdPerWinc = Number(usdPerGiB) / Number(wincPerGiB) // Calculate required USD amount const requiredUsd = Number(wincAmount) * usdPerWinc // Get token price const tokenPrice = await getTokenPrice(tokenType) const tokenAmount = (requiredUsd / tokenPrice) * PRICE_BUFFER return tokenAmount } // Function to check balance and fund if needed const ensureSufficientBalance = async ( fileSize: number, tokenType: string ) => { // Check current balance const balance = await turbo.getBalance() const currentWinc = BigInt(balance.controlledWinc) // If file is under 100KB, it's free if (fileSize <= FREE_UPLOAD_SIZE) { return true } // Get upload cost const costs = await turbo.getUploadCosts({ bytes: [fileSize] }) const requiredWinc = BigInt(costs[0].winc) // If we have enough balance, return true if (currentWinc >= requiredWinc) { return true } // Calculate and purchase required tokens const tokenAmount = await calculateTokenAmount( requiredWinc.toString(), tokenType ) // Create transaction const transaction = await arweave.createTransaction({ target: TURBO_AR_ADDRESS, quantity: arweave.ar.arToWinston(tokenAmount.toString()) }) // Sign and post transaction await window.arweaveWallet.sign(transaction) await arweave.transactions.post(transaction) // Wait for confirmation (typically 30-36 minutes) await new Promise((resolve) => setTimeout(resolve, 36 * 60 * 1000)) // Submit transaction to Turbo await turbo.submitFundTransaction({ txId: transaction.id }) return true } // Function to upload a file const uploadFile = async (file: File) => { try { // Ensure sufficient balance await ensureSufficientBalance(file.size, 'arweave') // Upload file const result = await turbo.uploadFile({ fileStreamFactory: () => file.stream(), fileSizeFactory: () => file.size, dataItemOpts: { tags: [ { name: 'Content-Type', value: file.type || 'application/octet-stream' } ] } }) console.log('File uploaded!', { id: result.id, url: `https://arweave.net/${result.id}`, owner: result.owner, dataCaches: result.dataCaches }) return result } catch (error) { console.error('Upload failed:', error) throw error } } // HTML input element <input type=\"file\" id=\"file-input\" accept=\"image/*,video/*,audio/*,.pdf,.txt\" /> // Example usage with file input const fileInput = document.getElementById('file-input') fileInput.addEventListener('change', async (event) => { const file = fileInput.files[0] if (!file) return await uploadFile(file) }) // Example usage with drag and drop const dropZone = document.getElementById('drop-zone') dropZone.addEventListener('dragover', (e) => { e.preventDefault() e.stopPropagation() dropZone.classList.add('drag-over') }) dropZone.addEventListener('dragleave', (e) => { e.preventDefault() e.stopPropagation() dropZone.classList.remove('drag-over') }) dropZone.addEventListener('drop', async (e) => { e.preventDefault() e.stopPropagation() dropZone.classList.remove('drag-over') const file = e.dataTransfer.files[0] if (!file) return await uploadFile(file) }) CopyCopied!Was this page helpful?YesNoComment",
          "estimatedWords": 1900,
          "lastModified": "2025-06-27T16:13:05.620Z",
          "siteKey": "ario",
          "siteName": "AR-IO Network",
          "depth": 3,
          "crawledAt": "2025-06-27T16:13:05.620Z"
        },
        {
          "url": "https://docs.ar.io/build/guides/arlink",
          "title": "Arlink Deploy",
          "content": "Arlink Deploy Overview Arlink is a third party tool that allows you to permanently deploy and manage web apps on the permaweb with ease. How it works Users can link their Github or Protocol.land repositories to their Arlink account through the Arlink dashboard. When a new project or build is deployed, Arlink will take the repository, build it, and upload the build folder to Arweave. Arlink also allows users to connect their project to an ArNS name they own, or an undername of the ArNS name ar://arlink. Dashboard After connecting your wallet to the Arlink web app using the button at the top right, you will be taken to your dashboard. This page will display any deployments associated with your wallet, and includes a \"+ New Deployment\" button in order to start the process of deploying a new project. New Deployment After clicking on the new deployment button, you will be prompted to import a repository from either Github or Protocol.land. Authorize Github If this is your first time importing from Github, you will be prompted to authorize Arlink to access your Github repositories. You can authorize all repositories, or limit authorization to any number of specific ones. Select Repository Once authorization is approved, select which repository and branch you want to deploy. Define Build and Output Steps Once you select what you want to deploy, you need to specify how the project needs to be built to get it ready. Arlink prompts for five inputs: Project Name: This is the name of your project. Install Command: The command for installing dependencies for your project. Usually npm install or yarn install Build Command: This is the command to run your build script. Usually npm run build or yarn build Sub Directory: If the front end for your project lives in a sub directory of your selected repository, you can specify that here. Output Directory: This is the path to the build folder being deployed. This will be different depending on the framework your project uses. Select ArNS The last thing to do is select an ArNS name to deploy your project to. If you own your own name, you can connect to it here with the \"Use existing ArNS\" toggle. Otherwise, you can select an undername of the ArNS name arlink to deploy to. Duplicate undernames cannot exist, so you can only select an undername that is not already being used. Logs Once you select your ArNS name and click \"Deploy\", your project will be deployed. Logs from the build and deploy process will be displayed so you can monitor for errors. Updates To deploy a new build of your project, select it from the dashboard. The project page gives you the option to update any settings or configurations, and has a \"Deploy Latest\" button which will redeploy your project. Was this page helpful?YesNoComment",
          "estimatedWords": 474,
          "lastModified": "2025-06-27T16:13:05.661Z",
          "siteKey": "ario",
          "siteName": "AR-IO Network",
          "depth": 3,
          "crawledAt": "2025-06-27T16:13:05.661Z"
        },
        {
          "url": "https://docs.ar.io/guides/permaweb-deploy",
          "title": "Deploy a Website or Application",
          "content": "Deploy a Website or Application Overview With the growing popularity of permanently deployed apps, hosted on Arweave, along with the growing list of tools offered by AR.IO, several methods have been developed to automate the process of deploying a website and updating the ArNS name pointed at it. A particularly useful tool for this is permaweb-deploy from Forward Research. permaweb-deploy is a cli tool that handles uploading a build folder to Arweave using Turbo, creating a manifest, and then updating an ArNS name to point at the new manifest. It being a cli tool makes it very easy to incorporate into a github actions flow. Setting up an automated deployment with permaweb-deploy is simple, but does require a few steps. ENV SecurityBefore automating your deployments, be sure to build your app and check for exposed environmental secrets. Some app frameworks or build flows will build your app with the secrets exposed, and if you are using a tool like permaweb-deploy, those secrets will be uploaded to Arweave. Since the permaweb is permanent, this could pose a security risk, especially with a copy of your wallet keyfile required for the deployment automation. Getting Started Installing package permaweb-deploy is an npm package, and must be installed in any project before it can be used. If you are using npm, you can install the package with the below command: npm install permaweb-deploy --save-dev CopyCopied! If you prefer yarn for your package installations, the process is slightly more involved. permaweb-deploy is not designed for installation with yarn, so you must provide the additional argument ignore-engines in order to skip over the yarn version error you would normally get with installation. There are two methods for doing so: Directly in the install command yarn add permaweb-deploy -D --ignore-engines CopyCopied! In a .yarnc file You can provide a file, named .yarnc in the same directory as your package.json in order to assign specific instructions to all of your yarn commands. Creating a .yarnc file with the line ignore-engines true CopyCopied! will have the same effect as providing the flag directly in your yarn command Adding a Deploy Script The simplest way to utilize the permaweb-deploy tool is to build it into a script in your package.json. Here you will provide all of the variables that permaweb-deploy needs in order to function properly, as well as ensure that your app is statically built before being uploaded. \"scripts\": { \"build\": \"vuepress build src\", \"deploy\": \"npm run build && permaweb-deploy --deploy-folder ./src/.vuepress/dist --arns-name <YOUR_ARNS_NAME>\" }, CopyCopied! Be sure to replace <YOUR_ARNS_NAME> with the name of the ArNS name you want to deploy to. The above example shows a build script for a vuepress app, which will build the app into a static folder for deployment, and a deploy script which runs build and then permaweb-deploy. Your build script will look different depending on the framework you are using, but most will provide that for you when you create your app. The permaweb-deploy command has two required arguments: --deploy-folder This is the relative path (from your package.json) to the build folder you want to upload. In a vuepress app, that will be ./src/.vuepress/dist unless you manually specify otherwise in your vuepress configuration. It will be different depending on your chosen framework and if you have modified the default location. --arns-name This is the ArNS name you want to deploy to. It must be an ArNS name that the wallet used to authenticate has ownership or controller privileges over, otherwise the deployment will fail at authentication in the ao process that controls the ArNS name. UndernamesThe --arns-name flag MUST be the top level name, not and undername. That is, if you want to deploy to undername_arnsname you must set --arns-name arnsname and not --arns-name undername_arnsname.There is the additional, optional flag --undername. If you want to deploy your app to an undername on an ArNS name, provide that name with this flag.--arns-name arnsname --undername undername Testnet Permaweb-deploy supports both Mainnet and Testnet deployments. By default, it will deploy to Mainnet. To deploy to Testnet, you can provide the --ario-process flag as \"testnet\". If not provided, deployments will default to Mainnet. Providing Arweave Wallet Keys While using permaweb-deploy, you will be uploading data to Arweave using Turbo, as well as performing protected actions on an Arweave Name Token. Because of this, you will need to provide the keys to an Arweave wallet in order for the actions to be successful. The wallet must contain Turbo Credits to pay for the upload, and it must either be a controller or the owner of the ArNS name you are trying to update. permaweb-deploy requires your wallet keyfile be encoded in base64 format. You can convert a local keyfile to base64, and copy the new value to your clipboard by using one of the below commands, depending on your operating system: Linux base64 wallet.json | xclip -selection clipboard CopyCopied! Mac base64 -i wallet.json | pbcopy CopyCopied! Windows (CMD) base64 wallet.json | clip CopyCopied! Be sure to replace wallet.json with the path to your chosen wallet keyfile. Once you have this value saved to your clipboard, you can move on to the next step. Create Github Secrets Anyone who has your wallet keyfile (including the base64 formatted keyfile) has full control over your wallet and any of its assets. Because of this, you do not want to include it directly in your package.json script. Instead, keep the value safe by storing it in a github secret. You will create the secrets in the settings tab on your github repo, and the secrets will act as environmental variables in the github actions workflow. You will need to create 1 secret DEPLOY_KEY: This is the base64 encoded version of your Arweave wallet keyfile. Create Action Workflow Github Actions allow you to perform specific actions whenever you push code to github. They are handled by using .yaml files provided in <root-of-project>/.github/workflows. To get started, create a new file named deploy.yaml in the workflows directory, then paste the below inside of it: name: Arweave Deploy on: push: branches: - main jobs: Arweave-build-and-deploy: runs-on: ubuntu-latest steps: - uses: actions/checkout@v2 - name: Setup Node.js environment uses: actions/setup-node@v2 with: node-version: \"20\" - name: Run deployment script env: DEPLOY_KEY: ${{ secrets.DEPLOY_KEY }} run: | npm install npm run deploy CopyCopied! The above tells github to perform these actions when you push new code to the branch main It then sets up a vps with nodejs v 20. When that is complete, it installs dependencies for your project using npm (You will need to add a step to install yarn if that is your preferred package manager), and runs your deploy script, which builds your static folder and then runs permaweb-deploy. It also loads your github secrets into environmental variables that can be used by your deploy script. Deploying App With the above setup complete, the only thing you need to do to deploy a new version of a permasite app to Arweave is push the updated code to branch main on github. Everything else is fully automated.Was this page helpful?YesNoComment",
          "estimatedWords": 1175,
          "lastModified": "2025-06-27T16:13:06.060Z",
          "siteKey": "ario",
          "siteName": "AR-IO Network",
          "depth": 4,
          "crawledAt": "2025-06-27T16:13:06.060Z"
        },
        {
          "url": "https://docs.ar.io/guides/arlink",
          "title": "Arlink Deploy",
          "content": "Arlink Deploy Overview Arlink is a third party tool that allows you to permanently deploy and manage web apps on the permaweb with ease. How it works Users can link their Github or Protocol.land repositories to their Arlink account through the Arlink dashboard. When a new project or build is deployed, Arlink will take the repository, build it, and upload the build folder to Arweave. Arlink also allows users to connect their project to an ArNS name they own, or an undername of the ArNS name ar://arlink. Dashboard After connecting your wallet to the Arlink web app using the button at the top right, you will be taken to your dashboard. This page will display any deployments associated with your wallet, and includes a \"+ New Deployment\" button in order to start the process of deploying a new project. New Deployment After clicking on the new deployment button, you will be prompted to import a repository from either Github or Protocol.land. Authorize Github If this is your first time importing from Github, you will be prompted to authorize Arlink to access your Github repositories. You can authorize all repositories, or limit authorization to any number of specific ones. Select Repository Once authorization is approved, select which repository and branch you want to deploy. Define Build and Output Steps Once you select what you want to deploy, you need to specify how the project needs to be built to get it ready. Arlink prompts for five inputs: Project Name: This is the name of your project. Install Command: The command for installing dependencies for your project. Usually npm install or yarn install Build Command: This is the command to run your build script. Usually npm run build or yarn build Sub Directory: If the front end for your project lives in a sub directory of your selected repository, you can specify that here. Output Directory: This is the path to the build folder being deployed. This will be different depending on the framework your project uses. Select ArNS The last thing to do is select an ArNS name to deploy your project to. If you own your own name, you can connect to it here with the \"Use existing ArNS\" toggle. Otherwise, you can select an undername of the ArNS name arlink to deploy to. Duplicate undernames cannot exist, so you can only select an undername that is not already being used. Logs Once you select your ArNS name and click \"Deploy\", your project will be deployed. Logs from the build and deploy process will be displayed so you can monitor for errors. Updates To deploy a new build of your project, select it from the dashboard. The project page gives you the option to update any settings or configurations, and has a \"Deploy Latest\" button which will redeploy your project. Was this page helpful?YesNoComment",
          "estimatedWords": 474,
          "lastModified": "2025-06-27T16:13:06.211Z",
          "siteKey": "ario",
          "siteName": "AR-IO Network",
          "depth": 4,
          "crawledAt": "2025-06-27T16:13:06.211Z"
        },
        {
          "url": "https://docs.ar.io/guides/ardrive-web",
          "title": "ArDrive Web Deployment Guide",
          "content": "ArDrive Web Deployment Guide Overview This guide will outline the simple steps needed to deploy your dApp or website onto the Arweave blockchain using the ArDrive web app and friendly UI. Simple apps and websites should work right out of the box. However, for advanced applications, this assumes you have already prepared your dApp to use hash routing and relative file paths, and built static files for any dApp in a language or framework that requires it (like React). Learn more about preparing your dApp for deployment here. Deploying Step 1: Log into ArDrive Go to the ArDrive web app and log in using the method of your choosing. If you don't already have an account, you will need to follow the instructions to set one up. Step 2: Select or Create a Drive Once logged in, navigate to the drive where you want your project to be hosted. If you haven't created a drive yet, or if you want a new one specifically for this project, click the big red \"New\" button at the top left and create a new drive. Remember, the drive needs to be set to public for your dApp to be accessible to others. Step 3: Upload your project With your drive selected, click the big red \"New\" button again, but this time, select \"Upload Folder\". Navigate to your project's root directory, or the built directory if required, and select it. This will upload the entire directory, maintaining your project's file structure. Step 4: Confirm Upload You'll be given a chance to review the upload and the associated cost. If everything looks right, click \"Confirm\". Remember, uploading to Arweave isnt free, but the cost is usually quite small and the benefits of having your dApp or website hosted on the permaweb are significant. Step 5: Create the Manifest While ArDrive displays your uploaded files as a traditional file structure, with files and folders inside other folders, thats not how they actually exist on Arweave. The manifest acts as a map to all the files your dApp needs to function. After you confirm your upload, navigate into your newly created folder by double clicking on it. Click the big red \"New\" button again and select \"New Manifest\" in the \"Advanced\" section. You'll be prompted to name the manifest and choose where to save it. Be sure to save it inside the folder you just created. Step 6: Get the Data TX ID Once the manifest is created, click on it to expand its details. In the \"details\" tab, on the bottom right, there's a line labeled \"Data TX ID\". This is the unique identifier for your uploaded dApp on Arweave. Copy this value. Step 7: View and Share your dApp Your dApp or website is now available on the permaweb forever! Append the Data TX ID you just copied to the end of an Arweave gateway URL, like https://arweave.net/ . It might take a few minutes for all of your files to finish propagating through the network, but once they do your dApp or website will be accessible to anyone, anywhere, at any time. Step 8: Assign a Friendly Name The Data TX ID you copied in Step 6 is long and difficult to remember. To make it easier to access your dApp or website, you can assign a friendly name to it using ArNS. If you already own an ArNS name, you will be prompted during the creation of your manifest if you want to assign one. If you do not, you can purchase one from arns.app. You can also assign an ArNS name to an existing manifest (or any other file) by clicking on the three dots on the right side of the file and selecting \"Assign ArNS name\". Updating your dApp Files uploaded to Arweave are permanent and immutable. They cannot be changed. However, the Arweave File System (ArFS) protocol used (and created) by ArDrive lets you \"replace\" them with new versions while still being able to access the old ones. You can do this with entire dApps as well. The old files won't be displayed in the ArDrive web app unless you click on a file to view its history. Once you have made changes to your dApp or website, and built the static directory for it, you can upload the entire folder again to the same location where you uploaded the original. Follow all the same steps listed above for uploading your dApp. You will need to create a new manifest to correctly point to the updated files. Give it the same name as the old manifest in order to \"replace\" it. Creating the new manifest will generate a new TX ID used to view the updated dApp. The old version of the dApp will always be available to anyone who has the correct TX ID.Was this page helpful?YesNoComment",
          "estimatedWords": 808,
          "lastModified": "2025-06-27T16:13:06.611Z",
          "siteKey": "ario",
          "siteName": "AR-IO Network",
          "depth": 4,
          "crawledAt": "2025-06-27T16:13:06.611Z"
        },
        {
          "url": "https://docs.ar.io/learn/guides/ants-on-bazar",
          "title": "Trading ANTs on Bazar",
          "content": "Trading ANTs on Bazar Overview Arweave Name Tokens are Atomic Asset Spec compliant AO tokens that manage records and permission for ArNS names. Because the ANT spec is compliant with the Atomic Asset Spec, they are tradable on Bazar, which is a decentralized market place for Atomic Assets on AO. There are a few simple steps that are required in order to make an ANT available on Bazar to be traded. Bazar Profile Bazar relies on profiles for displaying user information and tradable assets. Profiles are AO processes that contain user specified information like a name, a nickname, and images associated with the profile. Profiles also track assets held by the profile in order to provide their information to bazar. Create a Profile If you do not already have a profile associated with your wallet, you can easily create one on using the \"Create your profile\" button on bazar after connecting your wallet: You will be prompted to add, at a minimum, a name and handle (nickname) to associate with the profile. These values can be changed later. Click \"Save\" at the bottom to finish creation of your profile. Once your profile is created, you can get its ao process Id at any time by clicking on the user icon in Bazar, and then the \"Copy profile address\" button from the menu. Transfer ANT to the Profile Bazar profiles only track assets that are held in the profile process, not in a user wallet. In order for an ANT to be displayed and transferred on Bazar, it must first be transferred into the Bazar profile. This can be done easily using arns.app in your manage page for a given name. Once an ANT is transferred into the profile process, it will automatically be detected and displayed by Bazar. It can be transferred or sold just like any other atomic asset on the marketplace, with no additional steps required. Restore Controllers OptionalThis is an optional step that will enable updating an ANT's Target Id without transferring it back into your wallet. This step may be safely skipped without affecting the ANT's functionality or tradability on Bazar. Transferring an ANT to a new wallet or AO process resets all authorized controllers, or non-owner entities that are allowed to update some settings on the ArNS name. It does not reset the Target Id that the ArNS name is pointing to. If you want to be able to update the Target ID and undernames from your wallet using arns.app, you will need to set your wallet address as a controller for the ANT while it is in your profile. The easiest way to do this is using aos. If you have not used aos before, you can find installation instructions here Using aos, you can log directly into your profile process with the command: aos <profile-address> --wallet \"/path/to/your/keyfile\" CopyCopied! Be sure to replace <profile-address> with the process Id for your profile process, and /path/to/your/keyfile with the path to the keyfile for the wallet you created the profile with. Once you are logged in with aos, you can send a message to the ANT in your profile to set your wallet as a controller: Send({ Target = \"<Ant-Process-ID>\", Action = \"Add-Controller\", Controller = \"<Wallet-Address>\" }) CopyCopied! Replace <Ant-Process-ID> with the process Id of the ANT you transferred into your profile, and <Wallet-Address> with your wallet address.Was this page helpful?YesNoComment",
          "estimatedWords": 564,
          "lastModified": "2025-06-27T16:13:06.765Z",
          "siteKey": "ario",
          "siteName": "AR-IO Network",
          "depth": 3,
          "crawledAt": "2025-06-27T16:13:06.765Z"
        },
        {
          "url": "https://docs.ar.io/build/guides/arns-viewer",
          "title": "ArNS Viewer",
          "content": "ArNS Viewer Overview This guide will walk you through creating a project that uses the AR.IO SDK to interact with ArNS names in a web environment. It provides all the steps and context needed to help you get up and running smoothly, allowing you to effectively use these technologies. We will be using ARNext, a new framework based on Next.js, to simplify deployment to the Arweave permaweb. ARNext provides flexibility for deploying seamlessly to Arweave using an ArNS name, an Arweave transaction ID, or traditional services like Vercel—all without requiring major code modifications. This means you can deploy the same project across different environments with minimal effort. The guide will focus on the following core functionalities of the AR.IO SDK: Retrieving a List of All Active ArNS Names: Learn how to use the SDK to get and display a list of active ArNS names. Querying Detailed Records for a Specific ArNS Name: Learn how to access detailed records for a specific ArNS name using its ANT (Arweave Name Token). Updating and Creating Records on an ArNS Name: Learn how to modify and add records to an ArNS name, showcasing the capabilities of ANT for dynamic web content. By the end of this guide, you will have a complete, functional project that not only demonstrates how to use the AR.IO SDK but also shows the ease and flexibility of deploying applications to the Arweave permaweb. Whether you are an experienced developer or just starting out, this guide will help you understand the key aspects of building and deploying on Arweave. Getting Started Prerequisites Node v20.17 or greater git Install ARNext ARNext is a brand new framework that is still in development. It supports installation using npx, and you will need the proper Node version for the installation to be successful. npx create-arnext-app arnext CopyCopied! You can then move your terminal into that newly created folder with: cd arnext CopyCopied! or open the folder in an IDE like VSCode, and open a new terminal inside that IDE in order to complete the next steps. Sanity Check It is good practice when starting a new project to view it in localhost without any changes, to make sure everything is installed and working correctly. To do this, run: npm run dev CopyCopied! or, if you prefer yarn: yarn dev CopyCopied! By default, the project will be served on port 3000, so you can access it by navigating to localhost:3000 in any browser. You should see something that looks like this: With this complete, you are ready to move on to customizing for your own project. Install AR.IO SDK Next, install the AR.IO SDK. npm install @ar.io/sdk CopyCopied! or yarn add @ar.io/sdk --ignore-engines CopyCopied! Polyfills Polyfills are used to provide missing functionality in certain environments. For example, browsers do not have direct access to a computer's file system, but many JavaScript libraries are designed to work in both browser and Node.js environments. These libraries might include references to fs, the module used by Node.js to interact with the file system. Since fs is not available in browsers, we need a polyfill to handle these references and ensure the application runs properly in a browser environment. Polyfills are actually evil voodoo curse magic. No one understands what they are or how they work, but front end devs sell their souls to Bill Gates in exchange for their stuff working properly in browsers. The below polyfill instructions were stolen, at great personal cost, from one of these front end devs in order to save your soul. This is one of many convenient services offered by AR.IO Installation The below command will install several packages as development dependencies, which should be sufficient to handle most polyfill needs for projects that interact with Arweave. npm install webpack browserify-fs process buffer --save-dev CopyCopied! or yarn add webpack browserify-fs process buffer --dev --ignore-engines CopyCopied! Next Config With the polyfill packages installed, we need to tell our app how to use them. In NextJS, which ARNext is built on, this is done in the next.config.js file in the root of the project. The default config file will look like this: const arnext = require(\"arnext/config\") const nextConfig = { reactStrictMode: true } module.exports = arnext(nextConfig) CopyCopied! This configuration allows the app to determine if it is being served via an Arweave transaction Id, or through a more traditional method. From here, we need to add in the additional configurations for resolving our polyfills. The updated next.config.js will look like this: const arnext = require(\"arnext/config\"); const webpack = require(\"webpack\"); const nextConfig = { reactStrictMode: true, webpack: (config) => { config.resolve.fallback = { ...config.resolve.fallback, fs: false, process: \"process/browser\", buffer: \"buffer/\", }; config.plugins.push( new webpack.ProvidePlugin({ process: \"process/browser\", Buffer: [\"buffer\", \"Buffer\"], }) ); return config; }, }; module.exports = arnext(nextConfig); CopyCopied! With that, you are ready to start customizing your app. Strip Default Content The first step in building your custom app is to remove the default content and create a clean slate. Follow these steps: Update the Home Page Navigate to pages > index.js, which serves as the main home page. Delete everything in this file and replace it with the following placeholder: export default function Home() {} CopyCopied! Remove Unused Pages The folder pages > posts > [id].js will not be used in this project. Delete the entire posts folder to keep the project organized and free of unnecessary files. Create Header Create a new components folder Inside that, create a Header.js file, leave it blank for now. Create Routes Create a new file at components > ArweaveRoutes.js to handle routing between pages. Leave it simple for now. import { Routes, Route } from \"react-router-dom\"; import { createBrowserRouter, RouterProvider } from \"react-router-dom\"; import Home from \"../pages/index\"; import NotFound from \"../pages/404\"; const ArweaveRoutes = () => ( <Routes> <Route path=\"/\" element={<Home />} /> <Route path=\"*\" element={<NotFound />} /> </Routes> ); export default ArweaveRoutes; CopyCopied! Your project is now a blank slate, ready for your own custom design and functionality. This clean setup will make it easier to build and maintain your application as you move forward. Add Utilities There are a few functions that we might end up wanting to use in multiple different pages in our finished product. So we can put these in a separate file and export them, so that other pages can import them to use. Start by creating a utils folder in the root of the project, then create 2 files inside of it: auth.js: This will contain the functions required for connecting an Arweave wallet using ArConnect /** * Connect to the Arweave wallet using ArConnect and request permissions. * @returns {Promise<string>} The active wallet address. */ export const connectWallet = async () => { await window.arweaveWallet.connect([ \"ACCESS_ADDRESS\", \"SIGN_TRANSACTION\", \"ACCESS_PUBLIC_KEY\", \"SIGNATURE\", ]); const address = await window.arweaveWallet.getActiveAddress(); return address; }; /** * Truncate a wallet address for display purposes. * @param {string} address - The wallet address to truncate. * @returns {string} The truncated address. */ export const truncateAddress = (address) => { return `${address.slice(0, 3)}...${address.slice(-3)}`; }; CopyCopied! arweave.js: This is where we will put most of our AR.IO SDK functions for interacting with Arweave import { ARIO, ANT, ArconnectSigner } from \"@ar.io/sdk/web\"; /** * Initialize ArIO and fetch all ArNS records. * @returns {Promise<Object>} All ArNS records. */ export const fetchArNSRecords = async () => { const ario = ARIO.init(); let allRecords = []; let hasMore = true; let cursor; // Paginates through all records to get the full registry. while (hasMore) { const response = await ario.getArNSRecords({ limit: 1000, // You can adjust the limit as needed, max is 1000 sortBy: \"name\", sortOrder: \"asc\", cursor: cursor, }); allRecords = [...allRecords, ...response.items]; cursor = response.nextCursor; hasMore = response.hasMore; } // console.log(allRecords); return allRecords; }; /** * Initialize ANT with the given processId. * @param {string} processId - The processId. * @returns {Object} ANT instance. */ export const initANT = (processId) => { return ANT.init({ processId }); }; /** * Fetch detailed records, owner, and controllers for a given processId. * @param {string} contractTxId - The processId. * @returns {Promise<Object>} Detailed records, owner, and controllers. */ export const fetchRecordDetails = async (processId) => { const ant = initANT(processId); const detailedRecords = await ant.getRecords(); const owner = await ant.getOwner(); const controllers = await ant.getControllers(); return { detailedRecords, owner, controllers }; }; /** * Set a new record in the ANT process. * @param {string} processId - The processId. * @param {string} subDomain - The subdomain for the record. * @param {string} transactionId - The transaction ID the record should resolve to. * @param {number} ttlSeconds - The Time To Live (TTL) in seconds. * @returns {Promise<Object>} Result of the record update. */ export const setANTRecord = async ( processId, name, transactionId, ttlSeconds ) => { console.log(`Pid: ${processId}`); console.log(`name: ${name}`); console.log(`txId: ${transactionId}`); const browserSigner = new ArconnectSigner(window.arweaveWallet); const ant = ANT.init({ processId, signer: browserSigner }); const result = await ant.setRecord({ undername: name, transactionId, ttlSeconds, }); console.log(result); return result; }; CopyCopied! Build Home Page Header We want the Header component to contain a button for users to connect their wallet to the site, and display their wallet address when Connected. To do this, we will use the functions we exported from the utils > auth.js file, and pass in a state and set state function from each page rendering the header: import React from \"react\"; import { connectWallet, truncateAddress } from \"../utils/auth\"; /** * Header component for displaying the connect wallet button and navigation. * @param {Object} props - Component props. * @param {string} props.address - The connected wallet address. * @param {function} props.setAddress - Function to set the connected wallet address. */ const Header = ({ address, setAddress }) => { const handleConnectWallet = async () => { try { const walletAddress = await connectWallet(); setAddress(walletAddress); } catch (error) { console.error(\"Failed to connect wallet:\", error); } }; return ( <div className=\"header\"> <button className=\"connect-wallet\" onClick={handleConnectWallet}> {address ? `Connected: ${truncateAddress(address)}` : \"Connect Wallet\"} </button> </div> ); }; export default Header; CopyCopied! Grid Component Our home page is going to fetch a list of all ArNS names and display them. To make this display cleaner and more organized, we are going to create a component to display the names as a grid. Create a new file in components named RecordsGrid.js import React from \"react\"; import { Link } from \"arnext\"; /** * RecordsGrid component for displaying a grid of record keys. * @param {Object} props - Component props. * @param {Array<string>} props.keys - Array of record keys to display. */ const RecordsGrid = ({ keys }) => { return ( <div className=\"records-grid\"> {keys.map((key, index) => ( <button key={index} className=\"record-key\" onClick={() => { console.log(`clicked on ${key}`); }} > {key} </button> ))} </div> ); }; export default RecordsGrid; CopyCopied! This will take an individual ArNS record and display it as a button that logs the record name when clicked. We will update this later to make the button act as a link to the more detailed record page after we build that, which is why we are importing Link from arnext Home Page Go back to pages > index.js and lets build out our home page. We want to fetch the list of ArNS names when the page loads, and then feed the list into the grid component we just created. Because there are so many names, we also want to include a simple search bar to filter out displayed names. We will also need several states in order to manage all of this info: \"use client\"; import { useEffect, useState } from \"react\"; import Header from \"@/components/Header\"; import { fetchArNSRecords } from \"@/utils/arweave\"; import RecordsGrid from \"@/components/RecordsGrid\"; export default function Home() { const [arnsRecords, setArnsRecords] = useState(null); // State for storing all ArNS records const [isProcessing, setIsProcessing] = useState(true); // State for processing indicator const [searchTerm, setSearchTerm] = useState(\"\") // used to filter displayed results by search input const [address, setAddress] = useState(null); // State for wallet address useEffect(() => { const fetchRecords = async () => { const allRecords = await fetchArNSRecords(); setArnsRecords(allRecords); setIsProcessing(false); }; fetchRecords(); }, []); return ( <div> <Header address={address} setAddress={setAddress} /> {isProcessing ? ( \"processing\" ) : ( <div> <h2>Search</h2> <input type=\"text\" value={searchTerm} className =\"search-bar\" onChange = {(e) => {setSearchTerm(e.target.value)}} /> <RecordsGrid keys={arnsRecords .map((r) => r.name) .filter((key) => key.toLowerCase().includes(searchTerm?.toLowerCase()))} /></div> )} </div> ); } CopyCopied! Names Page NextJS, and ARNext by extension, supports dynamic routing, allowing us to create dedicated pages for any ArNS name without needing to use query strings, which makes the sharable urls much cleaner and more intuitive. We can do this by creating a page file with the naming convention [variable].js. Since we want to make a page for specific ArNS names we will create a new folder inside the pages folder named names, and then a new file pages > names > [name].js. This will be our largest file so far, including different logic for the displayed content depending on if the connected wallet is authorized to make changes the the name. We also need to make the page see what the name being looked at is, based on the url. We can do this using the custom useParams function from ARNext. The finished page will look like this: import Header from \"@/components/Header\"; import { useParams, Link } from \"arnext\"; // Import from ARNext, not NextJS import { useEffect, useState } from \"react\"; import { ARIO } from \"@ar.io/sdk/web\"; import { fetchRecordDetails, setANTRecord } from \"@/utils/arweave\"; export async function getStaticPaths() { return { paths: [], fallback: \"blocking\" }; } export async function getStaticProps({ params }) { const { name } = params; return { props: { name } }; // No initial record, just returning name } export default function NamePage() { const { name } = useParams(); const [nameState, setNameState] = useState(\"\"); const [nameRecord, setNameRecord] = useState(null); // Initialize record to null const [arnsRecord, setArnsRecord] = useState(null); const [resultMessage, setResultMessage] = useState(\"\"); const [address, setAddress] = useState(null); // State for wallet address useEffect(() => { if (name && name !== nameState) { setNameState(name); // Fetch the record dynamically whenever routeName changes const fetchRecord = async () => { console.log(\"fetching records\"); try { const ario = ARIO.init(); const newRecord = await ario.getArNSRecord({ name }); console.log(newRecord); setNameRecord(newRecord); } catch (error) { console.error(\"Failed to fetch record:\", error); setRecord(null); } }; fetchRecord(); } if (nameRecord && nameRecord.processId) { const fetchArnsRecord = async () => { try { const arnsRecord = await fetchRecordDetails(nameRecord.processId); console.log(arnsRecord); setArnsRecord(arnsRecord); } catch (error) { console.error(error); } }; fetchArnsRecord(); } }, [nameState, nameRecord]); const handleUpdateRecord = async (key, txId) => { const result = await setANTRecord(nameRecord.processId, key, txId, 900) console.log(`result Message: ${result}`) console.log(result) setResultMessage(result.id) }; if (nameRecord === null) { return ( <div> <Header address={address} setAddress={setAddress} /> <p>Loading...</p> </div> ); } const owner = arnsRecord?.owner || \"N/A\"; const controllers = arnsRecord?.controllers || []; return ( <div> <Header address={address} setAddress={setAddress} /> <div className=\"record-details\"> <h3>Record Details for {nameState}</h3> <div> {arnsRecord?.detailedRecords && Object.keys(arnsRecord.detailedRecords).map((recordKey, index) => ( <div key={index} className=\"record-txid\"> <strong>{recordKey}:</strong>{\" \"} <a href={`https://arweave.net/${arnsRecord.detailedRecords[recordKey].transactionId}`} target=\"_blank\" rel=\"noopener noreferrer\" > {arnsRecord.detailedRecords[recordKey].transactionId} </a> </div> ))} </div> <p>Owner: {owner}</p> <p> Controllers: {controllers.length > 0 ? controllers.join(\", \") : \"N/A\"} </p> {owner === address && ( <> {arnsRecord?.detailedRecords && Object.keys(arnsRecord.detailedRecords).map( (recordKey, index) => ( <div key={index} className=\"record-update\"> <label> {recordKey}: <input type=\"text\" placeholder=\"Enter new TxID\" id={`input-${index}`} /> <button onClick={() => { const inputElement = document.getElementById(`input-${index}`); const inputValue = inputElement ? inputElement.value : \"\"; handleUpdateRecord( recordKey === \"@\" ? \"@\" : `${recordKey}`, inputValue ); }} > Update </button> </label> </div> ) )} <div className=\"new-record\"> <input type=\"text\" placeholder=\"New Subdomain\" id={`new-subdomain-input`} /> <input type=\"text\" placeholder=\"New TxID\" id={`new-txid-input`} /> <button onClick={() => { const subdomainElement = document.getElementById(\"new-subdomain-input\"); const txIdElement = document.getElementById(\"new-txid-input\"); const newSubdomainValue = subdomainElement ? subdomainElement.value : \"\"; const newTxIdValue = txIdElement ? txIdElement.value : \"\"; console.log(newSubdomainValue) console.log(newTxIdValue) handleUpdateRecord(newSubdomainValue, newTxIdValue); }} > Set New Record </button> </div> </> )} <Link href=\"/\"> <button>Back to list</button> </Link> {resultMessage && <p>Successfully updated with message ID: {resultMessage}</p>} </div> </div> ); } CopyCopied! When this page loads, it gets the name being queried by using useParams and our custom getStaticPaths and getStaticProps functions. It then uses the AR.IO sdk to get the process Id of the ANT that controls the name, and queries the ANT for its info and detailed records list. Once the page has that info, it renders the ArNS name, its owner address, any addresses authorized to make changes, and every record that name contains. If the user has connected a wallet authorized to make changes, the page also renders input fields for each record for making those updates. It also provides the option to create an entirely new undername record. Finish the Grid Component Now that we have a path for our main page displays to link to, we can update the components > RecordsGrid.js file to include that link when clicked. import React from \"react\"; import { Link } from \"arnext\"; /** * RecordsGrid component for displaying a grid of record keys. * @param {Object} props - Component props. * @param {Array<string>} props.keys - Array of record keys to display. */ const RecordsGrid = ({ keys }) => { return ( <div className=\"records-grid\"> {keys.map((key, index) => ( <Link href={`/names/${key}`} key={index}> <button key={index} className=\"record-key\" onClick={() => {console.log(`clicked on ${key}`)}} > {key} </button> </Link> ))} </div> ); }; export default RecordsGrid; CopyCopied! View Project The ArNS viewer should be fully functional now. You can view it locally in your browser using the same steps as the initial Sanity Check Run yarn dev in your terminal Navigate to localhost:3000 in a browser CSS You will likely notice that everything functions correctly, but it doesnt look very nice. This is because we havent updated our css at all. The primary css file for this project is css > App.css. You can make whatever css rules here that you like to make the page look the way you want. Deploy With Turbo Once your app is looking the way you want it, you can deploy it to the permaweb using Turbo. For this, you will need an Arweave wallet with some Turbo Credits. Make sure you don't place your keyfile for the wallet inside the project directory, or you risk it getting uploaded to Arweave by mistake. In your terminal, run the command: yarn deploy:turbo -w <path-to-your-wallet> CopyCopied! Make sure to replace <path-to-your-wallet> with the actual path to your Arweave wallet. This will create a static build of your entire project, upload it to Arweave, and print out in the terminal all of the details of the upload. Find the section in the print out manifestResponse which will have a key named id. That will be the Arweave transaction id for your project. You can view a permanently deployed version of your project at https://arweave.net/<transaction-id> References Completed Project example: github Deployed Project: transaction id Was this page helpful?YesNoComment",
          "estimatedWords": 3114,
          "lastModified": "2025-06-27T16:13:07.245Z",
          "siteKey": "ario",
          "siteName": "AR-IO Network",
          "depth": 3,
          "crawledAt": "2025-06-27T16:13:07.245Z"
        },
        {
          "url": "https://docs.ar.io/guides/arns-viewer",
          "title": "ArNS Viewer",
          "content": "ArNS Viewer Overview This guide will walk you through creating a project that uses the AR.IO SDK to interact with ArNS names in a web environment. It provides all the steps and context needed to help you get up and running smoothly, allowing you to effectively use these technologies. We will be using ARNext, a new framework based on Next.js, to simplify deployment to the Arweave permaweb. ARNext provides flexibility for deploying seamlessly to Arweave using an ArNS name, an Arweave transaction ID, or traditional services like Vercel—all without requiring major code modifications. This means you can deploy the same project across different environments with minimal effort. The guide will focus on the following core functionalities of the AR.IO SDK: Retrieving a List of All Active ArNS Names: Learn how to use the SDK to get and display a list of active ArNS names. Querying Detailed Records for a Specific ArNS Name: Learn how to access detailed records for a specific ArNS name using its ANT (Arweave Name Token). Updating and Creating Records on an ArNS Name: Learn how to modify and add records to an ArNS name, showcasing the capabilities of ANT for dynamic web content. By the end of this guide, you will have a complete, functional project that not only demonstrates how to use the AR.IO SDK but also shows the ease and flexibility of deploying applications to the Arweave permaweb. Whether you are an experienced developer or just starting out, this guide will help you understand the key aspects of building and deploying on Arweave. Getting Started Prerequisites Node v20.17 or greater git Install ARNext ARNext is a brand new framework that is still in development. It supports installation using npx, and you will need the proper Node version for the installation to be successful. npx create-arnext-app arnext CopyCopied! You can then move your terminal into that newly created folder with: cd arnext CopyCopied! or open the folder in an IDE like VSCode, and open a new terminal inside that IDE in order to complete the next steps. Sanity Check It is good practice when starting a new project to view it in localhost without any changes, to make sure everything is installed and working correctly. To do this, run: npm run dev CopyCopied! or, if you prefer yarn: yarn dev CopyCopied! By default, the project will be served on port 3000, so you can access it by navigating to localhost:3000 in any browser. You should see something that looks like this: With this complete, you are ready to move on to customizing for your own project. Install AR.IO SDK Next, install the AR.IO SDK. npm install @ar.io/sdk CopyCopied! or yarn add @ar.io/sdk --ignore-engines CopyCopied! Polyfills Polyfills are used to provide missing functionality in certain environments. For example, browsers do not have direct access to a computer's file system, but many JavaScript libraries are designed to work in both browser and Node.js environments. These libraries might include references to fs, the module used by Node.js to interact with the file system. Since fs is not available in browsers, we need a polyfill to handle these references and ensure the application runs properly in a browser environment. Polyfills are actually evil voodoo curse magic. No one understands what they are or how they work, but front end devs sell their souls to Bill Gates in exchange for their stuff working properly in browsers. The below polyfill instructions were stolen, at great personal cost, from one of these front end devs in order to save your soul. This is one of many convenient services offered by AR.IO Installation The below command will install several packages as development dependencies, which should be sufficient to handle most polyfill needs for projects that interact with Arweave. npm install webpack browserify-fs process buffer --save-dev CopyCopied! or yarn add webpack browserify-fs process buffer --dev --ignore-engines CopyCopied! Next Config With the polyfill packages installed, we need to tell our app how to use them. In NextJS, which ARNext is built on, this is done in the next.config.js file in the root of the project. The default config file will look like this: const arnext = require(\"arnext/config\") const nextConfig = { reactStrictMode: true } module.exports = arnext(nextConfig) CopyCopied! This configuration allows the app to determine if it is being served via an Arweave transaction Id, or through a more traditional method. From here, we need to add in the additional configurations for resolving our polyfills. The updated next.config.js will look like this: const arnext = require(\"arnext/config\"); const webpack = require(\"webpack\"); const nextConfig = { reactStrictMode: true, webpack: (config) => { config.resolve.fallback = { ...config.resolve.fallback, fs: false, process: \"process/browser\", buffer: \"buffer/\", }; config.plugins.push( new webpack.ProvidePlugin({ process: \"process/browser\", Buffer: [\"buffer\", \"Buffer\"], }) ); return config; }, }; module.exports = arnext(nextConfig); CopyCopied! With that, you are ready to start customizing your app. Strip Default Content The first step in building your custom app is to remove the default content and create a clean slate. Follow these steps: Update the Home Page Navigate to pages > index.js, which serves as the main home page. Delete everything in this file and replace it with the following placeholder: export default function Home() {} CopyCopied! Remove Unused Pages The folder pages > posts > [id].js will not be used in this project. Delete the entire posts folder to keep the project organized and free of unnecessary files. Create Header Create a new components folder Inside that, create a Header.js file, leave it blank for now. Create Routes Create a new file at components > ArweaveRoutes.js to handle routing between pages. Leave it simple for now. import { Routes, Route } from \"react-router-dom\"; import { createBrowserRouter, RouterProvider } from \"react-router-dom\"; import Home from \"../pages/index\"; import NotFound from \"../pages/404\"; const ArweaveRoutes = () => ( <Routes> <Route path=\"/\" element={<Home />} /> <Route path=\"*\" element={<NotFound />} /> </Routes> ); export default ArweaveRoutes; CopyCopied! Your project is now a blank slate, ready for your own custom design and functionality. This clean setup will make it easier to build and maintain your application as you move forward. Add Utilities There are a few functions that we might end up wanting to use in multiple different pages in our finished product. So we can put these in a separate file and export them, so that other pages can import them to use. Start by creating a utils folder in the root of the project, then create 2 files inside of it: auth.js: This will contain the functions required for connecting an Arweave wallet using ArConnect /** * Connect to the Arweave wallet using ArConnect and request permissions. * @returns {Promise<string>} The active wallet address. */ export const connectWallet = async () => { await window.arweaveWallet.connect([ \"ACCESS_ADDRESS\", \"SIGN_TRANSACTION\", \"ACCESS_PUBLIC_KEY\", \"SIGNATURE\", ]); const address = await window.arweaveWallet.getActiveAddress(); return address; }; /** * Truncate a wallet address for display purposes. * @param {string} address - The wallet address to truncate. * @returns {string} The truncated address. */ export const truncateAddress = (address) => { return `${address.slice(0, 3)}...${address.slice(-3)}`; }; CopyCopied! arweave.js: This is where we will put most of our AR.IO SDK functions for interacting with Arweave import { ARIO, ANT, ArconnectSigner } from \"@ar.io/sdk/web\"; /** * Initialize ArIO and fetch all ArNS records. * @returns {Promise<Object>} All ArNS records. */ export const fetchArNSRecords = async () => { const ario = ARIO.init(); let allRecords = []; let hasMore = true; let cursor; // Paginates through all records to get the full registry. while (hasMore) { const response = await ario.getArNSRecords({ limit: 1000, // You can adjust the limit as needed, max is 1000 sortBy: \"name\", sortOrder: \"asc\", cursor: cursor, }); allRecords = [...allRecords, ...response.items]; cursor = response.nextCursor; hasMore = response.hasMore; } // console.log(allRecords); return allRecords; }; /** * Initialize ANT with the given processId. * @param {string} processId - The processId. * @returns {Object} ANT instance. */ export const initANT = (processId) => { return ANT.init({ processId }); }; /** * Fetch detailed records, owner, and controllers for a given processId. * @param {string} contractTxId - The processId. * @returns {Promise<Object>} Detailed records, owner, and controllers. */ export const fetchRecordDetails = async (processId) => { const ant = initANT(processId); const detailedRecords = await ant.getRecords(); const owner = await ant.getOwner(); const controllers = await ant.getControllers(); return { detailedRecords, owner, controllers }; }; /** * Set a new record in the ANT process. * @param {string} processId - The processId. * @param {string} subDomain - The subdomain for the record. * @param {string} transactionId - The transaction ID the record should resolve to. * @param {number} ttlSeconds - The Time To Live (TTL) in seconds. * @returns {Promise<Object>} Result of the record update. */ export const setANTRecord = async ( processId, name, transactionId, ttlSeconds ) => { console.log(`Pid: ${processId}`); console.log(`name: ${name}`); console.log(`txId: ${transactionId}`); const browserSigner = new ArconnectSigner(window.arweaveWallet); const ant = ANT.init({ processId, signer: browserSigner }); const result = await ant.setRecord({ undername: name, transactionId, ttlSeconds, }); console.log(result); return result; }; CopyCopied! Build Home Page Header We want the Header component to contain a button for users to connect their wallet to the site, and display their wallet address when Connected. To do this, we will use the functions we exported from the utils > auth.js file, and pass in a state and set state function from each page rendering the header: import React from \"react\"; import { connectWallet, truncateAddress } from \"../utils/auth\"; /** * Header component for displaying the connect wallet button and navigation. * @param {Object} props - Component props. * @param {string} props.address - The connected wallet address. * @param {function} props.setAddress - Function to set the connected wallet address. */ const Header = ({ address, setAddress }) => { const handleConnectWallet = async () => { try { const walletAddress = await connectWallet(); setAddress(walletAddress); } catch (error) { console.error(\"Failed to connect wallet:\", error); } }; return ( <div className=\"header\"> <button className=\"connect-wallet\" onClick={handleConnectWallet}> {address ? `Connected: ${truncateAddress(address)}` : \"Connect Wallet\"} </button> </div> ); }; export default Header; CopyCopied! Grid Component Our home page is going to fetch a list of all ArNS names and display them. To make this display cleaner and more organized, we are going to create a component to display the names as a grid. Create a new file in components named RecordsGrid.js import React from \"react\"; import { Link } from \"arnext\"; /** * RecordsGrid component for displaying a grid of record keys. * @param {Object} props - Component props. * @param {Array<string>} props.keys - Array of record keys to display. */ const RecordsGrid = ({ keys }) => { return ( <div className=\"records-grid\"> {keys.map((key, index) => ( <button key={index} className=\"record-key\" onClick={() => { console.log(`clicked on ${key}`); }} > {key} </button> ))} </div> ); }; export default RecordsGrid; CopyCopied! This will take an individual ArNS record and display it as a button that logs the record name when clicked. We will update this later to make the button act as a link to the more detailed record page after we build that, which is why we are importing Link from arnext Home Page Go back to pages > index.js and lets build out our home page. We want to fetch the list of ArNS names when the page loads, and then feed the list into the grid component we just created. Because there are so many names, we also want to include a simple search bar to filter out displayed names. We will also need several states in order to manage all of this info: \"use client\"; import { useEffect, useState } from \"react\"; import Header from \"@/components/Header\"; import { fetchArNSRecords } from \"@/utils/arweave\"; import RecordsGrid from \"@/components/RecordsGrid\"; export default function Home() { const [arnsRecords, setArnsRecords] = useState(null); // State for storing all ArNS records const [isProcessing, setIsProcessing] = useState(true); // State for processing indicator const [searchTerm, setSearchTerm] = useState(\"\") // used to filter displayed results by search input const [address, setAddress] = useState(null); // State for wallet address useEffect(() => { const fetchRecords = async () => { const allRecords = await fetchArNSRecords(); setArnsRecords(allRecords); setIsProcessing(false); }; fetchRecords(); }, []); return ( <div> <Header address={address} setAddress={setAddress} /> {isProcessing ? ( \"processing\" ) : ( <div> <h2>Search</h2> <input type=\"text\" value={searchTerm} className =\"search-bar\" onChange = {(e) => {setSearchTerm(e.target.value)}} /> <RecordsGrid keys={arnsRecords .map((r) => r.name) .filter((key) => key.toLowerCase().includes(searchTerm?.toLowerCase()))} /></div> )} </div> ); } CopyCopied! Names Page NextJS, and ARNext by extension, supports dynamic routing, allowing us to create dedicated pages for any ArNS name without needing to use query strings, which makes the sharable urls much cleaner and more intuitive. We can do this by creating a page file with the naming convention [variable].js. Since we want to make a page for specific ArNS names we will create a new folder inside the pages folder named names, and then a new file pages > names > [name].js. This will be our largest file so far, including different logic for the displayed content depending on if the connected wallet is authorized to make changes the the name. We also need to make the page see what the name being looked at is, based on the url. We can do this using the custom useParams function from ARNext. The finished page will look like this: import Header from \"@/components/Header\"; import { useParams, Link } from \"arnext\"; // Import from ARNext, not NextJS import { useEffect, useState } from \"react\"; import { ARIO } from \"@ar.io/sdk/web\"; import { fetchRecordDetails, setANTRecord } from \"@/utils/arweave\"; export async function getStaticPaths() { return { paths: [], fallback: \"blocking\" }; } export async function getStaticProps({ params }) { const { name } = params; return { props: { name } }; // No initial record, just returning name } export default function NamePage() { const { name } = useParams(); const [nameState, setNameState] = useState(\"\"); const [nameRecord, setNameRecord] = useState(null); // Initialize record to null const [arnsRecord, setArnsRecord] = useState(null); const [resultMessage, setResultMessage] = useState(\"\"); const [address, setAddress] = useState(null); // State for wallet address useEffect(() => { if (name && name !== nameState) { setNameState(name); // Fetch the record dynamically whenever routeName changes const fetchRecord = async () => { console.log(\"fetching records\"); try { const ario = ARIO.init(); const newRecord = await ario.getArNSRecord({ name }); console.log(newRecord); setNameRecord(newRecord); } catch (error) { console.error(\"Failed to fetch record:\", error); setRecord(null); } }; fetchRecord(); } if (nameRecord && nameRecord.processId) { const fetchArnsRecord = async () => { try { const arnsRecord = await fetchRecordDetails(nameRecord.processId); console.log(arnsRecord); setArnsRecord(arnsRecord); } catch (error) { console.error(error); } }; fetchArnsRecord(); } }, [nameState, nameRecord]); const handleUpdateRecord = async (key, txId) => { const result = await setANTRecord(nameRecord.processId, key, txId, 900) console.log(`result Message: ${result}`) console.log(result) setResultMessage(result.id) }; if (nameRecord === null) { return ( <div> <Header address={address} setAddress={setAddress} /> <p>Loading...</p> </div> ); } const owner = arnsRecord?.owner || \"N/A\"; const controllers = arnsRecord?.controllers || []; return ( <div> <Header address={address} setAddress={setAddress} /> <div className=\"record-details\"> <h3>Record Details for {nameState}</h3> <div> {arnsRecord?.detailedRecords && Object.keys(arnsRecord.detailedRecords).map((recordKey, index) => ( <div key={index} className=\"record-txid\"> <strong>{recordKey}:</strong>{\" \"} <a href={`https://arweave.net/${arnsRecord.detailedRecords[recordKey].transactionId}`} target=\"_blank\" rel=\"noopener noreferrer\" > {arnsRecord.detailedRecords[recordKey].transactionId} </a> </div> ))} </div> <p>Owner: {owner}</p> <p> Controllers: {controllers.length > 0 ? controllers.join(\", \") : \"N/A\"} </p> {owner === address && ( <> {arnsRecord?.detailedRecords && Object.keys(arnsRecord.detailedRecords).map( (recordKey, index) => ( <div key={index} className=\"record-update\"> <label> {recordKey}: <input type=\"text\" placeholder=\"Enter new TxID\" id={`input-${index}`} /> <button onClick={() => { const inputElement = document.getElementById(`input-${index}`); const inputValue = inputElement ? inputElement.value : \"\"; handleUpdateRecord( recordKey === \"@\" ? \"@\" : `${recordKey}`, inputValue ); }} > Update </button> </label> </div> ) )} <div className=\"new-record\"> <input type=\"text\" placeholder=\"New Subdomain\" id={`new-subdomain-input`} /> <input type=\"text\" placeholder=\"New TxID\" id={`new-txid-input`} /> <button onClick={() => { const subdomainElement = document.getElementById(\"new-subdomain-input\"); const txIdElement = document.getElementById(\"new-txid-input\"); const newSubdomainValue = subdomainElement ? subdomainElement.value : \"\"; const newTxIdValue = txIdElement ? txIdElement.value : \"\"; console.log(newSubdomainValue) console.log(newTxIdValue) handleUpdateRecord(newSubdomainValue, newTxIdValue); }} > Set New Record </button> </div> </> )} <Link href=\"/\"> <button>Back to list</button> </Link> {resultMessage && <p>Successfully updated with message ID: {resultMessage}</p>} </div> </div> ); } CopyCopied! When this page loads, it gets the name being queried by using useParams and our custom getStaticPaths and getStaticProps functions. It then uses the AR.IO sdk to get the process Id of the ANT that controls the name, and queries the ANT for its info and detailed records list. Once the page has that info, it renders the ArNS name, its owner address, any addresses authorized to make changes, and every record that name contains. If the user has connected a wallet authorized to make changes, the page also renders input fields for each record for making those updates. It also provides the option to create an entirely new undername record. Finish the Grid Component Now that we have a path for our main page displays to link to, we can update the components > RecordsGrid.js file to include that link when clicked. import React from \"react\"; import { Link } from \"arnext\"; /** * RecordsGrid component for displaying a grid of record keys. * @param {Object} props - Component props. * @param {Array<string>} props.keys - Array of record keys to display. */ const RecordsGrid = ({ keys }) => { return ( <div className=\"records-grid\"> {keys.map((key, index) => ( <Link href={`/names/${key}`} key={index}> <button key={index} className=\"record-key\" onClick={() => {console.log(`clicked on ${key}`)}} > {key} </button> </Link> ))} </div> ); }; export default RecordsGrid; CopyCopied! View Project The ArNS viewer should be fully functional now. You can view it locally in your browser using the same steps as the initial Sanity Check Run yarn dev in your terminal Navigate to localhost:3000 in a browser CSS You will likely notice that everything functions correctly, but it doesnt look very nice. This is because we havent updated our css at all. The primary css file for this project is css > App.css. You can make whatever css rules here that you like to make the page look the way you want. Deploy With Turbo Once your app is looking the way you want it, you can deploy it to the permaweb using Turbo. For this, you will need an Arweave wallet with some Turbo Credits. Make sure you don't place your keyfile for the wallet inside the project directory, or you risk it getting uploaded to Arweave by mistake. In your terminal, run the command: yarn deploy:turbo -w <path-to-your-wallet> CopyCopied! Make sure to replace <path-to-your-wallet> with the actual path to your Arweave wallet. This will create a static build of your entire project, upload it to Arweave, and print out in the terminal all of the details of the upload. Find the section in the print out manifestResponse which will have a key named id. That will be the Arweave transaction id for your project. You can view a permanently deployed version of your project at https://arweave.net/<transaction-id> References Completed Project example: github Deployed Project: transaction id Was this page helpful?YesNoComment",
          "estimatedWords": 3114,
          "lastModified": "2025-06-27T16:13:07.373Z",
          "siteKey": "ario",
          "siteName": "AR-IO Network",
          "depth": 4,
          "crawledAt": "2025-06-27T16:13:07.373Z"
        },
        {
          "url": "https://docs.ar.io/build/guides/permaweb-deploy",
          "title": "Deploy a Website or Application",
          "content": "Deploy a Website or Application Overview With the growing popularity of permanently deployed apps, hosted on Arweave, along with the growing list of tools offered by AR.IO, several methods have been developed to automate the process of deploying a website and updating the ArNS name pointed at it. A particularly useful tool for this is permaweb-deploy from Forward Research. permaweb-deploy is a cli tool that handles uploading a build folder to Arweave using Turbo, creating a manifest, and then updating an ArNS name to point at the new manifest. It being a cli tool makes it very easy to incorporate into a github actions flow. Setting up an automated deployment with permaweb-deploy is simple, but does require a few steps. ENV SecurityBefore automating your deployments, be sure to build your app and check for exposed environmental secrets. Some app frameworks or build flows will build your app with the secrets exposed, and if you are using a tool like permaweb-deploy, those secrets will be uploaded to Arweave. Since the permaweb is permanent, this could pose a security risk, especially with a copy of your wallet keyfile required for the deployment automation. Getting Started Installing package permaweb-deploy is an npm package, and must be installed in any project before it can be used. If you are using npm, you can install the package with the below command: npm install permaweb-deploy --save-dev CopyCopied! If you prefer yarn for your package installations, the process is slightly more involved. permaweb-deploy is not designed for installation with yarn, so you must provide the additional argument ignore-engines in order to skip over the yarn version error you would normally get with installation. There are two methods for doing so: Directly in the install command yarn add permaweb-deploy -D --ignore-engines CopyCopied! In a .yarnc file You can provide a file, named .yarnc in the same directory as your package.json in order to assign specific instructions to all of your yarn commands. Creating a .yarnc file with the line ignore-engines true CopyCopied! will have the same effect as providing the flag directly in your yarn command Adding a Deploy Script The simplest way to utilize the permaweb-deploy tool is to build it into a script in your package.json. Here you will provide all of the variables that permaweb-deploy needs in order to function properly, as well as ensure that your app is statically built before being uploaded. \"scripts\": { \"build\": \"vuepress build src\", \"deploy\": \"npm run build && permaweb-deploy --deploy-folder ./src/.vuepress/dist --arns-name <YOUR_ARNS_NAME>\" }, CopyCopied! Be sure to replace <YOUR_ARNS_NAME> with the name of the ArNS name you want to deploy to. The above example shows a build script for a vuepress app, which will build the app into a static folder for deployment, and a deploy script which runs build and then permaweb-deploy. Your build script will look different depending on the framework you are using, but most will provide that for you when you create your app. The permaweb-deploy command has two required arguments: --deploy-folder This is the relative path (from your package.json) to the build folder you want to upload. In a vuepress app, that will be ./src/.vuepress/dist unless you manually specify otherwise in your vuepress configuration. It will be different depending on your chosen framework and if you have modified the default location. --arns-name This is the ArNS name you want to deploy to. It must be an ArNS name that the wallet used to authenticate has ownership or controller privileges over, otherwise the deployment will fail at authentication in the ao process that controls the ArNS name. UndernamesThe --arns-name flag MUST be the top level name, not and undername. That is, if you want to deploy to undername_arnsname you must set --arns-name arnsname and not --arns-name undername_arnsname.There is the additional, optional flag --undername. If you want to deploy your app to an undername on an ArNS name, provide that name with this flag.--arns-name arnsname --undername undername Testnet Permaweb-deploy supports both Mainnet and Testnet deployments. By default, it will deploy to Mainnet. To deploy to Testnet, you can provide the --ario-process flag as \"testnet\". If not provided, deployments will default to Mainnet. Providing Arweave Wallet Keys While using permaweb-deploy, you will be uploading data to Arweave using Turbo, as well as performing protected actions on an Arweave Name Token. Because of this, you will need to provide the keys to an Arweave wallet in order for the actions to be successful. The wallet must contain Turbo Credits to pay for the upload, and it must either be a controller or the owner of the ArNS name you are trying to update. permaweb-deploy requires your wallet keyfile be encoded in base64 format. You can convert a local keyfile to base64, and copy the new value to your clipboard by using one of the below commands, depending on your operating system: Linux base64 wallet.json | xclip -selection clipboard CopyCopied! Mac base64 -i wallet.json | pbcopy CopyCopied! Windows (CMD) base64 wallet.json | clip CopyCopied! Be sure to replace wallet.json with the path to your chosen wallet keyfile. Once you have this value saved to your clipboard, you can move on to the next step. Create Github Secrets Anyone who has your wallet keyfile (including the base64 formatted keyfile) has full control over your wallet and any of its assets. Because of this, you do not want to include it directly in your package.json script. Instead, keep the value safe by storing it in a github secret. You will create the secrets in the settings tab on your github repo, and the secrets will act as environmental variables in the github actions workflow. You will need to create 1 secret DEPLOY_KEY: This is the base64 encoded version of your Arweave wallet keyfile. Create Action Workflow Github Actions allow you to perform specific actions whenever you push code to github. They are handled by using .yaml files provided in <root-of-project>/.github/workflows. To get started, create a new file named deploy.yaml in the workflows directory, then paste the below inside of it: name: Arweave Deploy on: push: branches: - main jobs: Arweave-build-and-deploy: runs-on: ubuntu-latest steps: - uses: actions/checkout@v2 - name: Setup Node.js environment uses: actions/setup-node@v2 with: node-version: \"20\" - name: Run deployment script env: DEPLOY_KEY: ${{ secrets.DEPLOY_KEY }} run: | npm install npm run deploy CopyCopied! The above tells github to perform these actions when you push new code to the branch main It then sets up a vps with nodejs v 20. When that is complete, it installs dependencies for your project using npm (You will need to add a step to install yarn if that is your preferred package manager), and runs your deploy script, which builds your static folder and then runs permaweb-deploy. It also loads your github secrets into environmental variables that can be used by your deploy script. Deploying App With the above setup complete, the only thing you need to do to deploy a new version of a permasite app to Arweave is push the updated code to branch main on github. Everything else is fully automated.Was this page helpful?YesNoComment",
          "estimatedWords": 1175,
          "lastModified": "2025-06-27T16:13:07.809Z",
          "siteKey": "ario",
          "siteName": "AR-IO Network",
          "depth": 3,
          "crawledAt": "2025-06-27T16:13:07.809Z"
        },
        {
          "url": "https://docs.ar.io/build/guides/managing-undernames",
          "title": "Managing Undernames",
          "content": "Managing Undernames Overview ArNS undernames are subdomains of top level ArNS domains. They are separated from the main ArNS domain using an underscore \"_\" in place of the more typically used dot \".\". Records for undernames can be set using the setRecord method on the AR.IO SDK, or removed by using the removeRecord method. The process for setting/removing a record for an undername vs. a top level ArNS domain is nearly identical, the only difference being the undername parameter. When managing a record on a top level ArNS domain, this must be set to @, while updates to an undername should provide the undername being updated. Chaining UndernamesUndernames can be created on other undernames, for example ar://og_logo_ardrive. In this example the undername og exists under the undername logo on the ArNS name ardrive.For the purpose of the undername parameter in the AR.IO SDK, this should be written as a single undername, including the separating underscores:og_logo Creating an Undername There are no special steps required to create an undername (provided the selected ArNS name has available undername space). Simply setting a record for an undername that does not exist will create the undername. Create an UndernameNodeJSWeb const fs = require(\"fs\"); const { ANT, ArweaveSigner } = require(\"@ar.io/sdk\"); async function main() { const jwk = JSON.parse(fs.readFileSync(\"KeyFile.json\")); const ant = ANT.init({ signer: new ArweaveSigner(jwk), processId: \"bh9l1cy0aksiL_x9M359faGzM_yjralacHIUo8_nQXM\" }); const { id: txId } = await ant.setRecord( { subDomain: 'brand-new-undername', transactionId: '432l1cy0aksiL_x9M359faGzM_yjralacHIUo8_nQXM', ttlSeconds: 3600 }, // optional additional tags { tags: [{ name: 'App-Name', value: 'My-Awesome-App' }] }, ); } main(); CopyCopied! Updating an Undername If an undername already exists, its record can easily be updated using the same setRecord method. Update an UndernameNodeJSWeb const fs = require(\"fs\"); const { ANT, ArweaveSigner } = require(\"@ar.io/sdk\"); async function main() { const jwk = JSON.parse(fs.readFileSync(\"KeyFile.json\")); const ant = ANT.init({ signer: new ArweaveSigner(jwk), processId: \"bh9l1cy0aksiL_x9M359faGzM_yjralacHIUo8_nQXM\" }); const { id: txId } = await ant.setRecord( { subDomain: 'undername-to-update', transactionId: '432l1cy0aksiL_x9M359faGzM_yjralacHIUo8_nQXM', ttlSeconds: 3600 }, // optional additional tags { tags: [{ name: 'App-Name', value: 'My-Awesome-App' }] }, ); } main(); CopyCopied! Removing an Undername An existing undername can be removed by using the removeRecord method on the AR.IO SDK. The undername parameter should be set to the undername being removed. Remove UndernameNodeJSWeb const fs = require(\"fs\"); const { ANT, ArweaveSigner } = require(\"@ar.io/sdk\"); async function main() { const jwk = JSON.parse(fs.readFileSync(\"KeyFile.json\")); const ant = ANT.init({ signer: new ArweaveSigner(jwk), processId: \"bh9l1cy0aksiL_x9M359faGzM_yjralacHIUo8_nQXM\" }); const { id: txId } = await ant.removeRecord( { undername: 'remove-domain', }, // optional additional tags { tags: [{ name: 'App-Name', value: 'My-Awesome-App' }] }, ); } main(); CopyCopied! Increasing Undername Support By default, ArNS names support up to 10 undernames. This number can be increased, for a fee. This is done using the increaseUndernameLimit method on the ARIO class of the AR.IO SDK, rather than the ANT class. The quantity (qty) parameter specifies the number of ADDITIONAL undernames to be supported. i.e. increasing from 10 undernames to 15 would require the qty parameter set to 5. Increasing Undername SupportNodeJSWeb const fs = require(\"fs\"); const { ARIO, ArweaveSigner, ARIOToken } = require(\"@ar.io/sdk\"); async function main() { const jwk = JSON.parse(fs.readFileSync(\"KeyFile.json\")); const ario = ARIO.init({ signer: new ArweaveSigner(jwk), }); const { id: txId } = await ario.increaseUndernameLimit( { name: 'ar-io', qty: 5, }, // optional additional tags { tags: [{ name: 'App-Name', value: 'My-Awesome-App' }] }, ); } main(); CopyCopied!Was this page helpful?YesNoComment",
          "estimatedWords": 563,
          "lastModified": "2025-06-27T16:13:07.948Z",
          "siteKey": "ario",
          "siteName": "AR-IO Network",
          "depth": 3,
          "crawledAt": "2025-06-27T16:13:07.948Z"
        },
        {
          "url": "https://docs.ar.io/build/guides/gql",
          "title": "GraphQL",
          "content": "GraphQL Overview GraphQL is a powerful query language designed for modern web applications to efficiently fetch data. It enables precise queries, allowing users to specify exactly which data they need and in what format, significantly reducing the amount of unnecessary data transferred. This approach is ideal for dealing with complex systems and large datasets, as it minimizes bandwidth and improves performance. GraphQL operates through a single endpoint, streamlining the way applications communicate with databases. The integration of GraphQL with Arweave introduces a refined method for interacting with decentralized data storage. Arweave allows for the tagging of uploaded data, facilitating enhanced searchability and retrievability within its blockchain network. Utilizing GraphQL, users can perform targeted queries that leverage these tags, ensuring the retrieval of specific data swiftly and efficiently. This capability is particularly beneficial for the development of decentralized applications (dApps), the archival of content in a permanent and unalterable form, and the establishment of data marketplaces where precision and efficiency in data access are paramount. Together, GraphQL and Arweave form a compelling combination, offering developers and users a robust framework for managing and querying data in a decentralized environment. This integration not only promotes the efficient and scalable retrieval of data but also supports the creation of more sophisticated and data-intensive applications on the decentralized web, maintaining a balance between technical depth and accessibility. Constructing a Query Basic Syntax In GraphQL, you start with a root field and use braces to outline the fields you want to retrieve, allowing for precise, hierarchical data requests. For instance: { transactions { edges { node { id tags { name value } } } } } CopyCopied! This query demonstrates fetching transactions and their tags, illustrating the hierarchical nature of GraphQL queries. Customizing Searches with Tags Arweave utilizes a tagging system for transactions, enabling intricate search capabilities. You can filter queries using these tags: { transactions(tags: [{name: \"App-Name\", values: \"YourAppName\"}]) { edges { node { id data { size type } } } } } CopyCopied! This example filters transactions by a specific application name, and returns the id, size, and type of the transaction, showcasing how to customize queries for targeted data retrieval. NOTE: Tags are not the only option for filtering results, but are extremely useful due to the ability to add custom tags during the upload process. Understanding Edges and Nodes In the realm of GraphQL queries, especially when interfacing with Arweave, grasping the concept of edges and nodes is pivotal for constructing efficient and effective queries. This structure is not unique to Arweave but is particularly relevant due to the decentralized and interconnected nature of the data stored on its blockchain. Nodes: At the heart of GraphQL's query structure, nodes represent individual data points or entities. In the context of Arweave, a node could be a transaction, a block, or any piece of data stored within the network. Nodes are the primary targets of your query, containing the data you wish to retrieve, such as transaction IDs, tags, or the content of data transactions. Edges: Serving as the glue between nodes, edges are constructs that outline the relationship between different nodes. They can contain metadata about the connection, such as the nature of the relationship or additional attributes that describe how nodes are linked. In many GraphQL implementations, including those that interact with Arweave, edges are used to navigate through collections of related data, making them crucial for understanding the data's structure and lineage. This hierarchical model is especially useful for querying complex and relational data sets, allowing for detailed navigation and efficient data retrieval within Arweave's decentralized storage system. By effectively utilizing the edges and nodes structure, you can precisely target the data you need, whether it's filtering transactions by tags, fetching related transactions, or exploring the blockchain's structure. Pagination To add pagination to your GraphQL queries, you can use the first, last, before, and after parameters. These parameters control the slice of data you're querying, making data retrieval more efficient and manageable. first: Specify the number of items to retrieve from the start of the list or dataset. last: Specify the number of items to retrieve from the end of the list or dataset. { transactions(first: 10) { edges { node { id } } } } CopyCopied! This query fetches the first 10 transactions. To navigate through your dataset, you can use after and before in conjunction with first or last. These parameters accept cursors, which are typically provided in the response of your initial query. after: Fetch items after the specified cursor, used with first. before: Fetch items before the specified cursor, used with last. { transactions(first: 10, after: \"cursorOfLastItem\") { edges { node { id } } } } CopyCopied! This query fetches the next 10 transactions following the transaction with the cursor \"cursorOfLastItem\". If no pagination terms are set, GraphQL servers may apply default limits to prevent excessively large datasets from being returned in a single query, potentially impacting performance. The default behavior can vary based on the server's configuration but often involves returning a predefined maximum number of items. For instance, without specifying first or last, a query to the transactions field might return the first 5-10 transactions by default, depending on the server settings. This behavior ensures that server resources are not overwhelmed by large requests and that client applications receive data in manageable chunks. General Tips for Optimizing Queries To optimize your GraphQL queries in Arweave, follow these general guidelines: Specificity: Query with the most precise tags possible to narrow the search scope and enhance performance. Minimalism: Limit your query to the essential set of tags to reduce processing time and data transfer. Schema Design: Design your app's schema to reflect query patterns, possibly introducing tags that encapsulate frequent combinations of criteria. Include Non-tag Fields: Adding fields like owner can refine your search, making your queries more efficient. Order Your Tags: Arrange tags from most specific to most general to leverage Arweave's indexing more effectively. By incorporating these strategies, developers can achieve faster and more precise data access from Arweave, enhancing the performance and responsiveness of decentralized applications. This balanced approach to query construction and optimization is key to navigating the expansive and decentralized storage landscape Arweave provides. Making a Query Executing GraphQL queries within the Arweave ecosystem offers flexibility and multiple avenues for developers and users alike. Whether you prefer a hands-on, manual approach to constructing and testing queries, or you aim for automation and integration within your applications, Arweave provides the tools necessary to interact with its decentralized data storage seamlessly. GraphQL Playground For those new to GraphQL or seeking to fine-tune their queries before implementation, the GraphQL playground offers an invaluable resource. This interactive interface allows users to manually construct queries, explore the schema, and immediately see the results of their queries. Accessible via web browsers, the playground can be found at the /graphql endpoint of most Arweave indexing services, such as https://arweave.dev/graphql. Here, you can experiment with different queries, understand the structure of the data, and refine your approach without writing a single line of code in your application. Steps for Accessing the GraphQL Playground: Navigate to https://arweave.dev/graphql, or the graphql endpoint of any AR.IO gateway, in your web browser. Enter your GraphQL query in the provided interface. Press the \"play\" button to execute the query to see real-time results and debug as needed. Using an API For application development and automation, making GraphQL queries programmatically is essential. You can send POST requests directly to the GraphQL endpoint of any indexing service that supports it, such as arweave.net or any AR.IO gateway. These requests should contain your query in the body, allowing for dynamic and automated data retrieval within your application. When selecting an indexing service, consider the data coverage and reliability of the gateway to ensure it meets your application's needs. Different gateways might have varying degrees of indexed data available, so choosing one that is consistently up-to-date and comprehensive is key. Example of making a programmatic query: const axios = require('axios'); const query = { query: ` { transactions(tags: [{name: \"App-Name\", values: \"YourAppName\"}]) { edges { node { id tags { name value } } } } } ` }; axios.post('https://arweave.net/graphql', query, { headers: { 'Content-Type': 'application/json' }, }) .then(response => console.log(response.data)) .catch(error => console.error('Error:', error)); CopyCopied! Using an SDK For an even more integrated experience, some Software Development Kits (SDKs) offer direct methods for executing GraphQL queries. The Arweave SDK, for example, provides built-in functionalities to interact with the blockchain, simplifying the process of making queries. By leveraging these SDKs, developers can bypass the intricacies of manual HTTP request construction, focusing instead on the logic and design of their applications. Example of using the Arweave SDK for GraphQL queries: // Assuming the Arweave SDK is already set up and initialized const query = { query: ` { transactions(tags: [{name: \"App-Name\", values: \"YourAppName\"}]) { edges { node { id tags { name value } } } } } ` }; arweave.api.post('/graphql', query) .then(response => { console.log(response.data); }) .catch(error => { console.error('Error:', error); }); CopyCopied!Was this page helpful?YesNoComment",
          "estimatedWords": 1512,
          "lastModified": "2025-06-27T16:13:08.377Z",
          "siteKey": "ario",
          "siteName": "AR-IO Network",
          "depth": 3,
          "crawledAt": "2025-06-27T16:13:08.377Z"
        },
        {
          "url": "https://docs.ar.io/build/guides/ardrive-web",
          "title": "ArDrive Web Deployment Guide",
          "content": "ArDrive Web Deployment Guide Overview This guide will outline the simple steps needed to deploy your dApp or website onto the Arweave blockchain using the ArDrive web app and friendly UI. Simple apps and websites should work right out of the box. However, for advanced applications, this assumes you have already prepared your dApp to use hash routing and relative file paths, and built static files for any dApp in a language or framework that requires it (like React). Learn more about preparing your dApp for deployment here. Deploying Step 1: Log into ArDrive Go to the ArDrive web app and log in using the method of your choosing. If you don't already have an account, you will need to follow the instructions to set one up. Step 2: Select or Create a Drive Once logged in, navigate to the drive where you want your project to be hosted. If you haven't created a drive yet, or if you want a new one specifically for this project, click the big red \"New\" button at the top left and create a new drive. Remember, the drive needs to be set to public for your dApp to be accessible to others. Step 3: Upload your project With your drive selected, click the big red \"New\" button again, but this time, select \"Upload Folder\". Navigate to your project's root directory, or the built directory if required, and select it. This will upload the entire directory, maintaining your project's file structure. Step 4: Confirm Upload You'll be given a chance to review the upload and the associated cost. If everything looks right, click \"Confirm\". Remember, uploading to Arweave isnt free, but the cost is usually quite small and the benefits of having your dApp or website hosted on the permaweb are significant. Step 5: Create the Manifest While ArDrive displays your uploaded files as a traditional file structure, with files and folders inside other folders, thats not how they actually exist on Arweave. The manifest acts as a map to all the files your dApp needs to function. After you confirm your upload, navigate into your newly created folder by double clicking on it. Click the big red \"New\" button again and select \"New Manifest\" in the \"Advanced\" section. You'll be prompted to name the manifest and choose where to save it. Be sure to save it inside the folder you just created. Step 6: Get the Data TX ID Once the manifest is created, click on it to expand its details. In the \"details\" tab, on the bottom right, there's a line labeled \"Data TX ID\". This is the unique identifier for your uploaded dApp on Arweave. Copy this value. Step 7: View and Share your dApp Your dApp or website is now available on the permaweb forever! Append the Data TX ID you just copied to the end of an Arweave gateway URL, like https://arweave.net/ . It might take a few minutes for all of your files to finish propagating through the network, but once they do your dApp or website will be accessible to anyone, anywhere, at any time. Step 8: Assign a Friendly Name The Data TX ID you copied in Step 6 is long and difficult to remember. To make it easier to access your dApp or website, you can assign a friendly name to it using ArNS. If you already own an ArNS name, you will be prompted during the creation of your manifest if you want to assign one. If you do not, you can purchase one from arns.app. You can also assign an ArNS name to an existing manifest (or any other file) by clicking on the three dots on the right side of the file and selecting \"Assign ArNS name\". Updating your dApp Files uploaded to Arweave are permanent and immutable. They cannot be changed. However, the Arweave File System (ArFS) protocol used (and created) by ArDrive lets you \"replace\" them with new versions while still being able to access the old ones. You can do this with entire dApps as well. The old files won't be displayed in the ArDrive web app unless you click on a file to view its history. Once you have made changes to your dApp or website, and built the static directory for it, you can upload the entire folder again to the same location where you uploaded the original. Follow all the same steps listed above for uploading your dApp. You will need to create a new manifest to correctly point to the updated files. Give it the same name as the old manifest in order to \"replace\" it. Creating the new manifest will generate a new TX ID used to view the updated dApp. The old version of the dApp will always be available to anyone who has the correct TX ID.Was this page helpful?YesNoComment",
          "estimatedWords": 808,
          "lastModified": "2025-06-27T16:13:08.484Z",
          "siteKey": "ario",
          "siteName": "AR-IO Network",
          "depth": 3,
          "crawledAt": "2025-06-27T16:13:08.484Z"
        },
        {
          "url": "https://docs.ar.io/network-composition",
          "title": "ARIO Network composition",
          "content": "AR.IO Network composition Overview The permanent web, or \"permaweb,\" is the collection of all webpages, applications, and files stored on the Arweave network and made accessible by the AR.IO permanent cloud. These range from simple tools for viewing and managing data to sophisticated decentralized applications integrating immutable storage and smart contracts. For users and developers, the permaweb offers low-cost, maintenance-free, and permanent hosting for web apps, data, and pages – serving both traditional and emerging industries. Composition of the Permanent Cloud The AR.IO Network integrates decentralized protocols, services, and applications to power the permanent web alongside the traditional internet. Foundational components like Arweave and AO are independently developed, while AR.IO introduces essential services and incentives that enable seamless interaction and accessibility. Diagram 1: The Permanent Cloud Network Major Components of the Permanent Cloud: Storage: Arweave At the foundation lies the Arweave protocol, providing decentralized, immutable data storage. This layer ensures data is preserved indefinitely with clear provenance records for long-term reliability. Compute: AO This layer comprises decentralized compute platforms, such as Arweave-native solutions like AO and other Layer 1 smart contract platforms like Ethereum. These systems enable flexible, data-driven computation and smart contract execution, broadening the ecosystem's capabilities. Services: AR.IO Sitting atop the compute layer, the AR.IO Network provides essential services like data upload, retrieval, indexing, querying, and domain name resolution. AR.IO gateways ensure the permanent web remains functional, accessible, and usable for developers, creators, and end users. Together, these layers form a cohesive ecosystem, combining data permanence, decentralized computation, and seamless cloud services. Each layer strengthens the others, creating a resilient foundation for the permaweb while bridging the traditional and decentralized internet paradigms.Was this page helpful?YesNoComment",
          "estimatedWords": 277,
          "lastModified": "2025-06-27T16:13:08.925Z",
          "siteKey": "ario",
          "siteName": "AR-IO Network",
          "depth": 2,
          "crawledAt": "2025-06-27T16:13:08.925Z"
        },
        {
          "url": "https://docs.ar.io/token",
          "title": "The ARIO Token",
          "content": "The ARIO Token Overview ARIO is the multifunction AO Computer based token that powers the AR.IO Network and its suite of permanent cloud applications. The ARIO Token uses include: Gateway Participation: Gateway operators must stake ARIO tokens to join and actively participate in the network. Eligibility for Protocol Rewards: Both individuals who stake tokens as gateway operators and those who delegate tokens to a gateway are positioned to receive protocol rewards. ArNS Name Purchases: Acquiring friendly names through the Arweave Name System (ArNS) requires ARIO tokens. These transactions directly contribute to the protocol, with the proceeds being redistributed through the Observation and Incentive Protocol. Universal Currency: Within the AR.IO ecosystem, ARIO tokens serve as a versatile currency, enabling network participants to make purchases and exchange value. Moreover, ARIO tokens play a crucial role in driving ecosystem growth, fueling incentive programs, investments, bounties, and grants designed for active participants. Adding ARIO Token to Wander To view your ARIO token balance in Wander, formerly ArConnect, follow these steps to add the token to your wallet: Open your Wander wallet (available on both desktop and mobile) Access Settings: Mobile: Click the 3 vertical dots in the top right, then select \"Settings\" Desktop: Click the hamburger menu icon in the top left Select \"Tokens\" Click \"Import Token\" For Desktop users: Ensure \"Token Type\" is set to \"ao Token\" Enter the AO process ID: qNvAoz0TgcH7DMg8BCVn8jF32QH5L6T29VjHxhHqqGE CopyCopied! The token ticker \"ARIO\" and name \"AR.IO Network\" will appear automatically Click \"Import Asset\" to complete the process Once imported, you'll be able to view your total ARIO balance in your Wander wallet.Was this page helpful?YesNoComment",
          "estimatedWords": 267,
          "lastModified": "2025-06-27T16:13:09.026Z",
          "siteKey": "ario",
          "siteName": "AR-IO Network",
          "depth": 2,
          "crawledAt": "2025-06-27T16:13:09.026Z"
        },
        {
          "url": "https://docs.ar.io/gateways/observer",
          "title": "Observation and Incentives (OIP)",
          "content": "Observation and Incentives (OIP) Overview The Observation and Incentive Protocol is designed to maintain and enhance the operational integrity of gateways on the AR.IO Network. It achieves this through a combination of incentivizing gateways for good performance and tasking those gateways to fulfill the role of \"observers\". The protocol is intentionally simple and adaptable, employing a smart contract-based method for onchain “voting” to assess peer performance while being flexible on how that performance is measured. This setup permits gateway and observer nodes to experiment and evolve best practices for performance evaluation, all while operating within the bounds of the network's immutable smart contract, thus eliminating the need for frequent contract updates (forks). In this protocol, observers evaluate their gateway peers' performance to resolve ArNS names. Their aim is to ensure each gateway in the network accurately resolves a subset of names and assigning a pass / fail score based on their findings. A key component of the protocol is its reward mechanism. This system is predicated on gateway performance and compliance with observation duties. Gateways that excel are tagged as \"Functional Gateways\" and earn rewards, while those that do not meet the criteria, “Deficient Gateways” risk facing penalties – namely, the lack of rewards. Funds for incentive rewards are derived from the protocol balance, which consists of ARIO tokens initially allocated at network genesis as well as those collected from ArNS asset purchases. Every epoch, this balance is utilized to distribute rewards to qualifying gateways and observers based on certain performance metrics. Observation Protocol The Observation protocol is organized around daily epochs, periods of time that are broken into an observation reporting and tallying phase. The protocol is followed across each epoch, promoting consistent healthy network activity that can form pro-social behaviors and react to malicious circumstances. Onchain Reports The to-be-evaluated ArNS names include a set of two (2) names randomly determined by the protocol, known as “prescribed names”, which are common across all observers within the epoch, as well as a set of eight (8) “chosen names” picked at the discretion of each individual observer. “Prescribed names” are assigned to act as a common denominator / baseline while “chosen names” allow each observer to evaluate names that may be important to their operation. Observers shall upload their completed reports (in JSON format) to the Arweave network as an onchain audit trail. In addition, observers shall submit an interaction to the AR.IO smart contract detailing each gateway that they observed to have “failed” their assessments. These “votes” are tallied and used to determine the reward distribution. Selection of Observers The observer selection process commences at the beginning of each epoch and employs a random-weighted selection method. By combining random selection with weighted criteria like stake, tenure, and past rewards, the process aims to ensure both fairness and acknowledgment of consistent performance. This method allows for a systematic yet randomized approach to selecting gateways for observation tasks. Criteria for Selection Up to fifty (50) gateways can be chosen as observers per epoch. If the GAR is below that amount, then every gateway is designated as an observer for that epoch. If there are greater than 50, then randomized selection shall be utilized. The weighted selection criteria will consider the following for each gateway: Stake Weight (SW): This factor considers how financially committed a gateway is to the network. It is the ratio of the total amount of ARIO tokens staked by the gateway (plus any delegated stake) relative to the network minimum and is expressed as: SW = (Gateway Stake + Delegated Stake) / (Minimum Network Join Stake) Tenure Weight (TW): This factor considers how long a gateway has been part of the network, with a maximum value capped at four (4). This means that the maximum value is achieved after 2-years of participation in the network. It is calculated as: TW = (Gateway Network Tenure) / (6-months) Gateway Performance Ratio Weight (GPRW): This factor is a proxy for a gateway’s performance at resolving ArNS names. The weight represents the ratio of epochs in which a gateway received rewards for correctly resolving names relative to their total time on the network. To prevent division by zero conditions, it is calculated as: GPRW = (1 + Passed Epochs) / (1 + Participated Epochs) Observer Performance Ratio Weight (OPRW): This factor is a proxy for a gateway’s performance at fulfilling observation duties. The weight reflects the ratio of epochs in which a gateway, as an observer, successfully submitted observation reports relative to their total periods of service as an observer. To prevent division by zero conditions thus unfairly harming a newly joined gateway, it is calculated as: OPRW = (1 + Submitted Epochs) / (1 + Selected Epochs) Weight Calculation and Normalization For each gateway, a composite weight (CW) is computed, combining the Stake Weight, Tenure Weight, Gateway Performance Ratio Weight, and Observer Performance Ratio Weight. The formula used is: CW = SW x TW x GPRW x OPRW These weights are then normalized across the network to create a continuous range, allowing for proportional random selection based on the weighted scores. The normalized composite weight (N_CW) for each gateway indicates its likelihood of being chosen as an observer and is calculated by dividing the gateway's CW by the sum of all CWs. Any gateway with a composite weight equal to zero shall be ineligible for selection as an observer during the associated epoch. Random Selection Process The selection of observers is randomized within the framework of these weights. A set of unique random numbers is generated with entropy within the total range of normalized weights. For each random number, the gateway whose normalized weight range encompasses this number is selected. This system ensures that while gateways with higher weights are more likely to be chosen, all gateways maintain a non-zero chance of selection, preserving both fairness and meritocracy in the observer assignment process. The current epoch’s selected / prescribed observers as well as prescribed ArNS names to be evaluated shall be saved in the contract state at the beginning of the epoch to ensure that any activities during that epoch do not affect the selection of observers or awards distribution. Performance Evaluation Consider the following classifications: Functional or Passed Gateways: are gateways that meet or surpass the network’s performance and quality standards. Deficient or Failed Gateways: are gateways that fall short of the network's performance expectations. Functional or Submitted Observers: are selected observers who diligently perform their duties and submit observation reports and contract interactions. Deficient or Failed Observers: are selected observers who do not fulfill their duty of submitting observation reports and contract interactions. At the end of an epoch, the smart contract will assess the results from the observers and determine a pass / fail score for each gateway: If greater than or equal to 50% of submitted observer contract interactions indicate a PASS score, then that gateway is considered Functional and eligible for gateway rewards. Else, if greater than 50% of submitted observer contract interactions indicate a FAIL score, then that gateway is considered Deficient and ineligible for gateway rewards. These results will determine how reward distributions are made for that epoch. Rewards shall be distributed after forty (40) minutes (approx. twenty (20) Arweave blocks) in the following epoch have elapsed. This delay ensures that all observation contract interactions are safely confirmed by the Arweave network without risk of “forking out” prior to the evaluation and reward distribution process. Reward Distribution Each epoch, a portion of the protocol balance is earmarked for distribution as rewards. This value shall begin at 0.1% per epoch for the first year of operation, then linearly decline down to and stabilize at 0.05% over the following 6 months. From this allocation, two distinct reward categories are derived: Base Gateway Reward (BGR): This is the portion of the reward allocated to each Functional Gateway within the network and is calculated as: BGR = [Epoch Reward Allocation x 90% / Total Gateways in the Network] Base Observer Reward (BOR): Observers, due to their additional responsibilities, have a separate reward calculated as: BOR = [Epoch Reward Allocation x 10% / Total Selected Observers for the Epoch] Distribution Based on Performance The reward distribution is contingent on the performance classifications derived from the Performance Evaluation: Functional Gateways: Gateways that meet the performance criteria receive the Base Gateway Reward. Deficient Gateways: Gateways falling short in performance do not receive any gateway rewards. Functional Observers: Observers that fulfilled their duty receive the Base Observer Reward. Deficient Observers: Observers failing to meet their responsibilities do not receive observer rewards. Furthermore, if they are also Functional Gateways, their gateway reward is reduced by 25% for that epoch as a consequence for not performing their observation duty. Gateways shall be given the option to have their reward tokens “auto-staked” to their existing stake or sent to their wallet as unlocked tokens. The default setting shall be “auto-staked”. Distribution to Delegates The protocol will automatically distribute a Functional Gateway’s shared rewards with its delegates. The distribution will consider the gateway’s total reward for the period (including observation rewards), the gateway’s “Delegate Reward Share Ratio”, and each delegate’s stake proportional to the total delegation. Each individual delegate reward is calculated as: DRi = Total Rewards x Reward Share Ratio x (Delegate’s Stake / Total Delegated Stake) Unlike gateways, token reward distributions to delegated stakers will only be “auto-staked” in that they will be automatically added to the delegate’s existing stake associated with the rewarded gateway. The delegated staker is then free to withdraw their staked rewards at any time (subject to withdrawal delays). Undistributed Rewards In cases where rewards are not distributed, either due to the inactivity or deficiency of gateways or observers, the allocated tokens shall remain in the protocol balance and carry forward to the next epoch. This mechanism is in place to discourage observers from frivolously marking their peers as offline in hopes of attaining a higher portion of the reward pool. Note that if a gateway (and its delegates) leaves the network or a delegate fully withdraws stake from a gateway, they become ineligible to receive rewards within the corresponding epoch and the earmarked rewards will not be distributed. Handling Deficient Gateways To maintain network efficiency and reduce contract state bloat, gateways that are marked as deficient, and thus fail to receive rewards, for thirty (30) consecutive epochs will automatically trigger a “Network Leave” action and be subjesct to the associated stake withdrawal durations for both gateway stake and any delegated stake. In addition, the gateway shall have its minimum network-join stake slashed by 100%. The slashed stake shall be immediately sent to the protocol balance.Was this page helpful?YesNoComment",
          "estimatedWords": 1778,
          "lastModified": "2025-06-27T16:13:09.672Z",
          "siteKey": "ario",
          "siteName": "AR-IO Network",
          "depth": 3,
          "crawledAt": "2025-06-27T16:13:09.672Z"
        },
        {
          "url": "https://docs.ar.io/gateways/release-notes",
          "title": "ARIO Release Notes",
          "content": "AR.IO Release Notes Overview Welcome to the documentation page for the AR.IO gateway release notes. Here, you will find detailed information about each version of the AR.IO gateway, including the enhancements, bug fixes, and any other changes introduced in every release. This page serves as a comprehensive resource to keep you informed about the latest developments and updates in the AR.IO gateway. For those interested in exploring the source code, each release's code is readily accessible at our GitHub repository: AR.IO gateway change logs. Stay updated with the continuous improvements and advancements in the AR.IO gateway by referring to this page for all release-related information. [Release 39] - 2025-06-17 This release enhances observability and reliability with new cache metrics, improved data verification capabilities, and automatic failover between chain data sources. The addition of ArNS-aware headers enables better data prioritization across the gateway network. This is a recommended but not urgent upgrade. Added Added filesystem cache metrics with cycle-based tracking. Two new Prometheus metrics track cache utilization: cache_objects_total (number of objects in cache) and cache_size_bytes (total cache size in bytes). Both metrics include store_type and data_type labels to differentiate between cache types (e.g., headers, contiguous_data). Metrics are updated after each complete cache scan cycle, providing accurate visibility into filesystem cache usage. Added X-AR-IO-Data-Id header to all data responses. This header shows the actual data ID being served, whether from a direct ID request or manifest path resolution, providing transparency about the content being delivered. Added automatic data item indexing when data verification is enabled. When ENABLE_BACKGROUND_DATA_VERIFICATION is set to true, the system now automatically enables data item indexing (ANS104_UNBUNDLE_FILTER) with an always: true filter if no filter is explicitly configured. This ensures bundles are unbundled to verify that data items are actually contained in the bundle associated with the Arweave transaction's data root. Added ArNS headers to outbound gateway requests to enable data prioritization. The generateRequestAttributes function now includes ArNS context headers (X-ArNS-Name, X-ArNS-Basename, X-ArNS-Record) in requests to other gateways and Arweave nodes, allowing downstream gateways to effectively prioritize ArNS data requests. Added configurable Docker Compose host port environment variables (CORE_PORT, ENVOY_PORT, CLICKHOUSE_PORT, CLICKHOUSE_PORT_2, CLICKHOUSE_PORT_3, OBSERVER_PORT) to allow flexible port mapping while maintaining container-internal port compatibility and security. Added Envoy aggregate cluster configuration for automatic failover between primary and fallback chain data sources. The primary cluster (default: arweave.net:443) uses passive outlier detection while the fallback cluster (default: peers.arweave.xyz:1984) uses active health checks. This enables zero-downtime failover between HTTPS and HTTP endpoints with configurable FALLBACK_NODE_HOST and FALLBACK_NODE_PORT environment variables. Changed Streamlined background data retrieval to reduce reliance on centralized sources. The default BACKGROUND_RETRIEVAL_ORDER now only includes chunks,s3, removing trusted-gateways and tx-data from the default configuration. This prioritizes verifiable chunk data and S3 storage for background operations like unbundling. Removed ar-io.net from default trusted gateways list and removed TRUSTED_GATEWAY_URL default value to reduce load on ar-io.net now that P2P data retrieval is re-enabled. Existing deployments with TRUSTED_GATEWAY_URL explicitly set will continue to work for backwards compatibility. [Release 38] - 2025-06-09 This release focuses on data integrity and security improvements, introducing trusted data verification and enhanced header information for data requests. Upgrading to this release is recommended but not urgent. Added Added X-AR-IO-Trusted header to indicate data source trustworthiness in responses. This header helps clients understand whether data comes from a trusted source and works alongside the existing X-AR-IO-Verified header to provide data integrity information. The system now filters peer data by requiring peers to indicate their content is either verified or trusted, protecting against misconfigured peers that may inadvertently serve unintended content (e.g., provider default landing pages) instead of actual Arweave data. Added If-None-Match header support for HTTP conditional requests enabling better client-side caching efficiency. When clients send an If-None-Match header that matches the ETag, the gateway returns a 304 Not Modified response with an empty body, reducing bandwidth usage and improving performance. Added digest and hash headers for data HEAD requests to enable client-side data integrity verification. Added EC2 IMDS (instance-profile) credential support for S3 data access, improving AWS authentication in cloud environments. Added trusted data flag to prevent caching of data from untrusted sources, ensuring only verified and reliable content is stored locally while still allowing serving of untrusted data when necessary. Changed Re-enabled ar-io-peers as fallback data source in configuration for improved data availability. Updated trusted node configuration to use arweave.net as the default trusted node URL. Updated ETag header format to use properly quoted strings (e.g., \"hash\" instead of hash) following HTTP/1.1 specification standards for improved compatibility with caching proxies and clients. [Release 37] - 2025-06-03 This is a recommended release due to the included observer robustness improvements. It also adds an important new feature - data verification for preferred ArNS names. When preferred ArNS names are set, the bundles containing the data they point to will be locally unbundled (verifying data item signatures), and the data root for the bundle will be compared to the data root in the Arweave chain (establishing that the data is on Arweave). To enable this feature, set your preferred ArNS names, turn on unbundling by setting ANS104_DOWNLOAD_WORKERS and ANS104_UNBUNDLE_WORKERS both to 1, and set your ANS104_INDEX_FILTER to a filter that will match the data items for your preferred names. If you don't know the filter, use {\"always\": true}, but be aware this will index the entire bundle for the IDs related to your preferred names. Note: this release contains migrations to data.db. If your node appears unresponsive please check core service logs to determine whether migrations are running and wait for them to finish. Added Added prioritized data verification system for preferred ArNS names, focusing computational resources on high-priority content while enabling flexible root transaction discovery through GraphQL fallback support. Added verification retry prioritization system with tracking of retry counts, priority levels, and attempt timestamps to ensure bundles do not get stuck retrying forever. Added improved observer functionality with best-of-2 observations and higher compression for more reliable network monitoring. Added MAX_VERIFICATION_RETRIES environment variable (default: 5) to limit verification retry attempts and prevent infinite loops for consistently failing data items. Added retry logic with exponential backoff for GraphQL queries to handle rate limiting (429) and server errors with improved resilience when querying trusted gateways for root bundle IDs. Changed Updated dependencies: replaced deprecated express-prometheus-middleware with the actively maintained express-prom-bundle library and updated prom-client to v15.1.3 for better compatibility and security. Updated Linux setup documentation to use modern package installation methods, replacing apt-key yarn installation with npm global install and updating Node.js/nvm versions. Improved route metrics normalization with explicit whitelist function for better granularity and proper handling of dynamic segments. Fixed Fixed docker-compose configuration to use correct NODE_MAX_OLD_SPACE_SIZE environment variable name. Fixed production TypeScript build configuration to exclude correct \"test\" directory path. Fixed Parquet exporter to properly handle data item block_transaction_index exports, preventing NULL value issues. Fixed bundles system to copy root_parent_offset when flushing data items to maintain data integrity. Fixed ClickHouse auto-import script to handle Parquet export not_started status properly. Fixed docker-compose ClickHouse configuration to not pass conflicting PARQUET_PATH environment variable to container scripts. Fixed verification process for data items that have not been unbundled by adding queue bundle support and removing bundle join constraint to ensure proper verification of data items without indexed root parents. [Release 36] - 2025-05-27 This is a recommended but not essential upgrade. The most important changes are the preferred ArNS caching feature for improved performance on frequently accessed content and the observer's 80% failure threshold to prevent invalid reports during network issues. Added Added preferred ArNS caching functionality that allows configuring lists of ArNS names to be cached longer via PREFERRED_ARNS_NAMES and PREFERRED_ARNS_BASE_NAMES environment variables. When configured, these names will be cleaned from the filesystem cache after PREFERRED_ARNS_CONTIGUOUS_DATA_CACHE_CLEANUP_THRESHOLD instead of the standard cleanup threshold (CONTIGUOUS_DATA_CACHE_CLEANUP_THRESHOLD). This is accomplished by maintaining an MRU (Most Recently Used) list of ArNS names in the contiguous metadata cache. When filesystem cleanup runs, it checks this list to determine which cleanup threshold to apply. This feature enables gateway operators to ensure popular or important ArNS names remain cached longer, improving performance for frequently accessed content. Added ArNS headers to responses: X-ArNS-Name, X-ArNS-Basename, and X-ArNS-Record to help identify which ArNS names were used in the resolution. Changed Updated observer to prevent report submission when failure rate exceeds 80%. This threshold helps guard against both poorly operated observers and widespread network issues. In the case of a widespread network issue, the assumption is that most gateway operators are well intentioned and will work together to troubleshoot and restore both observations and network stability, rather than submitting reports that would penalize functioning gateways. Updated default trusted gateway in docker-compose Envoy configuration to ar-io.net for improved robustness and alignment with core service configuration. Improved range request performance by passing ranges directly to getData implementations rather than streaming all data and extracting ranges. Fixed Fixed missing cache headers (X-Cache and other data headers) in range request responses to ensure consistent cache header behavior across all request types. Fixed async streaming for multipart range requests by using async iteration instead of synchronous reads, preventing potential data loss. Fixed ArNS resolution to properly exclude www subdomain from resolution logic. Fixed test reliability issues by properly awaiting stream completion before making assertions. Fixed chunk broadcasting to not await peer broadcasts, as they are best-effort operations. [Release 35] - 2025-05-19 This is a low upgrade priority release. It contains a small caching improvement and routing fix. Upgrading to help test it is appreciated but not essential. Changed Adjusted filesystem data expiration to be based on last request times rather than file access times which may be inaccurate. Adjusted CORS headers to include content-* headers. Fixed Fixed regex used to expose /api-docs when an apex ArNS name is set. [Release 34] - 2025-05-05 Given the resilience provided by adding a second trusted gateway URL, it is recommended that everyone upgrade to this release. Added Added peer list endpoints for retrieving information about Arweave peers and ar.io gateway peers. Added ar-io.net as a secondary trusted gateway to increase data retrieval resilience by eliminating a single point of failure. Added circuit breaker for Arweave peer chunk posting. Changed Created directories for DuckDB and Parquet to help avoid permission issues by the directories being created by containers. Fixed Fixed GraphQL ClickHouse error when returning block ID and timestamp. Fixed the tx-chunks-data-source to throw a proper error (resulting in a 404) when the first chunk is missing rather than streaming a partial response. [Release 33] - 2025-05-05 Added Added a [Parquet and ClickHouse usage guide]. Using ArDrive as an example, it provides step by step instructions about how to bulk load Parquet and configure continuous ingest of bundled data items into ClickHouse. This allows the ar-io-node to support performant GraphQL queries on larger data sets and facilitates sharing indexing work across gateways via distribution of Parquet files. Added support for configurable ArNS 404 pages using either: ARNS_NOT_FOUND_TX_ID: Transaction ID for custom 404 content ARNS_NOT_FOUND_ARNS_NAME: ArNS name to resolve for 404 content Added experimental /chunk/ GET route for serving chunk data by absolute offset either the local cache. Added support for AWS_SESSION_TOKEN in the S3 client configuration. Expanded ArNS OTEL tracing to improve resolution behavior observability. Added support for setting a ClickHouse username and password via the CLICKHOUSE_USERNAME and CLICKHOUSE_PASSWORD environment variable. When using ClickHouse, CLICKHOUSE_PASSWORD should always be set. However, CLICKHOUSE_USERNAME can be left unset. The username default will be used in that case. Added support for configuring the port used to connect to ClickHouse via the CLICKHOUSE_PORT environment variable. Changed Disabled ClickHouse import timing logging by default. It can be enabled via environment variable - DEBUG when running the service standalone or CLICKHOUSE_DEBUG when using Docker Compose Upgraded to ClickHouse 25.4. Fixed Ensure .env is read in clickhouse-import script. [Release 32] - 2025-04-22 Changed Reenabled parallel ArNS resolution with removal of misplaced global limit. Refer to release 30 notes for more details on configuration and rationale. Added a timeout for the last ArNS resolver in ARNS_RESOLVER_PRIORITY_ORDER. It defaults to 30 seconds and is configurable using ARNS_COMPOSITE_LAST_RESOLVER_TIMEOUT_MS. This helps prevent promise build up if the last resolver stalls. Fixed Fixed apex ArNS name handling when a subdomain is present in ARNS_ROOT_HOST. Fixed a case where fork recovery could stall due to early flushing of unstable chain data. Restored observer logs by removing unintentional default log level override in docker-compose.yaml. [Release 31] - 2025-04-11 Changed Improved peer TX header fetching by fetching from a wider range of peers and up/down weighting peers based on success/failure. Fixed Rolled back parallel ArNS resolution changes that were causing ArNS resolution to slow down over time. [Release 30] - 2025-04-04 Added Added support for filtering Winston logs with a new LOG_FILTER environment variable. Example filter: {\"attributes\":{\"class\":\"ArweaveCompositeClient\"}} to only show logs from that class. Use CORE_LOG_FILTER environment variable when running with docker-compose. Added parallel ArNS resolution capability. Configured via ARNS_MAX_CONCURRENT_RESOLUTIONS (default: 1). This foundation enables future enhancements to ArNS resolution and should generally not be adjusted at present. Changed Improved ClickHouse auto-import script with better error handling and continuous operation through errors. Reduced maximum header request rate per second to trusted node to load on community gateways. Optimized single owner and recipient queries on ClickHouse with specialized sorted tables. Used ID sorted ClickHouse table for ID queries to improve performance. Fixed Fixed data alignment in Parquet file name height boundaries to ensure consistent import boundaries. Removed trailing slashes from AO URLs to prevent issues when passing them to the SDK. Only prune SQLite data when ClickHouse import succeeds to prevent data loss during exports. [Release 29] - 2025-03-21 Changed Temporarily default to trusted gateway ArNS resolution to reduce CU load as much possible. On-demand CU resolution is still available as a fallback and the order can be modified by setting ARNS_RESOLVER_PRIORITY_ORDER. Remove duplicate network process call in on-demand resolver. Don't wait for network process debounces in the on-demand resolver. Slow network process dry runs no longer block fallback to next resolver. Added Added support for separate CUs URLs for the network and ANT processes via the NETWORK_AO_CU_URL and ANT_AO_CU_URL process URLs respectively. If either is missing the AO_CU_URL is used instead with a fallback to the SDK default URL if AO_CU_URL is also unspecified. Added CU URLs to on-demand ArNS resolver logs. Added circuit breakers for AR.IO network process CU dry runs. By default they use a 1 minute timeout and open after 30% failure over a 10 minute window and reset after 20 minutes. Fixed Owners in GraphQL results are now correctly retrieved from data based on offsets when using ClickHouse. [Release 28] - 2025-03-17 Changed Raised name not found name list refresh interval to 2 minutes to reduce load on CUs. This increases the maximum amount of time a user may wait for a new name to be available. Future releases will introduce other changes to mitigate this delay. Adjusted composite ArNS resolver to never timeout resolutions from the last ArNS resolver in the resolution list. Added Added support for serving a given ID or ArNS name from the apex domain of a gateway. If using an ID, set the APEX_TX_ID environment variable. If using an ArNS name, set the APEX_ARNS_NAME environment variable. Added BUNDLE_REPAIR_UPDATE_TIMESTAMPS_INTERVAL_SECONDS, BUNDLE_REPAIR_BACKFILL_INTERVAL_SECONDS, and BUNDLE_REPAIR_FILTER_REPROCESS_INTERVAL_SECONDS environment variables to control the interval for retrying failed bundles, backfilling bundle records, and reprocessing bundles after a filter change. Note: the latter two are rarely used. Queuing bundles for reprocessing via the /ar-io/admin/queue-bundle endpoint is usually preferable to automatic reprocessing as it is faster and offers more control over the reprocessing behavior. Fixed Signatures in GraphQL results are now correctly retrieved from data based on offsets when using ClickHouse. Adjusted exported Parquet file names to align with expectations of ClickHouse import script. Ensured that bundle indexing status is properly reset when bundles are manually queued after an unbundling filter change has been made. [Release 27] - 2025-02-20 Changed Set process IDs for mainnet. Increase default AO CU WASM memory limit to 17179869184 to support mainnet process. [Release 26] - 2025-02-13 Added Added a per resolver timeout in the composite ArNS resolver. When the composite resolver attempts resolution it is applied to each resolution attempt. It is configurable via the ARNS_COMPOSITE_RESOLVER_TIMEOUT_MS and defaults to 3 seconds in order to allow a fallback attempt before the default observer timeout of 5 seconds. Added a TURBO_UPLOAD_SERVICE_URL environment variable to support configuration of the bundler used by the observer (TurboSDK defaults are used if not set). Added a REPORT_DATA_SINK environment variable that enables switching the method used to post observer reports. With the default, turbo, it sends data items via a Turbo compatible bundler. Switching it to arweave will post base layer transactions directly to Arweave instead. Added a /ar-io/admin/bundle-status/<id> endpoint that returns the counters and timestamps from the bundles row in data.db. This can be used for monitoring unbundling progress and scripting (e.g., to skip requeuing already queued bundles). Added more complete documentation for filters. Changed Use arweave.net as the default GraphQL URL for AO CUs since most gateways will not have a complete local AO data item index. Use a default timeout of 5 seconds when refreshing Arweave peers to prevent stalled peer refreshes. Cache selected gateway peer weights for the amount of time specified by the GATEWAY_PEERS_WEIGHTS_CACHE_DURATION_MS environment variable with a default of 5 seconds to avoid expensive peer weight recomputation on each request. Chunk broadcasts to primary nodes occur in parallel with a concurrency limit defaulting to 2 and configurable via the CHUNK_POST_CONCURRENCY_LIMIT environment variable. Added circuit breakers for primary chunk node POSTs to avoid overwhelming chunk nodes when they are slow to respond. Fixed Properly cleanup timeout and event listener when terminating the data root computation worker. Count chunk broadcast exceptions as errors in the arweave_chunk_broadcast_total metric. [Release 25] - 2025-02-07 Added Added support for indexing and querying ECDSA signed Arweave transactions. Expanded the OpenAPI specification to cover the entire gateway API and commonly used Arweave node routes. ArNS undername record count limits are now enforced. Undernames are sorted based on their ANT configured priority with a fallback to name comparisons when priorities conflict or are left unspecified. Enforcement is enabled by default but can be disabled by setting the ARNS_RESOLVER_ENFORCE_UNDERNAME_LIMIT to false. Changed Renamed the ario-peer data source to ar-io-peers for consistency and clarity. ario-peer will continue to work for backwards compatibility but is considered deprecated. Use AR.IO gateway peers from the ar.io gateway address registry (GAR) as the last fallback for fetching data when responding to client data requests. This has the benefit of making the network more resilient to trusted gateway disruptions, but it can also result in nodes serving data from less trusted sources if it is not found in the trusted gateway. This can be disabled by using a custom ON_DEMAND_RETRIEVAL_ORDER that does not include ar-io-peers. Arweave data chunk requests are sent to the trusted node first with a fallback to Arweave peers when chunks are unavailable on the trusted node. This provides good performance by default with a fallback in case there are issues retrieving chunks from the trusted node. Increased the observer socket timeout to 5 seconds to accommodate initial slow responses for uncached ArNS resolutions. Disabled writing base layer Arweave signatures to the SQLite DB by default to save disk space. When signatures are required to satisfy GraphQL requests, they are retrieved from headers on the trusted node. Fixed Updated dependencies to address security issues. Improved reliability of failed bundle indexing retries. Fixed failure to compute data roots for verification for base layer data larger than 2GiB. Fixed observer healthcheck by correcting node.js path in healthcheck script. [Release 24] - 2025-02-03 Added Added a ARNS_ANT_STATE_CACHE_HIT_REFRESH_WINDOW_SECONDS environment variable that determines the number of seconds before the end of the TTL at which to start attempting to refresh the ANT state. Added a TRUSTED_GATEWAYS_REQUEST_TIMEOUT_MS environment that defaults to 10,000 and sets the number of milliseconds to wait before timing out request to trusted gateways. Added BUNDLE_REPAIR_RETRY_INTERVAL_SECONDS and BUNDLE_REPAIR_RETRY_BATCH_SIZE environment variables to control the time between queuing batches of bundle retries and the number of data items retrieved when constructing batches of bundles to retry. Added support for configuring the ar.io SDK log level via the AR_IO_SDK_LOG_LEVEL environment variable. Added a request_chunk_total Prometheus counter with status, source (a URL) and source_type (trusted or peer) labels to track success/failure of chunk retrieval in the Arweave network per source. Added a get_chunk_total Prometheus metric to count chunk retrieval success/failure per chunk. Added arns_cache_hit_total and arns_cache_miss_total Prometheus counters to track ArNS cache hits and misses for individual names respectively. Added arns_name_cache_hit_total and arns_name_cache_miss_total Prometheus counters to track ArNS name list cache hits and misses respectively. Added a arns_resolution_duration_ms Prometheus metric that tracks summary statistics for the amount of time it takes to resolve ArNS names. Changed In addition to the trusted node, the Arweave network is now searched for chunks by default. All chunks retrieved are verified against data roots indexed from a trusted Arweave node to ensure their validity. Default to a 24 hour cache TTL for the ArNS name cache. Record TTLs still override this, but in cases where resolution via AO CU is slow or fails, the cache will be used. In the case of slow resolution, CU based resolution will proceed in the background and update the cache upon completion. Switched to the ioredis library for better TLS support. Updated minor dependency minor versions (more dependencies will be updated in the next release). Bundles imports will no longer be re-attempted for bundles that have already been fully unbundled using the current filters if they are matched or manually queued again. Replaced references docker-compose in the docs with the more modern docker compose. Fixed Ensure duplicate data item IDs are ignored when comparing counts to determine if a bundle has been fully unbundled. Fixed worker threads failing to shut down properly when the main process stopped. Ensure bundle import attempt counts are incremented when bundles are skipped to avoid repeatedly attempting to import skipped bundles. Use observe that correctly ensure failing gateways are penalized in the AR.IO AO process. [Release 23] - 2025-01-13 Added Added FS_CLEANUP_WORKER_BATCH_SIZE, FS_CLEANUP_WORKER_BATCH_PAUSE_DURATION, and FS_CLEANUP_WORKER_RESTART_PAUSE_DURATION environment variables to allow configuration of number of contiguous data files cleaned up per batch, the pause between each batch, and the pause before restarting the entire cleanup process again. Added data_items_unbundled_total Prometheus metric that counts the total number of data items unbundled, including those that did not match the unbundling filter. Added a parent_type label that can be one of transaction or data_item to data item indexing metrics. Added a files_cleaned_total total Prometheus metric to enable monitoring of contiguous data cleanup. Added support for specifying the admin API via a file specified by the ADMIN_API_KEY_FILE environment variable. Added experimental support for posting chunks in a non-blocking way to secondary nodes specified via a comma separate list in the SECONDARY_CHUNK_POST_URLS environment variable. Changed Renamed the parent_type lable to contiguous_data_type on bundle metrics to more accurately reflect the meaning of the label. Reduced the maximum time to refresh the ArNS name list to 10 seconds to minimize delays in ArNS availability after a new name is registered. Changed /ar-io/admin/queue-bundle to wait for bundles rows to be written to the DB before responding to ensure that errors that occur due to DB contention are not silently ignored. Data items are now flushed even when block indexing is stopped. This allows for indexing batches of data items using the admin API with block indexing disabled. Adjust services in docker-compose to use unless-stopped as their restart policy. This guards against missing restarts in the case where service containers exit with a success status even when they shouldn't. Fixed Added missing created_at field in blocked_names table. Fixed broken ArNS undername resolution. [Release 22] - 2024-12-18 Added Added the ability to block and unblock ArNS names (e.g., to comply with hosting provider TOS). To block a name, POST { \"name\": \"<name to block>\" } to /ar-io/admin/block-name. To unblock a name, POST { \"name\": \"<name to unblock>\" } to /ar-io/admin/unblock-name. Changed Return an HTTP 429 response to POSTs to /ar-io/admin/queue-bundle when the bundle data import queue is full so that scripts queuing bundles can wait rather than overflowing it. Fixed Adjust ArNS length limit from <= 48 to <= 51 to match the limit enforced by the AO process. [Release 21] - 2024-12-05 Added Added a ClickHouse auto-import service. When enabled, it calls the Parquet export API, imports the exported Parquet into ClickHouse, moves the Parquet files to an imported subdirectory, and deletes data items in SQLite up to where the Parquet export ended. To use it, run Docker Compose with the clickhouse profile, set the CLICKHOUSE_URL to http://clickhouse:8123, and ensure you have set an ADMIN_KEY. Using this configuration, the core service will also combine results from ClickHouse and SQLite when querying transaction data via GraphQL. Note: if you have a large number of data items in SQLite, the first export and subsequent delete may take an extended period. Also, this functionality is considered experimental. We expect there are still bugs to be found in it and we may make breaking changes to the ClickHouse schema in the future. If you choose to use it in production (not yet recommended), we suggest backing up copies of the Parquet files found in data/parquet/imported so that they can be reimported if anything goes wrong or future changes require it. Added a background data verification process that will attempt to recompute data roots for bundles and compare them to data roots indexed from Arweave nodes. When the data roots match, all descendant data items will be marked as verified. This enables verification of data initially retrieived from sources, like other gateways, that serve contiguous data instead of verifiable chunks. Data verification can be enabled by setting the ENABLE_BACKGROUND_DATA_VERIFICATION environment variable to true. The interval between attempts to verify batches of bundles is configurable using the BACKGROUND_DATA_VERIFICATION_INTERVAL_SECONDS environment variable. Added a CHUNK_POST_MIN_SUCCESS_COUNT environment variable to configure how many Arweave nodes must accept a chunk before a chunk broadcast is considered successful. Added arweave_chunk_post_total and arweave_chunk_broadcast_total Prometheus metrics to respectively track the number of successful chunk POSTs to Arweave nodes and the number of chunks successfully broadcast. When resolving ArNS names, the entire list of names is now cached instead of individually checking whether each name exists. This reduces the load on AO CUs since the entire list can be reused across multiple requests for different names. Note: due to the default 5 minute interval between name list refreshes, newly registered may now take longer to resolver after initial registration. We intend to make further caching refinements to address this in the future. Added support for multiple prioritized trusted gateways configurable by setting the TRUSTED_GATEWAYS_URLS environment variable to a JSON value containing a mapping of gateway hosts to priorities. Data requests are sent to other gateways in ascending priority order. If multiple gateways share the same priority, all the gateways with the same priority are tried in a random order before continuing on to the next priority. Added support for caching contiguous data in S3. It is enabled by default when the AWS_S3_CONTIGUOUS_DATA_BUCKET and AWS_S3_CONTIGUOUS_DATA_PREFIX environment variables are set. Changed trusted-gateway was changed to trusted-gateways in ON_DEMAND_RETRIEVAL_ORDER and BACKGROUND_RETRIEVAL_ORDER. Renamed the S3 contiguous environment variables - AWS_S3_BUCKET to AWS_S3_CONTIGUOUS_DATA_BUCKET and AWS_S3_PREFIX to AWS_S3_CONTIGUOUS_DATA_PREFIX. [Release 20] - 2024-11-15 Added Exposed the core service chunk POST endpoint via Envoy. It accepts a Arweave data chunk and broadcasts it to either the comma separated list of URLs specified by the CHUNK_POST_URLs environment variable or, if none are specified, the /chunk path on URL specified by the TRUST_GATEWAY_URL environment variable. Added a X-AR-IO-Root-Transaction-Id HTTP header to data responses containing the root base layer transaction ID for the ID in question if it's been indexed. Added a X-AR-IO-Data-Item-Data-Offset HTTP header containing the offset of the data item relative to the root bundle base layer transaction for it. In conjunction with X-AR-IO-Root-Transaction-Id, it enables retrieving data for data item IDs from base layer data using first a HEAD request to retrieve the root ID and data offset followed by a range request into the root bundle. This greatly increases the likelihood of retriving data item data by ID since only an index into the base layer and Arweave chunk availability is needed for this access method to succeed. Added an experimental ClickHouse service to docker-compose.yaml (available via the clickhouse profile). This will be used as a supplemental GraphQL DB in upcoming releases. Added a data item indexing healthcheck that can be enabled by setting the RUN_AUTOHEAL environment variable to true. When enabled, it will restart the core service if no data items have been indexed since the value specified by the MAX_EXPECTED_DATA_ITEM_INDEXING_INTERVAL_SECONDS environment variable. [Release 19] - 2024-10-21 Fixed Adjusted data item flushing to use the bundle DB worker instead of the core DB worker to prevent write contention and failed flushes under heavy unbundling load. Added Added X-AR-IO-Digest, X-AR-IO-Stable, X-AR-IO-Verified, and ETag headers. X-AR-IO-Digest contains a base64 URL encoded representation of the SHA-256 hash of the data item data. It may be empty if the gateway has not previously cached the data locally. X-AR-IO-Stable contains either true or false depending on whether the associated Arweave transaction is more than 18 blocks old or not. X-AR-IO-Verified contains either true if the gateway has verified the data root of the L1 transaction or the L1 root parent of the data item or false if it has not. ETag contains the same value a X-AR-IO-Digest and is used to improve HTTP caching efficiency. Added support for using a different data source for on-demand and background data retrieval. Background data retrieval is used when unbundling. The background retrieval data source order is configurable using the BACKGROUND_RETRIEVAL_ORDER environment variable and defaults to chunks,s3,trusted-gateway,tx-data. Priority is given to chunk retrieval since chunks are verifiable. Added an /ar-io/admin/export-parquet/status to support monitoring of in-progress Parquet export status. Added sqlite_in_flight_ops Prometheus metric with worker (core, bundles, data, or moderation) and role (read or write) labels to support monitoring the number of in-flight DB operations. Added experimental Grafana and Prometheus based observability stack. See the \"Monitoring and Observability\" section of the README for more details. Changed Bundle data is now retrieved as chunks from Arweave nodes by default so that data roots can be compared against the chain (see entry about background retrieval above). Changed observer configuration to use 8 instead of 5 chosen names. These are combined with 2 names prescribed from the contract for a total of 10 names observed each epoch to provide increased ArNS observation coverage. Verification status is set on data items when unbundling a parent that has already been verified. [Release 18] - 2024-10-01 Fixed Improved performance of data attributes query that was preventing data.db WAL flushing. Added Added WAL sqlite_wal_checkpoint_pages Prometheus metric to help monitor WAL flushing. Added a POST /ar-io/admin/export-parquet endpoint that can be used to export the contents of the SQLite3 core and bundle DBs as Parquet. To trigger an export, POST JSON containing outputDir, startHeight, endHeight, and maxFileRows keys. The resulting Parquet files can then be queried directly using DuckDB or loaded into another system (e.g. ClickHouse). Scripts will be provided to help automate the latter in a future release. Added ARNS_RESOLVER_OVERRIDE_TTL_SECONDS that can be used to force ArNS names to refresh before their TTLs expire. Added a GET /ar-io/resolver/:name endpoint that returns an ArNS resolution for the given name. Changed Removed ArNS resolver service in favor of integrated resolver. If a standalone resolver is still desired, the core service can be run with the START_WRITERS environment variable set to false. This will disable indexing while preserving resolver functionality. Deduplicated writes to data.db to improve performance and reduce WAL growth rate. [Release 17] - 2024-09-09 Notes This release includes a LONG RUNNING MIGRATION. Your node may appear unresponsive while it is running. It is best to wait for it to complete. If it fails or is interrupted, removing your SQLite DBs (in data/sqlite by default) should resolve the issue, provided you are willing to lose your GraphQL index and let your node rebuild it. Fixed Use the correct environment variable to populate WEBHOOK_BLOCK_FILTER in docker-compose.yaml. Don't cache data regions retrieved to satisfy range requests to avoid unnecessary storage overhead and prevent inserting invalid ID to hash mappings into the data DB. Added Added a new ClickHouse based DB backend. It can be used in combination with the SQLite DB backend to enable batch loading of historical data from Parquet. It also opens up the possibility of higher DB performance and scalability. In its current state it should be considered a technology preview. It won't be useful to most users until we either provide Parquet files to load into it or automate flushing of the SQLite DB to it (both are planned in future release). It is not intended to be standalone solution. It supports bulk loading and efficient GraphQL querying of transactions and data items, but it relies on SQLite (or potentially another OLTP in the future) to index recent data. These limitations allow greatly simplified schema and query construction. Querying the new ClickHouse DB for transaction and data items via GraphQL is enabled by setting the CLICKHOUSE_URL environment variable. Added the ability to skip storing transaction signatures in the DB by setting WRITE_TRANSACTION_DB_SIGNATURES to false. Missing signatures are fetched from the trusted Arweave node when needed for GraphQL results. Added a Redis backed signature cache to support retrieving optimistically indexed data item signatures in GraphQL queries when writing data items signatures to the DB has been disabled. Added on-demand and composite ArNS resolvers. The on-demand resolver fetches results directly from an AO CU. The composite resolver attempts resolution in the order specified by the ARNS_RESOLVER_PRIORITY_ORDER environment variable (defaults to on-demand,gateway). Added a queue_length Prometheus metric to fasciliate monitoring queues and inform future optimizations Added SQLite WAL cleanup worker to help manage the size of the data.db-wal file. Future improvements to data.db usage are also planned to further improve WAL management. Changed Handle data requests by ID on ArNS sites. This enables ArNS sites to use relative links to data by ID. Replaced ARNS_RESOLVER_TYPE with ARNS_RESOLVER_PRIORITY_ORDER (defaults to on-demand,gateway). Introduced unbundling back pressure. When either data item data or GraphQL indexing queue depths are more than the value specified by the MAX_DATA_ITEM_QUEUE_SIZE environment variable (defaults to 100000), unbundling is paused until the queues length falls bellow that threshold. This prevents the gateway from running out of memory when the unbundling rate exceeds the indexing rate while avoiding wasteful bundle reprocessing. Prioritized optimistic data item indexing by inserting optimistic data items at the front of the indexing queues. Prioritized nested bundle indexing by inserting nested bundles at the front of the unbundling queue. [Release 16] - 2024-08-09 Fixed Fixed promise leak caused by missing await when saving data items to the DB. Modified ArNS middleware to not attempt resolution when receiving requests for a different hostname than the one specified by ARNS_ROOT_HOST. Added Added support for returning Content-Encoding HTTP headers based on user specified Content-Encoding tags. Added isNestedBundle filter enables that matches any nested bundle when indexing. This enables composite unbundling filters that match a set of L1 tags and bundles nested under them. Added ability to skip writing ANS-104 signatures to the DB and load them based on offsets from the data instead. This significantly reduces the size of the bundles DB. It can be enabled by setting the WRITE_ANS104_DATA_ITEM_DB_SIGNATURES environment variable to false. Added data_item_data_indexed_total Prometheus counter to count data items with data attributes indexed. Changed Queue data attributes writes when serving data rather than writing them syncronously. Reduced the default data indexer count to 1 to lessen the load on the data DB. Switched a number of overly verbose info logs to debug level. Removed docker-compose on-failure restart limits to ensure that services restart no matter how many times they fail. Modified the data_items_indexed_total Prometheus counter to count data items indexed for GraphQL querying instead of data attributes. Increased aggressiveness of contiguous data cleanup. It now pauses 5 seconds instead of 10 seconds per batch and runs every 4 hours instead of every 24 hours. [Release 15] - 2024-07-19 Fixed Fixed query error that was preventing bundles from being marked as fully imported in the database. Added Adjusted data item indexing to record data item signature types in the DB. This helps distinguish between signatures using different key formats, and will enable querying by signature type in the future. Adjusted data item indexing to record offsets for data items within bundles and signatures and owners within data items. In the future this will allow us to avoid saving owners and signatures in the DB and thus considerably reduce the size of the bundles DB. Added ARNS_CACHE_TTL_MS environment variable to control the TTL of ARNS cache entries (defaults to 1 hour). Added support for multiple ranges in a single HTTP range request. Added experimental chunk POST endpoint that broadcasts chunks to the comma-separate list of URLS in the CHUNK_BROADCAST_URLS environment variable. It is available at /chunk on the internal gateway service port (4000 by default) but is not yet exposed through Envoy. Added support for running an AO CU adjacent to the gateway (see README.md for details). Added X-ArNS-Process-Id to ArNS resolved name headers. Added a set of AO_... environment variables for specifying which AO URLs should be used (see docker-compose.yaml for the complete list). The AO_CU_URL is of particular use since the core and resolver services only perform AO reads and only the CU is needed for reads. Changed Split the monolithic docker-compose.yaml into docker-compose.yaml, docker-compose.bundler.yaml, and docker-compose.ao.yaml (see README for details). Replaced references to 'docker-compose' with 'docker compose' in the docs since the former is mostly deprecated. Reduce max fork depth from 50 to 18 inline to reflect Arweave 2.7.2 protocol changes. Increased the aggressiveness of bundle reprocessing by reducing reprocessing interval from 10 minutes to 5 minutes and raising reprocessing batch size from 100 to 1000. Use a patched version of Litestream to work around insufficient S3 multipart upload size in the upstream version. [Release 14] - 2024-06-26 Fixed Correctly handle manifest index after paths. [Release 13] - 2024-06-24 Added Added support for optimistically reading data items uploaded using the integrated Turbo bundler via the LocalStack S3 interface. Added X-AR-IO-Origin-Node-Release header to outbound data requests. Added hops, origin, and originNodeRelease query params to outbound data requests. Added support for fallback in v0.2 manifests that is used if no path in the manifest is matched. Changed Updated Observer to read prescribed names from and write observations to the ar.io AO network process. Updated Resolver to read from the ar.io AO network process. Fixed Modified optimistic indexing of data items to use a null parent_id when inserting into the DB instead of a placeholder value. This prevents unexpected non-null bundledIn values in GraphQL results for optimistically indexed data items. Modified GraphQl query logic to require an ID for single block GraphQL queries. Previously queries missing an ID were returning an internal SQLite error. This represents a small departure from arweave.net's query logic which returns the latest block for these queries. We recommend querying blocks instead of block in cases where the latest block is desired. Adjusted Observer health check to reflect port change to 5050. Security Modified docker-compose.yaml to only expose Redis, PostgreSQL, and LocalStack ports internally. This protects gateways that neglect to deploy behind a firewall, reverse proxy, or load balancer. [Release 12] - 2024-06-05 Added Added /ar-io/admin/queue-data-item endpoint for queuing data item headers for indexing before the bundles containing them are processed. This allows trusted bundlers to make their data items quickly available to be queried via GraphQL without having to wait for bundle data submission or unbundling. Added experimental support for retrieving contiguous data from S3. See AWS_* environment variables documentation for configuration details. In conjuction with a local Turbo bundler this allows optimistic bundle (but not yet data item) retrieval. Add experimental support for fetching data from gateway peers. It can be enabled by adding ario-peer to ON_DEMAND_RETRIEVAL_ORDER. Note: do not expect this work reliably yet! This functionality is in active development and will be improved in future releases. Add import_attempt_count to bundle records to enable future bundle import retry optimizations. Changed Removed version from docker-compose.yaml to avoid warnings with recent versions of docker-compose. Switched default observer port from 5000 to 5050 to avoid conflict on OS X. Since Envoy is used to provide external access to the observer API this should have no user visible effect. [Release 11] - 2024-05-21 Added Added arweave_tx_fetch_total Prometheus metric to track counts of transaction headers fetched from the trusted node and Arweave network peers. Changed Revert to using unnamed bind mounts due to cross platform issues with named volumes. [Release 10] - 2024-05-20 Added Added experimental support for streaming SQLite backups to S3 (and compatible services) using Litestream. Start the service using the docker-compose \"litestream\" profile to use it, and see the AR_IO_SQLITE_BACKUP_* environment variables documentation for further details. Added /ar-io/admin/queue-bundle endpoint for queueing bundles for import for import before they're in the mempool. In the future this will enable optimistic indexing when combined with a local trusted bundler. Added support for triggering webhooks when blocks are imported matching the filter specified by the WEBHOOK_BLOCK_FILTER environment variable. Added experimental support for indexing transactions and related data items from the mempool. Enable it by setting ENABLE_MEMPOOL_WATCHER to 'true'. Made on-demand data caching circuit breakers configurable via the GET_DATA_CIRCUIT_BREAKER_TIMEOUT_MS environment variable. This allows gateway operators to decide how much latency they will tolerate when serving data in exchange for more complete data indexing and caching. Rename cache header from X-Cached to X-Cache to mimic typical CDN practices. Add X-AR-IO-Hops and X-AR-IO-Origin headers in preparation for future peer-to-peer functionality. Upgrade to Node.js v20 and switch to native test runner. [Release 9] - 2024-04-10 Added Added experimental Farcaster Frames support, enabling simple Arweave based Frames with button navigation. Transaction and data item data is now served under /local/farcaster/frame/<ID>. /local is used as a prefix to indicate this functionality is both experimental and local to a particular gateway rather than part of the global gateway API. Both GET and POST requests are supported. Added an experimental local ArNS resolver. When enabled it removes dependence on arweave.net for ArNS resolution! Enable it by setting RUN_RESOLVER=TRUE, TRUSTED_ARNS_RESOLVER_TYPE=resolver, and TRUSTED_ARNS_RESOLVER_URL=http://resolver:6000 in your .env file. Added an X-Cached header to data responses to indicate when data is served from the local cache rather than being retrieved from an external source. This is helpful for interfacing with external systems, debugging, and end-to-end testing. Save hashes for unbundled data items during indexing. This enables reduction in data storage via hash based deduplication as well as more efficient peer-to-peer data retrieval in the future. [Release 8] - 2024-03-14 Added Added GraphQL SQL query debug logging to support trouble-shooting and performance optimization. Added support for indexing data items (not GraphQL querying) based solely on tag name. (example use case: indexing all IPFS CID tagged data items). Changes Observer data sampling now uses randomized ranges to generate content hashes. Reference gateway ArNS resolutions are now cached to improve report generation performance. Contract interactions are now tested before posting using dryWrite to avoid submitting interactions that would fail. /ar-io/observer/info now reports INVALID for wallets that fail to load. Fixed Fix data caching failure caused by incorrect method name in getData circuit breakers. Fix healthcheck when ARNS_ROOT_HOST includes a subdomain. [Release 7] - 2024 - 02 - 14 Added Add support for notifying other services of transactions and data items using webhooks (see README for details). Add support for filter negation (particularly useful for excluding large bundles from indexint). Improve unbundling throughput by decoupling data fetching from unbundling. Add Envoy and core service ARM builds. Changed Improve resouce cleanup and shutdown behavior. Don't save Redis data to disk by default to help prevent memory issues on startup for small gateways. Reduce the amount of data sampled from large files by the observer. Ensure block poa2 field is not chached to reduce memory consumption. [Release 6] - 2024-01-29 Fixed Update observer to improve reliability of contract state synchronization and evaluation. [Release 5] - 2024-01-25 Added Added transaction offset indexing to support future data retrieval capabilities. Enabled IPv6 support in Envoy config. Added ability to configure observer report generation interval via the REPORT_GENERATION_INTERVAL_MS environmental variable. (Intended primarily for development and testing) Changed Updated observer to properly handle FQDN conflicts. Renamed most created_at columns to index to indexed_at for consistency and clarity. Fixed Updated LMDB version to remove Buffer workaround and fix occasional block cache errors. [Release 4] - 2024-01-11 Added Added circuit breakers around data index access to reduce impact of DB access contention under heavy requests loads. Added support for configuring data source priority via the ON_DEMAND_RETRIEVAL_ORDER environment variable. Updated observer to a version that retrieves epoch start and duration from contract state. Changed Set the Redis max memory eviction policy to allkeys-lru. Reduced default Redis max memory from 2GB to 256MB. Improved predictability and performance of GraphQL queries. Eliminated unbundling worker threads when filters are configured to skip indexing ANS-104 bundles. Reduced the default number of ANS-104 worker threads from 2 to 1 when unbundling is enabled to conserve memory. Increased nodejs max old space size to 8GB when ANS-104 workers > 1. Fixed Adjusted paths for chunks indexed by data root to include the full data root. [Release 3] - 2023-12-05 Added Support range requests (PR 61, PR 64) Note: serving multiple ranges in a single request is not yet supported. Release number in /ar-io/info response. Redis header cache implementation (PR 62). New default header cache (replaces old FS cache). LMDB header cache implementation (PR 60). Intended for use in development only. Enable by setting CHAIN_CACHE_TYPE=lmdb. Filesystem header cache cleanup worker (PR 68). Enabled by default to cleanup old filesystem cache now that Redis is the new default. Support for parallel ANS-104 unbundling (PR 65). Changed Used pinned container images tags for releases. Default to Redis header cache when running via docker-compose. Default to LMDB header cache when running via yarn start. Fixed Correct GraphQL pagination for transactions with duplicate tags. Was this page helpful?YesNoComment",
          "estimatedWords": 7670,
          "lastModified": "2025-06-27T16:13:10.145Z",
          "siteKey": "ario",
          "siteName": "AR-IO Network",
          "depth": 4,
          "crawledAt": "2025-06-27T16:13:10.145Z"
        },
        {
          "url": "https://docs.ar.io/arns",
          "title": "Arweave Name System (ArNS)",
          "content": "Arweave Name System (ArNS) Overview Arweave URLs and transaction IDs are long, difficult to remember, and occasionally miscategorized as spam. The Arweave Name System (ArNS) aims to resolve these problems in a decentralized manner. ArNS is a censorship-resistant naming system stored on Arweave, powered by ARIO tokens, enabled through AR.IO gateway domains, and used to connect friendly domain names to permaweb apps, web pages, data, and identities. It's an open, permissionless, domain name registrar that doesn’t rely on a single TLD. This system works similarly to traditional DNS services, where users can purchase a name in a registry and DNS Name servers resolve these names to IP addresses. The system shall be flexible and allow users to purchase names permanently or lease them for a defined duration based on their use case. With ArNS, the registry is stored permanently on Arweave via AO, making it immutable and globally resilient. This also means that apps and infrastructure cannot just read the latest state of the registry but can also check any point in time in the past, creating a “Wayback Machine” of permanent data. Users can register a name, like ardrive, within the ArNS Registry. Before owning a name, they must create an Arweave Name Token (ANT), an AO Computer based token and open-source protocol used by ArNS to track the ownership and control over the name. ANTs allow the owner to set a mutable pointer to any type of permaweb data, like a page, app or file, via its Arweave transaction ID. Each AR.IO gateway acts as an ArNS Name resolver. They will fetch the latest state of both the ArNS Registry and its associated ANTs from an AO compute unit (CU) and serve this information rapidly for apps and users. AR.IO gateways will also resolve that name as one of their own subdomains, e.g., https://ardrive.arweave.net and proxy all requests to the associated Arweave transaction ID. This means that ANTs work across all AR.IO gateways that support them: https://ardrive.ar-io.dev, https://ardrive.g8way.io/, etc. Users can easily reference these friendly names in their browsers, and other applications and infrastructure can build rich solutions on top of these ArNS primitives. Name Registration There are two different types of name registrations that can be utilized based upon the needs of the user: Lease: a name may be leased on a yearly basis. A leased name can have its lease extended or renewed but only up to a maximum active lease of five (5) years at any time. Permanent (permabuy): a name may be purchased for an indefinite duration. Registering a name requires spending ARIO tokens corresponding to the name’s character length and purchase type. Name Registry The ArNS Registry is a list of all registered names and their associated ANT Process IDs. Key rules embedded within the smart contract include: Genesis Prices: Set within the contract as starting conditions. Dynamic Pricing: Varies based on name length, purchase type (lease vs buy), lease duration, and current Demand Factor. Name Records: Include a pointer to the Arweave Name Token process identifier, lease end time (if applicable), and undername allocation. Reassignment: Name registrations can be reassigned from one ANT to another. Lease Extension: Anyone with available ARIO Tokens can extend any name’s active lease. Lease to Permanent Buy: Anyone with available ARIO Tokens can convert a name’s lease to a permanent buy. Undername Capacity: Additional undername capacity can be purchased for any actively registered name. There is no cap on the maximum amount of undernames that a top-level ArNS name can have associated with it. Name Removal: Name records can only be removed from the registry if a lease expires, or a permanent name is returned to the protocol. Name Validation Rules All names registered shall meet the following criteria: Valid names include only numbers 0-9, characters a-z and dashes. Dashes cannot be leading or trailing characters. Dashes cannot be used in single character domains. 1 character minimum, 51 characters maximum. Shall not be an invalid name predesignated to prevent unintentional use/abuse such as www. Lease Expirations When a lease term ends, there is a grace period of two (2) weeks where the lease can be renewed before it fully expires. If this grace period elapses, the name is considered expired and returns to the protocol for public registration. Once expired, a name’s associated undername registrations and capacity also expire. A recently expired name’s registration shall be priced subject to the “Returned Name Premium” mechanics detailed below. Lease to Permabuy Conversions An actively leased name may be converted to a permanent registration. The price for this conversion shall be treated as if it were a new permanent name purchase. This functionality allows users to transition from leasing to permanent ownership based on changing needs and available resources. It generates additional protocol revenue through conversion fees, contributing to the ecosystem's financial health and reward system. Additionally, by maintaining fair value for name conversions, it ensures prices reflect current market conditions, promoting a balanced and fair environment. Permanent Name Return Users have the option to “return” their permanently registered names back to the protocol. This process allows users to relinquish their ownership, returning the name to the protocol for public re-registration. Only the Owner of a name can initiate a name return. When a permanent name is returned, the name is subject to a \"Returned Name Premium”, similar to expired leases. A key difference is that if the name is repurchased during the premium window, the proceeds are split between the returning owner and the protocol balance. Primary Names The Arweave Name System (ArNS) supports the designation of a \"Primary Name\" for users, simplifying how Arweave addresses are displayed across applications. A Primary Name is a user-friendly alias that replaces complex wallet addresses, making interactions and profiles easier to manage and identify. Users can set one of their owned ArNS names as their Primary Name, subject to a small fee. This allows applications to use a single, human-readable identifier for a wallet, improving user experience across the network. Arweave Name Token (ANT) To establish ownership of a record in the ArNS Registry, each record contains both a friendly name and a reference to an Arweave Name Token, ANT. Name Tokens are unique AO Computer based tokens / processes that give their owners the ability to update the Arweave Transaction IDs that their associated friendly names point to. The ANT smart contract process is a standardized contract that implements the specific Arweave Name Process specification required by AR.IO gateways who resolve ArNS names and their Arweave Transaction IDs. It also contains other basic functionality to establish ownership and the ability to transfer ownership and update the Arweave Transaction ID. Name Tokens have an owner, who can transfer the token and control its modifiable settings. These settings include modifying the address resolution time to live (ttl) for each name contained in the ANT, and other settings like the ANT Name, Ticker, and an ANT Controller. The controller can only manage the ANT and set and update records, name, and the ticker, but cannot transfer the ANT. Note that ANTs are initially created in accordance with network standards by an end user who then has to ability to transfer its ownership or assign a controller as they see fit. Owners of names should ensure their ANT supports evolve ability if future modifications are desired. Loss of a private key for a permanently purchased name can result in the name being \"bricked”. Secondary markets could be created by ecosystem partners that facilitate the trading of Name Tokens. Additionally, tertiary markets could be created that support the leasing of these friendly names to other users. Such markets, if any, would be created by third parties unrelated to and outside of the scope of this paper or control of the Foundation. The table below indicates some of the possible interactions with the ArNS registry, corresponding ANTs, and who can perform them: ← Swipe to see more →ANT InteractionsTypeANT OwnerANT ControllerAny ARIO Token HolderTransfer ownership✔Add / remove controllers✔Set or change primary name✔Reassign name to new ANT process✔Return a permanent name✔Set records (pointers)✔✔Update records, name, ticker✔✔Update descriptions and keywords✔✔Create and assign undernames✔✔Extend / renew lease✔✔✔Increase undernames✔✔✔Convert lease to permanent✔✔✔← Swipe to see more → ANT Interactions Under_names ANT owners and controllers can configure multiple subdomains for their registered ArNS name known as “under_names” or more easily written “undernames”. These undernames are assigned individually at the time of registration or can be added on to any registered name at any time. Under_names use an underscore “_” in place of a more typically used dot “.“ to separate the subdomain from the main ArNS domain. Addressing Variable Market Conditions The future market landscape is unpredictable, and the AR.IO Network smart contract is designed to be immutable, operating without governance or manual intervention. Using a pricing oracle to fix name prices relative to a stable currency is not viable due to the infancy of available solutions and reliance on external dependencies. To address these challenges, ArNS is self-contained and adaptive, with name prices reflecting network activity and market conditions over time. To achieve this, ArNS incorporates: A dynamic pricing model that adjusts fees using a \"Demand Factor\" based on ArNS purchase activity. A Returned Name Premium (RNP) system that applies a timed, descending multiplier to registration prices for names that have recently expired or been returned to the protocol. This approach ensures that name valuations adapt to market conditions within the constraints of an immutable, maintenance-free smart contract framework. Dynamic Pricing Model ArNS employs an adaptive pricing model to balance market demand with pricing fairness for name registration within the network. This model integrates static and dynamic elements, adjusting prices based on name length and purchase options like leasing, permanent acquisition, and undername amounts. A key element is the Demand Factor (DF), which dynamically adjusts prices according to network activity and revenue trends, ensuring prices reflect market conditions while remaining accessible and affordable. A detailed description of the variables and formulas used for dynamic pricing can be found in the Appendix. Returned Name Premiums (RNP) ArNS applies a Returned Name Premium (RNP) to names that re-enter the market after expiration or permanent return. This premium starts at a maximum value and decreases linearly over a predefined window, ensuring fair and transparent pricing for re-registered names. The RNP multiplier is applied to the registration price of both permanently purchased and leased names. Gateway Operator ArNS Discount Gateway operators who demonstrate consistent, healthy participation in the network are eligible for a 20% discount on certain ArNS interactions. To qualify: The gateway must maintain a “Gateway Performance Ratio Weight” (GPRW) of 0.85 or higher. The gateway must have a “Tenure Weight” (TW) of 0.5 or greater, indicating at least a 3-month prior commitment to the network. A gateway marked as “Leaving” shall not be eligible for this discount. Eligible ArNS Discounted Interactions: Purchasing a name Extending a lease Upgrading a lease to permabuy Increasing undernames capacity Was this page helpful?YesNoComment",
          "estimatedWords": 1826,
          "lastModified": "2025-06-27T16:13:10.185Z",
          "siteKey": "ario",
          "siteName": "AR-IO Network",
          "depth": 2,
          "crawledAt": "2025-06-27T16:13:10.185Z"
        },
        {
          "url": "https://docs.ar.io/ar-io-sdk/getting-started",
          "title": "Getting Started",
          "content": "Getting Started Prerequisites node >= v18.0.0 npm or yarn Installation Installation Methodsnpmyarnnpm install @ar.io/sdk CopyCopied! Quick Start The following examples demonstrate how to use the AR.IO SDK to retrieve a list of active gateways from the Gateway Address Registry (GAR) across different environments. Node Example UsageESMCJSimport { ARIO } from '@ar.io/sdk/node' // set up client const ario = ARIO.init() // fetch gateways const gateways = await ario.getGateways() console.log(gateways) CopyCopied! Web PolyfillsPolyfills are not provided by default for bundled web projects (Vite, ESBuild, Webpack, Rollup, etc.) . Depending on your apps bundler configuration and plugins, you will need to provide polyfills for various imports including crypto, process and buffer. Refer to examples/webpack and examples/vite for examples. For other project configurations, refer to your bundler's documentation for more information on how to provide the necessary polyfills. Environment SetupBundlersBrowserimport { ARIO } from '@ar.io/sdk/web' // set up client const ario = ARIO.init() // fetch gateways const gateways = await ario.getGateways() console.log(gateways) CopyCopied! Output The output for obtaining a list of gateways, regardless of the environment used, will follow the structure outlined below: { \"items\": [ { \"gatewayAddress\": \"QGWqtJdLLgm2ehFWiiPzMaoFLD50CnGuzZIPEdoDRGQ\", \"observerAddress\": \"IPdwa3Mb_9pDD8c2IaJx6aad51Ss-_TfStVwBuhtXMs\", \"operatorStake\": 250000000000, \"settings\": { \"fqdn\": \"ar-io.dev\", \"label\": \"AR.IO Test\", \"note\": \"Test Gateway operated by PDS for the AR.IO ecosystem.\", \"port\": 443, \"properties\": \"raJgvbFU-YAnku-WsupIdbTsqqGLQiYpGzoqk9SCVgY\", \"protocol\": \"https\" }, \"startTimestamp\": 1720720620813, \"stats\": { \"failedConsecutiveEpochs\": 0, \"passedEpochCount\": 30, \"submittedEpochCount\": 30, \"totalEpochCount\": 31, \"totalEpochsPrescribedCount\": 31 }, \"status\": \"joined\", \"vaults\": {}, \"weights\": { \"compositeWeight\": 0.97688888893556, \"gatewayRewardRatioWeight\": 1, \"tenureWeight\": 0.19444444444444, \"observerRewardRatioWeight\": 1, \"normalizedCompositeWeight\": 0.19247316211083, \"stakeWeight\": 5.02400000024 } } ], \"hasMore\": true, \"nextCursor\": \"-4xgjroXENKYhTWqrBo57HQwvDL51mMdfsdsxJy6Y2Z_sA\", \"totalItems\": 316, \"sortBy\": \"operatorStake\", \"sortOrder\": \"desc\" } CopyCopied!Was this page helpful?YesNoComment",
          "estimatedWords": 264,
          "lastModified": "2025-06-27T16:13:10.585Z",
          "siteKey": "ario",
          "siteName": "AR-IO Network",
          "depth": 2,
          "crawledAt": "2025-06-27T16:13:10.585Z"
        },
        {
          "url": "https://docs.ar.io/ar-io-sdk/release-notes",
          "title": "ARIO SDK Changelog",
          "content": "AR.IO SDK Changelog Overview Welcome to the documentation page for the AR.IO SDK release notes. Here, you will find detailed information about each version of the AR.IO SDK, including the enhancements, bug fixes, and any other changes introduced in every release. This page serves as a comprehensive resource to keep you informed about the latest developments and updates in the AR.IO SDK. For those interested in exploring the source code, each release's code is readily accessible at our GitHub repository: AR.IO SDK change logs. Stay updated with the continuous improvements and advancements in the AR.IO SDK by referring to this page for all release-related information. 3.9.1 (2025-03-31) Bug Fixes ario: throw errors if fail to find epoch data (b1bb024) cli: update other vault APIs (0916e96) cli: update types and errors in cli (4b5aebd) io: do not return undefined for any API (ce6a077) 3.9.0 (2025-03-27) Bug Fixes ao: update retry logic on send, include more verbose messaging (70e6678) comments: cleanup (7444be3) exports: move types/browser exports above import/require (280d8bd) gql: add fallback to go to CU for epoch distribution data (12be216) logging: log errors more verbosely on read fails (c9fab18) messaging: only retry messaging if no id was received (9cbffef) options: add write options to addVersion (95ccef0) PE-7802: add logo to SpawnAntState type and options (c7adfca) send: add comment on not retrying on send (692938a) send: log on max attempts in send as well (2399d23) versions: export versions class (6368c44) Features versions: add ANT version class (c9ec5c5) versions: add versioning handlers to ant registry (c681909) 3.8.4 (2025-03-12) Bug Fixes types: add vault controller as optional to vault (f26bdb3) 3.8.3 (2025-03-05) Bug Fixes add missing maxDelegateRewardSharePct field from AoGatewayRegistrySettings (87942ad) schema: remove viem and use string for AOAddressSchema (090c799) schemas: update ant schema to accept eth support (7bc7df4) tests: update unit test to check loosely on eth address (b8e202b) 3.8.2 (2025-02-25) Bug Fixes missing break for happy path through send (e55ecc1) modify retry logic for send to only retry on exceptions from ao.message or ao.result (229df6b) modify retry logic to only occur on dryrun exceptions (c578893) protect against if retries is 0 (6aa1b58) 3.8.1 (2025-02-21) Bug Fixes mainnet: default to the mainnet process ID (a96713c) [3.8.0] (2025-02-20) View changes on Github Features mainnet: update constant to the mainnet process ID (4a11840) [3.7.1] (2025-02-19) View changes on Github Bug Fixes types: overload getEpoch to provide correct types on specific param requests (bafce74) 3.7.1-alpha.1 (2025-02-18) Bug Fixes types: overload getEpoch to provide correct types on specific param requests (bafce74) [3.7.0] (2025-02-17) View changes on GitHub Bug Fixes types: fix epoch settings type (a306baa) Features distributions: init paginated distributions cli command PE-7641 (8810ec6) distributions: init paginated getEligibleDistributions PE-7641 (9ba192f) distributions: remove eligible rewards from get epoch return PE-7641 (4437eaa) read commands: add commands epoch-settings, demand-factor-settings, read-action (821b6f6) [3.7.0-alpha.1] (2025-02-17) View changes on GitHub Features distributions: init paginated distributions cli command PE-7641 (8810ec6) distributions: init paginated getEligibleDistributions PE-7641 (9ba192f) distributions: remove eligible rewards from get epoch return PE-7641 (4437eaa) read commands: add commands epoch-settings, demand-factor-settings, read-action (821b6f6) [3.6.2-alpha.1] (2025-02-17) View changes on GitHub Bug Fixes types: fix epoch settings type (a306baa) [3.6.1] (2025-02-17) View changes on GitHub Bug Fixes types: correct types for demand factor and gateway settings, update tests (583ffeb) [3.6.0] (2025-02-17) View changes on GitHub Bug Fixes ant ids: update module and lua source ids to ant 15 (b8d6c96) deps: bump aoconnect sdk (3896ee8) ids: bump module and lua source ids to ant 16 (cf6d0de) page size: set page size to 1000 on fetch all records util (5fa802e) request name: add fund from tag on request primary name api (be362ad) Features ant: add sorting to ANT records responses by default (4e74825) [3.5.3] (2025-02-12) View changes on GitHub Bug Fixes arns: use buy-name buy default for getCostDetails (d71f402) [3.5.2] (2025-02-12) View changes on GitHub Bug Fixes arns: use buy-name when fetching token cost by default (5585b4d) [3.5.1] (2025-02-09) View changes on GitHub Bug Fixes gql: use goldsky by default for fetching gql data (57f4948) zod: fix zod enforcement (08d5168) [3.5.0] (2025-02-06) View changes on GitHub Bug Fixes ant: add priority as an attribute on ANTs (f0c6758) ant: update types and add index for easy enforcement (3dd6df5) ant: use deterministic sort with no locale comparison (7f2e067) evolve: use fetch for data instead of arweave (6deb91c) module ids: update ant lua and module id (97e0628) Features arns stats: include arns stats type on epoch PE-7562 (f92ee91) [3.4.1] (2025-02-03) View changes on GitHub Bug Fixes epochs: getPrescribedObservers and getPrescribedNames should get data from GQL/arweave vs. the contract (d8fa25d) gar: mark old fields as deprecated, add new ones (18ca1b4) [3.4.0] (2025-01-31) View changes on GitHub Bug Fixes ant: add setUndername and setBasename apis (ce4abfe) Features ant apis: add commands for new methods (931e621) revokable vaults: init revokeVault and vaultedTransfer commands and methods PE-7514 (6ca44a1) vault apis: assert lock length in range PE-7541 (3585643) vault apis: init write methods+commands for create/extend/increase vault PE-7541 (b2e3cab) [3.3.1] (2025-01-29) View changes on GitHub Bug Fixes ant: bumps ids (8eb2e38) ants: module bump _wSmbjfSlX3dZNcqE8JqKmj-DKum9uQ_jB08LwOKCyw (8d442e0) ant: update ANT ids (9f37e76) boot: add boot loader logic to ant spawn util (f00ab47) lua: update lua code id (76822a2) tags: remove extra tags from spawn util (f308b84) [3.3.0] (2025-01-24) View changes on GitHub Bug Fixes ants: tag with ao authority (f08af65) ao: add ao client for ants in emitter (489c040) arconnect: use signDataItem method, signature is deprecated (11e2378) arweave: use defaultArweave when fetching data (acf3e02) error handling: trim escape codes from thrown error PE-7417 (6dcf641) error handling: use a consolidated regexp for msg.Error and msg.Tags.Error PE-7417 (770a81e) gql: add retries when fetching epoch distribution data from arweave (42c1534) ids: add module and code ids (7474ccd) import: use import from file (f8fe7b4) logs: add processId to read error logs, include stack trace (51b7e38) module id: update ant module id (9e122af) pagination: allow nested keys in sortBy pagination params utility type PE-7428 (8ae8d88) spawn: spawn ANTs with a custom ANT module instead of aos module (2359b5b) test: double test timeout (4a52b81) ts: add root dir (e33eba5) types: simplify types for init functions, cleanup contructors (2197d99) types: simplify types for init functions, cleanup contructors (cd0afa6) Features add writeAction sdk/cli command for utility PE-7417 (1953504) get all delegates: init getAllDelegates type/handler PE-7221 (b015582) get all delegates: init list-all-delegates command PE-7221 (a632563) get all vaults: init command PE-7220 (e74a6e4) get all vaults: init type and ARIO method PE-7220 (e8f5a74) io: fetch historical epoch data from gql (b627d55) [3.2.0] (2025-01-13) View changes on GitHub Bug Fixes ant: add getLogo api (eddc3a8) ario: use standardize tags for registration fees and cost details (3f5fdbe) io: remove new APIs (d916ab6) types: add Buy-Name to supported intent types (b5a6d01) Features ario: add new APIs to ario class, update ant removePrimaryNames tags (61e0ee8) cost-details: include returnedNameDetails when they exist on cost-details PE-7371 (9edfb79) [3.1.0] (2025-01-02) View changes on GitHub Bug Fixes don't get old arweave block timestamps on read actions (1792ee8) don't return null when stringified null is found in message data on ao.read (c5873e6) eth signer: use a unique anchor in ans-104 headers (8cd5587) format process errors to be more user friendly PE-7327 (3449e32) io: fix AoEpochData type, add prescribedNames (1ba3588) tags: prune out empty tags (de0ec83) types: fix funding plan vaults type (1cea7db) types: revert prescribedObserver type (ca60f6f) Features cost-details: init cli command get-cost-details PE-7114 (674626e) cost-details: init new cost method for exposing fundingPlan and discounts PE-7114 (c6910c8) fund-from: add Fund-From tag to eligible methods/commands PE-7291 (4d47270) primary names: add processID to read APIs PE-7307 (e01e6ce) remove usage of Tags.Timestamp in favor of computing epoch indexes PE-7338 (ee1bea0) [3.0.0] (2024-12-10) View changes on GitHub Bug Fixes ar.io cli: use global program from cli.ts scope for ar.io command PE-5854 (3e83298) expose instant param for decreaseOperatorStake function arg type (2fd1f5d) lua id: change lua id (d4907db) remove un-used import (5db9ac0) spawn-ant: use a valid default ttlSeconds (aea4aa7) use Keywords for setKeywords (19ab3ad) [3.0.0] (2024-12-10) View changes on GitHub Bug Fixes ar.io cli: use global program from cli.ts scope for ar.io command PE-5854 (3e83298) expose instant param for decreaseOperatorStake function arg type (2fd1f5d) lua id: change lua id (d4907db) remove un-used import (5db9ac0) spawn-ant: use a valid default ttlSeconds (aea4aa7) use Keywords for setKeywords (19ab3ad) Features ar-io cli: init balance command and CLI setup (94c630b) ar-io cli: init join-network command (fc9dc07) ar.io cli: add --cu-url global parameter PE-5854 (2346f5b) ar.io cli: enable confirmation prompts on each write action PE-5854 (9ac88bb) ar.io cli: include --tags input in write actions PE-5854 (4b9d03e) ar.io cli: init buy/upgrade/extend-record, inc-undernames, sub-auc-bid, req-prim-name PE-5854 (5eb3df2) ar.io cli: init decrease-delegate-stake instant/cancel-withdraw commands PE-5854 (f0e7b9e) ar.io cli: init epoch read commands PE-5854 (61e0fc3) ar.io cli: init get token cost and auction prices PE-5854 (867807d) ar.io cli: init get-delegations, get-arns-record, list-arns-records commands PE-5854 (d7cbde3) ar.io cli: init get-gateway-delegates and get-gateways commands PE-5854 (35a33ef) ar.io cli: init get-vault and get-gateway commands (d262243) ar.io cli: init increase/decrease-operator-stake commands PE-5854 (1312860) ar.io cli: init info command (c721374) ar.io cli: init leave-network, delegate-stake PE-5854 (40ebe06) ar.io cli: init pagination from CLI layer PE-5854 (f52ce1f) ar.io cli: init read/write ANT commands PE-5854 (392a9ef) ar.io cli: init redelegate-stake PE-5854 (7bf4a8e) ar.io cli: init save-observations PE-5854 (f80bb8c) ar.io cli: init spawn-ant and get-ant-state PE-5854 (119c765) ar.io cli: init token-supply command (b58d782) ar.io cli: init transfer command (5553584) ar.io cli: init update-gateway-settings PE-5854 (7a6aa4b) ar.io cli: stringify outputs for command line compatibility (3c04cac) ARIO token: change all IO references to ARIO (4f8135d) ARIO token: update all IO references to ARIO (8fb2188) returned names: remove/replace auction APIs in favor returned names (2c9826f) BREAKING CHANGES ARIO token: All exported IO and IOToken are now repleced with ARIO and ARIOToken respectively PE-7225 [2.6.0] (2024-12-05) View changes on GitHub Bug Fixes lua id: bump lua id for ANT 9 (9e8e7e8) use Keywords for setKeywords (99cccd4) Features get demand factor settings: init new IO method PE-6894 (ad2eb36) init get gateway registry settings PE-6895 (bb7b6b4) [2.5.5] (2024-11-28) View changes on GitHub Bug Fixes io: update gateway delegates api, add to README (65aa6a8) [2.5.4] (2024-11-28) View changes on GitHub Bug Fixes primary: support primary name in token cost API (b4edf47) [2.5.3] (2024-11-27) View changes on GitHub Bug Fixes ant lua id: update ant lua id (54ff68b) ant: update write handler types removes evolve handler name (d9f5de4) handler names: add primary name handlers (5192c09) [2.5.2] (2024-11-25) View changes on GitHub Bug Fixes io: fix tag for requestPrimaryName API (bdaeaaf) io: updated types and fixed apis for primary name requests (a297628) [2.5.1] (2024-11-22) View changes on GitHub Bug Fixes primary names: update type for getPrimaryNameRequest (bdd3a9f) [2.5.0] (2024-11-22) View changes on GitHub Bug Fixes ant: revert breaking change on records for ANT (58db878) arns: update reserved names to pagaination api (dacf0c5) cjs: remove ant validation from cjs test (50b8290) errors: we should be checking the result.Error as well as tags (7ffe131) eslint: remove unnecessary rule config (03a0552) getHandlers: remove redundant check (b0c9548) handlers: update handler name list (251695e) id and test: add test for old ant and add lua source id for new code (77601b2) io: add getDelegations to AoIORead (7c30c9b) io: use helper for computing timestamp (ffe6ff3) lint: ignore underscore vars (2c84d3d) lint: update lint rule for ignore args (136e44a) lint: update linter to allow nullable string (b985139) lua id: rollback lua id (89b8392) primary: add additional ANT handlers for primary names (c98b136) readme: make api headers h4 (395f7fb) readme: update readme with new apis on ant class (bce76d2) readme: use real outputs in example (1529f79) setLogo: call param txId instead of logo (cda5e1d) source id: name the source id tags the same on evolve and spawn (058c829) spawn: add lua source id to spawn (8850ed2) test: remove old test for validate (14a77dc) tests: add test for old ant (0489cb6) tests: add unit tests for util and move parsing of records to uitl (2d08c9a) tests: update ANT in tests to use v8 ant (1eff8a9) types: modify AoDelegation type (18bb755) types: restructure type construction (2ef04db) validation util: remove validation util (d803e59) validator: add comments and reformat into a more clear loop for creating the validation config (ea3e70c) vaults: add API for gateway vaults (923b2cd) Features delegations: add getter for staked and vaulted delegations PE-7093 (7182942) delegations: add SDK function to retrieve an address's delegations PE-7093 (07c9107) getRecords: update getRecords to return as flat array of objects (b9808c1) io: add getAllowedDelegates to IO (7d143e0) PE-6910: support primary name APIs (6ace606) PE-6910: support primary name APIs (82a5b44) redelegate stake: init IO methods PE-7159 (7539dd2) setLogo: add set logo api to ant class (c5812b1) util: move validation util to ant class (cad7149) validation util: simplify validation util (cd57929) validations: add write validation util (69fc131) [2.4.0] (2024-11-12) View changes on GitHub Bug Fixes ant: add reassignName to ant implementation (9e705a9) auctions: fix submitAuctionApi to accept type and years (6780a80) auctions: update auction APIs and types (5fd2ccc) auctions: update read APIs to fetch auctions, use vite example display active auction (32001c2) auctions: update types and add intervalMs (bc21200) corrected AoVaultData field to be startTimestamp (b9888bf) delegates: fixes type (ae7be5c) emitter: do non strict checks on state in arns emitter (6566a3c) emitter: provide strictness in constuctor (060df05) exports: add exports to barrel file (fec094e) exports: dont export http stuff) (d6369aa) io: consolidate instantGatewayWithdrawal and instantGatewayWithdrawal to just instantWithdrawal, update `cancelWithdrawal (ea9f3eb) io: include address in delegate type for gateway (46ef1a7) lint: add lint fix and missing bracket (72446aa) PE-7080: add apis for fetching paginated delegates (e3d4af2) schema: add strict mode to ANT with default to false (4864abf) schemas: add passthrough on schema checks for ants (9cb2776) schemas: add zod schemas and tests (feba587) schema: specify HandlerNames instead of Handlers (44cc472) schemas: update ant schema and tests (f3284ed) schema: update handlers schema (6ec52e4) strict: allow for passing in strict mode on apis (e147220) tag: small tweak to instant tag (663de6f) test: correct params for get record (f999c49) tests: add esm tests and remove redundant cjs tests (95244ea) tests: add js path on imports (db1520a) tests: simplify strict check on test (62c9140) types: add back delegates for AoGateway (d337a74) types: update types to match contract (cb7d2b4) types: use generic on PageParms for sortBy, update delegate types (7a1abc4) util: create schema parsing util to pretty format errors (367537a) validations: add zod schema validations on ant returns (163c2f1) withdrawls: update API for cancelling withdrawls to allow delegate and operator withdrawls (5cb680a) Features ant: adds set-keywords and set-description methods for ants) (3b260a2) ant: support releasing of name of ANTs (16363e8) arns: add upgradeRecord API (9c1726d) auctions: add auctions api to IO classes (974897b) delegates: add instant delegate withdrawal for a fee (4b4cb8f) getVault: init IO method PE-7081 (0e3cde2) paginated vaults: init SDK paginated vaults PE-7081 (6d079f9) paginated vaults: use flat array over nested vaults PE-7081 (e17cfb7) [2.3.2] (2024-10-16) View changes on GitHub Bug Fixes io: add getDemandFactor api (feab461) io: update getTokenSupply to type that returns full breakdown of tokens (e790055) types: add totalEligibleGateways to AoEpochDistributionData type (9a35d39) types: update gateways to include services (a3fe5b4) [2.3.1] (2024-10-09) View changes on GitHub Bug Fixes use AoEpochObservationData type to match what is coming back from contract (684abf3) [2.3.0] (2024-10-08) View changes on GitHub Bug Fixes ao: check messages is not empty to avoid .length error when evaluating outputs of dryrun (a7b4953) logs: enable logging in spawn and evolve utils (08ce71a) luaID: update lua id to latest for ant source code (9c13dd3) main: merge main back to alpha, release hotfixes on alpha (9299427) types: add source code tx id to ant state type (8949f04) types: fix types on ant (3bdb3a6) types: remove restricted type (b1fac75) types: update type and tests (877b03f) types: update types (883ffb3) Features delegates: add cancel delegate withdrawal method (a3827dc) io: add api for querying get registration fees handler to AoIORead class (7b3909f) [2.2.5] (2024-09-26) View changes on GitHub Bug Fixes ant: allow sending tags on ant write interactions (99c24f8) [2.2.4] (2024-09-26) View changes on GitHub Bug Fixes types: update getInfo types on IO (7a0d20d) [2.2.3] (2024-09-25) View changes on GitHub Bug Fixes types: update type and tests (877b03f) [2.2.2] (2024-09-23) View changes on GitHub Bug Fixes deps: update arbundles to @dha-team/arbundles (c41e4e4) [2.2.1] (2024-09-16) View changes on GitHub Bug Fixes types: correct totalEpochCount for gateway stats (f82fed8) [2.2.0] (2024-08-30) View changes on GitHub Bug Fixes logger: permit logger as argument for typeguard util and default it (45df626) register: update spawn ant to register at end of spawn (4320c80) signer: add typeguard util for aoSigner (0d7f210) signing: add aosigner to contract signer (3b0495a) tests: dont send messages to ao in e2e tests (e7108da) tests: reconfigure test structure (1872a26) tests: use test-wallet fixture in tests instead of generating anew each time (27a5dc2) typeguard: return true or false in typeguard and log the error (4b851c5) types: update types for epoch distributions (5aedf50) util: use ANTRegistry class for registering ant on spawn instead of aoconnect (350112d) Features ant id: update lua ant id to latest (968c30e) util: add AoAntState typeguard util (c6f457f) [2.1.0] (2024-08-07) View changes on GitHub Bug Fixes actions: ignore engines in action (7f6f87d) ant lua id: update to version Flwio4Lr08g6s6uim6lEJNnVGD9ylvz0_aafvpiL8FI (8cbd564) ant: remove data from ant object, none of our ant methods require data attributes (0f267c1) ao: update AoProcess to only support string | undefined (584aee1) arns: update event emitter to provide more events and logs while loading arns records (8775896) constants: do not set env var for ant registry (9e61cc7) deps: move arconnect to dev deps (34f07d2) emiter: use a set to filter out duplicate (7887af9) emitter: add page size param for emitter to increase amount of records per page to 50k (b6f2157) errors: use any type on error (f14ed5a) events: use arns name space for events (1d67dfe) evolve: call eval twice to ensure evolve txid is set (a6261e5) evolve: dont double eval (a2a9121) evolve: fixed evolve somehow (b06503b) example: dont spawn in example (d1d5147) example: remove unused arweave instance (d0035c0) format: fix linting issues in format (b72dc1f) gateway stats: update gateway stat types (a59b166) io: add api that returns the total token supply (261c85c) io: no longer add data to save observations (c017b52) lint: fix lint errors and warnings (e532f4e) lua id: set new lua id in constants (e4c3aaf) naming: name AoSigner property aoSigner (4604524) records: update arns emitter to use ant registry (e55a67b) signer: describe signing function as signer vs aoSigner in case of signer type changes (3b23f80) signer: move createAoSigner to be a util (7f7a0e6) signer: pass in signing function instead of signer class (cba16e3) signer: use AoSigner type as return type (8e95edd) spawn: update spawn to use ant registry id in the tags (28dae7f) tests: check the return of ACL on ant tests more granularly (350bab1) tests: update e2e tests to only read from ant registry (a61e0bf) tests: update web test to use ANT registry in app (38ca913) tests: use const for unchanging test vars (9f965e1) test: update browser test with data test id and render checks (93741cb) test: use a known wallet adddress in tests (9dac280) todo: remove completed todo comment (c868522) types: add gateway weights to AoGateway (e725198) types: check info on evolve util first (a44cca1) types: remove deprecated types (c674876) types: update AoGateway to include weights (5368668) types: update type name to what contract returns (99edbad) use custom event names to avoid overlap (5b919ac) utils: revert new util (c959c81) utils: update util to use ant registry (b2223d4) Features ant registry: add ant registry class (2056674) evolve: add evolve util (47bfe20) signing: add window arweave wallet to available signing options (7596aec) [2.0.2] (2024-07-12) View changes on GitHub Bug Fixes types: update gateway settings type to only support observerAddress (13e073b) [2.0.1] (2024-07-11) View changes on GitHub Bug Fixes logger: fixes the console logger to respect the log level provided by web clients (99d7993) [2.0.0] (2024-07-11) View changes on GitHub Bug Fixes arweave: use default arweave in IO (21d25b9) deps: replace bunyan or console depending on the client environment (9d940aa) log: allow log level configuration for clients (9cb0981) log: replace bunyan with winston to ensure browser compatibility (80b38e0) Features io: add paginated gateway support for larger state objects (e.g. balances, records, and gateways) (b23efa8) util: add utility for fetching all records (8df2aac) io: add leaveNetwork API (54222ce) BREAKING CHANGES deps: removes all smartweave implementations using warp-sdk. The result is an only AO compatible ANT and IO network contracts. Some utilities are preserved due to their usefulness. imports: modifies web named exports to provide esm and cjs exports instead of minified bundle. The web bundle was causing issues in bundled projects, and polyfills are no longer provided by default. Refer to the README for specifications on how to use the SDK for a web project. [1.2.2] (2024-07-11) View changes on GitHub Bug Fixes api: ensure timestamps are always in miliseconds (93b162f) [1.2.1] (2024-07-04) View changes on GitHub Bug Fixes io: default the IO process to use testnet (61bca5c) [1.2.0] (2024-07-03) View changes on GitHub Bug Fixes ant: add event emitter util for fetching ants (ee5287b) ant: fix read api and update types (977e0e3) ant: handle when no data is returned (1de6610) ants: separate out interfaces (60fd593) ant: update apis to implement interface (9c54db0) ant: update interface to expect undername instead of name for ant records (416cb3d) ao ant: add handler for get state (fd20aa7) ao reads: safely parse json (1ff5410) ao: add AR-IO-SDK tag to process interaction (e5b5603) ao: add default timestamp to getTokenCost (36fed1b) ao: add getPrescribedNames for epoch api (747fad2) ao: add retries to read interactions (67d59e2) ao: fix tag for join network, update observation response (556f5d5) ao: prune tags on joinNetwork (31978f9) ao read: fix interface to have ant getState api (4e95bbd) aos: update aos module id and lua id (e19139e) ao: support connection config params in AO (3e6a246) ao: support tags for all write interactions (67f8da9) ao: update APIs for ao interface to be more descriptive (f07ac36) ao: update epoch interfaces to support various inputs (ddc4c10) ao: update send on process to use proper signer and evalute result (4e2f65d) ao: update stake interface (427e8ba) ao: use types and connect config in ao process to wrap connect from ao (05b07cf) buy: require processId on buyRecord (cc5859f) deps: add eventemitter3 dep (1d50cd1) deps: use p-limit-lit to avoid jest issues (05e0673) emitter: add a end and some console logs in the example (bc4e6b8) emmiter: rename and move throttle to be variable powered (f9cf40d) epochs: fix epoch default timestamp (ffb9df7) events: return process ids on end of fetching (15e3f44) handlers: update handler names (720b178) io: add buyRecord API (30d5e74) io: add epoch-settings api and tests (56555ea) io: add init to provide custom process (8811016) io: separate out io/ao contract interfaces (d96fa59) io: update arns interactions on registry contract (9befe2a) pLimit: add pLimit for util to avoid ao throttling (5b13560) readds incorrectly removed descriptions (c77217a) revert purchasetype tag (2dc08df) spawn: add option state contractTxID to track where init state is from (1745766) tags: make remaining tags ans-116 compliant (d034c8c) tags: use updated ans-116 tag format for actions (261b788) timeout: increase timeout period on arns emitter (b5ddb5f) type: default to unknown return type for json (0bddce0) types: add ao ant state type (02dbacd) types: update some types for arns names and contract state (2d23241) updates to use IO class and process terminology (ec45d66) util: initial implementation of get ant process for wallet (885fa31) Features ant: add balance APIs to ant interface (ec67440) ant: add utility for fetchint ant modules owned by wallet (01f7ec9) ants: support ANT apis in SDK (b187aeb) ao utils: add spawn ant util (d02566e) ao: experiment with initial implementation of ao contract (6118cea) getInfo io: add getInfo method to io class (4ef25ec) IO: implement io/ao classes that call process apis (aab8967) [1.1.1] (2024-06-06) View changes on GitHub Bug Fixes api: default evaluation options on getArNSReservedNames api (0a1f22e) [1.1.0] (2024-06-03) View changes on GitHub Bug Fixes api: make evaluation options optional on the interface (9e5a1c0) api: remove unused variable for epochBlockHeight (98c5ebc) arweave: default to arweave.net (84c9653) axios: add back axios-retry (9aae4de) errors: throw AbortError on signal aborted (63bd395) getContracts: only implement util for now (6b29c2f) gql query: don't abstract the data protocol query (f0b8f77) imports: import type from base route warp-contracts (bf99a85) init: allow signer to be undefined and if so return readable (b6a05e2) init: fix type for init to allow undefined signer (0a64ea9) init: remove unnecessary destructuring (81af1af) interface: remove epochBlockHeight from interface (b646f08) types:remove DataItem from WriteInteractionResult (eadb1a1) types: use gql node interface for dataProtocolTransaction (79cebd9) warp: ensure contract init on read interactions (bc3d1b8) Features getContracts: add get contracts on network specific providers like WarpContract (603d36e) gql util: add smartweave gql utils (5ea3aab) write: add tags support to write interactions on warp-contract and saveObservations (46eb4c9) [1.0.8] (2024-05-29) View changes on GitHub Bug Fixes api: add getPriceForInteration api to ario contract (3b8083c) bundle: minify web bundle (9266676) api: use function map for method name (439ec1f) reserved: add reserved arns name get methods (ad203ef) signer: check if method is property of signer before using (c52783c) signer: modify signer to assume the signer type based on public key being undefined (b775c96) test: add dockerfile for running tests in certain node environments (86cf2ad) [1.0.7] (2024-05-23) View changes on GitHub Bug Fixes contract: add extendLease and increaseUndernameSupport apis (1b13b5e) types: fix the AtLeastOne type (ffd0869) deps: force arweavve to 1.15.1 (2448598) contract: make params required - properties and note (89db674) types: update tests and use overwrite type to allow mIOtoken for certain paramaters (badcece) api: change to increaseUndernameLimit (9b72c1e) docs: update ario apis (4af0862) tests: update extend test util to include a test domain (e959b7c) token: add mIO and IO token classes to exports (f47f7d5) types: add delegated gateway type (c877496) types: export the token types (dfc83ae) types: remove visible types (6ab1fc3) types: update Gateway delegates type to use the new GatewayDelegate (ac7e924) warp: bump warp version (db7344d) [1.0.6] (2024-05-07) View changes on GitHub Bug Fixes warp: bump warp to fix AbortError issue on warp imports for web (c9a5613) [1.0.5] (2024-05-02) View changes on GitHub Bug Fixes cjs: provide path alias for warp in cjs export (7f9bf9a) logger: replace winston with bunyan (0488f75) util: add FQDN regex that matches ArNS contract (e6d7396) utils: manally conver from b64 to b64url to avoid web polyfill issues (766035c) utils: use base64 for fromB64url util (42302ef) warp-contract: correctly throw error in write interaction (c2368dd) [1.0.4] (2024-04-30) View changes on GitHub Bug Fixes ario: update joinNetwork to accept observerWallet param (6a32dd1) [1.0.3] (2024-04-26) View changes on GitHub Bug Fixes signer: set owner before signing data (0b558f5) [1.0.2] (2024-04-25) View changes on GitHub Bug Fixes arweave: default to the arweave node import to avoid issues with browser environments (fc8c26e) cacheurl: use default cache url in warpcontract (a676a3c) init: cleanup init overload methods and tests (fa328d2) lint: address lint issue in ArIOWriteable (4a3ee89) tsconfig: modify some tsconfig settings to get isolated configs for web/cjs/esm (46b7acc) typeguards: make type guards accept unknowns (7f285bb) types: use generic types and modify the requirements for init functions (9350f78) utils: add writeInteraction types and update base64url logic (4f5476b) [1.0.1] (2024-04-23) View changes on GitHub Bug Fixes docs: improve README docs interface documentation for ArIO clients (b0da48c) [1.0.0] (2024-04-23) Bug Fixes actions: bump node setup action (4eb49cd) actions: freeze lockfile (dba7313) contractadd cache config in ario constructor (1f3c0ba) ant: add ant contract to exports (a2ff57b) ant: add signer to ant test (4581b8d) ant: default evaluation options for ant apis that do not take an… (#25) (0c8b55d) ant: default evaluation options for ant apis that do not take another parameter (7c59033) ant: default evaluation options for apis that do not require them (72b57d5) ant: fix API for getRecords (c714aa3) apis: remove epoch from distributions and observations (7b2d279) arbundle version: pin version (35ffab6) arbundles: update arbundles import (f02d83f) ario: add cache config in ario constructor (#11) (ecb279d) ario: formatting (c61570a) ario: make state provider nullable and default to remote arns-service provider (fa1cb72) ario: re-add contract default config (2296cc3) ario: remove unused cache property (7f2d02e) build: add setImmediate polyfill for web only (ad36776) build: remove redundant exported type (134319b) cache: remove cache folder (2ac9427) cacheURL: update ario cache url setting pattern to use custom url appropriately (c76e67d) cache: validate arweave id before setting it (5ba1175) casing: revert to lower case casing (b5da0ab) comments: make class logger private, remove comments (7483246) connect: add init static function on ario class to create interaction classes (765f39c) contract configuration: return cache url as well (b4a7bc3) contract functions: correct contract function names (ad9bc56) contracts: add configuration view method and update types (4fae4a2) contracts: remove write method and type from remote contract (740d8b8) contracttxid: make contractTxID require in remote state cache instance (dc82d21) contracttxid: make contractTxID required in remote state cache instance (#10) (bf651bb) ctrl flow: remove else from control flow (4b3c4c2) deps: pin arweave (d39391c) deps: remove axios-retry, will implement later (0218e95) deps: remove extra crypto-browserify (9b42898) deps: remove warp-contracts-deploy from deps (9d4f9fa) docs: remove docs folder (47e8403) drywrite: throw on bad drywrite and continue if successful (5052c0a) eslintignore: remove old file names (415c163) eslint: remove eslint comments and use this signer (32530eb) esm: add polyfills for crypto (dd8fbfe) esm: add polyfills for crypto (#27) (553822c) example web: update ario instatiation (77c6842) example: escape quotes in packagejson for example package json (fb47de0) example: simplify example and remove unused method on remote cache (81637f8) examples: update comments and fix package.json (db7140b) examples: update examples to use devnet (cc037ac) examples: update examples with records methods, and balance methods (a2d2a02) exports: add arweavesigner and arconnectsigner to exports, clean up docs (c7860ed) exports: update exports in indices (f794437) exports: update package exports to have index in src folder (2cce9e3) files: clean git cache of duplicate casing (e9eaa2d) filters: punt filters (1c23cb3) fixture: add type to arns state fixture (5bcac32) formating: format (3f30f77) gar write: fix types and flow on gar write (f5e7774) gateway: update gateway settings to support autostake (82c6840) generics: use named generic (4b647f0) gitignore: remove cache from gitignore (2867abc) git: test fix with file casing issue (c3611ee) headers: use source-version for header (2b26d88) http: add headers sdk headers to http config (94810ed) husky: add commit hooks (885ce68) imports: update to use indexed imports from warp (1242568) indentation: fix indentation in examples (a266731) interface: removed filters and added base records types (849834d) interface: rename interface to ContractCache (2a0a765) jest: remove extra config (014fbde) lint: disable no-any warning certain types (de5f108) lint: formatting (21224e2) logger, errors, http: Updated to axios and axios-retry, added winston logger, more extensive custom error objects (b944f4d) logger: remove unused logger property (9501d1d) logs: removing debug logs (f025171) mixin: filter private methods in mixin util (beb8610) naming: change epoch to epochStartHeight (908971c) naming: rename getRecord[s] to getArNSRecord[s] (bd3d4bc) overloads: only accept warp contract as a contract config for ariowritable (e3c97e9) polyfills: rollback polyfill on logger (0cdb2f0) postinstall: remove husky postinstall script (c74a135) readme: add grammar and example recs (ecc07f7) readme: condense quick start (b35e5bd) readme: refactor api list to header tags (817d99b) readme: update ant header (77235ce) readme: update ANT usage description (70c8520) readme: update joinNetwork docs (9fcf440) readme: update quick start (a60d96a) readme: update readme with default provider example (68a5a16) readme: update readme with examples (d9ee23e) record records: update key to use result instead of record (90314db) records: remove contractTxId filter remove lodash shrink readme (50669e1) records: use state endpoint to fetch records (2f02c53) recs: modify the interfaces for contracts and implement with warp and remote service (#13) (56ebb08) release: remove release assets entirely (9d5a1b3) release: update github release config to publish packages to github (5534d9d) remote: getState not properly setting evalTo in http requests (55745c1) safety: update type safety checks (32eebbc) setimmediate: make set immediate a build dependency as it is required by the node winston (9292eaa) signer: check that contract is connected before trying to write (d352e9c) signer: check that contract is connected before trying to write (#29) (536a116) signer: fix signer in WarpContracts - update tests (ea9448f) signer: fix signer in WarpContracts - update tests (#32) (16d69d8) signer: remove jwk use, ignore web example for now (bc7e577) signer: remove signer, will do in other pr (d02276d) signer: remove use of JWK, simplify constructor (#22) (d2ef573) signer: update ANT to have signer (c7f8eee) structure: update cache provider folder to be named caches (844c1aa) structure: use snake case for file and folder names (37f27d3) test warp-contract: use beforeAll to read env vars (95cc019) tests: add test cases as a const (8458185) tests: add test for custom arIO client config (0e6142b) tests: change control flow pattern to .catch instead of trycatch (883de51) tests: dont make blockHeight or sortKey undefined but rather evalTo (f76a201) tests: instantiate new ant to connect in tests (9869415) tests: remove dryWrite from writeInteraction, update tests (bc1becc) tests: remove fixture and use live service for tests (30d3e8c) tests: test 404 response (590dea6) tests: update ario test (4208bd0) tests: update client instantiation test to check read vs write clients (059653c) tests: update docker compose params (a71befd) tests: update gateways test (1fcb3e6) tests: update stubs in tests (e4bbc6e) tests: update test to match jest syntax (553bdbb) tests: update tests for named prop expectation (4ea04a7) tests: update tests to use younger contract, add evalParams config (ae890c8) tests: update tests with constants and update types (1bdcfeb) tests: update tests with new name (2cd1b5c) tests: update with new names on methods (619c193) tests: use angela for testing (10f30fe) tests: use http not https in tests (fddba1e) tests: use process vars as priority url (faab4f3) test: update test to use ArweaveTransactionID class (f6c4f8b) tsconfig, names: reverted tsconfig to nodenext resolution, changed naming convention on provider, removed extraeneous error classes, rolled back axios-retry to match our tsconfig settings (d412d44) tyeps: set types to objects rather than top level params for easier readability (edfd77b) type: rename all type implementations (5959045) types and tests: update evalTo to allow undefined sortKey and block and test that (a59f05c) types: add @ to records (53601c1) types: make props nullable on certain read apis (f8ff552) types: remove any type (5c80242) types: remove any types (d8d910b) types: remove ArweaveTransactionID type for now (3adf53b) types: remove unnecesssary empty defaults (7d14edb) types: rename signer to ContractSigner (87d6c90) types: require atleast one param to update gateway settings (857ebdc) types: update interaction type to only use read for now (2c02e90) types: update tests, readme, and types (e9985dd) types: use partial write type (fa6a638) types: use string instead of any (014a262) validate id: make validator a private method (dce4a94) validity util: isBlockheight check more strict (2b28675) warp contract: added test for getting state after connecting with warp (060ee2c) warp-contract: provide logger - update isTransaction flow ctrl - use typed props (5f6e0a1) warp-contracts: bump warp to 1.4.38 - fixed warp exports (af4a20b) winston: move the winston polyfill - this will prevent any esm based web projects from getting polyfill issues (c8b7998) write: add dry run - sync state - abortSignal - update interface (970bdef) write: update utils - change error flow - update arweave constructor props (0a81c92) write: update write methods on warp (9c0540b) yarn: update lockfile (fd5e0ee) Features ant: add ANT read interface (c941c96) ant: create ant contract class for interacting with ant contracts (6eb7ef5) ants: add readable-writable framework to the ant client and implement write methods (3019f53) ario contract: add distributions and observation apis (21e38d1) arioContract: update ArIO interface and ArIOContract interface (5d87e2e) auctions: add auctions apis (faf08c5) contract: add distribution, observations apis, update readme and examples (0208317) contract: create new contract classes that impelement both warp and remote cache for ant contract and ar-io contracts (855da2d) first issue: setup examples, readme, and initial gateways provider (5a9e232) gar methods: add gar write methods to the ario client (e01b08b) inital providers: scaffold initial providers (4949514) io transfer: add transfer api to ario writable client (0d37623) observerations: add saveObservations write interaction (8dd977c) observers: add API for fetching prescribed observers (a18e130) observers: add API for fetching prescribed observers (#17) (17ce6de) PE-5742: add records api to arns remote cache (#8) (c46cd39) PE-5751: add blockheight and sortkey eval filters (#12) (832a1ad) PE-5758: add signer to ario class (#20) (1b82077) PE-5759: observations and distributions apis (#16) (dded361) PE-5773: add auctions read apis (#18) (e0c6fca) PE-5800: add epoch apis (48ee4ba) PE-5800: epoch apis (#15) (70563b1) PE-5825: ANT read interface (#19) (6a0c477) records: add records api to arns remote cache (1b7f54f) signer: add arweave signer to ario class (7e08097) write: add write interface and base implementation on warp-contract (6dfc969) Was this page helpful?YesNoComment",
          "estimatedWords": 5995,
          "lastModified": "2025-06-27T16:13:11.033Z",
          "siteKey": "ario",
          "siteName": "AR-IO Network",
          "depth": 2,
          "crawledAt": "2025-06-27T16:13:11.033Z"
        },
        {
          "url": "https://docs.ar.io/(https://github.com/ar-io/ar-io-sdk/compare/v3.7.1...v3.8.0)",
          "title": "Our vision is an internet with no more 404s but it looks like you just found one",
          "content": "404s suck!Our vision is an internet with no more 404s, but it looks like you just found one.Rest assured that if this page ever existed, it is still available on the permaweb.Back to Home",
          "estimatedWords": 34,
          "lastModified": "2025-06-27T16:13:11.129Z",
          "siteKey": "ario",
          "siteName": "AR-IO Network",
          "depth": 3,
          "crawledAt": "2025-06-27T16:13:11.129Z"
        },
        {
          "url": "https://docs.ar.io/(https://github.com/ar-io/ar-io-sdk/compare/v3.7.0...v3.7.1)",
          "title": "Our vision is an internet with no more 404s but it looks like you just found one",
          "content": "404s suck!Our vision is an internet with no more 404s, but it looks like you just found one.Rest assured that if this page ever existed, it is still available on the permaweb.Back to Home",
          "estimatedWords": 34,
          "lastModified": "2025-06-27T16:13:11.593Z",
          "siteKey": "ario",
          "siteName": "AR-IO Network",
          "depth": 3,
          "crawledAt": "2025-06-27T16:13:11.593Z"
        },
        {
          "url": "https://docs.ar.io/gateways",
          "title": "Gateway Architecture",
          "content": "Gateway Architecture Overview Gateways are the workhorses of the AR.IO Network. Their primary role is to act as a bridge between the Arweave network and the outside world. This means that a gateway's main task is to make it easier for users to interact with the Arweave network by simplifying the technical processes of writing, reading, and discovering data on the blockweave in a trust-minimized fashion. Gateway functions The functions of an AR.IO gateway are broken down into the following categories: Writing data involves: Proxying base layer transaction headers to one or more healthy and active Arweave nodes (miners) to facilitate inclusion in the mempools of as many nodes as possible. Proxying chunks for base layer Arweave transactions to Arweave nodes to help facilitate storage and replication of the chunks on the blockweave. Receiving and bundling so-called bundled data items (e.g., ANS-104 spec) as base layer transactions. Reading involves retrieving: Transaction headers for a base layer Arweave transaction. Individual data chunks for a base layer Arweave transaction. Blocks from the blockweave. Storage pricing rates for data from the Arweave node network. Contiguous streams of chunks representing an entire base layer transaction. Bundled data items (e.g., ANS-104). Wallet information (e.g., token balance). Discovering data involves: Facilitating efficient, structured queries for base layer transactions, bundled data items, and wallet data by: examining incoming streams of data (i.e., directly ingested transactions and data items, blocks emitted by the chain, etc.). managing index data in a database or analogous data store. Parsing and executing user queries. Facilitating friendly-path routing via Arweave manifest indexing. Including other benefits and capabilities such as: Facilitating friendly-subdomain-name routing to Arweave transactions via a direct integration with the Arweave Name System (ArNS). Providing the modularity and configurability necessary for operating extensible gateways that can be deployed at small or large scales to meet the needs of specific applications, use cases, communities, or business models. Providing pluggable means for consuming telemetry data for internal and external monitoring and alerting. Facilitating configurable content moderation policies. Providing connectivity to a decentralized network of other AR.IO gateways, enabling data sharing and other shared workloads. AR.IO Gateway Benefits AR.IO gateways provide many new benefits and capabilities beyond general Arweave gateways: Providing the modularity and configurability necessary for operating extensible gateways that can be deployed at small or large scales to meet the needs of specific applications, use cases, communities, or business models. Providing pluggable means for consuming telemetry data for internal and external monitoring and alerting. Facilitating friendly-subdomain-name routing to Arweave transactions via a direct integration with the Arweave Name System (ArNS). Facilitating configurable content moderation policies. Providing connectivity to a decentralized network of other AR.IO gateways, enabling data sharing and other shared workloads. Gateway Modularity A design principle of AR.IO gateways is that their core components should be interchangeable with compatible implementations. The core services in the gateway are written in Typescript, with flexible interfaces to the various subsystems and databases. This allows operators to customize their gateway to meet their specific requirements. Gateway services can be turned on or off depending on the operator's needs. For example, an operator might choose to have their gateway serve data, but not actively index Layer 2 bundled data. This flexibility also allows operators to utilize the technologies that are appropriate for the scale and environments in which they operate. For example, small scale operators might want to use low-overhead relational databases to power their indexing while larger scale operators might opt to use cloud-native, horizontally scalable databases. Analogous examples for storage and caching exist as well. ← Swipe to see more →Gateway Tech Stack OptionsTopologyChain IndexBundle IndexData IndexData StoreSmallSQLiteSQLiteSQLiteLocal File SystemLargePostgreSQLCassandraCassandraS3 Compatible← Swipe to see more → ARNS Indexing and Routing The Arweave Name System’s (ArNS) state is managed by the ARIO token’s smart contract. AR.IO gateways shall perform the following minimum functions relative to ArNS: Actively track state changes in the contract. Maintain up-to-date indexes for routing configurations based on the state of the ARIO contract as well as the states of the Arweave Name Token (ANT) contracts to which each name is affiliated. Manage the expiration of stale records. Facilitate ArNS routing based on the subdomains specified on incoming requests where appropriate. Provide a custom HTTP response header for ArNS requests indicating the corresponding Arweave transaction ID. Was this page helpful?YesNoComment",
          "estimatedWords": 717,
          "lastModified": "2025-06-27T16:13:11.866Z",
          "siteKey": "ario",
          "siteName": "AR-IO Network",
          "depth": 1,
          "crawledAt": "2025-06-27T16:13:11.866Z"
        },
        {
          "url": "https://docs.ar.io/gateways/gateway-network",
          "title": "Gateway network",
          "content": "Gateway network Overview The AR.IO Network consists of AR.IO gateway nodes, which are identified by their registered Arweave wallet addresses and either their IP addresses or hostnames, as stored in the network's smart contract Gateway Address Registry (GAR). These nodes adhere to the AR.IO Network’s protocols, creating a collaborative environment of gateway nodes that vary in scale and specialization. The network promotes a fundamental level of service quality and trust minimization among its participants. Being part of the network grants AR.IO gateways an array of advantages, such as: Simplified advertising of services and discovery by end users via the Gateway Address Registry. More rapid bootstrapping of key gateway operational data due to prioritized data request fulfillment among gateways joined to the network. Sharing of data processing results. Auditability and transparency through the use of AGPL-3 licenses, which mandate public disclosure of any software changes, thereby reinforcing the network's integrity and reliability. Improved network reliability and performance through an incentive protocol, which uses a system of evaluations and rewards to encourage high-quality service from gateways. Eligibility to accept delegated staking improving a gateway’s discoverability and reward opportunities. Gateway Address Registry (GAR) Any gateway operator that wishes to join the AR.IO Network must register their node in the AR.IO smart contract’s “Gateway Address Registry”, known as the GAR. Registration involves staking a minimum amount of ARIO tokens and providing additional metadata describing the gateway service offered. After joining the network, the operator’s gateway can be easily discovered by permaweb apps, its health can be observed, and it can participate in data sharing protocols. A gateway becomes eligible to participate in the network’s incentive protocol in the epoch following the one they joined in. The GAR advertises the specific attributes of each gateway including its stake, delegates, settings and services. This enables permaweb apps and users to discover which gateways are currently available and meet their needs. Apps that read the GAR can sort and filter it using the gateway metadata, for example, ranking gateways with the highest stake, reward performance, or feature set at the top of the list. This would allow users to prefer the higher staked, more rewarded gateways with certain capabilities over lower staked, less rewarded gateways. Data Sharing A key advantage and incentive for networked AR.IO gateways over standalone gateways is their ability to preferentially share various kinds of Arweave data among one another. Each gateway advertises its registered Arweave wallet address, so other network participants know who they are. Gateways can identify AR.IO Network peers by evaluating the Gateway Address Registry (GAR) within the AR.IO smart contract. They utilize that peer list to request as-yet-uncached data on behalf of their requesting clients or in service of their internal workflows. This can include requests for transaction header data, data items, and chunks. The Arweave Network shall act as the backstop for all block data, transaction header data, and chunk data. Additionally, gateways that receive requests for cache-missed data from other gateways can provide a higher quality of service to other AR.IO gateways than that which is provided to general users, apps, and infrastructure. However, gateways are not forced to share data with one another and can choose not to share their data if the intended recipient is acting maliciously. Such behaviors might include failure to reciprocate in data sharing, engaging in dishonest activities / observation, or distributing invalid data. Data Verification Gateway data verification is achieved by linking content hashes of transactions and data items to data roots on the Arweave base layer chain. Gateways index the chain from a trusted Arweave node and compute data roots for the base layer transaction data they download, ensuring that their data aligns with what was originally uploaded to Arweave. For base layer bundles that have already been verified, gateways compute hashes of individual data items, establishing a connection between the data root, the verified bundle, and the data items it contains. Gateways then expose these hashes and their verification status to users via HTTP headers on data responses.Was this page helpful?YesNoComment",
          "estimatedWords": 671,
          "lastModified": "2025-06-27T16:13:12.321Z",
          "siteKey": "ario",
          "siteName": "AR-IO Network",
          "depth": 2,
          "crawledAt": "2025-06-27T16:13:12.321Z"
        },
        {
          "url": "https://docs.ar.io/gateways/linux-setup",
          "title": "Linux Installation Instructions",
          "content": "Linux Installation Instructions Overview The following instructions will guide you through the process of installing the AR.IO node on a Linux machine, specifically Ubuntu 22.04.3 desktop on a home computer. Actual steps may differ slightly on different versions or distributions. This guide will cover how to set up your node, point a domain name to your home network, and create an nginx server for routing traffic to your node. No prior coding experience is required. System Requirements Please note, The AR.IO Node software is still in development and testing, all system requirements are subject to change. External storage devices should be formatted as ext4. Minimum requirements The hardware specifications listed below represent the minimum system requirements at which the AR.IO Node has been tested. While your Node may still operate on systems with lesser specifications, please note that AR.IO cannot guarantee performance or functionality under those conditions. Use below-minimum hardware at your own risk. 4 core CPU 4 GB Ram 500 GB storage (SSD recommended) Stable 50 Mbps internet connection Recommended 12 core CPU 32 GB Ram 2 TB SSD storage Stable 1 Gbps internet connection Install Packages If you would like to quickly install all required and suggested packages, you can run the following 4 commands in your terminal, and skip to installing the Node. sudo apt update -y && sudo apt upgrade -y && sudo apt install -y curl openssh-server git certbot nginx sqlite3 build-essential && sudo systemctl enable ssh && curl -o- https://raw.githubusercontent.com/nvm-sh/nvm/v0.40.3/install.sh | bash && source ~/.bashrc && sudo ufw allow 22 80 443 && sudo ufw enable CopyCopied! # Add Docker's official GPG key: sudo apt-get update sudo apt-get install ca-certificates curl sudo install -m 0755 -d /etc/apt/keyrings sudo curl -fsSL https://download.docker.com/linux/ubuntu/gpg -o /etc/apt/keyrings/docker.asc sudo chmod a+r /etc/apt/keyrings/docker.asc # Add the repository to Apt sources: echo \\ \"deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.asc] https://download.docker.com/linux/ubuntu \\ $(. /etc/os-release && echo \"$VERSION_CODENAME\") stable\" | \\ sudo tee /etc/apt/sources.list.d/docker.list > /dev/null sudo apt-get update CopyCopied! sudo apt-get install docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin CopyCopied! nvm install 20.11.1 && nvm use 20.11.1 && npm install -g yarn@1.22.22 CopyCopied! Required packages Update your software: sudo apt update sudo apt upgrade CopyCopied! Enable your firewall and open necessary ports: sudo ufw enable # Optional: If using SSH, allow port 22 sudo ufw allow 22 # Allow ports 80 and 443 for HTTP and HTTPS sudo ufw allow 80 sudo ufw allow 443 CopyCopied! Install nginx: sudo apt install nginx -y CopyCopied! Install git: sudo apt install git -y CopyCopied! Install Docker: # Add Docker's official GPG key: sudo apt-get update sudo apt-get install ca-certificates curl sudo install -m 0755 -d /etc/apt/keyrings sudo curl -fsSL https://download.docker.com/linux/ubuntu/gpg -o /etc/apt/keyrings/docker.asc sudo chmod a+r /etc/apt/keyrings/docker.asc # Add the repository to Apt sources: echo \\ \"deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.asc] https://download.docker.com/linux/ubuntu \\ $(. /etc/os-release && echo \"$VERSION_CODENAME\") stable\" | \\ sudo tee /etc/apt/sources.list.d/docker.list > /dev/null sudo apt-get update CopyCopied! sudo apt-get install docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin CopyCopied! Test Docker installation: sudo docker run hello-world CopyCopied! Install Certbot: sudo apt install certbot -y CopyCopied! Suggested packages These packages are not required to run a node in its basic form. However, they will become necessary for more advanced usage or customization. Install ssh (optional, for remote access to your Linux machine): sudo apt install openssh-server -y sudo systemctl enable ssh CopyCopied! Install NVM (Node Version Manager): curl -o- https://raw.githubusercontent.com/nvm-sh/nvm/v0.40.3/install.sh | bash source ~/.bashrc CopyCopied! Install Node.js: nvm install 20.11.1 CopyCopied! Install Yarn: npm install -g yarn@1.22.22 CopyCopied! Install build tools: sudo apt install build-essential CopyCopied! Install SQLite: sudo apt install sqlite3 -y CopyCopied! Install the Node Navigate to the desired installation location: NOTE: Your indexing databases will be created in the project directory unless otherwise specified in your .env file, not your Docker environment. So, if you are using an external hard drive, you should install the node directly to that external drive. Clone the ar-io-node repository and navigate into it: git clone -b main https://github.com/ar-io/ar-io-node cd ar-io-node CopyCopied! Create an environment file: nano .env CopyCopied! Paste the following content into the new file, replacing <your-domain> with the domain address you are using to access the node, and <your-public-wallet-address> with the public address of your Arweave wallet, save, and exit: GRAPHQL_HOST=arweave.net GRAPHQL_PORT=443 START_HEIGHT=1000000 RUN_OBSERVER=true ARNS_ROOT_HOST=<your-domain> AR_IO_WALLET=<your-public-wallet-address> OBSERVER_WALLET=<hot-wallet-public-address> CopyCopied! The GRAPHQL values set the proxy for GQL queries to arweave.net, You may use any available gateway that supports GQL queries. If omitted, your node can support GQL queries on locally indexed transactions, but only L1 transactions are indexed by default. START_HEIGHT is an optional line. It sets the block number where your node will start downloading and indexing transactions headers. Omitting this line will begin indexing at block 0. RUN_OBSERVER turns on the Observer to generate Network Compliance Reports. This is required for full participation in the AR.IO Network. Set to false to run your gateway without Observer. ARNS_ROOT_HOST sets the starting point for resolving ARNS names, which are accessed as a subdomain of a gateway. It should be set to the url you are pointing to your node, excluding any protocol prefix. For example, use node-ar.io and not https://node-ar.io. If you are using a subdomain to access your node and do not set this value, the node will not understand incoming requests. AR_IO_WALLET is optional, and sets the wallet you want associated with your Gateway. An associated wallet is required to join the AR.IO network. OBSERVER_WALLET is the public address of the wallet used to sign Observer transactions. This is required for Observer to run, but may be omitted if you are running a gateway outside of the AR.IO network and do not plan to run Observer. You will need to supply the keyfile to this wallet in the next step. More advanced configuration options can be found at ar.io/docs Supply Your Observer Wallet Keyfile: If you are running Observer, you need to provide a wallet keyfile in order to sign report upload transactions. The keyfile must be saved in the wallets directory in the root of the repository. Name the file <Observer-Wallet-Address>.json, replacing \"<Observer-Wallet-Address>\" with the public address of the wallet. This should match your OBSERVER_WALLET environmental variable. Learn more about creating Arweave wallets and obtaining keyfiles here Payment For Observer Report UploadsBy default, the Observer will use Turbo Credits to pay for uploading reports to Arweave. This allows reports under 100kb to be uploaded for free, but larger reports will fail if the Observer wallet does not contain Credits. Including REPORT_DATA_SINK=arweave in your .env file will configure the Observer to use AR tokens instead of Turbo Credits, without any free limit. Start the Docker container: sudo docker compose up -d CopyCopied! Explanation of flags: up: Start the Docker containers. -d: Run the containers as background processes (detached mode). NOTE: Effective with Release #3, it is no longer required to include the --build flag when starting your gateway. Docker will automatically build using the image specified in the docker-compose.yaml file. To ensure your node is running correctly, follow the next two steps. Check the logs for errors: sudo docker compose logs -f --tail=0 CopyCopied! Explanation of flags: -f: Follow the logs in real time. --tail=0: Ignore all logs from before running the command. NOTE: Previous versions of these instructions advised checking a gateway's ability to fetch content using localhost. Subsequent security updates prevent this without first unsetting ARNS_ROOT_HOST in your .env. Set up Networking The following guide assumes you are running your node on a local home computer. Register a Domain Name: Choose a domain registrar (e.g., Namecheap) to register a domain name. Point the Domain at Your Home Internet: Obtain your public IP address by visiting https://www.whatsmyip.org/ or running: curl ifconfig.me CopyCopied! Create an A record with your registrar for your domain and wildcard subdomains, using your public IP address. For example, if your domain is \"ar.io,\" create a record for \"ar.io\" and \"*.ar.io.\" Set up Port Forwarding: Obtain the local IP address of the machine where the node is installed by running: ip addr show | grep -w inet | awk '{print $2}' | awk -F'/' '{print $1}' CopyCopied! If there are multiple lines of output, choose the one starting with 192 (usually). Enter your router's IP address in the address bar of a browser (e.g., 192.168.0.1). If you're unsure of your router's IP address, consult your router's documentation or contact your Internet Service Provider (ISP). Navigate to the port forwarding settings in your router configuration. The exact steps may vary depending on your router model. Consult your router's documentation or support for detailed steps. Set up port forwarding rules to forward incoming traffic on ports 80 (HTTP) and 443 (HTTPS) to the same ports on the machine running your node. You may also forward port 22 if you want to enable SSH access to your node from outside your home network. Create SSL (HTTPS) Certificates for Your Domain: sudo certbot certonly --manual --preferred-challenges dns -d <your-domain>.com -d '*.<your-domain>.com' CopyCopied! Follow the instructions to create the required TXT records for your domain in your chosen registrar. Use a DNS checker to verify the propagation of each record. Email NotificationsPrevious versions of these instructions advised providing an email address to Certbot. As of June 2025, LetsEncrypt (the certificate authority used by Certbot) no longer supports email notifications. IMPORTANT: Wild card subdomain (*.<your-domain>.com) cannot auto renew without obtaining an API key from your domain registrar. Not all registrars offer this. Certbot certificates expire every 90 days. Be sure to consult with your chosen registrar to see if they offer an API for this purpose, or run the above command again to renew your certificates. You will receive an email warning at the address you provided to remind you when it is time to renew. Configure nginx: nginx is a free and open-source web server and reverse proxy server. It will handle incoming traffic, provide SSL certificates, and redirect the traffic to your node. Open the default configuration file: sudo nano /etc/nginx/sites-available/default CopyCopied! Replace the file's contents with the following configuration (replace \"<your-domain>\" when necessary): # Force redirects from HTTP to HTTPS server { listen 80; listen [::]:80; server_name <your-domain>.com *.<your-domain>.com; location / { return 301 https://$host$request_uri; } } # Forward traffic to your node and provide SSL certificates server { listen 443 ssl; listen [::]:443 ssl; server_name <your-domain>.com *.<your-domain>.com; ssl_certificate /etc/letsencrypt/live/<your-domain>.com/fullchain.pem; ssl_certificate_key /etc/letsencrypt/live/<your-domain>.com/privkey.pem; location / { proxy_pass http://localhost:3000; proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_http_version 1.1; } } CopyCopied! Save and exit nano. Test the configuration: sudo nginx -t CopyCopied! If there are no errors, restart nginx: sudo service nginx restart CopyCopied! Your node should now be running and connected to the internet. Test it by entering https://<your-domain>/3lyxgbgEvqNSvJrTX2J7CfRychUD5KClFhhVLyTPNCQ in your browser. Note: If you encounter any issues during the installation process, please seek assistance from the AR.IO community.Was this page helpful?YesNoComment",
          "estimatedWords": 1795,
          "lastModified": "2025-06-27T16:13:12.437Z",
          "siteKey": "ario",
          "siteName": "AR-IO Network",
          "depth": 2,
          "crawledAt": "2025-06-27T16:13:12.437Z"
        },
        {
          "url": "https://docs.ar.io/gateways/troubleshooting",
          "title": "Gateway Troubleshooting  FAQ",
          "content": "Gateway Troubleshooting & FAQ Welcome to the unified troubleshooting and FAQ resource for AR.IO Gateway operators. Use the quick lookup table below for fast answers, or browse the detailed sections for in-depth guidance. Quick Lookup Below is a quick summary of what you should check when troubleshooting your gateway. Find more detailed information in the sections below. ← Swipe to see more →IssueWhat to CheckMy release number is wrongPull the latest github updates and make sure you are on the main branchGateway appears offline on Viewblock or ar://gatewaysProbably fine, but verify that your gateway is still running.'/ar-io/observer/reports/current' just says \"report pending\"Normal behavior, wait for the report to complete.Observer error \"Cannot read properties of undefined\"Normal behavior, Observer is checking for data not implemented yet.Observing my gateway shows failuresCheck AR_IO_WALLET and ARNS_ROOT_HOST settings.Updated .env settings not reflected on gatewayRebuild your gateway after editing .env file.Out of disk space errorCheck for inode exhaustion and delete files if necessary.Can't load ArNS namesCheck ARNS_ROOT_HOST setting in .env file, and DNS records.\"Your connection is not private\" errorGenerate or renew SSL certificates.404/Nginx error when accessing domainCheck Nginx settings and restart Nginx if necessary.502 error from NginxCheck for errors in your gateway.Trouble generating SSL certificatesEnsure TXT records have propagated and follow certbot instructions.← Swipe to see more → General Troubleshooting My Gateway Seems to be Running but... My release number doesn't match the latest version, or includes \"-pre\"If your release number when you go to <your-gateway>/ar-io/info is lower than the current release, you simply need to upgrade your gateway in order to reach the latest release.If your release number includes the suffix \"-pre\" it means you are running your gateway from the development branch of the github repository, instead of the main branch. The development branch is used for staging work that the engineering team is in the middle of. Because of this, it can be much less stable than the main branch used for production and can cause significant issues.Ensure that you are running the latest release, from the main branch, by running the below commands in your terminal:sudo docker-compose down --rmi all git checkout main git pull sudo docker-compose up -d CopyCopied!If this doesn't resolve the issue, you can also try a more extreme method of clearing out the incorrect docker images:sudo docker-compose down sudo docker system prune -a sudo docker-compose up -d CopyCopied! It appears offline on Viewblock or ar://gatewaysViewblock and ar://gateways use a very simple ping method for determining if a gateway is \"up\". There are plenty of reasons why this ping may fail while the gateway is running perfectly, so showing as down is not cause for concern. Just verify that your gateway is still running, and wait. Your gateway will show as up again soon. < gateway >/ar-io/observer/reports/current just says \"report pending\"This is normal. Your Observer is working to generate a report and that report will be displayed once it is complete. My Observer is showing me the error \"error: Error reading interaction: Cannot read properties of undefined\"This is not an issue with your observer. The short explanation is that your Observer is looking for tasks assigned to it by the AR.IO network contract, but there isnt anything there. You can safely ignore this error message. Observing my gateway shows failuresWhen observing a gateway, there are two main pass/fail tests. \"Ownership\" and \"ArNS Assessment\" Ownership: This tests to see if the value set in your gateway AR_IO_WALLET value (in .env) matches the wallet used to join the AR.IO Network. If they don't match, update the value in your .env file and restart your gateway. ArNS Assessment: This tests to see if a gateway is able to resolve ArNS names correctly. The first thing you should check is if you have the ARNS_ROOT_HOST value set in your .env file. If not, set the value and restart your gateway. If this value is set, check to make sure you have current DNS records and SSL certificates for wildcard subdomains on your gateway. I updated my .env settings, but nothing changed on my gatewayOnce you edit your .env file, you need to \"rebuild\" your gateway for the changes to take effect. As of release 3, every time you start your gateway with docker-compose it is automatically rebuilt. So all you need to do is shut your gateway down and restart it. I am getting an out of disk space error, but I still have open storage space on my computerThe most likely cause of this is inode exhaustion. Test this by running the command:df -i CopyCopied!If one of the lines in the output says 100%, you have run out of inodes and so your filesystem is not capable of creating new files, even if you have available space. The solution is to delete files from your data folder in order to free up inodes.This was a common issue prior to release #3, when Redis caching was introduced to reduce the number of small files created. If you are using an older version of the gateway, consider upgrading to mitigate the risk of inode exhaustion. I can't load ArNS namesThe first thing you should check if your gateway is not resolving ArNS names is that you have ARNS_ROOT_HOST set in your .env file. If not, set it to your domain name used for the gateway. For example, ARNS_ROOT_HOST=arweave.dev.Once this value is set, restart your gateway for the changes to take effect.If that doesn't resolve the issue, check your dns records. You need to have a wildcard subdomain ( *.< your-domain > ) set with your domain registrar so that ArNS names will actually point at your gateway. You can set this record, and generate an SSL certificate for it, in the same way you set the records for your primary domain. When I try to access my gateway in a browser I get a \"Your connection is not private\" errorThis error message means that your SSL certificates have expired. You need to renew your certificates by running the same certbot command you used when you initially started your gateway:sudo certbot certonly --manual --preferred-challenges dns --email <your-email-address> -d <your-domain>.com -d '*.<your-domain>.com' CopyCopied!Certbot SSL certificates expire after 90 days, and you will need to rerun this command to renew every time. If you provide an email address, you will receive an email letting you know when it is time to renew. I set my gateway up, but when I go to my domain I get a 404/Nginx errorIf you navigate to your domain and see a 404 error from Nginx (the reverse proxy server used in the setup guide) it means that your domain is correctly pointed at the machine running your gateway, but you have not properly configured your Nginx settings (or your gateway is not running).The Set up Networking section of the setup guide has detailed instructions on configuring your Nginx server. If all else fails, try restarting Nginx, that usually clears any issues with the server clinging to old configurations.sudo service nginx restart CopyCopied! When I visit my domain I see a 502 error from NginxA 502 error from Nginx means that Nginx is working correctly, but it is receiving an error from your gateway when it tries to forward traffic. I am having trouble generating my SSL certificatesWhen using the manual certbot command provided in the setup guide:sudo certbot certonly --manual --preferred-challenges dns --email <your-email-address> -d <your-domain>.com -d '*.<your-domain>.com' CopyCopied!You need to be sure that you are waiting after creating your TXT records for them to completely propagate. You can check propagation using a tool like dnschecker.org.If you continue to have issues, you can check the official certbot instructions guide. My gateway was working, but it just stopped Visit your gateway in a browser and see if your SSL certs are expired. This is the most common issue causing sudden stops in proper operation. I updated my SSL certs, but it still shows as bad in a browser Try restarting nginx, it sometimes has trouble looking at the new certs without a restart. My gateway won't resolve ArNS names Make sure ARNS_ROOT_HOST is properly set in your .env file. Updating this requires restarting your gateway. Make sure you have a DNS record set for *.<your-gateway-domain>. Since ArNS names are served as subdomains, you need to make sure all subdomains are pointed at your gateway. If your gateway is attempting to resolve the name, but times out, it's most likely a CU issue. I see an error in my logs, but everything appears to be working AR.IO gateways are very robust, they can handle temporary errors gracefully and not affect normal operation. You should only be concerned if the error is consistent or it is causing your gateway to not function properly. I was selected as an observer, but my logs say a report was not saved Observers generate and submit their reports at specific times throughout the epoch. This is to ensure a healthy network throughout the entire epoch, not just at the start. Your observer wallet must match the observer wallet associated with your gateway in the AR.IO contract. You can check this by navigating to your gateway in ar://gateways. I see an error in my logs that says <h\"... is not valid JSON This happens when a request to a CU fails, and your gateway receives an html failure message instead of the expected JSON response. This will normally clear up on its own after congestion on that CU dies down, but if it is persistent try switching to a different CU. My gateway logs just changed, instead of importing blocks I see \"polling for block\" This is normal. It means you have reached the current Arweave block and need to wait for more before you can index them. Error resolving name with resolver Promise timed out This is normal. If a gateway fails to resolve an arns name within 3 seconds, it will fall back to a trusted gateway (arweave.net by default) to help resolve the name. My gateway failed an epoch There are many reasons a gateway could fail an epoch. Following these steps is usually enough to identify and correct the issue: Try to visit your gateway in a browser and see if your SSL certs are bad Try to resolve an ArNS name on your gateway. If it fails to resolve, check the console and your gateway logs for errors Look at the observation reports that failed your gateway, they will list the reason for failure Troubleshooting Failed Epochs Overview The ARIO Network provides several tools to help troubleshoot problems with a gateway. The most powerful among these is the Observer. The Observer, which is a component of every gateway joined to the ARIO Network, checks all gateways in the network to ensure that they are functioning properly, and returning the correct data. The Observer then creates a report of the results of these checks, including the reasons why a gateway might have failed the checks. If a gateway fails the checks from more than half of the prescribed observers, the gateway is marked as failed for the epoch, and does not receive any rewards for that epoch. The first step in troubleshooting a failed gateway is always to attempt to resolve data on that gateway in a browser, but if that does not make the issue clear, the Observer report can be used to diagnose the problem. Manual Observation Manual observations may be run on a gateway at any time buy using the Network Portal. This allows operators (or anyone with an interest in the gateway's performance) to check the gateway's performance at any time. To run a manual observation: Navigate to the Network Portal Select the gateway you are interested in from the list of gateways Click on the \"Observe\" button in the top right corner of the page. Click on the \"Run Observation\" button in the bottom right corner of the page. Two randomly selected ArNS names will be entered automatically in the \"ArNS names\" field to the left of the \"Run Observation\" button. These can be changed, or additional ArNS names can be added to the list before running the observation. The Manual observation will run the same checks as the observer, and will display the results on the right side of the page. Accessing the Observer Report The simplest way to access an observer report is via the Network Portal, following the steps below: Navigate to the Network Portal Select the gateway you are interested in from the list of gateways In the Observation window, select the epoch you are interested in. This will display a list of the observers that failed the gateway for that epoch. Click on the \"View Report\" button to the right any observer on that list. This will display the entire report that observer generated. Locate the gateway you are interested in in the report, and click on that row. This will display the report for that gateway. Understanding the Observer Report The observer report will display a list of checked ArNS names, and a reason if the gateway failed to return the correct data for that name. There are several reasons why a gateway might fail to return the correct data for an ArNS name. Below is a list of the most common reasons, and how to resolve them. Timeout awaiting 'socket', or Timeout awaiting 'connect' This failure means that the observer was unable to connect to the gateway when it tried to check the ArNS name. There are lots of reasons why this might happen, many of them unrelated to the gateway itself. If an observer report has a small number of these failures, among a larger number of successful checks, it is unlikely to be an issue with the gateway. If this failure occurs persistently for a large number, or all ArNS names checked, it likely means that the observer is having trouble connecting to the gateway at all. You can verify this by: Attempting to connect to the gateway in a browser Running manual observations on the gateway using the Network Portal Using tools like curl or ping to check the gateway's connectivity If these methods consistently fail to connect to the gateway, it is likely that the gateway is not properly configured or powered on. If this is the case: Check Docker and the gateway's logs to see if the gateway is on. Ensure that the SSL certificates are valid for the gateway's domain. Check DNS records for the gateway's domain, misconfigured or conflicting DNS records can cause connectivity issues. Some gateway operators who run their gateways on their personal home networks have also reported issues with their ISP blocking, throttling, or otherwise delaying traffic to a gateway. If none of the above steps resolve the issue, it may be worth checking with your ISP to see if they are blocking or throttling traffic to the gateway. Using Grafana can also provide a visual representation of the gateway's ArNS resolution times. If this is consistently high (above 10 seconds), it is likely that the gateway is not properly configured to resolve ArNS names. Ensure that the gateway is operating on the latest Release. certificate has expired This failure means that the gateway's SSL certificate has expired. Obtaining a new SSL certificate and updating the gateway's reverse proxy (nginx, etc) configuration to use the new certificate is the only solution to this issue. dataHashDigest mismatch This failure means that the gateway did respond to a resolution request, but the data it returned did not match the data that was expected. This could be due to a number of reasons, including: Cached data was returned by the gateway that doesnt match the most current data on the network. The gateway is configured to operate on testnet or devnet. Gateways joined to the ARIO Network MUST operate on mainnet in order to pass observation checks. The gateway is intentionally returning fraudulent data. A gateway will not return fraudulent data unless that operator intentionally rewrote the gateway's code to do so, and a major purpose of the Observation and Incentive Protocol is to catch and prevent this behavior. A gateway may return mistaken data on occasion, usually due to a cache mismatch between the gateway and the observer's authority (usually arweavae.net). This is a relatively rare occurrence, and should only be considered an issue if it occurs persistently. If most or all of the ArNS names checked are failing for this reason, it is likely that the gateway is not operating on mainnet. Response code 502 (Bad Gateway) This failure means that the observer was able to connect to the gateway's network, but the reverse proxy returned a 502 error. This is almost always a reverse proxy issue. Ensure that the gateway's reverse proxy is running, and that it is configured to forward requests to the gateway. Testing the validity of the reverse proxy's configuration file (sudo nginx -t on Nginx) may provide more information about the issue, and restarting the reverse proxy (sudo nginx -s reload) often resolves the issue if there are no problems with the configuration file. It is also possible that the gateway itself is not running at all. Check Docker and the gateway's logs to see if the gateway is on. Response code 503 (Service Unavailable) This failure means that the observer was able to connect to the gateway's network, but the reverse proxy was unable to forward the request to the gateway. It differs from the 502 error in that the reverse proxy is likely able to see that the gateway is running, but is unable to communicate with it. This is often a temporary issue, caused by the gateway not being able to handle a heavy load of requests, or the gateway being in the process of restarting. If this failure occurs once or twice in a report, it is likely a temporary issue and should not be considered an issue with the gateway. However, when this failure occurs persistently, particularly for every ArNS name checked on the report, it is likely that the gateway may have crashed. Manually restarting the gateway can likely resolve the issue. connect EHOSTUNREACH This failure means that the observer was unable to connect to the gateway at all. The connection was either refused, or the gateway was not able to find a target based on the domain name's DNS records. This is almost always an issue with DNS records or local network configuration. Ensure that the gateway domain has correct DNS records, and that the local network is set up to allow connections. Checking logs from the local network's reverse proxy (nginx, etc) may provide more information about the issue. getaddrinfo ENOTFOUND This is another DNS related issue. Likely, the gateway does not have a valid DNS record either for the top level domain or the required wildcard subdomain. Having this failure occur once or twice in a report could mean that the DNS server being used by the observer is having temporary issues and should not be considered an issue with the gateway. However, when this failure occurs persistently, particularly for every ArNS name checked on the report, it is likely that the gateway's DNS records are not set, or are misconfigured. Hostname/IP does not match certificate's altnames: Host: <gateway-domain>. is not in the cert's altnames: DNS:<gateway-domain> This failure means that the observer's SSL certificate does not match the gateway's domain name. This is almost always an issue with the gateway's SSL certificate. This most likely occurred because the gateway's operator did not update the gateway's SSL certificate when the gateway's domain name was changed. Obtaining a new SSL certificate and updating the gateway's reverse proxy configuration to use the new certificate is the only solution to this issue. write EPROTO <connection-id>:error:<error-code>:SSL routines:ssl3_read_bytes:tlsv1 unrecognized name:<path-to-openssl-source>:SSL alert number 112 This failure almost always means that the gateway operator did not properly obtain SSL certificates for the gateway's wildcard subdomain. Obtaining a new SSL certificate and updating the gateway's reverse proxy configuration to use the new certificate is the only solution to this issue. FAQ Why was my reward different this epoch? Show answer Gateway protocol rewards are calculated as 0.1% of the protocol balance (0.05% after August 2025) split between all gateways in the network. A change in the protocol balance or the number of gateways in the network between epochs will result in the reward for an individual gateway changing. The Observer rewards are separate from protocol rewards, and if your gateway is selected as an observer for an epoch, assuming it performs its duties well, it will receive additional rewards I have a high stake on my gateway, why am I not an observer? Show answerThe observer selection process uses a weighted random selection method that considers multiple factors beyond just stake: Stake Weight (SW): Ratio of your total staked ARIO tokens (including delegated stake) to the network minimum Tenure Weight (TW): How long your gateway has been part of the network (capped at 4 after 2 years) Gateway Performance Ratio Weight (GPRW): Ratio of epochs where you correctly resolved names vs total participation Observer Performance Ratio Weight (OPRW): Ratio of epochs where you successfully submitted reports vs total observer periods A composite weight (CW) is calculated as: CW = SW × TW × GPRW × OPRWUp to 50 gateways are chosen as observers per epoch. If there are more than 50 gateways, selection is randomized based on these normalized weights. Even with a high stake, other factors like performance and tenure affect your chances of being selected. I withdrew my stake, but now I have less Show answer There is a 90 day locking period when withdrawing stake, either from delegated stake or operator stake on your gateway. This locking period can be skipped, for a fee. The fee starts at 50% of the withdrawal amount, and goes down over time. If you selected instant withdrawal, you paid the fee to skip the locking period. Why Can't I withdraw my stake? Show answer The minimum operator stake for gateways (10,000 ARIO) cannot be instantly withdrawn, it is subject to the full 90 day locking period, and withdrawal can only be started by removing your gateway from the network. I would like to move my node to a new server - how? Show answer If possible, leave your original server running while you prepare the new one Set up the new server following the same steps you used to set up the original server This includes setting up SSL certificates for the new server You must use the same gateway wallet when setting up the new server The observer wallet may be changed at any point, but requires extra steps. It is recommended you use the original observer wallet as well Once the new server is set up, change your DNS A records to point at the new server After your DNS records are set and you have verified your gateway is operating correctly, shut down the original server No changes need to be made in the network contract or on ar://gateways Can I change my nodes FQDN? Show answer Yes Configure your new domain to point at your gateway, including setting up SSL certificates Update your NGINX (or other reverse proxy) server to recognize the new domain. This usually requires a restart of NGINX Update the ARNS_ROOT_HOST variable in your .env and restart the gateway Using ar://gateways, update your gateway settings to change the FQDN in the contract Your gateway is now using the new domain name for normal operation. Was this page helpful?YesNoComment",
          "estimatedWords": 3923,
          "lastModified": "2025-06-27T16:13:13.148Z",
          "siteKey": "ario",
          "siteName": "AR-IO Network",
          "depth": 3,
          "crawledAt": "2025-06-27T16:13:13.148Z"
        },
        {
          "url": "https://docs.ar.io/gateways/upgrading",
          "title": "Upgrading your Gateway",
          "content": "Upgrading your Gateway To ensure the optimal performance and security of your AR.IO Gateway, it's essential to regularly upgrade to the latest version. Notably, indexed data resides separate from Docker. As a result, neither upgrading the Gateway nor pruning Docker will erase your data or progress. Here's how you can perform the upgrade: Prerequisites Your Gateway should have been cloned using git. If you haven't, follow the installation instructions for windows or linux. Checking your Release Number Effective with release 3, you can view the currently implemented release on any gateway by visiting https://<gateway>/ar-io/info in a browser. Be sure to replace <gateway> with the domain of the gateway you are checking. If the release number displayed includes -pre it means that your gateway is using the develop branch of the github repo for the gateway code. Follow steps in our troubleshooting guide to switch over to the more stable main branch. Announcements will be made in our discord server showing each new release. Upgrade Steps Pull the latest changes from the repository Navigate to your cloned repository directory and execute the following command: git pull CopyCopied! Shut down Docker Depending on your operating system, use the respective commands: Linuxsudo docker-compose down -v CopyCopied! Windows docker-compose down -v CopyCopied! Prune Docker (Optional) It's a good practice to clean up unused Docker resources. Again, use the command based on your OS: NOTE: This will erase all inactive docker containers on your machine. If you use docker for anything beyond running a gateway be extremely careful using this command. Linux sudo docker system prune CopyCopied! Windows docker system prune CopyCopied! Check for New Environmental Variables Read the update release change logs and community announcements to see if the new version includes any new environmental variables that you should set before restarting your gateway. Restart the Docker container Finally, start the Docker container again to implement the changes: Linuxsudo docker-compose up -d CopyCopied! Windowsdocker-compose up -d CopyCopied!NOTE: Effective with Release #3, it is no longer required to include the --build flag when starting your gateway. Docker will automatically build using the image specified in the docker-commpose.yaml file. That's it! Your AR.IO Gateway is now upgraded to the latest version. Ensure to test and verify that everything is functioning as expected. If you encounter any issues, reach out to the AR.IO community for assistance.Was this page helpful?YesNoComment",
          "estimatedWords": 392,
          "lastModified": "2025-06-27T16:13:13.481Z",
          "siteKey": "ario",
          "siteName": "AR-IO Network",
          "depth": 3,
          "crawledAt": "2025-06-27T16:13:13.481Z"
        },
        {
          "url": "https://docs.ar.io/gateways/ar-io-node/windows-setup",
          "title": "Our vision is an internet with no more 404s but it looks like you just found one",
          "content": "404s suck!Our vision is an internet with no more 404s, but it looks like you just found one.Rest assured that if this page ever existed, it is still available on the permaweb.Back to Home",
          "estimatedWords": 34,
          "lastModified": "2025-06-27T16:13:13.525Z",
          "siteKey": "ario",
          "siteName": "AR-IO Network",
          "depth": 4,
          "crawledAt": "2025-06-27T16:13:13.525Z"
        },
        {
          "url": "https://docs.ar.io/gateways/ar-io-node/linux-setup",
          "title": "Our vision is an internet with no more 404s but it looks like you just found one",
          "content": "404s suck!Our vision is an internet with no more 404s, but it looks like you just found one.Rest assured that if this page ever existed, it is still available on the permaweb.Back to Home",
          "estimatedWords": 34,
          "lastModified": "2025-06-27T16:13:14.003Z",
          "siteKey": "ario",
          "siteName": "AR-IO Network",
          "depth": 4,
          "crawledAt": "2025-06-27T16:13:14.003Z"
        },
        {
          "url": "https://docs.ar.io/gateways/moderation",
          "title": "Content Moderation",
          "content": "Content Moderation Overview Arweave is a network designed for permanent storage of data. It is a practical impossibility for data to be wholly removed from the network once it has been uploaded. The AR.IO Network has adopted Arweave's voluntary content moderation model, whereby every participant of the network has the autonomy to decide which content they want to (or can legally) store, serve, and see. Each gateway operating on the network has the right and ability to blocklist any content, ArNS name, or address that is deemed in violation of its content policies or is non-compliant with local regulations. NOTEOverly restrictive content policies may impact a gateway's likelihood of receiving protocol rewards. Gateway operators may set content to be blocked by their gateway by submitting a Put request to their gateway defining the content to be blocked. This requires that the ADMIN_API_KEY environmental variable to be set in order to authenticate the moderation request. The simplest method for submitting moderation requests to a gateway is to use curl in a terminal. Authentication Moderation requests must contain the gateway's ADMIN_API_KEY in the request Header, as Authorization: Bearer. For example, if a gateway's ADMIN_API_KEY is set to secret, any request must contain Authorization: Bearer secret in the Header. Block Data Specific data items can be blocked by a gateway operator by submitting a Put request containing a json object with three keys: id: The Arweave transaction Id of the data item to be blocked. notes: Any note the gateway operator wants to leave him/herself as to the reason the content is blocked. source: A note as to where the content was identified as requiring moderation. i.e. a public block list. Requests to block data must be submitted to the gateway's /ar-io/admin/block-data endpoint. Block Datacurl -X 'PUT' \\ 'http://localhost:3000/ar-io/admin/block-data' \\ -H 'accept: */*' \\ -H 'Authorization: Bearer secret' \\ -H 'Content-Type: application/json' \\ -d '{ \"id\": \"3lyxgbgEvqNSvJrTX2J7CfRychUD5KClFhhVLyTPNCQ\", \"notes\": \"This content is offensive\", \"source\": \"Public Block list\" }' CopyCopied! Unblock Data At this time, blocked data items can only be unblocked by manually deleting the corresponding row from the data/sqlite/moderation.db database. The Arweave transaction Id of the blocked data item is stored in the database as raw bytes, which sqlite3 accepts as a BLOB (Binary Large OBject), and so cannot be accessed easily using the original transaction Id, which is a base64url. Sqlite3 is able to interact with a hexadecimal representation of the BLOB, by using a BLOB literal. To do so, wrap a hexadecimal representation of the Arweave transaction Id in single quotes, and prepend an X i.e. X'de5cb181b804bea352bc9ad35f627b09f472721503e4a0a51618552f24cf3424'. Where possible, consider using the notes or source values to identify rows for deletion rather than the id. Unblock Dataidsourcesqlite3 data/sqlite/moderation.db \"DELETE FROM blocked_ids WHERE id=X'de5cb181b804bea352bc9ad35f627b09f472721503e4a0a51618552f24cf3424';\" # Note that the id in this command is a BLOB literal using the hexadecimal representation of the Arweave transaction Id, not the transaction Id in its normal base64url format CopyCopied! Block ArNS Name ArNS names can be blocked so that a gateway will refuse to serve their associated content even if the name holder updates the Arweave transaction Id that the name points at. This is done via an authenticated PUT request to the endpoint /ar-io/admin/block-name containing a json object with three keys: name: The ArNS name to be blocked. notes: Any note the gateway operator wants to leave him/herself as to the reason the content is blocked. source: A note as to where the content was identified as requiring moderation. i.e. a public block list. Block ArNS Namecurl -X 'PUT' \\ 'http://localhost:3000/ar-io/admin/block-name' \\ -H 'accept: */*' \\ -H 'Authorization: Bearer secret' \\ -H 'Content-Type: application/json' \\ -d '{ \"name\": \"i-bought-a-potato\", \"notes\": \"Potatoes are offensive\", \"source\": \"Public Block list\" }' CopyCopied! UndernamesFor moderation purposes, each undername of an ArNS name is treated as a separate name and must be moderated separately. Unblock ArNS Name Gateway operators can unblock ArNS names that were previously blocked. This is done via an authenticated PUT request to the endpoint /ar-io/admin/unblock-name containing a json object with a single key: name: The ArNS name to be unblocked Unblock ArNS Namecurl -X 'PUT' \\ 'http://localhost:3000/ar-io/admin/unblock-name' \\ -H 'accept: */*' \\ -H 'Authorization: Bearer secret' \\ -H 'Content-Type: application/json' \\ -d '{ \"name\": \"i-bought-a-potato\", }' CopyCopied!Was this page helpful?YesNoComment",
          "estimatedWords": 704,
          "lastModified": "2025-06-27T16:13:14.302Z",
          "siteKey": "ario",
          "siteName": "AR-IO Network",
          "depth": 3,
          "crawledAt": "2025-06-27T16:13:14.302Z"
        },
        {
          "url": "https://docs.ar.io/gateways/filters",
          "title": "ARIO Node Filtering System",
          "content": "AR.IO Node Filtering System The AR.IO Node filtering system provides a flexible way to match and filter items based on various criteria. The system is built around JSON-based filter definitions that can be combined to create both simple and complex matching patterns. Unbundling and Indexing Filters When processing bundles, the AR.IO Node applies two filters obtained from environment variables: ANS104_UNBUNDLE_FILTER=\"\" ANS104_INDEX_FILTER=\"\" CopyCopied! The ANS104_UNBUNDLE_FILTER determines which base layer transactions and data items, in the case of bundles nested in other bundles, are processed, and the ANS104_INDEX_FILTER determines which data items within the processed bundles are indexed for querying. Webhook Filters There are also two filters available that are used to trigger webhooks. When a transaction is processed that matches one of the webhook filters, the gateway will send a webhook to the specified WEBHOOK_TARGET_SERVERS urls containing the transaction data. WEBHOOK_INDEX_FILTER=\"\" WEBHOOK_BLOCK_FILTER=\"\" CopyCopied! The WEBHOOK_INDEX_FILTER is used to trigger a webhook when a transaction is indexed. The WEBHOOK_BLOCK_FILTER is used to trigger a webhook when a block is processed. Important Notes All tag names and values are base64url-decoded before matching Owner addresses are automatically converted from owner public keys Empty or undefined filters default to \"never match\" Tag matching requires all specified tags to match Attribute matching requires all specified attributes to match The filter system supports nested logical operations to any depth, allowing for very precise control over what data gets processed All these filters can be used in various contexts within the AR.IO Node, such as configuring webhook triggers, controlling ANS-104 bundle processing, or setting up data indexing rules. The filtering system is designed to be intuitive yet powerful, allowing for precise control over which items get processed while maintaining readable and maintainable filter definitions. Filter Construction .env formattingWhile the filters below are displayed on multiple lines for readability, they must be stored in the .env file as a single line for proper processing. Basic Filters The simplest filters you can use \"always\" and \"never\" filters. The \"never\" filter is the default behavior and will match nothing, while the \"always\" filter matches everything. Basic FiltersNever MatchAlways Match{ \"never\": true //default behavior } CopyCopied! Tag Filters Tag filters allow you to match items based on their tags in three different ways. You can match exact tag values, check for the presence of a tag regardless of its value, or match tags whose values start with specific text. All tag values are automatically base64url-decoded before matching. Tag FiltersExact MatchMatch Tag Name OnlyStarts With Match{ \"tags\": [ { \"name\": \"Content-Type\", \"value\": \"image/jpeg\" } ] } CopyCopied! Attribute Filters Attribute filtering allows you to match items based on their metadata properties. The system automatically handles owner public key to address conversion, making it easy to filter by owner address. You can combine multiple attributes in a single filter: Attribute Filters{ \"attributes\": { \"owner_address\": \"xyz123...\", \"data_size\": 1000 } } CopyCopied! Nested Bundle Filter The isNestedBundle filter is a specialized filter that checks whether a data item is part of a nested bundle structure. It's particularly useful when you need to identify or process data items in bundles that are contained within other bundles. The filter checks for the presence of a parent_id field in the item. Nested Bundle Filter{ \"isNestedBundle\": true } CopyCopied! Note: When processing nested bundles, be sure to include filters that match the nested bundles in both ANS104_UNBUNDLE_FILTER and ANS104_INDEX_FILTER. The bundle data items (nested bundles) need to be indexed to be matched by the unbundle filter. Complex Filters Using Logical Operators For more complex scenarios, the system provides logical operators (AND, OR, NOT) that can be combined to create sophisticated filtering patterns. These operators can be nested to any depth: Logical OperatorsAND OperationOR OperationNOT Operation{ \"and\": [ { \"tags\": [ { \"name\": \"App-Name\", \"value\": \"ArDrive-App\" } ] }, { \"tags\": [ { \"name\": \"Content-Type\", \"valueStartsWith\": \"image/\" } ] } ] } CopyCopied! Advanced Examples Advanced ExamplesComplex Data FilterMulti-condition Tag FilterExclusion Filter{ \"and\": [ { \"tags\": [ { \"name\": \"App-Name\", \"value\": \"ArDrive-App\" }, { \"name\": \"Content-Type\", \"valueStartsWith\": \"image/\" } ] }, { \"attributes\": { \"data_size\": 1000000 } }, { \"not\": { \"isNestedBundle\": true } } ] } CopyCopied!Was this page helpful?YesNoComment",
          "estimatedWords": 690,
          "lastModified": "2025-06-27T16:13:14.573Z",
          "siteKey": "ario",
          "siteName": "AR-IO Network",
          "depth": 3,
          "crawledAt": "2025-06-27T16:13:14.573Z"
        },
        {
          "url": "https://docs.ar.io/gateways/optimize-data",
          "title": "Optimizing Data Handling in ARIO Gateway",
          "content": "Optimizing Data Handling in AR.IO Gateway The AR.IO Gateway provides powerful tools for optimizing how you access and serve specific types of data. By configuring filters and worker settings, you can focus your gateway on efficiently handling the data that matters most to your use case, ensuring quick and reliable access to relevant information. Understanding the Filtering System The AR.IO Gateway uses two filters to control how ANS104 data items are processed: ANS104_UNBUNDLE_FILTER: Controls which bundles are processed and unbundled ANS104_INDEX_FILTER: Controls which data items from unbundled bundles are stored in the database for querying These filters are configured through environment variables: ANS104_UNBUNDLE_FILTER=\"\" ANS104_INDEX_FILTER=\"\" CopyCopied! By default, the gateway processes no bundles and indexes no data items. This allows you to selectively enable processing for the specific data types you need. For a detailed explanation of how to construct these filters, see our Filtering System documentation. Key Environment Variables Several environment variables control how your gateway processes data: Core Configuration# Enable backfilling of missing bundle records BACKFILL_BUNDLE_RECORDS=true # Alternative: Manually queue bundles for backfilling via the admin API # This can be more efficient than automatic backfilling when you know specific bundle IDs # Requires admin API key to be set. See <gateway>/api-docs for more information # Reprocess all indexed bundles with new filters FILTER_CHANGE_REPROCESS=true # Number of ANS-104 bundles to download in parallel ANS104_DOWNLOAD_WORKERS=5 # Number of workers for unbundling (0 = disabled, 1 = enabled if filters set) ANS104_UNBUNDLE_WORKERS=1 CopyCopied! Data Management# Number of new data items before flushing to stable storage DATA_ITEM_FLUSH_COUNT_THRESHOLD=1000 # Maximum time between flushes (in seconds) MAX_FLUSH_INTERVAL_SECONDS=600 # Maximum number of data items to queue for indexing MAX_DATA_ITEM_QUEUE_SIZE=100000 # Enable background verification of unverified data ENABLE_BACKGROUND_DATA_VERIFICATION=true CopyCopied! Data Item Flushing The gateway uses a two-stage storage system for indexed data items: Temporary Storage: Newly indexed data items are first stored in a temporary table Stable Storage: Data items are periodically \"flushed\" from temporary to stable storage This process is controlled by two environment variables: DATA_ITEM_FLUSH_COUNT_THRESHOLD: Number of items to queue before flushing (default: 1000) MAX_FLUSH_INTERVAL_SECONDS: Maximum time between flushes (default: 600 seconds) The gateway will flush data items when either: The number of items in temporary storage reaches the threshold The time since the last flush exceeds the interval This batching approach helps optimize database performance by reducing the number of write operations. GraphQL Configuration The GRAPHQL_HOST setting determines how your gateway handles GraphQL queries. You have two options: GraphQL Host Options# Set to arweave.net to proxy queries to a gateway with full index GRAPHQL_HOST=arweave.net # Or unset to enable local-only GraphQL queries # GRAPHQL_HOST= # Port for GraphQL queries (default: 443) GRAPHQL_PORT=443 CopyCopied! Using arweave.net (Recommended for new gateways) Proxies queries to a gateway with a complete index of the blockweave Provides immediate access to all historical data No need to wait for local indexing May introduce additional latency from proxying Local-only Queries (Unset GRAPHQL_HOST) Responds to queries using only locally indexed data Faster response times for indexed data Requires complete local indexing (can take weeks for L1 transactions) No proxying overhead Only returns data that matches your indexing filters Note: Even with GRAPHQL_HOST set to arweave.net, your gateway will still maintain its own index based on your filters. This allows for quick access to frequently requested data while ensuring availability of all historical data. Common Use Cases Optimizing for Specific Data Types By configuring your filters and workers appropriately, you can optimize your gateway for different types of data: High-Volume Data: Configure workers to handle large amounts of data efficiently Specific Applications: Filter for particular app names or content types Filter Examples The AR.IO Gateway uses two distinct filters to control how ANS104 bundle data is processed: ANS104_UNBUNDLE_FILTER: Determines which bundles (including nested bundles) are unbundled ANS104_INDEX_FILTER: Determines which data items within a bundle have their data indexed Here are some practical examples of how to configure these filters for specific use cases: Specific Application Data This configuration demonstrates how to focus your gateway on data from a specific application. In this example, we show how to process and index all ArDrive-related transactions, but you can adapt this pattern for any application, using the App-Name tag. This approach is perfect for: Building application-specific services Creating application data archives Running application-focused analytics Supporting application infrastructure Reducing processing overhead by focusing only on relevant data In this example, the index filter uses the ArFS tag to only index ArFS-compliant data, which is a specific aspect of ArDrive applications. Index filters can be adjusted for any application's needs - the App-Name tag is particularly useful here, as data items within a bundle can have a different App-Name than the bundle that contains them. All ArDrive DataUnbundle FilterIndex Filter{ \"or\": [ { \"tags\": [{ \"name\": \"App-Name\", \"value\": \"ArDrive-App\" }]}, { \"tags\": [{ \"name\": \"App-Name\", \"value\": \"ArDrive-Web\" }]}, { \"tags\": [{ \"name\": \"App-Name\", \"value\": \"ArDrive-CLI\" }]}, { \"tags\": [{ \"name\": \"App-Name\", \"value\": \"ArDrive-Desktop\" }]}, { \"tags\": [{ \"name\": \"App-Name\", \"value\": \"ArDrive-Mobile\" }]}, { \"tags\": [{ \"name\": \"App-Name\", \"value\": \"ArDrive-Core\" }]}, { \"tags\": [{ \"name\": \"App-Name\", \"value\": \"ArDrive-Sync\" }]}, { \"tags\": [{ \"name\": \"App-Name\", \"value\": \"ArDrive Turbo\" }]} ] } // Alternative using valueStartsWith for a more concise filter { \"tags\": [{ \"name\": \"App-Name\", \"valueStartsWith\": \"ArDrive\" }] } CopyCopied! Personal Data Gateway This configuration is designed for users who want to run a personal gateway that only processes their own ArDrive data. It: Excludes common specific use case bundlers to reduce unnecessary processing Only indexes data owned by your wallet address Includes all ArDrive and Turbo app data Perfect for personal data management Personal Data GatewayUnbundle FilterIndex Filter{ \"and\": [ { \"not\": { // Exclude common specific use case bundlers \"or\": [ { \"tags\": [{ \"name\": \"Bundler-App-Name\", \"value\": \"Warp\" }]}, { \"tags\": [{ \"name\": \"Bundler-App-Name\", \"value\": \"Redstone\" }]}, { \"tags\": [{ \"name\": \"Bundler-App-Name\", \"value\": \"Kyve\" }]}, { \"tags\": [{ \"name\": \"Bundler-App-Name\", \"value\": \"AO\" }]}, { \"attributes\": { \"owner_address\": \"-OXcT1sVRSA5eGwt2k6Yuz8-3e3g9WJi5uSE99CWqsBs\" }}, { \"attributes\": { \"owner_address\": \"ZE0N-8P9gXkhtK-07PQu9d8me5tGDxa_i4Mee5RzVYg\" }}, { \"attributes\": { \"owner_address\": \"6DTqSgzXVErOuLhaP0fmAjqF4yzXkvth58asTxP3pNw\" }} ] } }, { \"or\": [ // Include ArDrive and Turbo apps { \"tags\": [{ \"name\": \"App-Name\", \"value\": \"ArDrive-App\" }]}, { \"tags\": [{ \"name\": \"App-Name\", \"value\": \"ArDrive-Web\" }]}, { \"tags\": [{ \"name\": \"App-Name\", \"value\": \"ArDrive-CLI\" }]}, { \"tags\": [{ \"name\": \"App-Name\", \"value\": \"ArDrive-Desktop\" }]}, { \"tags\": [{ \"name\": \"App-Name\", \"value\": \"ArDrive-Mobile\" }]}, { \"tags\": [{ \"name\": \"App-Name\", \"value\": \"ArDrive-Core\" }]}, { \"tags\": [{ \"name\": \"App-Name\", \"value\": \"ArDrive-Sync\" }]}, { \"tags\": [{ \"name\": \"App-Name\", \"value\": \"ArDrive Turbo\" }]} ] } ] } // Alternative using valueStartsWith for a more concise filter while maintaining bundler exclusions { \"and\": [ { \"not\": { // Exclude common specific use case bundlers \"or\": [ { \"tags\": [{ \"name\": \"Bundler-App-Name\", \"value\": \"Warp\" }]}, { \"tags\": [{ \"name\": \"Bundler-App-Name\", \"value\": \"Redstone\" }]}, { \"tags\": [{ \"name\": \"Bundler-App-Name\", \"value\": \"Kyve\" }]}, { \"tags\": [{ \"name\": \"Bundler-App-Name\", \"value\": \"AO\" }]}, { \"attributes\": { \"owner_address\": \"-OXcT1sVRSA5eGwt2k6Yuz8-3e3g9WJi5uSE99CWqsBs\" }}, { \"attributes\": { \"owner_address\": \"ZE0N-8P9gXkhtK-07PQu9d8me5tGDxa_i4Mee5RzVYg\" }}, { \"attributes\": { \"owner_address\": \"6DTqSgzXVErOuLhaP0fmAjqF4yzXkvth58asTxP3pNw\" }} ] } }, {\"tags\": [{ \"name\": \"App-Name\", \"valueStartsWith\": \"ArDrive\" }]} ] } CopyCopied! All ArDrive Bundles (Excluding Common Bundlers) This configuration is useful for gateways that want to process ArDrive data while avoiding common bundlers. It's ideal for: Reducing processing overhead by excluding known bundlers Maintaining a clean dataset focused on direct ArDrive transactions Optimizing storage and processing resources Supporting ArDrive infrastructure with reduced resource requirements All ArDrive Bundles (Excluding Common Bundlers)Unbundle FilterIndex Filter{ \"and\": [ // both of the wrapped filters must be true to process a bundle { \"not\": { // exclude common specific use case bundlers \"or\": [ { \"tags\": [{ \"name\": \"Bundler-App-Name\", \"value\": \"Warp\" }]}, { \"tags\": [{ \"name\": \"Bundler-App-Name\", \"value\": \"Redstone\" }]}, { \"tags\": [{ \"name\": \"Bundler-App-Name\", \"value\": \"Kyve\" }]}, { \"tags\": [{ \"name\": \"Bundler-App-Name\", \"value\": \"AO\" }]}, { \"attributes\": { \"owner_address\": \"-OXcT1sVRSA5eGwt2k6Yuz8-3e3g9WJi5uSE99CWqsBs\" }}, { \"attributes\": { \"owner_address\": \"ZE0N-8P9gXkhtK-07PQu9d8me5tGDxa_i4Mee5RzVYg\" }}, { \"attributes\": { \"owner_address\": \"6DTqSgzXVErOuLhaP0fmAjqF4yzXkvth58asTxP3pNw\" }} ] } }, { \"or\": [ // include ArDrive and Turbo apps { \"tags\": [{ \"name\": \"App-Name\", \"value\": \"ArDrive-App\" }]}, { \"tags\": [{ \"name\": \"App-Name\", \"value\": \"ArDrive-Web\" }]}, { \"tags\": [{ \"name\": \"App-Name\", \"value\": \"ArDrive-CLI\" }]}, { \"tags\": [{ \"name\": \"App-Name\", \"value\": \"ArDrive-Desktop\" }]}, { \"tags\": [{ \"name\": \"App-Name\", \"value\": \"ArDrive-Mobile\" }]}, { \"tags\": [{ \"name\": \"App-Name\", \"value\": \"ArDrive-Core\" }]}, { \"tags\": [{ \"name\": \"App-Name\", \"value\": \"ArDrive-Sync\" }]}, { \"tags\": [{ \"name\": \"App-Name\", \"value\": \"ArDrive Turbo\" }]} ] } ] } // Alternative using valueStartsWith for a more concise filter while maintaining bundler exclusions { \"and\": [ // both of the wrapped filters must be true to process a bundle { \"not\": { // exclude common specific use case bundlers \"or\": [ { \"tags\": [{ \"name\": \"Bundler-App-Name\", \"value\": \"Warp\" }]}, { \"tags\": [{ \"name\": \"Bundler-App-Name\", \"value\": \"Redstone\" }]}, { \"tags\": [{ \"name\": \"Bundler-App-Name\", \"value\": \"Kyve\" }]}, { \"tags\": [{ \"name\": \"Bundler-App-Name\", \"value\": \"AO\" }]}, { \"attributes\": { \"owner_address\": \"-OXcT1sVRSA5eGwt2k6Yuz8-3e3g9WJi5uSE99CWqsBs\" }}, { \"attributes\": { \"owner_address\": \"ZE0N-8P9gXkhtK-07PQu9d8me5tGDxa_i4Mee5RzVYg\" }}, { \"attributes\": { \"owner_address\": \"6DTqSgzXVErOuLhaP0fmAjqF4yzXkvth58asTxP3pNw\" }} ] } }, { \"tags\": [{ \"name\": \"App-Name\", \"valueStartsWith\": \"ArDrive\" }] } ] } CopyCopied! Important Filter Considerations When configuring your filters, keep these points in mind: The unbundle filter determines which bundles are processed and unbundled The index filter determines which data items from unbundled bundles are indexed in the database When filtering by owner addresses, use the modulus of the Arweave public address in the unbundle filter Common Bundler Exclusions When configuring filters, you may want to exclude data from common bundlers: Bundler Addresses: Irys Node 1: -OXcT1sVRSA5eGwt2k6Yuz8-3e3g9WJi5uSE99CWqsBs Irys Node 2: ZE0N-8P9gXkhtK-07PQu9d8me5tGDxa_i4Mee5RzVYg Irys Node 3: 6DTqSgzXVErOuLhaP0fmAjqF4yzXkvth58asTxP3pNw Bundler App Names: Warp Redstone Kyve AO ArDrive Best Practices Start Small: Begin with conservative worker counts and adjust based on system performance Monitor Resources: Watch system memory and CPU usage when adjusting worker counts Reprocess Bundles with New Filters: Use FILTER_CHANGE_REPROCESS to reprocess bundles after changing filters Regular Maintenance: Enable background verification and cleanup features Performance Considerations When optimizing your gateway, consider these factors: System Resources: Worker counts should be balanced against available CPU cores and memory Storage Space: Indexing filters affect database size and query performance Network Bandwidth: Unbundling workers can generate significant network traffic Query Performance: More indexed data means larger databases but better query capabilities Next Steps Review the Filtering System documentation for detailed filter syntax Check your system resources to determine optimal worker counts Start with basic filters and gradually refine based on your needs Monitor system performance and adjust settings as needed Optimization Strategy Focus on configuring your gateway to efficiently handle the specific data types you need. The default state processes no data, so you can selectively enable processing for your use case without worrying about unnecessary resource usage.Was this page helpful?YesNoComment",
          "estimatedWords": 1763,
          "lastModified": "2025-06-27T16:13:15.272Z",
          "siteKey": "ario",
          "siteName": "AR-IO Network",
          "depth": 3,
          "crawledAt": "2025-06-27T16:13:15.272Z"
        },
        {
          "url": "https://docs.ar.io/gateways/bundler",
          "title": "Bundler",
          "content": "Bundler Overview A Turbo ANS-104 data item bundler can be run alongside an AR.IO gateway. This allows gateways the ability to accept data items to be submit to the Arweave blockweave. The bundler service can be easily run inside Docker in the same way that the gateway is. It utilizes a separate docker compose file for configuration and deployment, which also allows for the use of a separate file for environmental variables specific to the bundler service. Additionally, the separation allows operators to spin their bundler service up or down at any time without affecting their core gateway service. Despite the use of separate docker compose files, the bundler service shares a docker network with the AR.IO gateway, and so is able to directly interact with the gateway service and data. Getting Started NOTE: The bundler service relies on GraphQL indexing of recently bundled and uploaded data to manage its pipeline operations. The AR.IO gateway should have its indexes synced up to Arweave's current block height before starting the bundler's service stack. Environmental Variables Environmental variables must be provided for the bundler to function and integrate properly with an existing AR.IO gateway. The gateway repository provides a .env.bundler.example file that can be renamed to .env.bundler and used as a starting point. It contains the following: BUNDLER_ARWEAVE_WALLET='Stringified JWK wallet. e.g: '{ \"n\": \"...\", ... }' BUNDLER_ARWEAVE_ADDRESS='Address for above wallet' APP_NAME='AR.IO bundler service' # Use localstack s3 bucket for shared data source between AR.IO gateway and bundler AWS_S3_BUCKET=ar.io AWS_S3_PREFIX='data' AWS_ACCESS_KEY_ID='test' AWS_SECRET_ACCESS_KEY='test' AWS_REGION='us-east-1' AWS_ENDPOINT='http://localstack:4566' CopyCopied! BUNDLER_ARWEAVE_WALLET must be the entire jwk of an Arweave wallet's keyfile, stringified. All uploads of bundled data items to Arweave will be signed and paid for by this wallet, so it must maintain a balance of AR tokens sufficient to handle the uploads. BUNDLER_ARWEAVE_ADDRESS must be the normalized public address for the provided Arweave wallet. APP_NAME is a GraphQL tag that will be added to uploaded bundles. The remaining lines in the .env.bundler.example file control settings that allow the bundler service to share data with the AR.IO gateway. Data sharing of contiguous data between a bundler and a gateway allows the gateway to serve optimistically cached data without waiting for it to fully settle on chain. Managing Bundler Access By default, the bundler will only accept data items uploaded by data item signers whose normalized wallet addresses are in the ALLOW_LISTED_ADDRESSES list. This is an additional environmental variable that can be added to your .env.bundler file, and must be a comma separated list of normalized public wallet addresses for wallets that should be allowed to bundle and upload data through your gateway. ALLOW_LISTED_ADDRESSES=<address1>,<address2> CopyCopied! The following permissioning configurations schemes are also possible: ← Swipe to see more →SchemeALLOW_LISTED_ADDRESSESSKIP_BALANCE_CHECKSALLOW_LISTED_SIGNATURE_TYPESPAYMENT_SERVICE_BASE_URLAllow Specific WalletsComma-separated normalized wallet addressesfalseEMPTY or suppliedEMPTYAllow Specific chainsEMPTY or suppliedfalsearbundles sigtype intEMPTYAllow Alln/atruen/an/aAllow NoneEMPTYfalseEMPTYEMPTYAllow PayersEMPTY or suppliedfalseEMPTY or suppliedYour payment service url← Swipe to see more → Indexing Bundlers submit data to the Arweave network as an ANS-104 data item bundle. This means it is several transactions wrapped into one. A gateway will need to unbundle these transactions in order to index them. A gateway should include the following ANS-104 filters in order to unbundle and index transactions from a particular bundler: ANS104_INDEX_FILTER={ \"always\": true } ANS104_UNBUNDLE_FILTER={ \"attributes\": { \"owner_address\": \"$BUNDLER_ARWEAVE_ADDRESS\" } } CopyCopied! $BUNDLER_ARWEAVE_ADDRESS should be replaced with the normalized public wallet address associated with the bundler. NOTE: The above filters must be placed in the .env file for the core gateway service, not the bundler. Gateways handle data item indexing asynchronously. This means they establish a queue of items to index, and work on processing the queue in the background while the gateway continues with its normal operations. If a gateway has broad indexing filters, there can be some latency in indexing data items from the bundler while the gateway works through its queue. Optimistic Indexing Gateway operators control access to their optimistic data item indexing API via an admin key that must be supplied by all bundling clients in order for their requests to be accepted. This key should be made available in the environment configuration files for BOTH the core gateway, and the bundler, and should be provided as AR_IO_ADMIN_KEY: AR_IO_ADMIN_KEY=\"Admin password\" CopyCopied! NOTE: If a gateway is started without providing the admin key, a random string will be generated to protect the gateway's admin endpoints. This can be reset by restarting the gateway with the admin key provided in the .env file. Starting and Stopping the Bundler Starting The bundler service is designed to run in conjunction with an AR.IO gateway, and so relies on the ar-io-network network created in Docker when the core gateway services are spun up. It is possible to spin up the bundler while the core services are down, but the network must exist in Docker. To start the bundler, specify the env and docker-compose files being used in a docker compose up command: docker compose --env-file ./.env.bundler --file docker-compose.bundler.yaml up -d CopyCopied! The -d flag runs the command in \"detached\" mode, so it will run in the background without requiring the terminal to remain active. Stopping To spin the bundler service down, specify the docker-compose file in a docker compose down command: docker compose --file docker-compose.bundler.yaml down CopyCopied! logs While the bundler service is running in detached mode, logs can be checked by specifying the docker-compose file in a docker compose logs command: docker compose --file docker-compose.bundler.yaml logs -f --tail=0 CopyCopied! -f runs the command in \"follow\" mode, so the terminal will continue to watch and display new logs. --tail= defines the number of logs to display that existed prior to running the command. 0 displays only new logs. Was this page helpful?YesNoComment",
          "estimatedWords": 944,
          "lastModified": "2025-06-27T16:13:15.639Z",
          "siteKey": "ario",
          "siteName": "AR-IO Network",
          "depth": 3,
          "crawledAt": "2025-06-27T16:13:15.639Z"
        },
        {
          "url": "https://docs.ar.io/gateways/grafana",
          "title": "Grafana Analytics",
          "content": "Grafana Analytics Overview AR.IO gateways track a significant number of performance and operation metrics using Prometheus. A Grafana sidecar can be deployed to visualize these metrics, and provide an easy way to monitor the health of the gateway. The Grafana sidecar is deployed as a separate docker container that uses the same network as the gateway, and is deployed in a similar manner. Deploying Grafana The file that controls the deployment of the Grafana sidecar is docker-compose.grafana.yaml. So to deploy Grafana, run the following command: docker compose -f docker-compose.grafana.yaml up -d CopyCopied! The -f flag is used to specify the path to the docker-compose file, and the up -d flag is used to deploy the container in detached mode. Terminal LocationThis command assumes that you are running the command from the root directory of the gateway. If you are running the command from a different directory, you will need to adjust the path to the docker-compose file. Checking the logs To check the logs of the Grafana sidecar, run the following command: docker compose -f docker-compose.grafana.yaml logs -f --tail=25 CopyCopied! The -f flag is used to follow the logs, and the --tail=25 flag is used to specify the number of lines to show from the end of the logs, in this case 25. Exit the logs by pressing Ctrl+C. Troubleshooting permission errors In some cases, the Grafana sidecar may encounter permission errors. There are two primary solutions to this issue: Modify Directory Permissions The simplest solution is to modify the permissions of the directory that contains the Grafana data. sudo chmod -R 777 ./data/grafana CopyCopied! This will give the grafana user ownership of the directory and all its contents. Terminal LocationThis command assumes that you are running the command from the root directory of the gateway. If you are running the command from a different directory, you will need to adjust the path to the docker-compose file. Check the logs again to ensure that the issue is resolved. Change the Grafana User The second solution is to change the user that Grafana runs as. This can be done by modifying the docker-compose.grafana.yaml file to use a different user. It is suggested to use \"root\" or \"0\" to ensure that the container has the necessary permissions. In any editor, open the docker-compose.grafana.yaml file and add \"user: root\" to the grafana service. grafana: image: grafana/grafana:latest user: root ports: - \"3000:3000\" CopyCopied! Once this is done, restart the Grafana sidecar by running the following command: docker compose -f docker-compose.grafana.yaml restart CopyCopied! Check the logs again to ensure that the issue is resolved. Configure Nginx The Grafana sidecar is deployed on the same network as the gateway, and can be accessed in a browser by navigating to http://localhost:1024 from the machine running the gateway. In order to be able to access Grafana from outside the network running the gateway, Nginx, which is already used to route gateway traffic, can be configured to route Grafana traffic to the correct port. In any editor, open the relevant Nginx configuration file. If the setup guide configuration was used, that file will be located at /etc/nginx/sites-available/default. Add the following block to the configuration file inside the server block for https (443) traffic: location /grafana/ { proxy_pass http://localhost:1024/grafana/; proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header X-Forwarded-Proto $scheme; } CopyCopied! The full configuration file should look like this: # force redirects http to https server { listen 80; listen [::]:80; server_name <domain> *.<domain>; location / { return 301 https://$host$request_uri; } } # forwards traffic into your node and provides ssl certificates server { listen 443 ssl; listen [::]:443 ssl; server_name <domain> *.<domain>; ssl_certificate /etc/letsencrypt/live/<domain>/fullchain.pem; ssl_certificate_key /etc/letsencrypt/live/<domain>/privkey.pem; location / { proxy_pass http://localhost:3000; proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_http_version 1.1; } location /grafana/ { proxy_pass http://localhost:1024/grafana/; proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header X-Forwarded-Proto $scheme; } } CopyCopied! Be sure to replace <domain> with the domain of the gateway. Once the configuration is saved, test the configuration by running the following command: sudo nginx -t CopyCopied! This will print out a message indicating that the configuration is valid. Then, restart Nginx by running the following command: sudo systemctl restart nginx CopyCopied! Once this is done, Grafana can be accessed by navigating to https://<domain>/grafana in a browser. Accessing Grafana To access Grafana, navigate to https://<domain>/grafana in a browser. The default credentials are: Username: admin Password: admin Once logged in for the first time, you will be prompted to change the password. Credential ResetUpdated credentials may be lost if the Grafana sidecar is restarted. Be sure to log into Grafana immediately after every start up to ensure Grafana cannot be accessed with the default credentials. Dashboards The Grafana sidecar comes preloaded with three dashboards: ar-io-node: Contains general gateway metrics, like the last block indexed, ArNS resolution times, and CPU usage. ar-io-node bundle indexing: Contains metrics related to bundle indexing, like the number of bundles and data items indexed. ar-io-node queue lengths: Contains metrics related to the queue lengths of the gateway, like Arweave Client requests and transaction importer data. Additional dashboards can be added in order to monitor different aspects of the gateway. The Grafana landing page contains tutorials for how to configure dashboards, as well as additional features such as alerting.Was this page helpful?YesNoComment",
          "estimatedWords": 881,
          "lastModified": "2025-06-27T16:13:16.056Z",
          "siteKey": "ario",
          "siteName": "AR-IO Network",
          "depth": 3,
          "crawledAt": "2025-06-27T16:13:16.056Z"
        },
        {
          "url": "https://docs.ar.io/gateways/cu",
          "title": "AO Compute Unit (CU)",
          "content": "AO Compute Unit (CU) Overview An AO Compute Unit (CU) is a critical component in the AO ecosystem responsible for executing AO processes and maintaining their state. CUs serve as the computational backbone of the AO network by: Processing Messages: CUs receive and process messages sent to AO processes Executing WASM Modules: CUs run the WebAssembly (WASM) code that defines process behavior Maintaining State: CUs track and update the state of AO processes Creating Checkpoints: CUs periodically save process state to the Arweave network as checkpoints Running a CU alongside your gateway allows you to: Process AO requests locally rather than relying on external services Improve response times for AO-related queries Contribute computational resources to the AO network Ensure your gateway has reliable access to AO functionality For more detailed information about Compute Units, please refer to the AO Cookbook: Units. System Requirements Before deploying a CU, ensure your system meets the following requirements: Recommended: At least 16GB RAM for optimal CU operation Minimum: 4GB RAM is possible with adjusted memory limits (see resource allocation settings) At least 100GB disk space dedicated to CU operation These requirements are separate from your gateway requirements Running a CU is resource-intensive. Make sure your system has sufficient resources to handle both the gateway and the CU. While you can run a CU with less than the recommended RAM, you'll need to adjust the memory limits accordingly. Deploying an AO CU Step 1: Navigate to Gateway Directory First, navigate to the root directory of your gateway: cd /path/to/your/gateway CopyCopied! Step 2: Configure Environment Variables Copy the example environment file: cp .env.ao.example .env.ao CopyCopied! Default .env.ao.example Contents The default .env.ao.example file contains the following settings: CU_WALLET='[wallet json here]' PROCESS_CHECKPOINT_TRUSTED_OWNERS=fcoN_xJeisVsPXA-trzVAuIiqO3ydLQxM-L4XbrQKzY GATEWAY_URL=http://envoy:3000 UPLOADER_URL=http://envoy:3000/bundler CopyCopied! These default settings are configured to work with a gateway running on the same machine, but you'll need to modify them as described below. Open the .env.ao file in your preferred text editor: nano .env.ao CopyCopied! Configure the following settings: CU_WALLET: Replace '[wallet json here]' with the JSON from an Arweave wallet. The entire JSON must be placed on a single line for proper registration. PROCESS_CHECKPOINT_TRUSTED_OWNERS: This is a comma-separated list of trusted wallet addresses: PROCESS_CHECKPOINT_TRUSTED_OWNERS=fcoN_xJeisVsPXA-trzVAuIiqO3ydLQxM-L4XbrQKzY CopyCopied! Adding Your Own WalletIf you are uploading your own checkpoints, you should add your own CU wallet address after the default value, separated by a comma:PROCESS_CHECKPOINT_TRUSTED_OWNERS=fcoN_xJeisVsPXA-trzVAuIiqO3ydLQxM-L4XbrQKzY,YOUR_WALLET_ADDRESS_HERE CopyCopied!This allows your CU to trust checkpoints from both the official source and your own wallet. GATEWAY_URL: By default, this is set to use your own gateway: GATEWAY_URL=http://envoy:3000 CopyCopied! A gateway must be set to index all ANS-104 data items from AO or the CU will not operate properly. Most users will want to set this to: GATEWAY_URL=https://arweave.net CopyCopied! UPLOADER_URL: By default, this is set to use a bundler sidecar run by your gateway: UPLOADER_URL=http://envoy:3000/bundler CopyCopied! Important: Checkpoint Uploads Require PaymentCheckpoints are uploaded to Arweave, so the upload must be paid for. You must ensure your wallet has sufficient funds: If using https://up.arweave.net (recommended), your CU_WALLET must contain Turbo Credits If using your own bundler or another service, you'll need the appropriate token (AR or other) Without proper funding, checkpoints will fail to upload and your CU may not function correctly The simplest option for most users is to use: UPLOADER_URL=https://up.arweave.net CopyCopied! This requires your CU_WALLET to contain Turbo Credits. Optional: Disable Checkpoint Creation: If you want to disable checkpoint uploads, add: DISABLE_PROCESS_CHECKPOINT_CREATION=true CopyCopied! Example of a Completed .env.ao File Here's an example of what your completed .env.ao file might look like with common settings: CU_WALLET='{\"kty\":\"RSA\",\"e\":\"AQAB\",\"n\":\"mYM07...\"}' PROCESS_CHECKPOINT_TRUSTED_OWNERS=fcoN_xJeisVsPXA-trzVAuIiqO3ydLQxM-L4XbrQKzY GATEWAY_URL=https://arweave.net UPLOADER_URL=https://up.arweave.net CopyCopied! After making your changes, save and exit the nano editor: Press Ctrl+X to exit Press Y to confirm saving changes Press Enter to confirm the filename Optional Resource Allocation Settings You can fine-tune the CU's resource usage by adding these optional environment variables: PROCESS_WASM_MEMORY_MAX_LIMIT: Sets the maximum memory limit (in bytes) for WASM processes. PROCESS_WASM_MEMORY_MAX_LIMIT=17179869184 # 16GB (16 * 1024^3) CopyCopied! Important Memory RequirementTo work with the AR.IO process, PROCESS_WASM_MEMORY_MAX_LIMIT must be at least 17179869184 (16GB).Note: This doesn't mean your server needs 16GB of RAM. This is the maximum memory limit the CU will support for processes. Most processes don't use their maximum allocated memory.You can set this value to 16GB even if your server only has 4GB of RAM. However, if a process requires more memory than your server has available, the CU will fail when evaluating messages that need more memory. WASM_EVALUATION_MAX_WORKERS: Sets the maximum number of worker threads for WASM evaluation. WASM_EVALUATION_MAX_WORKERS=4 # Example: Use 4 worker threads CopyCopied! Worker Thread ConfigurationThis will default to (available CPUs - 1) if not specified. If you're running a gateway and unbundling on the same server, consider setting this to 2 or less to avoid overloading your CPU. PROCESS_WASM_COMPUTE_MAX_LIMIT: The maximum Compute-Limit, in bytes, supported for ao processes (defaults to 9 billion) PROCESS_WASM_COMPUTE_MAX_LIMIT=9000000000 CopyCopied! NODE_OPTIONS: Sets Node.js memory allocation for the Docker container. NODE_OPTIONS=--max-old-space-size=8192 # Example: 8GB for Node.js heap CopyCopied! Resource TuningStart with conservative values and monitor performance. You can adjust these settings based on your system's capabilities and the CU's performance. Step 3: Start the CU Container Once your environment file is configured, start the CU container: docker compose --env-file .env.ao -f docker-compose.ao.yaml up -d CopyCopied! This command uses the following flags: --env-file .env.ao: Specifies the environment file to use -f docker-compose.ao.yaml: Specifies the Docker Compose file to use up: Creates and starts the containers -d: Runs containers in detached mode (background) Step 4: Check the Logs To check the logs of your CU container: docker compose -f docker-compose.ao.yaml logs -f --tail=20 CopyCopied! This command uses the following flags: -f: Follows the log output (continuous display) --tail=20: Shows only the last 20 lines of logs Exit the logs by pressing Ctrl+C. Connecting Your Gateway to the CU To make your gateway use your local CU: Add the following line to your gateway's .env file: AO_CU_URL=http://ao-cu:6363 CopyCopied! This assumes the CU is running on the same machine as the gateway. Restart your gateway: docker compose down docker compose up -d CopyCopied! A CU won't do anything until requests are being made of it. By connecting your gateway to the CU, you'll start generating these requests. Accessing Your CU Once properly set up and connected to your gateway, you can access your CU via: https://<your-gateway-domain>/ao/cu CopyCopied! This endpoint allows you to interact with your CU directly through your gateway's domain. Important Notes Initial Processing Time: A CU will need to process AO history before it can give valid responses. This process can take several hours. Gateway Fallback: A gateway on release 27 or above will fallback to arweave.net if its default CU is not responding quickly enough, so gateway operations will not be significantly impacted during the initial processing. Monitoring Progress: Check the CU logs after pointing a gateway at it to watch the process of working through AO history: docker compose -f docker-compose.ao.yaml logs -f --tail=20 CopyCopied! Resource Usage: Running a CU is resource-intensive. Monitor your system's performance to ensure it can handle both the gateway and CU workloads. Was this page helpful?YesNoComment",
          "estimatedWords": 1174,
          "lastModified": "2025-06-27T16:13:16.421Z",
          "siteKey": "ario",
          "siteName": "AR-IO Network",
          "depth": 3,
          "crawledAt": "2025-06-27T16:13:16.421Z"
        },
        {
          "url": "https://docs.ar.io/gateways/windows-setup",
          "title": "Windows Installation Instructions",
          "content": "Windows Installation Instructions Overview This guide provides step-by-step instructions for setting up the AR.IO node on a Windows computer. It covers installing necessary software, cloning the repository, creating an environment file, starting the Docker container, setting up networking, and installing and configuring NGINX Docker. No prior coding experience is required. Prerequisites Before starting the installation process, ensure you have the following: A Windows computer Administrative privileges on the computer Install Required Packages Install Docker: Download Docker Desktop for Windows from here. Run the installer and follow the prompts. During installation, make sure to select the option to use WSL (Windows Subsystem for Linux) rather than Hyper-V. Restart your PC. Update Windows Subsystem for Linux (WSL): Open the command prompt as an administrator: Press Windows Key + R. Type cmd and press Enter. Right-click on the \"Command Prompt\" application in the search results. Select \"Run as administrator\" from the context menu. Run the following commands: wsl --update wsl --shutdown CopyCopied! Restart Docker Desktop. Install Git: Download Git for Windows from here. Run the installer and use the default settings. Clone the Repository Clone the main repository: Open the command prompt: Press Windows Key + R. Type cmd and press Enter. Navigate to the directory where you want to clone the repository: Use the cd command to change directories. For example, to navigate to the Documents directory: cd Documents CopyCopied! More detailed instructions on navigating with the cd command can be found here NOTE: Your database of Arweave Transaction Headers will be created in the project directory, not Docker. So, if you are using an external hard drive to turn an old machine into a node, install the node directly to that external drive. Run the following command: git clone -b main https://github.com/ar-io/ar-io-node CopyCopied! Create the Environment File Create an environmental variables file: Open a text editor (e.g., Notepad): Press Windows Key and search for \"Notepad\". Click on \"Notepad\" to open the text editor. Paste the following content into the new file, replacing <your-domain> with the domain address you are using to access the node, and <your-public-wallet-address> with the public address of your Arweave wallet: GRAPHQL_HOST=arweave.net GRAPHQL_PORT=443 START_HEIGHT=0 RUN_OBSERVER=true ARNS_ROOT_HOST=<your-domain> AR_IO_WALLET=<your-public-wallet-address> OBSERVER_WALLET=<hot-wallet-public-address> CopyCopied! The GRAPHQL values set the proxy for GQL queries to arweave.net, You may use any available gateway that supports GQL queries. If omitted, your node can support GQL queries on locally indexed transactions, but only L1 transactions are indexed by default. START_HEIGHT is an optional line. It sets the block number where your node will start downloading and indexing transactions headers. Omitting this line will begin indexing at block 0. RUN_OBSERVER turns on the Observer to generate Network Compliance Reports. This is required for full participation in the AR.IO Network. Set to false to run your gateway without Observer. ARNS_ROOT_HOST sets the starting point for resolving ARNS names, which are accessed as a subdomain of a gateway. It should be set to the url you are pointing to your node, excluding any protocol prefix. For example, use node-ar.io and not https://node-ar.io. If you are using a subdomain to access your node and do not set this value, the node will not understand incoming requests. AR_IO_WALLET is optional, and sets the wallet you want associated with your Gateway. An associated wallet is required to join the AR.IO network. OBSERVER_WALLET is the public address of the wallet used to sign Observer transactions. This is required for Observer to run, but may be omitted if you are running a gateway outside of the AR.IO network and do not plan to run Observer. You will need to supply the keyfile to this wallet in the next step. Advanced configuration options can be found at docs.ar.io Save the file with the name \".env\" and make sure to select \"All Files\" as the file type. This helps to ensure the file saves as \".env\" and not \".env.txt\" Note: The .env file should be saved inside the same directory where you cloned the repository (e.g., ar-io-node). Supply Your Observer Wallet Keyfile: If you are running Observer, you need to provide a wallet keyfile in order to sign report upload transactions. The keyfile must be saved in the wallets directory in the root of the repository. Name the file <Observer-Wallet-Address>.json, replacing \"<Observer-Wallet-Address>\" with the public address of the wallet. This should match your OBSERVER_WALLET environmental variable. Learn more about creating Arweave wallets and obtaining keyfiles here Start the Docker Containers Start the Docker container: Open the command prompt: Press Windows Key + R. Type cmd and press Enter. Navigate to the directory where you cloned the repository (e.g., ar-io-node): Use the cd command to change directories. For example, if the repository is located in the Documents directory, you would enter: cd Documents\\ar-io-node CopyCopied! If the directory path contains spaces, enclose it in double quotation marks. For example: cd \"C:\\My Documents\\ar-io-node\" CopyCopied! Use the dir command to list the contents of the current directory and verify that you're in the correct location: dir CopyCopied! Once you are in the correct directory, run the following command to start the Docker container: docker compose up -d CopyCopied! Explanation of flags: up: Start the Docker containers. -d: Run the containers as background processes (detached mode). NOTE: Effective with Release #3, it is no longer required to include the --build flag when starting your gateway. Docker will automatically build using the image specified in the docker-commpose.yaml file. The gateway can be shut down using the command: docker compose down CopyCopied! If prompted by the firewall, allow access for Docker when requested. Set Up Router Port Forwarding To expose your node to the internet and use a custom domain, follow these steps: Obtain a Domain Name: Choose a domain registrar (e.g., Namecheap) and purchase a domain name. Point the Domain at Your Home Network: In your browser, go to https://www.whatsmyip.org/ to display your public ip address. It can be found at the top of the screen. Note this number down. Access your domain registrar's settings (e.g., Namecheap's cPanel). Navigate to the DNS settings for your domain. In cPanel this is under the \"Zone Editor\" tab. Create an A record with your registrar for your domain and wildcard subdomains, using your public IP address. For example, if your domain is \"ar.io,\" create a record for \"ar.io\" and \"*.ar.io.\" Instructions may vary depending on the domain registrar and cPanel. Consult your registrar's documentation or support for detailed steps. Obtain the Local IP Address of Your Machine: Open the command prompt: Press Windows Key + R. Type cmd and press Enter. Run the following command: ipconfig CopyCopied! Look for the network adapter that is currently connected to your network (e.g., Ethernet or Wi-Fi). Note down the IPv4 Address associated with the network adapter. It should be in the format of 192.168.X.X or 10.X.X.X. This IP address will be used for port forwarding. Set Up Router Port Forwarding: Access your home router settings: Open a web browser. Enter your router's IP address in the address bar (e.g., 192.168.0.1). If you're unsure of your router's IP address, consult your router's documentation or contact your Internet Service Provider (ISP). Navigate to the port forwarding settings in your router configuration. The exact steps may vary depending on your router model. Consult your router's documentation or support for detailed steps. Set up port forwarding rules to forward incoming traffic on ports 80 and 443 to the local IP address of your machine where the node is installed. Configure the ports to point to the local IP address noted in the previous step. Save the settings. Install and Configure NGINX Docker Clone the NGINX Docker repository: Open the command prompt: Press Windows Key + R. Type cmd and press Enter. Navigate to the directory where you want to clone the repository (This should not be done inside the directory for the node): Use the cd command to change directories. For example, to navigate to the Documents directory: cd Documents CopyCopied! Run the following command: git clone -b main https://github.com/bobinstein/dockerized-nginx CopyCopied! Note: This NGINX container was designed to easily automate many of the more technical aspects of setting up NGNIX and obtaining an ssl certificate so your node can be accessed with https. However, wildcard domain certifications cannot be universally automated due to significant security concerns. Be sure to follow the instructions in this project for obtaining wildcard domain certificates in order for your node to function properly. Follow the instructions provided in the repository for setting up NGINX Docker. Congratulations! Your AR.IO node is now running and connected to the internet. Test it by entering https://<your-domain>/3lyxgbgEvqNSvJrTX2J7CfRychUD5KClFhhVLyTPNCQ in your browser. Note: If you encounter any issues during the installation process, please seek assistance from the AR.IO community.Was this page helpful?YesNoComment",
          "estimatedWords": 1447,
          "lastModified": "2025-06-27T16:13:16.988Z",
          "siteKey": "ario",
          "siteName": "AR-IO Network",
          "depth": 2,
          "crawledAt": "2025-06-27T16:13:16.988Z"
        },
        {
          "url": "https://docs.ar.io/gateways/ar-io-node/advanced-config.html",
          "title": "Advanced Configuration",
          "content": "Advanced Configuration Overview The Getting Started guides for windows and linux contain all the information needed to start your AR.IO Gateway node successfully with basic configurations. There are also ever expanding advanced configuration options that allow you to run your node in a way that is customized to your specific use case. Most of the below options can be added to your .env file in order to customize its operation. Any changes made to your .env require you to stop the docker containers running your node, and restarting them with the --build flag in order for the changes to take effect. See ENV for a complete list of environmental variables you can set. Data Storage Location You can set a custom location for your AR.IO Gateway to save the data it pulls from the Arweave network. There are three primary types of data stored, and you can set a unique storage location for each of these independently. These are \"chunks data\", \"contiguous data\", and \"headers data\". The custom location for each of these can be set in your .env file like this: CHUNKS_DATA_PATH=<file path> CONTIGUOUS_DATA_PATH=<file path> HEADERS_DATA_PATH=<file path> CopyCopied! Be sure to replace \"<file path>\" with the path to the location where you would like the data stored. If these values are omitted, the data will be stored in the \"data\" directory inside your Gateway code repository. Admin API Key HTTP endpoints under \"/ar-io/admin\" are protected by an admin API key. These endpoints allow you to get certain analytics data or make adjustments to your node as it's running. When your node starts, it reads your environmental variables to see if a key is set. If not, a random key is generated. The key name is ADMIN_API_KEY and it should be set in your .env file like this: ADMIN_API_KEY=SUPER_SECRET_PASSWORD CopyCopied! View examples of the admin endpoints here Wallet Association In order to participate in the greater AR.IO network, Gateway nodes need to associate themselves with an Arweave wallet. This can be configured by setting the AR_IO_WALLET key value in your .env file. AR_IO_WALLET=1seRanklLU_1VTGowDZdD7s_-7k1qowT6oeFZHUZiZo CopyCopied! Unbundling AR.IO Gateway nodes support unbundling and indexing ANS-104 bundle data. This is disabled by default, but can be turned on with several different configuration options. You can set these configurations with the ANS104_UNBUNDLE_FILTER and ANS104_INDEX_FILTER keys in your .env: ANS104_UNBUNDLE_FILTER=\"<filter string>\" ANS104_INDEX_FILTER=\"<filter string>\" CopyCopied! The following types of filters are supported: { \"never\": true } # the default { \"always\": true } { \"attributes\": { \"owner\": <owner key>, ... }} { \"tags\": [{ \"name\": <utf8 tag name>, \"value\": <utf8 tag value> }, ...]} { \"and\": [ <nested filter>, ... ]} { \"or\": [ <nested filter>, ... ]} CopyCopied! Content Moderation You are able to set your Gateway to block specific transactions or data-items you don't want to serve. Unlike previous configuration options in this list, blocking content can be achieved without the need to add to your .env file and rebuild your Gateway. Instead, make a PUT request to your Gateway at /ar-io/admin/block-data. As this is an admin endpoint, you will need to have configured your ADMIN_API_KEY. Using curl as an example, the request should be formatted as follows: curl -X PUT -H \"Authorization: Bearer <ADMIN_KEY>\" \\ -H \"Content-Type: application/json\" \\ \"http://<HOST>:<PORT>/ar-io/admin/block-data\" \\ -d '{ \"id\": \"<ID>\", \"notes\": \"Example notes\", \"source\": \"Example source\" }' CopyCopied! id (string): This will be the transaction ID of the content you want to add to your block list. notes (string): Internal notes regarding why a particular ID is blocked. source (string): Identifier of a particular source of IDs to block. (e.g. the name of a block list) notes and source are used for documentation only, and have no effect on your block list itself. Contiguous Data Cleanup Transaction data on Arweave is stored in a chunked manner. It is commonly retrieved, however, in the the transaction data's original, contiguous form with all of its component chunks assembled end-to-end. Gateways cache contiguous representations of the transaction data to assist in various workloads, including serving transaction data to clients, allowing for efficient utilization of valuable system resources. Gateway operators will need to determine for themselves the best balance between disk space and other resource usage based on the size of their gateway and their particular use case. Contiguous data cache cleanup can be enabled using the CONTIGUOUS_DATA_CACHE_CLEANUP_THRESHOLD environmental variable. This variable sets the number of seconds from the creation of a file in the contiguous data cache after which that file will be deleted. For example: CONTIGUOUS_DATA_CACHE_CLEANUP_THRESHOLD=10000 CopyCopied! will clear items from the contiguous data cache after ten thousand (10,000) seconds. ArNS Resolver Gateways, by default, forward requests to resolve ArNS names to arweave.dev. Starting with Release 9 gateways can instead build and maintain their own local cache. Doing so removes external dependencies and allows faster resolution. View the code for the ArNS resolver service here: https://github.com/ar-io/arns-resolver NOTE: The ArNS resolver is still an experimental feature. It is possible it may behave in unexpected ways when presented with rare edge case scenarios. In order to enable the local ArNS resolver, three environmental variables will need to be set: RUN_RESOLVER=true TRUSTED_ARNS_RESOLVER_TYPE=resolver TRUSTED_ARNS_RESOLVER_URL=http://resolver:6000 CopyCopied! RUN_RESOLVER is a boolean representing an on/off switch for the local resolver. TRUSTED_ARNS_RESOLVER_TYPE sets the method the gateway uses for resolving ArNS names. Use resolver for the local resolver, or gateway for default functionality. TRUSTED_ARNS_RESOLVER_URL is the url a gateway will use to request ArNS name resolution. Was this page helpful?YesNoComment",
          "estimatedWords": 902,
          "lastModified": "2025-06-27T16:13:17.045Z",
          "siteKey": "ario",
          "siteName": "AR-IO Network",
          "depth": 3,
          "crawledAt": "2025-06-27T16:13:17.045Z"
        },
        {
          "url": "https://docs.ar.io/gateways/parquet",
          "title": "Parquet and ClickHouse Usage Guide",
          "content": "Parquet and ClickHouse Usage Guide Overview AR.IO gateway Release 33 introduces a new configuration option for using Parquet files and ClickHouse to improve performance and scalability of your AR.IO gateway for large datasets. This guide will walk you through the process of setting up ClickHouse with your AR.IO gateway, and importing Parquet files to bootstrap your ClickHouse database. What is Parquet? Apache Parquet is a columnar storage file format designed for efficient data storage and retrieval. Unlike row-based storage formats like SQLite, Parquet organizes data by column rather than by row, which provides several advantages for analytical workloads: Efficient compression: Similar data is stored together, leading to better compression ratios Columnar access: You can read only the columns you need, reducing I/O operations Predicate pushdown: Filter operations can be pushed down to the storage layer, improving query performance Current Integration with AR.IO Gateways In the current AR.IO gateway implementation, Parquet and ClickHouse run alongside SQLite rather than replacing it. This parallel architecture allows each database to handle what it does best: SQLite continues to handle transaction writes and updates ClickHouse with Parquet files is optimized for fast query performance, especially with large datasets The gateway continues to operate with SQLite just as it always has, maintaining all of its normal functionality. Periodically, the gateway will export batches of data from SQLite to Parquet files, which are then imported into ClickHouse. This batch-oriented approach is much more efficient than attempting to synchronize the databases in real-time, as it leverages Parquet's strength in handling large, immutable data sets. Note that despite Parquet's efficient compression, gateways may not see significant disk space reduction in all cases. While bundled transaction data is exported to Parquet, L1 data remains in SQLite. Without substantial unbundling and indexing filters, minimal data gets exported to Parquet, limiting potential storage savings. With ClickHouse integration enabled, GraphQL queries are primarily routed to ClickHouse, leveraging its superior performance for large datasets. This significantly improves response times while maintaining SQLite's reliability for transaction processing. Parquet vs. SQLite in AR.IO Gateways While SQLite is excellent for transactional workloads and small to medium datasets, it faces challenges with very large datasets: ← Swipe to see more →FeatureSQLiteParquet + ClickHouseStorage modelRow-basedColumn-basedQuery optimizationBasicAdvanced analytical optimizationCompressionLimitedHigh compression ratiosScalingLimited by single fileDistributed processing capableWrite speedFast for small transactionsOptimized for batch operationsRead speed for analyticsSlower for large datasetsOptimized for analytical queriesIdeal use caseRecent transaction data, OLTPHistorical data, OLAP workloads← Swipe to see more → Benefits for Gateway Operators Implementing Parquet and ClickHouse alongside SQLite in your AR.IO gateway offers several key advantages: Dramatically improved query performance for GraphQL endpoints, especially for large result sets Reduced storage requirements through efficient columnar compression Better scalability for growing datasets Faster bootstrapping of new gateways through Parquet file imports Reduced load on SQLite by offloading query operations to ClickHouse The primary focus of the Parquet/ClickHouse integration is the significant speed improvement for querying large datasets. Gateway operators managing significant volumes of data will notice substantial performance gains when using this configuration. Storage Considerations While Parquet files offer more efficient compression for the data they contain, it's important to understand the storage impact: Bundled transaction data is exported to Parquet and removed from SQLite, potentially saving space L1 data remains in SQLite regardless of Parquet configuration Space savings are highly dependent on your unbundling filters - without substantial unbundling configurations, minimal data gets exported to Parquet The more data you unbundle and export to Parquet, the greater the potential storage efficiency For gateway operators, this means proper filter configuration is crucial to realize storage benefits. The primary advantage remains significantly improved query performance for large datasets, with potential space savings as a secondary benefit depending on your specific configuration. The following sections will guide you through setting up ClickHouse with your AR.IO gateway, exporting data from SQLite to Parquet, and importing Parquet files to bootstrap your ClickHouse database. NoteThe below instructions are designed to be used in a linux environment. Windows and MacOS users must modify the instructions to use the appropriate package manager/ command syntax for their platform.Unless otherwise specified, all commands should be run from the root directory of the gateway. Installing ClickHouse ClickHouse is a powerful, open-source analytical database that excels at handling large datasets and complex queries. It is the tool used by the gateway to integrate with the Parquet format. To integrate ClickHouse with your AR.IO gateway, follow these steps: It is recommended to use official pre-compiled deb packages for Debian or Ubuntu. Run these commands to install packages: sudo apt-get install -y apt-transport-https ca-certificates curl gnupg curl -fsSL 'https://packages.clickhouse.com/rpm/lts/repodata/repomd.xml.key' | sudo gpg --dearmor -o /usr/share/keyrings/clickhouse-keyring.gpg ARCH=$(dpkg --print-architecture) echo \"deb [signed-by=/usr/share/keyrings/clickhouse-keyring.gpg arch=${ARCH}] https://packages.clickhouse.com/deb stable main\" | sudo tee /etc/apt/sources.list.d/clickhouse.list sudo apt-get update CopyCopied! This will verify the installation package from official sources and enable installation via apt-get. sudo apt-get install -y clickhouse-client CopyCopied! This will perform the actual installation of the ClickHouse server and client. During installation, you will be prompted to set a password for the default user. This is required to connect to the ClickHouse server. Advanced users may also choose to create a designated user account in clickhouse for the gateway to use, but the default gateway configuration will assume the default user. Configure Gateway to use ClickHouse Because the gateway will be accessing ClickHouse, host address andthe password for the selected user must be provided. This is done via the CLICKHOUSE_PASSWORD environment variable. Update your .env file with the following: CLICKHOUSE_URL=\"http://clickhouse:8123\" CLICKHOUSE_PASSWORD=<your-password> CopyCopied! If you set a specific user account for the gateway to use, you can set the CLICKHOUSE_USER environment variable to the username. CLICKHOUSE_USER=<your-username> CopyCopied! If omitted, the gateway will use the default user. Additionally, The Parquet file provided below contains an unbundled data set that includes all data items uploaded via an ArDrive product, including Turbo. Because of this, it is recommended to include unbundling filters that match, or expand, this configuration. ANS104_UNBUNDLE_FILTER='{ \"and\": [ { \"not\": { \"or\": [ { \"tags\": [ { \"name\": \"Bundler-App-Name\", \"value\": \"Warp\" } ] }, { \"tags\": [ { \"name\": \"Bundler-App-Name\", \"value\": \"Redstone\" } ] }, { \"tags\": [ { \"name\": \"Bundler-App-Name\", \"value\": \"KYVE\" } ] }, { \"tags\": [ { \"name\": \"Bundler-App-Name\", \"value\": \"AO\" } ] }, { \"attributes\": { \"owner_address\": \"-OXcT1sVRSA5eGwt2k6Yuz8-3e3g9WJi5uSE99CWqsBs\" } }, { \"attributes\": { \"owner_address\": \"ZE0N-8P9gXkhtK-07PQu9d8me5tGDxa_i4Mee5RzVYg\" } }, { \"attributes\": { \"owner_address\": \"6DTqSgzXVErOuLhaP0fmAjqF4yzXkvth58asTxP3pNw\" } } ] } }, { \"tags\": [ { \"name\": \"App-Name\", \"valueStartsWith\": \"ArDrive\" } ] } ] }' ANS104_INDEX_FILTER='{ \"tags\": [ { \"name\": \"App-Name\", \"value\": \"ArDrive-App\" } ] }' CopyCopied! Lastly, you must have a gateway admin password set. This is used for the periodic export of data from SQLite to Parquet. ADMIN_API_KEY=<example> CopyCopied! Once the .env file is updated, restart the gateway to apply the changes. Downloading the Parquet File A Parquet archive file is available for download from ar://JVmsuD2EmFkhitzWN71oi9woADE4WUfvrbBYgremCBM. This file contains an unbundled data set that includes all data items uploaded via an ArDrive product, current to April 23, 2025, and compressed using tar.gz. To download the file, run the following command: curl -L https://arweave.net/JVmsuD2EmFkhitzWN71oi9woADE4WUfvrbBYgremCBM -o 2025-04-23-ardrive-ans104-parquet.tar.gz CopyCopied! or visit the url https://arweave.net/JVmsuD2EmFkhitzWN71oi9woADE4WUfvrbBYgremCBM and download the file manually. NoteIf downloaded manually, it will download as a binary file named JVmsuD2EmFkhitzWN71oi9woADE4WUfvrbBYgremCBM. This is normal and must be converted to a tar.gz file by renaming it to 2025-04-23-ardrive-ans104-parquet.tar.gz.It should also be placed in the root directory of the gateway. The downloaded file will be approximately 3.5GB in size. Extracting and Importing the Parquet File With the parquet file downloaded and placed in the root directory of the gateway, you can extract the file and import it into ClickHouse. tar -xzf 2025-04-23-ardrive-ans104-parquet.tar.gz CopyCopied! This will extract the file into a directory named 2025-04-23-ardrive-ans104-parquet, and take a while to complete. Next, if you do not already have a data/parquet directory, you must create it. Release 33 does not have this directory by default, but future Releases will. You can create the directory by using the following command: mkdir -p data/parquet CopyCopied! or by starting the gateway ClickHouse container with the following command: docker compose --profile clickhouse up clickhouse -d CopyCopied! NoteDepending on your system configurations, allowing the gateway to create the directory may result in the directory being created with incorrect permissions. If this is the case, you can remove the restrictions by running the following command:sudo chmod -R 777 data/parquet CopyCopied! With the directory created, you can now move the extracted parquet files into it. mv 2025-04-23-ardrive-ans104-parquet/* data/parquet CopyCopied! When this is complete, you can run the import script to import the parquet files into ClickHouse. If you haven't done so already, start the ClickHouse container with the following command: docker compose --profile clickhouse up clickhouse -d CopyCopied! Then run the import script with the following command: ./scripts/clickhouse-import CopyCopied! This process will take several minutes, and will output the progress of the import. Verifying Successful Import To verify that the import was successful, run the following commands: clickhouse client --password <your-password> -h localhost -q 'SELECT COUNT(DISTINCT id) FROM transactions' CopyCopied! Being sure to replace <your-password> with the password you set for the selected ClickHouse user. This should return a count of the number of unique transactions in the parquet file, which is 32712311. You can also verify that the data is being served by the gateway's GraphQL endpoint by ensuring the gateway is not proxying its GraphQL queries (Make sure GRAPHQL_HOST is not set) and running the following command: curl -g -X POST \\ -H \"Content-Type: application/json\" \\ -d '{\"query\":\"query { transactions(ids: [\\\"YSNwoYB01EFIzbs6HmkGUjjxHW3xuqh-rckYhi0av4A\\\"]) { edges { node { block { height } bundledIn { id } } } } }\"}' \\ http://localhost:3000/graphql # Expected output: # {\"data\":{\"transactions\":{\"edges\":[{\"node\":{\"block\":{\"height\":1461918},\"bundledIn\":{\"id\":\"ylhb0PqDtG5HwBg00_RYztUl0x2RuKvbNzT6YiNR2JA\"}}}]}}} CopyCopied! Starting and Stopping the Gateway with ClickHouse The gateway ClickHouse container is run as a \"profile\" in the main docker compose file. That means you must specify the profile when starting or stopping the gateway if you want to include the ClickHouse container in the commands. To start the gateway with the ClickHouse profile, run the following command: docker compose --profile clickhouse up -d CopyCopied! This will start all of the containers normally covered by the docker compose up command, but will also start the ClickHouse container. To stop the gateway with the ClickHouse profile, run the following command: docker compose --profile clickhouse down CopyCopied! This will stop all of the containers normally covered by the docker compose down command, but will also stop the ClickHouse container. To start or stop only the ClickHouse container, you can use the following commands: docker compose --profile clickhouse up clickhouse -d CopyCopied! and docker compose --profile clickhouse down clickhouse CopyCopied!Was this page helpful?YesNoComment",
          "estimatedWords": 1759,
          "lastModified": "2025-06-27T16:13:17.783Z",
          "siteKey": "ario",
          "siteName": "AR-IO Network",
          "depth": 2,
          "crawledAt": "2025-06-27T16:13:17.783Z"
        },
        {
          "url": "https://docs.ar.io/gateways/snapshots",
          "title": "Importing SQLite Database Snapshots",
          "content": "Importing SQLite Database Snapshots Overview One of the challenges of running an AR.IO Gateway is the initial synchronization time as your gateway builds its local index of the Arweave network. This process can take days or even weeks, depending on your hardware and the amount of data you want to index. To accelerate this process, you can import a pre-synchronized SQLite database snapshot that contains transaction and data item records already indexed. This guide will walk you through the process of importing a database snapshot into your AR.IO Gateway. NoteThe below instructions are designed to be used in a linux environment. Windows and MacOS users must modify the instructions to use the appropriate package manager/ command syntax for their platform.Unless otherwise specified, all commands should be run from the root directory of the gateway. Obtaining a Database Snapshot SQLite database snapshots are very large and not easy to incrementally update. For these reasons, AR.IO is distributing them using BitTorrent. These snapshots can be downloaded using any preferred torrenting client, and below is instructions on doing so using transmission-cli from a terminal. transmission-cli \"magnet:?xt=urn:btih:62ca6e05248e6df59fac9e38252e9c71951294ed&dn=2025-04-23-sqlite.tar.gz&tr=udp%3A%2F%2Ftracker.opentrackr.org%3A1337%2Fannounce&tr=http%3A%2F%2Ftracker.opentrackr.org%3A1337%2Fannounce&tr=udp%3A%2F%2Fopen.demonii.com%3A1337%2Fannounce&tr=udp%3A%2F%2Ftracker.torrent.eu.org%3A451%2Fannounce&tr=udp%3A%2F%2Fp4p.arenabg.com%3A1337%2Fannounce&tr=https%3A%2F%2Ftracker.bt4g.com%3A443%2Fannounce\" CopyCopied! This will download a snapshot, current to April 23, 2025, of an unbundled data set that includes all data items uploaded via an ArDrive product, including Turbo. The file will be named 2025-04-23-sqlite.tar.gz and be approximately 42.8Gb in size. NoteWhile continuing to seed the torrent after download is not required, it is highly recommended to help ensure the continued availability of the snapshot for others, as well as the integrity of the data. Seeding this file should not cause any issues with your internet service provider. This is a compressed tarball, so it will need to be extracted before it can be used. Extracting the Database Snapshot Once the file has downloaded, you can extract it using the following command, be sure to replace the filename with the actual filename of the snapshot you are using, if not using the example above. tar -xzf 2025-04-23-sqlite.tar.gz CopyCopied! This will extract the file into a directory matching the filename, minus the .tar.gz extension. Importing the Database Snapshot Once you have an extracted database snapshot, you can import it into your AR.IO gateway by replacing the existing SQLite database files. Follow the instructions below to do so. IMPORTANTImporting a database snapshot will delete your existing database and replace it with the snapshot you are importing. Stop your AR.IO gateway. docker compose down CopyCopied! (Optional) Backup your existing SQLite database files. mkdir sqlite-backup mv data/sqlite/* sqlite-backup/ CopyCopied! Delete the existing SQLite database files. rm data/sqlite/* CopyCopied! Move the snapshot files into the data/sqlite directory. mv 2025-04-23-sqlite/* data/sqlite/ CopyCopied! Be sure to replace 2025-04-23-sqlite with the actual directory name of the extracted snapshot you are using. Start your AR.IO gateway. docker compose up -d CopyCopied! Verifying the Import The simplest way to verify the import is to check the gateway logs to see what block number is being imported. The 2025-04-23 snapshot was taken at block 1645229, so the gateway will start importing blocks after this height if the snapshot was imported successfully. You can also use the Grafana Sidecar to view the last block imported in a more human readable format.Was this page helpful?YesNoComment",
          "estimatedWords": 534,
          "lastModified": "2025-06-27T16:13:18.067Z",
          "siteKey": "ario",
          "siteName": "AR-IO Network",
          "depth": 2,
          "crawledAt": "2025-06-27T16:13:18.067Z"
        },
        {
          "url": "https://docs.ar.io/gateways/join-network",
          "title": "Join the ARIO Network",
          "content": "Join the AR.IO Network Prerequisites Must have a fully functional AR.IO gateway. This includes the ability to resolve ArNS subdomains. Follow installation instructions for windows or linux and get help from the ar.io community. Gateway must be associated with an Arweave Wallet. Learn about creating Arweave wallets here Arweave wallet must be funded with enough ARIO tokens to meet the minimum stake for gateway operators. Joining Via Network Portal The simplest method for joining a new gateway to the Gateway Address Registry (GAR) is to use the Network Portal. The Network portal has a prominent \"Start your own gateway\" button That will open a form where configurations can be set for your gateway in the network. Start Your Gateway Start Gateway Form Start Gateway Form The form is used to set basic configurations for a gateway when joining the network. It contains the following fields: Label: This is a friendly name for a gateway. It can be a maximum of 64 characters. Address: This is the fully qualified domain name of the gateway. That is, the standard web address used to access the gateway. i.e. arweave.net. The form prefills the https:// protocol prefix, and www should not be included. Gateways DO support using subdomains as their address, so long as the gateway is properly configured. Observer Wallet: This is the public wallet address of the wallet used for the gateway's observer. By default, the primary gateway wallet address is filled in this space; however, a different wallet may be utilized if desired for operational reasons. Properties ID: This is an Arweave Transaction Id for a JSON object that contains additional details about the gateway. The gateway network has not yet incorporated these properties into standard gateway participation, and so the space may safely be left as the default value. The contents of the default properties Id can be viewed here Stake: This is the amount of ARIO tokens to be staked to the gateway. It must be at least the network minimum. Delegated Stake: This toggle enables or disables delegated staking on a gateway. This may be changed later. Minimum Delegated Stake: This is the minimum number of ARIO tokens that a delegate must stake in order to stake to a gateway. The network minimum is 10 ARIO. Reward Share Ratio: The percentage of gateway rewards that will be distributed to delegated stakers. Note: A description of the gateway. It can be a maximum of 256 characters. Once all required fields of the form are completed, the \"Confirm\" button will become available. Clicking this will prompt a signature from the connected Arweave wallet in order to complete the joining process. Joining Programmatically Joining the network can also be completed programmatically through the AR.IO SDK. This is done using the join-network method on the ARIO class. The method must be called after authenticating the ARIO class using the wallet to be associated with the new gateway.Was this page helpful?YesNoComment",
          "estimatedWords": 488,
          "lastModified": "2025-06-27T16:13:18.311Z",
          "siteKey": "ario",
          "siteName": "AR-IO Network",
          "depth": 2,
          "crawledAt": "2025-06-27T16:13:18.311Z"
        },
        {
          "url": "https://docs.ar.io/ar-io-sdk/ario/gateways/join-network",
          "title": "joinNetwork",
          "content": "joinNetwork joinNetwork is a method on the ARIO class that joins a gateway to the ar.io network using its associated wallet. joinNetwork requires authentication. Parameters ← Swipe to see more →ParameterTypeDescriptionRequiredqtynumberAmount in mARIO to stake when joining networkYesautoStakebooleanWhether to automatically stake gateway rewardsYesallowDelegatedStakingbooleanWhether to allow third parties to delegate stakeYesminDelegatedStakenumberMinimum amount in mARIO that can be delegatedYesdelegateRewardShareRationumberPercentage of rewards to share with delegates (e.g., 10)YeslabelstringGateway name (1-64 characters)YesnotestringGateway description (max 256 characters)YespropertiesstringArweave transaction ID containing additional gateway configurationYesobserverWalletstringWallet address used for network observationsYesfqdnstringValid domain name owned by the gateway operatorYesportnumberPort number for gateway access (typically 443)YesprotocolstringAccess protocol (only 'https' supported)YestagsarrayAn array of GQL tag objects to attach to the transactionNo← Swipe to see more → Example joinNetworkNodeJSWeb const fs = require(\"fs\"); const { ARIO, ArweaveSigner, ARIOToken } = require(\"@ar.io/sdk\"); async function main() { const jwk = JSON.parse(fs.readFileSync(\"KeyFile.json\")); const ario = ARIO.init({ signer: new ArweaveSigner(jwk), }); const { id: txId } = await ario.joinNetwork( { qty: new ARIOToken(10_000).toMARIO(), // minimum operator stake allowed autoStake: true, // auto-stake operator rewards to the gateway allowDelegatedStaking: true, // allows delegated staking minDelegatedStake: new ARIOToken(100).toMARIO(), // minimum delegated stake allowed delegateRewardShareRatio: 10, // percentage of rewards to share with delegates (e.g. 10%) label: 'john smith', // min 1, max 64 characters note: 'The example gateway', // max 256 characters properties: 'FH1aVetOoulPGqgYukj0VE0wIhDy90WiQoV3U2PeY44', // Arweave transaction ID containing additional properties of the Gateway observerWallet: '0VE0wIhDy90WiQoV3U2PeY44FH1aVetOoulPGqgYukj', // wallet address of the observer, must match OBSERVER_WALLET on the observer fqdn: 'example.com', // fully qualified domain name - note: you must own the domain and set the OBSERVER_WALLET on your gateway to match `observerWallet` port: 443, // port number protocol: 'https', // only 'https' is supported }, // optional additional tags { tags: [{ name: 'App-Name', value: 'My-Awesome-App' }] }, ); } main(); CopyCopied!Was this page helpful?YesNoComment",
          "estimatedWords": 297,
          "lastModified": "2025-06-27T16:13:18.628Z",
          "siteKey": "ario",
          "siteName": "AR-IO Network",
          "depth": 3,
          "crawledAt": "2025-06-27T16:13:18.628Z"
        },
        {
          "url": "https://docs.ar.io/ar-io-sdk/ario/gateways/get-gateway",
          "title": "getGateway",
          "content": "getGateway getGateway is a method on the ARIO class that retrieves detailed information about a specific gateway using its wallet address. getGateway does not require authentication. Parameters ← Swipe to see more →ParameterTypeDescriptionOptionalgatewayAddressstring - WalletAddressThe wallet address of the gateway to retrievefalse← Swipe to see more → Examples getGatewayNodeJSWebconst { ARIO } = require('@ar.io/sdk'); async function main() { const ario = ARIO.init(); const gateway = await ario.getGateway({ gatewayAddress: 't4Xr0_J4Iurt7caNST02cMotaz2FIbWQ4Kbj616RHl3' }); console.log(gateway); } main(); CopyCopied! Output { \"gatewayAddress\": \"t4Xr0_J4Iurt7caNST02cMotaz2FIbWQ4Kbj616RHl3\", \"observerWallet\": \"t4Xr0_J4Iurt7caNST02cMotaz2FIbWQ4Kbj616RHl3\", \"operatorStake\": 500000000000, \"totalDelegatedStake\": 250000000000, \"settings\": { \"label\": \"AR.IO Test Gateway\", \"fqdn\": \"testgateway.ar.io\", \"port\": 443, \"protocol\": \"https\", \"allowDelegatedStaking\": true, \"minDelegatedStake\": 100000000, \"delegateRewardShareRatio\": 10, \"properties\": { \"description\": \"A test gateway for the AR.IO network\" } }, \"stats\": { \"prescribedEpochCount\": 150, \"observedEpochCount\": 148, \"totalEpochCount\": 150, \"passedEpochCount\": 148, \"failedEpochCount\": 2, \"totalEpochParticipationCount\": 148, \"totalEpochsPrescribedCount\": 150 }, \"status\": \"joined\", \"startTimestamp\": 1706227200000, \"endTimestamp\": 0, \"vaults\": { \"vault_1\": { \"balance\": 100000000, \"start\": 1726243200000, \"end\": 1726329600000 } }, \"delegates\": { \"delegate_address\": { \"delegatedStake\": 50000000, \"start\": 1726243200000, \"vaults\": {} } } } CopyCopied!Was this page helpful?YesNoComment",
          "estimatedWords": 166,
          "lastModified": "2025-06-27T16:13:18.879Z",
          "siteKey": "ario",
          "siteName": "AR-IO Network",
          "depth": 4,
          "crawledAt": "2025-06-27T16:13:18.879Z"
        },
        {
          "url": "https://docs.ar.io/ar-io-sdk/ario/gateways/get-gateways",
          "title": "getGateways",
          "content": "getGateways getGateways is a method on the ARIO class that retrieves all gateways with optional pagination and filtering support. getGateways does not require authentication. Parameters ← Swipe to see more →ParameterTypeDescriptionOptionalDefaultcursorstringThe gateway address to use as the starting point for paginationtrueNonelimitnumberThe maximum number of gateways to return (max: 1000)true100sortBystringThe property to sort gateways bytruestartTimestampsortOrderstringThe sort direction ('desc' or 'asc')truedesc← Swipe to see more → Examples getGatewaysNodeJSWebconst { ARIO } = require('@ar.io/sdk'); async function main() { const ario = ARIO.init(); const gateways = await ario.getGateways({ limit: 10, sortBy: 'operatorStake', sortOrder: 'desc' }); console.log(gateways); } main(); CopyCopied! Output { \"items\": [ { \"gatewayAddress\": \"t4Xr0_J4Iurt7caNST02cMotaz2FIbWQ4Kbj616RHl3\", \"observerWallet\": \"t4Xr0_J4Iurt7caNST02cMotaz2FIbWQ4Kbj616RHl3\", \"operatorStake\": 500000000000, \"totalDelegatedStake\": 250000000000, \"settings\": { \"label\": \"AR.IO Test Gateway\", \"fqdn\": \"testgateway.ar.io\", \"port\": 443, \"protocol\": \"https\", \"allowDelegatedStaking\": true, \"minDelegatedStake\": 100000000, \"delegateRewardShareRatio\": 10 }, \"status\": \"joined\", \"startTimestamp\": 1706227200000, \"stats\": { \"prescribedEpochCount\": 150, \"observedEpochCount\": 148, \"passedEpochCount\": 148, \"failedEpochCount\": 2 } } ], \"hasMore\": true, \"nextCursor\": \"anotherGatewayAddress\", \"totalItems\": 567, \"sortBy\": \"operatorStake\", \"sortOrder\": \"desc\" } CopyCopied!Was this page helpful?YesNoComment",
          "estimatedWords": 159,
          "lastModified": "2025-06-27T16:13:19.205Z",
          "siteKey": "ario",
          "siteName": "AR-IO Network",
          "depth": 4,
          "crawledAt": "2025-06-27T16:13:19.205Z"
        },
        {
          "url": "https://docs.ar.io/ar-io-sdk/ario/gateways/leave-network",
          "title": "leaveNetwork",
          "content": "leaveNetwork leaveNetwork is a method on the ARIO class that sets a gateway's status to leaving on the ar.io network. The gateway's operator and delegate stakes are vaulted and will be returned after the leave period. The gateway will be removed from the network once the leave period ends. leaveNetwork requires authentication. Parameters ← Swipe to see more →ParameterTypeDescriptionRequiredtagsarrayAn array of GQL tag objects to attach to the AO messageNo← Swipe to see more → Example leaveNetworkNodeJSWeb const fs = require(\"fs\"); const { ARIO, ArweaveSigner } = require(\"@ar.io/sdk\"); async function main() { const jwk = JSON.parse(fs.readFileSync(\"KeyFile.json\")); const ario = ARIO.init({ signer: new ArweaveSigner(jwk), }); const { id: txId } = await ario.leaveNetwork( // optional additional tags { tags: [{ name: 'App-Name', value: 'My-Awesome-App' }] }, ); } main(); CopyCopied!Was this page helpful?YesNoComment",
          "estimatedWords": 132,
          "lastModified": "2025-06-27T16:13:19.424Z",
          "siteKey": "ario",
          "siteName": "AR-IO Network",
          "depth": 4,
          "crawledAt": "2025-06-27T16:13:19.424Z"
        },
        {
          "url": "https://docs.ar.io/ar-io-sdk/ario/gateways/update-gateway-settings",
          "title": "updateGatewaySettings",
          "content": "updateGatewaySettings updateGatewaySettings is a method on the ARIO class that writes new gateway settings to the caller's gateway configuration. updateGatewaySettings requires authentication. Parameters ← Swipe to see more →ParameterTypeDescriptionOptionalautoStakebooleanIf true, automatically stakes gateway rewards.trueallowDelegatedStakingbooleanIf true, allows third parties to delegate stake to the gateway.trueminDelegatedStakenumberMinimum number of tokens, in mARIO that can be delegated to the gateway.truedelegateRewardShareRationumberPercentage of gateway rewards to share with delegates. e.g. 10%truelabelstringFriendly name for gateway, min 1 character, max 64 characters.truenotestringA note to be associated with gateway, max 256 characters.truepropertiesstring - ArweaveTxIdArweaveTxId to properties object containing additional gateway configuration details.trueobserverWalletstring - WalletAddressPublic wallet address for wallet used to upload network observations.truefqdnstringFully qualified domain name, must be valid domain owned by gateway operator.trueportnumberPort number to use when accessing gateway, generally 443 (https)trueprotocolstring - \"http\" || \"https\"Protocol to use when accessing gateway, only \"https\" is supported for network participation.truetagsarrayAn array of GQL tag objects to attach to the joinNetwork AO message.true← Swipe to see more → Example updateGatewaySettingsNodeJSWeb const fs = require(\"fs\"); const { ARIO, ArweaveSigner, ARIOToken } = require(\"@ar.io/sdk\"); async function main() { const jwk = JSON.parse(fs.readFileSync(\"KeyFile.json\")); const ario = ARIO.init({ signer: new ArweaveSigner(jwk), }); const { id: txId } = await ario.updateGatewaySettings( { // any other settings you want to update minDelegatedStake: new ARIOToken(100).toMARIO(), }, // optional additional tags { tags: [{ name: 'App-Name', value: 'My-Awesome-App' }] }, ); } main(); CopyCopied!Was this page helpful?YesNoComment",
          "estimatedWords": 228,
          "lastModified": "2025-06-27T16:13:19.780Z",
          "siteKey": "ario",
          "siteName": "AR-IO Network",
          "depth": 4,
          "crawledAt": "2025-06-27T16:13:19.780Z"
        },
        {
          "url": "https://docs.ar.io/ar-io-sdk/ario/gateways/increase-operator-stake",
          "title": "increaseOperatorStake",
          "content": "increaseOperatorStake increaseOperatorStake is a method on the ARIO class that increases the caller's operator stake. This method must be executed with a wallet registered as a gateway operator. increaseOperatorStake requires authentication. Parameters ← Swipe to see more →ParameterTypeDescriptionRequiredqtynumberAmount in mARIO to add to operator stakeYestagsarrayAn array of GQL tag objects to attach to the transactionNo← Swipe to see more → Examples increaseOperatorStakeNodeJSWeb const fs = require(\"fs\"); const { ARIO, ArweaveSigner, ARIOToken } = require(\"@ar.io/sdk\"); async function main() { const jwk = JSON.parse(fs.readFileSync(\"KeyFile.json\")); const ario = ARIO.init({ signer: new ArweaveSigner(jwk), }); const { id: txId } = await ario.increaseOperatorStake( { qty: new ARIOToken(100).toMARIO(), }, { tags: [{ name: 'App-Name', value: 'My-Awesome-App' }], }, ); } main(); CopyCopied!Was this page helpful?YesNoComment",
          "estimatedWords": 119,
          "lastModified": "2025-06-27T16:13:19.985Z",
          "siteKey": "ario",
          "siteName": "AR-IO Network",
          "depth": 4,
          "crawledAt": "2025-06-27T16:13:19.985Z"
        },
        {
          "url": "https://docs.ar.io/ar-io-sdk/ario/gateways/decrease-operator-stake",
          "title": "decreaseOperatorStake",
          "content": "decreaseOperatorStake decreaseOperatorStake is a method on the ARIO class that decreases the caller's operator stake. This method must be executed with a wallet registered as a gateway operator. decreaseOperatorStake requires authentication. Parameters ← Swipe to see more →ParameterTypeDescriptionRequiredqtynumberAmount in mARIO to remove from operator stake (cannot decrease below the network minimum)YestagsarrayAn array of GQL tag objects to attach to the transactionNo← Swipe to see more → Examples decreaseOperatorStakeNodeJSWeb const fs = require(\"fs\"); const { ARIO, ArweaveSigner, ARIOToken } = require(\"@ar.io/sdk\"); async function main() { const jwk = JSON.parse(fs.readFileSync(\"KeyFile.json\")); const ario = ARIO.init({ signer: new ArweaveSigner(jwk), }); const { id: txId } = await ario.decreaseOperatorStake( { qty: new ARIOToken(100).toMARIO(), }, { tags: [{ name: 'App-Name', value: 'My-Awesome-App' }], }, ); } main(); CopyCopied!Was this page helpful?YesNoComment",
          "estimatedWords": 125,
          "lastModified": "2025-06-27T16:13:20.340Z",
          "siteKey": "ario",
          "siteName": "AR-IO Network",
          "depth": 4,
          "crawledAt": "2025-06-27T16:13:20.340Z"
        },
        {
          "url": "https://docs.ar.io/ar-io-sdk/ario/gateways/increase-delegate-stake",
          "title": "increaseDelegateStake",
          "content": "increaseDelegateStake increaseDelegateStake is a method on the ARIO class that increases the caller's delegated stake on the target gateway. increaseDelegateStake requires authentication. Parameters ← Swipe to see more →ParameterTypeDescriptionRequiredqtynumberAmount in mARIO to add to delegated stakeYestargetstringThe gateway's wallet addressYestagsarrayAn array of GQL tag objects to attach to the transactionNo← Swipe to see more → Example increaseDelegateStakeNodeJSWeb const fs = require(\"fs\"); const { ARIO, ArweaveSigner, ARIOToken } = require(\"@ar.io/sdk\"); async function main() { const jwk = JSON.parse(fs.readFileSync(\"KeyFile.json\")); const ario = ARIO.init({ signer: new ArweaveSigner(jwk), }); const { id: txId } = await ario.increaseDelegateStake( { target: 't4Xr0_J4Iurt7caNST02cMotaz2FIbWQ4Kbj616RHl3', qty: new ARIOToken(100).toMARIO(), }, // optional additional tags { tags: [{ name: 'App-Name', value: 'My-Awesome-App' }] }, ); } main(); CopyCopied!Was this page helpful?YesNoComment",
          "estimatedWords": 119,
          "lastModified": "2025-06-27T16:13:20.547Z",
          "siteKey": "ario",
          "siteName": "AR-IO Network",
          "depth": 4,
          "crawledAt": "2025-06-27T16:13:20.547Z"
        },
        {
          "url": "https://docs.ar.io/ar-io-sdk/ario/gateways/decrease-delegate-stake",
          "title": "decreaseDelegateStake",
          "content": "decreaseDelegateStake decreaseDelegateStake is a method on the ARIO class that decreases the caller's delegated stake on the target gateway. decreaseDelegateStake requires authentication. Parameters ← Swipe to see more →ParameterTypeDescriptionRequiredqtynumberAmount in mARIO to remove from delegated stakeYestargetstringThe gateway's public wallet addressYesinstantbooleanIf true, pays a fee to make the withdrawn stake available instantlyNotagsarrayAn array of GQL tag objects to attach to the transactionNo← Swipe to see more → Example decreaseDelegateStakeNodeJS - standardNodeJS - instantWeb - standardWeb - instant const fs = require(\"fs\"); const { ARIO, ArweaveSigner, ARIOToken } = require(\"@ar.io/sdk\"); async function main() { const jwk = JSON.parse(fs.readFileSync(\"KeyFile.json\")); const ario = ARIO.init({ signer: new ArweaveSigner(jwk), }); const { id: txId } = await ario.decreaseDelegateStake( { target: 't4Xr0_J4Iurt7caNST02cMotaz2FIbWQ4Kbj616RHl3', qty: new ARIOToken(100).toMARIO(), }, { tags: [{ name: 'App-Name', value: 'My-Awesome-App' }], }, ); } main(); CopyCopied!Was this page helpful?YesNoComment",
          "estimatedWords": 135,
          "lastModified": "2025-06-27T16:13:20.892Z",
          "siteKey": "ario",
          "siteName": "AR-IO Network",
          "depth": 4,
          "crawledAt": "2025-06-27T16:13:20.892Z"
        },
        {
          "url": "https://docs.ar.io/ar-io-sdk/ario/gateways/instant-withdrawal",
          "title": "instantWithdrawal",
          "content": "instantWithdrawal instantWithdrawal is a method on the ARIO class that instantly withdraws funds from an existing vault on a gateway. If no gatewayAddress is provided, the signer's address will be used. instantWithdrawal requires authentication. Parameters ← Swipe to see more →ParameterTypeDescriptionRequiredgatewayAddressstringThe gateway address where the vault existsNovaultIdstringThe ID of the vault to withdraw fromYestagsarrayAn array of GQL tag objects to attach to the transactionNo← Swipe to see more → Examples instantWithdrawalNodeJSWeb const fs = require(\"fs\"); const { ARIO, ArweaveSigner } = require(\"@ar.io/sdk\"); async function main() { const jwk = JSON.parse(fs.readFileSync(\"KeyFile.json\")); const ario = ARIO.init({ signer: new ArweaveSigner(jwk), }); const { id: txId } = await ario.instantWithdrawal( { // gateway address where delegate vault exists gatewayAddress: 't4Xr0_J4Iurt7caNST02cMotaz2FIbWQ4Kbj616RHl3', // delegated vault id to cancel vaultId: 'fDrr0_J4Iurt7caNST02cMotaz2FIbWQ4Kcj616RHl3', }, // optional additional tags { tags: [{ name: 'App-Name', value: 'My-Awesome-App' }], }, ); // removes an operator vault from a gateway const { id: txId } = await ario.instantWithdrawal( { vaultId: 'fDrr0_J4Iurt7caNST02cMotaz2FIbWQ4Kcj616RHl3', }, ); } main(); CopyCopied!Was this page helpful?YesNoComment",
          "estimatedWords": 166,
          "lastModified": "2025-06-27T16:13:21.124Z",
          "siteKey": "ario",
          "siteName": "AR-IO Network",
          "depth": 4,
          "crawledAt": "2025-06-27T16:13:21.124Z"
        },
        {
          "url": "https://docs.ar.io/ar-io-sdk/ario/gateways/cancel-withdrawal",
          "title": "cancelWithdrawal",
          "content": "cancelWithdrawal cancelWithdrawal is a method on the ARIO class that cancels a pending withdrawal for a gateway, returning the stake back to the delegated amount. cancelWithdrawal requires authentication. Parameters ← Swipe to see more →ParameterTypeDescriptionOptionalgatewayAddressstring - WalletAddressThe wallet address of the gatewayfalsevaultIdstringThe ID of the vault containing the withdrawal to cancelfalsetagsarrayAn array of GQL tag objects to attach to the transfer AO messagetrue← Swipe to see more → Examples cancelWithdrawalNodeJSWebconst fs = require('fs'); const { ARIO, ArweaveSigner } = require('@ar.io/sdk'); async function main() { const jwk = JSON.parse(fs.readFileSync('KeyFile.json')); const ario = ARIO.init({ signer: new ArweaveSigner(jwk), }); const { id: txId } = await ario.cancelWithdrawal({ gatewayAddress: 't4Xr0_J4Iurt7caNST02cMotaz2FIbWQ4Kbj616RHl3', vaultId: 'vault_123' }); console.log(txId); } main(); CopyCopied!Was this page helpful?YesNoComment",
          "estimatedWords": 116,
          "lastModified": "2025-06-27T16:13:21.464Z",
          "siteKey": "ario",
          "siteName": "AR-IO Network",
          "depth": 4,
          "crawledAt": "2025-06-27T16:13:21.464Z"
        },
        {
          "url": "https://docs.ar.io/ar-io-sdk/ario/gateways/redelegate-stake",
          "title": "redelegateStake",
          "content": "redelegateStake redelegateStake is a method on the ARIO class that moves staked tokens from one gateway to another. A vault ID can be optionally included to redelegate from an existing withdrawal vault. The redelegation fee is calculated based on the fee rate and the stake amount. Users receive one free redelegation every seven epochs. Each additional redelegation increases the fee by 10%, up to a maximum of 60%. For example: If 1000 mARIO is redelegated with a 10% fee rate, the fee will be 100 mARIO. This results in 900 mARIO being redelegated to the new gateway and 100 mARIO being returned to the protocol balance. redelegateStake requires authentication. Parameters ← Swipe to see more →ParameterTypeDescriptionRequiredtargetstringDestination gateway address for the stakeYessourcestringCurrent gateway address of the stakeYesstakeQtynumberAmount in mARIO to redelegateYesvaultIdstringID of the vault to move stake fromNo← Swipe to see more → Examples redelegateStakeNodeJSWeb const fs = require(\"fs\"); const { ARIO, ArweaveSigner, ARIOToken } = require(\"@ar.io/sdk\"); async function main() { const jwk = JSON.parse(fs.readFileSync(\"KeyFile.json\")); const ario = ARIO.init({ signer: new ArweaveSigner(jwk), }); const { id: txId } = await ario.redelegateStake({ target: 't4Xr0_J4Iurt7caNST02cMotaz2FIbWQ4Kbj616RHl3', source: 'HwFceQaMQnOBgKDpnFqCqgwKwEU5LBme1oXRuQOWSRA', stakeQty: new ARIOToken(1000).toMARIO(), vaultId: 'fDrr0_J4Iurt7caNST02cMotaz2FIbWQ4Kcj616RHl3', }); } main(); CopyCopied!Was this page helpful?YesNoComment",
          "estimatedWords": 196,
          "lastModified": "2025-06-27T16:13:21.684Z",
          "siteKey": "ario",
          "siteName": "AR-IO Network",
          "depth": 4,
          "crawledAt": "2025-06-27T16:13:21.684Z"
        },
        {
          "url": "https://docs.ar.io/ar-io-sdk/ario/gateways/get-redelegation-fee",
          "title": "getRedelegationFee",
          "content": "getRedelegationFee getRedelegationFee is a method on the ARIO class that retrieves the redelegation fee rate as a percentage for a specific address. The fee rate ranges from 0% to 60% based on the number of redelegations since the last fee reset. getRedelegationFee does not require authentication. Parameters ← Swipe to see more →ParameterTypeDescriptionRequiredaddressstringThe wallet address to check for redelegation feesYes← Swipe to see more → Examples getRedelegationFeeNodeJSWebconst { ARIO } = require('@ar.io/sdk'); async function main() { const ario = ARIO.init(); const fee = await ario.getRedelegationFee({ address: 't4Xr0_J4Iurt7caNST02cMotaz2FIbWQ4Kbj616RHl3', }); console.log(fee); } main(); CopyCopied! Output { \"redelegationFeeRate\": 10, \"feeResetTimestamp\": 1730996691117 } CopyCopied!Was this page helpful?YesNoComment",
          "estimatedWords": 103,
          "lastModified": "2025-06-27T16:13:22.015Z",
          "siteKey": "ario",
          "siteName": "AR-IO Network",
          "depth": 4,
          "crawledAt": "2025-06-27T16:13:22.015Z"
        },
        {
          "url": "https://docs.ar.io/ar-io-sdk/ario/gateways/get-delegations",
          "title": "getDelegations",
          "content": "getDelegations getDelegations is a method on the ARIO class that retrieves all active and vaulted stakes across all gateways for a specific address. Results are paginated and sorted by the specified criteria. The cursor parameter represents the last delegationId (a combination of gateway address and delegation start timestamp) from the previous request. getDelegations does not require authentication. Parameters ← Swipe to see more →ParameterTypeDescriptionRequiredaddressstringThe wallet address to query for delegationsYescursorstringCursor for paginated resultsNolimitnumberMaximum number of results to return (max: 1000)NosortBystringProperty to sort results byNosortOrderstringSort direction (valid values: 'desc' or 'asc')No← Swipe to see more → Example getDelegationsNodeJSWebconst { ARIO } = require('@ar.io/sdk'); async function main() { const ario = ARIO.init(); const vaults = await ario.getDelegations({ address: 't4Xr0_J4Iurt7caNST02cMotaz2FIbWQ4Kbj616RHl3', cursor: 'QGWqtJdLLgm2ehFWiiPzMaoFLD50CnGuzZIPEdoDRGQ_123456789', limit: 2, sortBy: 'startTimestamp', sortOrder: 'asc', }); console.log(vaults); } main(); CopyCopied! Output { \"sortOrder\": \"asc\", \"hasMore\": true, \"totalItems\": 95, \"limit\": 2, \"sortBy\": \"startTimestamp\", \"items\": [ { \"type\": \"stake\", \"startTimestamp\": 1727815440632, \"gatewayAddress\": \"QGWqtJdLLgm2ehFWiiPzMaoFLD50CnGuzZIPEdoDRGQ\", \"delegationId\": \"QGWqtJdLLgm2ehFWiiPzMaoFLD50CnGuzZIPEdoDRGQ_1727815440632\", \"balance\": 1383212512 }, { \"type\": \"vault\", \"startTimestamp\": 1730996691117, \"gatewayAddress\": \"QGWqtJdLLgm2ehFWiiPzMaoFLD50CnGuzZIPEdoDRGQ\", \"delegationId\": \"QGWqtJdLLgm2ehFWiiPzMaoFLD50CnGuzZIPEdoDRGQ_1730996691117\", \"vaultId\": \"_sGDS7X1hyLCVpfe40GWioH9BSOb7f0XWbhHBa1q4-g\", \"balance\": 50000000, \"endTimestamp\": 1733588691117 } ], \"nextCursor\": \"QGWqtJdLLgm2ehFWiiPzMaoFLD50CnGuzZIPEdoDRGQ_1730996691117\" } CopyCopied!Was this page helpful?YesNoComment",
          "estimatedWords": 180,
          "lastModified": "2025-06-27T16:13:22.243Z",
          "siteKey": "ario",
          "siteName": "AR-IO Network",
          "depth": 4,
          "crawledAt": "2025-06-27T16:13:22.243Z"
        },
        {
          "url": "https://docs.ar.io/ar-io-sdk/ario/gateways/get-allowed-delegates",
          "title": "getAllowedDelegates",
          "content": "getAllowedDelegates getAllowedDelegates is a method on the ARIO class that retrieves all allowed delegates for a specific gateway address. The cursor parameter is used for pagination and represents the last address from the previous request. getAllowedDelegates does not require authentication. Parameters ← Swipe to see more →ParameterTypeDescriptionRequiredaddressstringThe gateway address to query for allowed delegatesYescursorstringCursor for paginated resultsNolimitnumberMaximum number of results to return (max: 1000)NosortBystringProperty to sort results byNosortOrderstringSort direction (valid values: 'desc' or 'asc')No← Swipe to see more → Example getAllowedDelegatesNodeJSWebconst { ARIO } = require('@ar.io/sdk'); async function main() { const ario = ARIO.init(); const allowedDelegates = await ario.getAllowedDelegates({ address: 'QGWqtJdLLgm2ehFWiiPzMaoFLD50CnGuzZIPEdoDRGQ', }); console.log(allowdDelegates); } main(); CopyCopied! Output { \"sortOrder\": \"desc\", \"hasMore\": false, \"totalItems\": 4, \"limit\": 100, \"items\": [ \"PZ5vIhHf8VY969TxBPQN-rYY9CNFP9ggNsMBqlWUzWM\", \"N4h8M9A9hasa3tF47qQyNvcKjm4APBKuFs7vqUVm-SI\", \"JcC4ZLUY76vmWha5y6RwKsFqYTrMZhbockl8iM9p5lQ\", \"31LPFYoow2G7j-eSSsrIh8OlNaARZ84-80J-8ba68d8\" ] } CopyCopied!Was this page helpful?YesNoComment",
          "estimatedWords": 128,
          "lastModified": "2025-06-27T16:13:22.552Z",
          "siteKey": "ario",
          "siteName": "AR-IO Network",
          "depth": 4,
          "crawledAt": "2025-06-27T16:13:22.552Z"
        },
        {
          "url": "https://docs.ar.io/ar-io-sdk/ario/gateways/get-gateway-delegates",
          "title": "getGatewayDelegates",
          "content": "getGatewayDelegates getGatewayDelegates is a method on the ARIO class that retrieves all delegates for a specific gateway. Results are paginated and sorted by the specified criteria. The cursor parameter represents the last delegate address from the previous request. getGatewayDelegates does not require authentication. Parameters ← Swipe to see more →ParameterTypeDescriptionRequiredaddressstringThe gateway address to query for delegatesYescursorstringCursor for paginated resultsNolimitnumberMaximum number of results to return (max: 1000)NosortBystringProperty to sort results byNosortOrderstringSort direction (valid values: 'desc' or 'asc')No← Swipe to see more → Example getGatewayDelegatesNodeJSWebconst { ARIO } = require('@ar.io/sdk'); async function main() { const ario = ARIO.init(); const delegates = await ario.getGatewayDelegates({ address: 'QGWqtJdLLgm2ehFWiiPzMaoFLD50CnGuzZIPEdoDRGQ', limit: 3, sortBy: 'startTimestamp', sortOrder: 'desc', }); console.log(delegates); } main(); CopyCopied! Output { \"nextCursor\": \"ScEtph9-vfY7lgqlUWwUwOmm99ySeZGQhOX0MFAyFEs\", \"limit\": 3, \"sortBy\": \"startTimestamp\", \"totalItems\": 32, \"sortOrder\": \"desc\", \"hasMore\": true, \"items\": [ { \"delegatedStake\": 600000000, \"address\": \"qD5VLaMYyIHlT6vH59TgYIs6g3EFlVjlPqljo6kqVxk\", \"startTimestamp\": 1732716956301 }, { \"delegatedStake\": 508999038, \"address\": \"KG8TlcWk-8pvroCjiLD2J5zkG9rqC6yYaBuZNqHEyY4\", \"startTimestamp\": 1731828123742 }, { \"delegatedStake\": 510926479, \"address\": \"ScEtph9-vfY7lgqlUWwUwOmm99ySeZGQhOX0MFAyFEs\", \"startTimestamp\": 1731689356040 } ] } CopyCopied!Was this page helpful?YesNoComment",
          "estimatedWords": 160,
          "lastModified": "2025-06-27T16:13:22.803Z",
          "siteKey": "ario",
          "siteName": "AR-IO Network",
          "depth": 4,
          "crawledAt": "2025-06-27T16:13:22.803Z"
        },
        {
          "url": "https://docs.ar.io/ar-io-sdk/ario/gateways/get-gateway-vaults",
          "title": "getGatewayVaults",
          "content": "getGatewayVaults getGatewayVaults is a method on the ARIO class that retrieves all vault information for a specific gateway, including delegated stakes and pending withdrawals. getGatewayVaults does not require authentication. Parameters ← Swipe to see more →ParameterTypeDescriptionOptionalgatewayAddressstring - WalletAddressThe wallet address of the gatewayfalse← Swipe to see more → Examples getGatewayVaultsNodeJSWebconst { ARIO } = require('@ar.io/sdk'); async function main() { const ario = ARIO.init(); const vaults = await ario.getGatewayVaults({ gatewayAddress: 't4Xr0_J4Iurt7caNST02cMotaz2FIbWQ4Kbj616RHl3' }); console.log(vaults); } main(); CopyCopied! Output { \"t4Xr0_J4Iurt7caNST02cMotaz2FIbWQ4Kbj616RHl3\": { \"balance\": 1000000000, \"start\": 1726243200000, \"end\": 1726329600000 }, \"anotherAddress\": { \"balance\": 500000000, \"start\": 1726243200000, \"end\": 1726329600000 } } CopyCopied!Was this page helpful?YesNoComment",
          "estimatedWords": 100,
          "lastModified": "2025-06-27T16:13:23.132Z",
          "siteKey": "ario",
          "siteName": "AR-IO Network",
          "depth": 4,
          "crawledAt": "2025-06-27T16:13:23.133Z"
        },
        {
          "url": "https://docs.ar.io/gateways/arns-resolution",
          "title": "ArNS Resolution",
          "content": "ArNS Resolution Overview One of the core functions of the AR.IO network gateway is to serve ArNS (Arweave Name System) records. Each ArNS name is assigned a specific \"time to live\" (TTL) value, which determines how often gateways should check for updates to the Arweave Transaction ID that the name points to. This TTL works similarly to a DNS TTL, which controls how often updates are checked for traditional websites. As a result, there may be a delay between when an ArNS record is updated and when users see the updated information in their browser. Effective with gateway Release 23, new features have been implemented on AR.IO gateways to optimize the resolution of ArNS records. These include an option for gateway operators to override the ArNS TTL, and set their own schedule for checking ArNS names for updates. Initial Caching When a gateway starts up, it will attempt to fetch the records of all ArNS names in order to create a local cache. Previously, this cache was stored in memory. After Release 23, this cache is saved to persistent storage so that the gateway's ArNS cache will survive restarting the gateway. This prevents delays in resolving ArNS names immediately after a gateway starts up. This cache is saved to the directory data/arns. Cache Refreshing When a new ArNS name is purchased on arns.app (or programmatically using the AR.IO SDK), gateways need to update their local cache to include this new name. Previously, gateway operators could not control how or when their gateway refreshed its cache. As a result, new names would often take several hours to resolve. With Release 23, once a new name is purchased and requested from a gateway, the gateway will check if it was already aware of the name's existence. If not, it will refresh its cache to include the records for the new name, allowing immediate resolution. Gateway operators can specify how often their gateway should refresh its cache when it fails to find an ArNS name that has been requested by setting the ARNS_NAME_LIST_CACHE_MISS_REFRESH_INTERVAL_SECONDS value in their .env file. The default value for this environmental variable is 10 seconds. Similarly, they can prompt their gateway to refresh their cache of ArNS names when a name is requested and successfully found in the local cache by setting the ARNS_NAME_LIST_CACHE_HIT_REFRESH_INTERVAL_SECONDS, which defaults to 1 hour. Both of these variables can be set to a number, which represents the number of seconds the gateway should wait before refreshing its cache when a name is requested that is, or is not, already in its local cache. .env# how long to wait before refreshing the base names cache on a miss (default is 10 seconds) ARNS_NAME_LIST_CACHE_MISS_REFRESH_INTERVAL_SECONDS=10 # how long to wait before refreshing the base names cache on a hit (default is 1 hour) ARNS_NAME_LIST_CACHE_HIT_REFRESH_INTERVAL_SECONDS=3600 CopyCopied! Gateway TTL Override Every ArNS record is set with a TTL specified by the name owner. Gateway operators can set the ARNS_RESOLVER_OVERRIDE_TTL_SECONDS variable in their .env file to override this TTL, and define for themselves how often the gateway should check for updated records. A shorter TTL value will result in more frequent outgoing requests to the ANT that controls the ArNS name, which can result in slower serving of the name data to users, while a longer TTL allows for faster serving of cached data, which may be out of date. TTL Override is disabled by default, and should be set to the number of seconds the gateway should use as its TTL when resolving names. If a gateway operator chooses to override the TTL set by ArNS owners, they must carefully weigh the trade-offs and decide on the balance between performance speed and record currency that best aligns with their priorities and use case. .env# override TTLs set on ANT records (defaults to disabled) ARNS_RESOLVER_OVERRIDE_TTL_SECONDS=60 CopyCopied! Note that the gateway TTL override does not override what is set in the cache headers for the name, it only overrides that TTL on the internal cache of the gateway (meaning the gateway will fetch is more frequently if it wants, but always respects the TTL when serving it)Was this page helpful?YesNoComment",
          "estimatedWords": 688,
          "lastModified": "2025-06-27T16:13:23.606Z",
          "siteKey": "ario",
          "siteName": "AR-IO Network",
          "depth": 2,
          "crawledAt": "2025-06-27T16:13:23.606Z"
        },
        {
          "url": "https://docs.ar.io/gateways/apex",
          "title": "Gateway Apex Domain Content Resolution",
          "content": "Gateway Apex Domain Content Resolution Overview Prior to gateway Release 28, the apex domain of a gateway would only display information about the Arweave network. Release 28 introduced two new environment variables that allow a gateway to serve custom content from the apex domain: APEX_TX_ID: Set to serve content from a specific transaction ID APEX_ARNS_NAME: Set to serve content from an ArNS name These variables enable gateway operators to customize their gateway's apex domain with useful information, details about the operator or associated projects, or any other content they wish to share. Quick Start If you want to serve your project's dApp from the apex domain of your gateway: Upload your dApp to Arweave Assign your dApp's transaction Id to an ArNS name Set the environment variable: APEX_ARNS_NAME=your-ArNS-name CopyCopied! Restart your gateway Your dApp will now be served from your gateway's apex domain Configuration Environment Variables You can configure your gateway to serve content from the apex domain by setting one of two environment variables: # Option 1: Serve content from a transaction ID APEX_TX_ID=your-transaction-id # Option 2: Serve content from an ArNS name APEX_ARNS_NAME=your-arns-name CopyCopied! IMPORTANTYou cannot set both variables simultaneously. Providing both variables will result in an error. Restart Requirements The gateway must be restarted after initially setting these environment variables If using APEX_ARNS_NAME, no restart is needed when the ArNS name points to a new transaction If using APEX_TX_ID, the gateway must be restarted when updating the transaction ID Use Cases Gateway operators can use this feature to: Display information about their gateway service Share details about the operator or organization Showcase associated projects and services Share educational content about Arweave and the permaweb Display any other content they wish to make available at their gateway's root domain Community Examples Several gateway operators have already implemented this feature to serve custom content from their apex domains: arnode.asia - Serves a custom landing page with information about their gateway service arlink.xyz - Serves the permaDapp for the Arlink project frostor.xyz / love4src.com - Serves information about the Memetic Block Software Guild and their projects vilenarios.com - Serves personalized portfolio/link tree information about the operator permagate.io - Serves personalized link tree information about the operator These examples demonstrate how gateway operators can leverage the apex domain feature to create a more personalized and informative experience for their users.Was this page helpful?YesNoComment",
          "estimatedWords": 392,
          "lastModified": "2025-06-27T16:13:23.876Z",
          "siteKey": "ario",
          "siteName": "AR-IO Network",
          "depth": 2,
          "crawledAt": "2025-06-27T16:13:23.876Z"
        },
        {
          "url": "https://docs.ar.io/gateways/admin",
          "title": "ARIO HTTP API Admin Endpoints",
          "content": "AR.IO HTTP API Admin Endpoints Overview The AR.IO HTTP API offers several endpoints that allow access to internal information and the ability to make adjustments without restarting your Gateway. Each of these endpoints behind /ar-io/admin/ have access restricted, so you will need to have set up your ADMIN_API_KEY variable and include \"Authorization: \"Bearer ${ADMIN_API_KEY}\" in the header of your request. When testing endpoints at <your-Gateway>/api-docs, you can enter your ADMIN_API_KEY using the green \"Authorize\" button near the top of the page, or by clicking any of the open lock icons next to a password protected end point. Debug The ar-io/admin/debug endpoint provides a comprehensive view of the current state of your Gateway. This endpoint has been designed to offer developers and administrators insights into the operational status of the gateway, including any errors or warnings that have occurred since the last startup. Example response{ db: { counts: { wallets: 137, tagNames: 61, tagValues: 892, stableTxs: 0, stableBlocks: 0, stableBlockTxs: 0, missingStableBlocks: 0, missingStableTxs: 0, missingTxs: 0, newBlocks: 32, newTxs: 4436, bundleCount: 159, bundleDataItems: 0, matcheDataItems: 0, dataItems: 0, nestedDataItems: null }, heights: { minStable: -1, maxStable: -1, minNew: 1000000, maxNew: 1000031 }, timestamps: { now: 1692230403, maxBundleQueuedAt: -1, maxBundleSkippedAt: 1692230390, maxBundleUnbundledAt: -1, maxBundleFullyIndexedAt: -1, maxNewDataItemIndexedAt: -1, maxStableDataItemIndexedAt: -1 }, errors: [], warnings: [] } } CopyCopied! Queue Transaction The ar-io/admin/queue-tx endpoint allows you to prioritize processing of a specific transaction, based on that transaction's ID. The id key must be set in the body of your request, and a POST request should be used. This endpoint will also enable you to prioritize opening and indexing bundles by providing the L1 TX ID for the bundle, but only if your Gateway is operating with the ANS104_UNBUNDLE_FILTER and ANS104_INDEX_FILTER keys set. Your Gateway will either respond with an error, or { message: 'TX queued' } Block Data The ar-io/admin/block-data endpoint allows you to tell your Gateway to refuse to serve certain data. In order to add to this block list, make a PUT request to this endpoint with the following in the body: { \"id\": \"<TX ID you want to block>\", \"notes\": \"Example notes\", \"source\": \"Example source\" } CopyCopied! id: This should be the transaction id of the content you want to block. notes: Notes regarding the reason this content was blocked. For documentation purposes only. source: Identifier for the source of TX IDs you are blocking. For example, the name of a public block list. For documentation purposes only. Your Gateway will either respond with an error, or { message: 'Content blocked' }Was this page helpful?YesNoComment",
          "estimatedWords": 425,
          "lastModified": "2025-06-27T16:13:24.357Z",
          "siteKey": "ario",
          "siteName": "AR-IO Network",
          "depth": 2,
          "crawledAt": "2025-06-27T16:13:24.357Z"
        },
        {
          "url": "https://docs.ar.io/gateways/advanced",
          "title": "Advanced Configuration",
          "content": "Advanced Configuration Overview The Getting Started guides for windows and linux contain all the information needed to start your AR.IO Gateway node successfully with basic configurations. There are also ever expanding advanced configuration options that allow you to run your node in a way that is customized to your specific use case. Most of the below options can be added to your .env file in order to customize its operation. Any changes made to your .env require you to stop the docker containers running your node, and restarting them with the --build flag in order for the changes to take effect. See ENV for a complete list of environmental variables you can set. Data Storage Location You can set a custom location for your AR.IO Gateway to save the data it pulls from the Arweave network. There are three primary types of data stored, and you can set a unique storage location for each of these independently. These are \"chunks data\", \"contiguous data\", and \"headers data\". The custom location for each of these can be set in your .env file like this: CHUNKS_DATA_PATH=<file path> CONTIGUOUS_DATA_PATH=<file path> HEADERS_DATA_PATH=<file path> CopyCopied! Be sure to replace \"<file path>\" with the path to the location where you would like the data stored. If these values are omitted, the data will be stored in the \"data\" directory inside your Gateway code repository. Admin API Key HTTP endpoints under \"/ar-io/admin\" are protected by an admin API key. These endpoints allow you to get certain analytics data or make adjustments to your node as it's running. When your node starts, it reads your environmental variables to see if a key is set. If not, a random key is generated. The key name is ADMIN_API_KEY and it should be set in your .env file like this: ADMIN_API_KEY=SUPER_SECRET_PASSWORD CopyCopied! View examples of the admin endpoints here Wallet Association In order to participate in the greater AR.IO network, Gateway nodes need to associate themselves with an Arweave wallet. This can be configured by setting the AR_IO_WALLET key value in your .env file. AR_IO_WALLET=1seRanklLU_1VTGowDZdD7s_-7k1qowT6oeFZHUZiZo CopyCopied! Unbundling AR.IO Gateway nodes support unbundling and indexing ANS-104 bundle data. This is disabled by default, but can be turned on with several different configuration options. You can set these configurations with the ANS104_UNBUNDLE_FILTER and ANS104_INDEX_FILTER keys in your .env: ANS104_UNBUNDLE_FILTER=\"<filter string>\" ANS104_INDEX_FILTER=\"<filter string>\" CopyCopied! The following types of filters are supported: { \"never\": true } # the default { \"always\": true } { \"attributes\": { \"owner\": <owner key>, ... }} { \"tags\": [{ \"name\": <utf8 tag name>, \"value\": <utf8 tag value> }, ...]} { \"and\": [ <nested filter>, ... ]} { \"or\": [ <nested filter>, ... ]} CopyCopied! Content Moderation You are able to set your Gateway to block specific transactions or data-items you don't want to serve. Unlike previous configuration options in this list, blocking content can be achieved without the need to add to your .env file and rebuild your Gateway. Instead, make a PUT request to your Gateway at /ar-io/admin/block-data. As this is an admin endpoint, you will need to have configured your ADMIN_API_KEY. Using curl as an example, the request should be formatted as follows: curl -X PUT -H \"Authorization: Bearer <ADMIN_KEY>\" \\ -H \"Content-Type: application/json\" \\ \"http://<HOST>:<PORT>/ar-io/admin/block-data\" \\ -d '{ \"id\": \"<ID>\", \"notes\": \"Example notes\", \"source\": \"Example source\" }' CopyCopied! id (string): This will be the transaction ID of the content you want to add to your block list. notes (string): Internal notes regarding why a particular ID is blocked. source (string): Identifier of a particular source of IDs to block. (e.g. the name of a block list) notes and source are used for documentation only, and have no effect on your block list itself. Contiguous Data Cleanup Transaction data on Arweave is stored in a chunked manner. It is commonly retrieved, however, in the the transaction data's original, contiguous form with all of its component chunks assembled end-to-end. Gateways cache contiguous representations of the transaction data to assist in various workloads, including serving transaction data to clients, allowing for efficient utilization of valuable system resources. Gateway operators will need to determine for themselves the best balance between disk space and other resource usage based on the size of their gateway and their particular use case. Contiguous data cache cleanup can be enabled using the CONTIGUOUS_DATA_CACHE_CLEANUP_THRESHOLD environmental variable. This variable sets the number of seconds from the creation of a file in the contiguous data cache after which that file will be deleted. For example: CONTIGUOUS_DATA_CACHE_CLEANUP_THRESHOLD=10000 CopyCopied! will clear items from the contiguous data cache after ten thousand (10,000) seconds. ArNS Resolver Gateways, by default, forward requests to resolve ArNS names to arweave.dev. Starting with Release 9 gateways can instead build and maintain their own local cache. Doing so removes external dependencies and allows faster resolution. View the code for the ArNS resolver service here: https://github.com/ar-io/arns-resolver NOTE: The ArNS resolver is still an experimental feature. It is possible it may behave in unexpected ways when presented with rare edge case scenarios. In order to enable the local ArNS resolver, three environmental variables will need to be set: RUN_RESOLVER=true TRUSTED_ARNS_RESOLVER_TYPE=resolver TRUSTED_ARNS_RESOLVER_URL=http://resolver:6000 CopyCopied! RUN_RESOLVER is a boolean representing an on/off switch for the local resolver. TRUSTED_ARNS_RESOLVER_TYPE sets the method the gateway uses for resolving ArNS names. Use resolver for the local resolver, or gateway for default functionality. TRUSTED_ARNS_RESOLVER_URL is the url a gateway will use to request ArNS name resolution. Was this page helpful?YesNoComment",
          "estimatedWords": 902,
          "lastModified": "2025-06-27T16:13:24.657Z",
          "siteKey": "ario",
          "siteName": "AR-IO Network",
          "depth": 2,
          "crawledAt": "2025-06-27T16:13:24.657Z"
        },
        {
          "url": "https://docs.ar.io/gateways/env",
          "title": "Environmental Variables",
          "content": "Environmental Variables Overview The AR.IO Gateway allows configuration customization through environmental variables. These variables dictate the gateway's behavior, from block synchronization settings to log formatting. Detailed below is a table enumerating all available environmental variables, their respective types, default values, and a brief description. Note that certain variables, such as SANDBOX_PROTOCOL, rely on others (e.g., ARNS_ROOT_HOST) to function effectively. Ensure proper understanding of these dependencies when configuring. Variables ← Swipe to see more →ENV NameTypeDefault ValueDescriptionGRAPHQL_HOSTStringarweave.netHost for GraphQL queries. You may use any available gateway that supports GQL queries. If omitted, your node can support GQL queries on locally indexed transactions, but only L1 transactions are indexed by default.GRAPHQL_PORTNumber443Port for GraphQL queries. Used in conjunction with GRAPHQL_HOST to set up the proxy for GQL queries.START_HEIGHTNumber or \"Infinity\"0Starting block height for node synchronization (0 = start from genesis block)STOP_HEIGHTNumber or \"Infinity\"\"Infinity\"Stop block height for node synchronization (Infinity = keep syncing until stopped)TRUSTED_NODE_URLString\"https://arweave.net\"Arweave node to use for fetching dataTRUSTED_GATEWAY_URLString\"https://arweave.net\"Arweave node to use for proxying reqeustsTRUSTED_GATEWAYS_URLSStringTRUSTED_GATEWAY_URLA JSON map of gateways and priorityTRUSTED_GATEWAYS_REQUEST_TIMEOUT_MSString\"10000\"Request timeout in milliseconds for trusted gatewaysTRUSTED_ARNS_GATEWAY_URLString\"https://NAME.arweave.dev\"ArNS gatewayWEIGHTED_PEERS_TEMPERATURE_DELTANumber0.1Any positive number above 0, best to keep 1 or less. Used to determine the sensitivity of which the probability of failing or succeeding peers decreases or increases.INSTANCE_IDString\"\"Adds an \"INSTANCE_ID\" field to output logsLOG_FORMATString\"simple\"Sets the format of output logs, accepts \"simple\" and \"json\"SKIP_CACHEBooleanfalseIf true, skips the local cache and always fetches headers from the nodePORTNumber4000AR.IO node exposed port numberSIMULATED_REQUEST_FAILURE_RATENumber0Number from 0 to 1, representing the probability of a request failingAR_IO_WALLETString\"\"Arweave wallet address used for staking and rewardsADMIN_API_KEYStringGeneratedAPI key used for admin API requests (if not set, it is generated and logged into the console)ADMIN_API_KEY_FILEStringGeneratedAlternative way to set the API key used for admin API requests via filepath, it takes precedence over ADMIN_API_KEY if definedBACKFILL_BUNDLE_RECORDSBooleanfalseIf true, AR.IO node will start indexing missing bundlesFILTER_CHANGE_REPROCESSBooleanfalseIf true, all indexed bundles will be reprocessed with the new filters (you can use this when you change the filters)ON_DEMAND_RETRIEVAL_ORDERStrings3,trusted-gateways,chunks,tx-dataData source retrieval order for on-demand data requestsBACKGROUND_RETRIEVAL_ORDERStringchunks,s3,trusted-gateways,chunks,tx-dataData source retrieval order for background data requests (i.e., unbundling)ANS104_UNBUNDLE_FILTERString{\"never\": true}Only bundles compliant with this filter will be unbundledANS104_INDEX_FILTERString{\"never\": true}Only bundles compliant with this filter will be indexedANS104_DOWNLOAD_WORKERSString5Sets the number of ANS-104 bundles to attempt to download in parallelANS104_UNBUNDLE_WORKERSNumber0, or 1 if filters are setSets the number of workers used to handle unbundlingDATA_ITEM_FLUSH_COUNT_THRESHOLDNumber1000Sets the number of new data items indexed before flushing to stable data itemsMAX_FLUSH_INTERVAL_SECONDSNumber600Sets the maximum time interval in seconds before flushing to stable data itemsWRITE_ANS104_DATA_ITEM_DB_SIGNATURESBooleanfalseIf true, the data item signatures will be written to the databaseWRITE_TRANSACTION_DB_SIGNATURESBooleantrueIf true, the transactions signatures will be written to the databaseENABLE_DATA_DB_WAL_CLEANUPBooleanfalseIf true, the data database WAL cleanup worker will be enabledENABLE_BACKGROUND_DATA_VERIFICATIONBooleanfalseIf true, unverified data will be verified in backgroundMAX_DATA_ITEM_QUEUE_SIZENumber100000Sets the maximum number of data items to queue for indexing before skipping indexing new data itemsARNS_ROOT_HOSTStringundefinedDomain name for ArNS hostSANDBOX_PROTOCOLStringundefinedProtocol setting in process of creating sandbox domains in ArNS (ARNS_ROOT_HOST needs to be set for this env to have any effect) accepts \"http\" or \"https\"START_WRITERSBooleantrueIf true, start indexing blocks, tx, ANS104 bundlesRUN_OBSERVERBooleantrueIf true, run observer (ARIO processes), requires WALLET env var to be setWALLETStringN/AWallet jwk file path for observer ario processLMDB_BLOCK_STORE_COMPRESSIONString\"gzip\"Accepts 'gzip', 'brotli', or 'none'. Compresses new blocks with specified algorithm before storing them in the local header store. Note: Changing this after blocks have been stored locally will require re-sync or remove local data to apply new compression setting to previously stored blocks.LMDB_BUNDLE_STORE_COMPRESSIONString\"gzip\"Accepts 'gzip', 'brotli', or 'none'. Compresses new bundles with specified algorithm before storing them in the local bundle store. Note: Changing this after bundles have been stored locally will require re-indexing to apply new compression setting to previously stored bundles.LMDB_DATA_ITEM_STORE_COMPRESSIONString\"gzip\"Accepts 'gzip', 'brotli', or 'none'. Compresses new data items with specified algorithm before storing them in the local data item store. Note: Changing this after data items have been stored locally will require re-indexing to apply new compression setting to previously stored data items.LMDB_TX_STORE_COMPRESSIONString\"gzip\"Accepts 'gzip', 'brotli', or 'none'. Compresses new transactions with specified algorithm before storing them in the local transaction store. Note: Changing this after transactions have been stored locally will require re-sync or remove local data to apply new compression setting to previously stored transactions.LMDB_DATA_STORE_COMPRESSIONString\"gzip\"Accepts 'gzip', 'brotli', or 'none'. Compresses new data with specified algorithm before storing them in the local data store. Note: Changing this after data has been stored locally will require re-sync or remove local data to apply new compression setting to previously stored data.CONTIGUOUS_DATA_CACHE_CLEANUP_THRESHOLDNumber1000Sets the number of contiguous data items to cache before cleaning upENABLE_FS_HEADER_CACHE_CLEANUPBooleantrueIf true, enable header cache cleanup for the fs cache (this will prune headers that are older than HEADER_CACHE_CLEANUP_THRESHOLD)HEADER_CACHE_CLEANUP_THRESHOLDNumber2000Sets the height threshold for which to clean up headersCHUNK_DATA_CACHE_CLEANUP_THRESHOLDNumber250000Sets the number of chunks to cache before cleaning upMANIFEST_CACHE_CLEANUP_THRESHOLDNumber250000Sets the number of data items to cache before cleaning up manifest cacheANS104_DATA_INDEX_CACHE_CLEANUP_THRESHOLDNumber50000Sets the number of data items to cache before cleaning up ANS-104 data index cacheREDIS_CACHE_URLStringundefinedRedis cache URL for external caching of data items, chunks, and tx headersREDIS_CACHE_TTL_SECONDSNumber3600TTL in seconds for Redis cache entriesAWS_S3_BUCKETStringundefinedAWS S3 bucket to save/retrieve block filesAWS_REGIONStringus-east-1AWS S3 bucket regionAWS_ENDPOINTString\"https://s3.amazonaws.com\"AWS S3 bucket endpointAWS_ACCESS_KEY_IDStringundefinedAWS S3 bucket access keyAWS_SECRET_ACCESS_KEYStringundefinedAWS S3 secret keyMIN_CONFIRMATIONSNumber10Minimum number of confirmations needed for a transaction to be returned by the /tx endpointINDEX_BLOCKSBooleantrueIf true, the gateway will index blocks as they're syncedINDEX_TXBooleantrueIf true, the gateway will index transactions as they're syncedINDEX_DATA_ITEMSBooleantrueIf true, the gateway will index data items as they're syncedINDEX_TX_OFFSET_LISTSBooleantrueIf true, the gateway will index the chunks of block data and transaction data offsetsENABLE_MEMPOOL_WATCHERBooleanfalseIf true, the gateway will watch the mempool for new transactions and save the txs headersENABLE_WEBHOOKSBooleanfalseIf true, allows the gateway to act as a client and execute webhooks when local state changesWEBHOOK_TARGET_SERVERSString\"\"Comma separated list of target webhook servers (URLs)WEBHOOK_INDEX_FILTERString{\"never\": true}Webhook events are emitted only if incoming transactions satisfy the specified filterWEBHOOK_BLOCK_FILTERString{\"never\": true}Block webhook events are emitted only if incoming block satisfies the specified filterPROMETHEUS_METRICS_ENABLEDBooleanfalseIf true, the gateway will expose Prometheus compatible metrics via the /metrics endpointNODE_ENVStringdevelopmentNode.js environment settingLOG_LEVELStringinfoLog verbosity level← Swipe to see more →Was this page helpful?YesNoComment",
          "estimatedWords": 975,
          "lastModified": "2025-06-27T16:13:25.193Z",
          "siteKey": "ario",
          "siteName": "AR-IO Network",
          "depth": 2,
          "crawledAt": "2025-06-27T16:13:25.193Z"
        }
      ],
      "lastCrawled": "2025-06-27T16:13:26.770Z",
      "stats": {
        "totalPages": 90,
        "averageWords": 808,
        "duration": 29593,
        "requestCount": 90,
        "averageResponseTime": 313.7111111111111,
        "pagesPerSecond": 3.041259757375055
      }
    }
  }
}