{"generated":"2025-10-20T12:15:30.422Z","sites":{"hyperbeam":{"name":"Hyperbeam","baseUrl":"https://hyperbeam.arweave.net","pages":[{"url":"https://hyperbeam.arweave.net/llms.html","title":"LLM Context Files","content":"LLM Context Files¶ This section provides access to specially formatted files intended for consumption by Large Language Models (LLMs) to provide context about the HyperBEAM documentation. LLM Summary (llms.txt) Content: Contains a brief summary of the HyperBEAM documentation structure and a list of relative file paths for all markdown documents included in the build. Usage: Useful for providing an LLM with a high-level overview and the available navigation routes within the documentation. LLM Full Content (llms-full.txt) Content: A single text file containing the complete, concatenated content of all markdown documents from the specified documentation directories (begin, run, guides, devices, resources). Each file's content is clearly demarcated. Usage: Ideal for feeding the entire documentation content into an LLM for comprehensive context, analysis, or question-answering based on the full documentation set. Generation Process These files are automatically generated by the docs/build-all.sh script during the documentation build process. They consolidate information from the following directories: docs/run docs/build Permaweb LLMs.txt¶ An interactive tool for selecting and curating Permaweb documentation into llms.txt format for feeding to LLMs. .llms-builder-dark { display: none; } .llms-builder-light { display: block; } [data-md-color-scheme=\"slate\"] .llms-builder-light { display: none; } [data-md-color-scheme=\"slate\"] .llms-builder-dark { display: block; } [data-md-color-scheme=\"default\"] .llms-builder-dark { display: none; } [data-md-color-scheme=\"default\"] .llms-builder-light { display: block; } October 3, 2025","estimatedWords":209,"lastModified":"2025-10-20T12:09:37.588Z","breadcrumbs":["llms"],"siteKey":"hyperbeam","siteName":"Hyperbeam","depth":1,"crawledAt":"2025-10-20T12:09:37.588Z"},{"url":"https://hyperbeam.arweave.net/build/reference/troubleshooting.html","title":"Developer Troubleshooting Guide","content":"Developer Troubleshooting Guide¶ This guide addresses common issues you might encounter when developing processes for HyperBEAM. Process Execution Fails¶ Symptoms: Errors when deploying or executing processes Solutions: Check both HyperBEAM and CU logs for specific error messages Verify that the WASM module is correctly compiled and valid Test with a simple example process to isolate the issue Adjust memory limits if the process requires more resources Memory Errors in Compute Unit¶ Symptoms: Out of memory errors or excessive memory usage during process execution Solutions: Adjust the PROCESS_WASM_MEMORY_MAX_LIMIT environment variable Enable garbage collection by setting an appropriate GC_INTERVAL_MS Monitor memory usage and adjust limits as needed If on a low-memory system, reduce concurrent process execution Getting Help¶ If you're still experiencing issues after trying these troubleshooting steps: Check the GitHub repository for known issues Join the Discord community for support Open an issue on GitHub with detailed information about your problem October 3, 2025","estimatedWords":153,"lastModified":"2025-10-20T12:09:38.215Z","breadcrumbs":["build","reference","troubleshooting"],"siteKey":"hyperbeam","siteName":"Hyperbeam","depth":3,"crawledAt":"2025-10-20T12:09:38.215Z"},{"url":"https://hyperbeam.arweave.net/build/reference/glossary.html","title":"Developer Glossary","content":"Developer Glossary¶ This glossary provides definitions for terms and concepts relevant to building on HyperBEAM. AO-Core Protocol¶ The underlying protocol that HyperBEAM implements, enabling decentralized computing and communication between nodes. AO-Core provides a framework into which any number of different computational models, encapsulated as primitive devices, can be attached. Asynchronous Message Passing¶ A communication paradigm where senders don't wait for receivers to be ready, allowing for non-blocking operations and better scalability. Compute Unit (CU)¶ The NodeJS component of HyperBEAM that executes WebAssembly modules and handles computational tasks. Device¶ A functional unit in HyperBEAM that provides specific capabilities to the system, such as storage, networking, or computational resources. Hashpaths¶ A mechanism for referencing locations in a program's state-space prior to execution. These state-space links are represented as Merklized lists of programs inputs and initial states. Message¶ A data structure used for communication between processes in the HyperBEAM system. Messages can be interpreted as a binary term or as a collection of named functions (a Map of functions). Module¶ A unit of code that can be loaded and executed by the Compute Unit, typically in WebAssembly format. Process¶ An independent unit of computation in HyperBEAM with its own state and execution context. Process ID¶ A unique identifier assigned to a process within the HyperBEAM system. WebAssembly (WASM)¶ A binary instruction format that serves as a portable compilation target for programming languages, enabling deployment on the web and other environments. Permaweb Glossary¶ For a more comprehensive glossary of terms used in the permaweb, try the Permaweb Glossary. Or use it below: .dark-mode-iframe-container { display: none; } .light-mode-iframe-container { display: block; } [data-md-color-scheme=\"slate\"] .light-mode-iframe-container { display: none; } [data-md-color-scheme=\"slate\"] .dark-mode-iframe-container { display: block; } [data-md-color-scheme=\"default\"] .dark-mode-iframe-container { display: none; } [data-md-color-scheme=\"default\"] .light-mode-iframe-container { display: block; } October 3, 2025","estimatedWords":295,"lastModified":"2025-10-20T12:09:39.015Z","breadcrumbs":["build","reference","glossary"],"siteKey":"hyperbeam","siteName":"Hyperbeam","depth":3,"crawledAt":"2025-10-20T12:09:39.015Z"},{"url":"https://hyperbeam.arweave.net/build/reference/faq.html","title":"Developer FAQ","content":"Developer FAQ¶ This page answers common questions about building applications and processes on HyperBEAM. What can I build with HyperBEAM?¶ You can build a wide range of applications, including: Decentralized applications (dApps) Distributed computation systems Peer-to-peer services Resilient microservices IoT device networks Decentralized storage solutions What is the current focus or phase of HyperBEAM development?¶ The initial development phase focuses on integrating AO processes more deeply with HyperBEAM. A key part of this is phasing out the reliance on traditional \"dryrun\" simulations for reading process state. Instead, processes are encouraged to use the ~patch@1.0 device to expose specific parts of their state directly via GET requests. This allows for more efficient and direct state access, particularly for web interfaces and external integrations. You can learn more about this mechanism in the Exposing Process State with the Patch Device guide. What is the difference between HyperBEAM and Compute Unit?¶ HyperBEAM: The Erlang-based node software that handles message routing, process management, and device coordination. Compute Unit (CU): A NodeJS implementation that executes WebAssembly modules and handles computational tasks. Together, these components form a complete execution environment for AO processes. What programming languages can I use with HyperBEAM?¶ You can use any programming language that compiles to WebAssembly (WASM) for creating modules that run on the Compute Unit. This includes languages like: Lua Rust C/C++ And many others with WebAssembly support How do I debug processes running in HyperBEAM?¶ Debugging processes in HyperBEAM can be done through: Logging messages to the system log (DEBUG=HB_PRINT rebar3 shell) Monitoring process state and message flow Inspecting memory usage and performance metrics Where can I get help if I encounter issues?¶ If you encounter issues: Check the Troubleshooting guide Search or ask questions on GitHub Issues Join the community on Discord October 3, 2025","estimatedWords":297,"lastModified":"2025-10-20T12:09:39.649Z","breadcrumbs":["build","reference","faq"],"siteKey":"hyperbeam","siteName":"Hyperbeam","depth":3,"crawledAt":"2025-10-20T12:09:39.649Z"},{"url":"https://hyperbeam.arweave.net/build/devices/building-devices.html","title":"Extending HyperBEAM with Devices","content":"Extending HyperBEAM with Devices¶ We encourage you to extend HyperBEAM with devices for functionality that is general purpose and reusable across different applications. What are Devices?¶ As explained in the introduction, devices are the core functional units within HyperBEAM. They are self-contained modules that process messages and perform specific actions, forming the building blocks of your application's logic. HyperBEAM comes with a set of powerful built-in devices that handle everything from process management (~process@1.0) and message scheduling (~scheduler@1.0) to executing WebAssembly (~wasm64@1.0) and Lua scripts (~lua@5.3a). Creating Your Own Devices (Coming Soon)¶ We will create more in depth guides for building devices in Lua and Erlang in the future. Further Reading¶ In the meantime, community-contributed guides are available that can walk you through the process. For example: Rust: Building Rust Devices with HyperBEAM M3 Beta: mini-Roam API (Vol. 1) - A tutorial from Decent Land Labs that covers how to build a custom Rust device from scratch. October 3, 2025","estimatedWords":160,"lastModified":"2025-10-20T12:09:40.259Z","breadcrumbs":["build","devices","building devices"],"siteKey":"hyperbeam","siteName":"Hyperbeam","depth":3,"crawledAt":"2025-10-20T12:09:40.259Z"},{"url":"https://hyperbeam.arweave.net/build/devices/application-features/auth-ecosystem-at-1-0.html","title":"Device Authentication Ecosystem (auth-hook10 secret10 cookie10 http-auth10)","content":"Device: Authentication Ecosystem (~auth-hook@1.0, ~secret@1.0, ~cookie@1.0, ~http-auth@1.0)¶ Overview¶ The Authentication Ecosystem provides wallet-less blockchain applications through server-side key management. The ecosystem consists of ~auth-hook@1.0 (authentication interceptor), ~secret@1.0 (wallet storage), ~cookie@1.0 (session management), and ~http-auth@1.0 (HTTP Basic authentication). Core Concept: Zero-Friction Authentication¶ Users interact with decentralized applications like traditional web apps while HyperBEAM handles cryptographic operations server-side. The wallet- patterns ~http-auth@1.0¶ Provides HTTP Basic authentication with PBKDF2 key derivation (1,200,000 iterations, SHA256). generate: Processes Authorization headers and derives signing keys Flow: Returns 401 with WWW-Authenticate when credentials missing, otherwise derives key and signs Performance PBKDF2 performs ~5-10 derivations/second for brute-force protection. Device Integration¶ The authentication workflow: HTTP request with &! → auth-hook intercepts → provider generates secret → secret device signs → authenticated response returned. Integration with Core Devices: ~process@1.0: Automatic signing of process communications ~query@1.0: Authenticated data discovery ~copycat@1.0: Authenticated data replication ~meta@1.0: Authentication provider configuration Multi-Provider Configuration: \"on\": { \"request\": [ {\"device\": \"auth-hook@1.0\", \"secret-provider\": {\"device\": \"cookie@1.0\"}}, {\"device\": \"auth-hook@1.0\", \"secret-provider\": {\"device\": \"http-auth@1.0\"}} ] } Security Considerations¶ Security Layers: Provider authentication (Cookie/HTTP) → Access control messages → Controller verification → Request signing (RSA-PSS/HMAC) Best Practices: HTTPS-only, secure cookie attributes, strong PBKDF2 parameters, session key rotation, audit logging Trust Model Intended for deployment in Trusted Execution Environments (TEE) with ~snp@1.0 or trusted nodes. Private keys never leave server memory. All operations create cryptographically auditable signatures. Configuration Examples¶ Cookie Authentication: \"on\": {\"request\": {\"device\": \"auth-hook@1.0\", \"secret-provider\": {\"device\": \"cookie@1.0\"}}} HTTP Basic Authentication: \"secret-provider\": {\"device\": \"http-auth@1.0\", \"realm\": \"Protected\", \"iterations\": 2000000} Multi-Signature: \"secret-provider\": { \"device\": \"cookie@1.0\", \"controllers\": [\"admin1\", \"admin2\"], \"required-controllers\": 2 } Practical Implications¶ For Application Developers: This ecosystem eliminates blockchain onboarding friction. Users access your dApp without installing wallets or managing keys—authentication happens transparently via cookies or HTTP Basic Auth. Your application simply adds &! to requests that need signing. For Node Operators: You control the trust model. Deploy in TEE for trustless environments or trusted nodes for enterprise scenarios. Cookie-based auth suits consumer applications, while HTTP Basic Auth serves API integrations and enterprise SSO systems. Multi-signature configurations enable shared custody models. For End Users: Single-click access to blockchain applications with traditional web experience. No seed phrases to manage, no browser extensions to install. Sessions persist across devices when using non-volatile storage mode. Authentication state syncs via cookies or HTTP headers. See Also¶ ~process@1.0 - Process communication signing ~query@1.0 - Authenticated data queries ~snp@1.0 - Trusted execution environment October 3, 2025","estimatedWords":395,"lastModified":"2025-10-20T12:09:40.874Z","breadcrumbs":["build","devices","application features","auth ecosystem at 1 0"],"siteKey":"hyperbeam","siteName":"Hyperbeam","depth":4,"crawledAt":"2025-10-20T12:09:40.874Z"},{"url":"https://hyperbeam.arweave.net/build/devices/foundational/meta-at-1-0.html","title":"Device meta10","content":"Device: ~meta@1.0¶ Overview¶ The ~meta@1.0 device provides access to metadata and configuration information about the local HyperBEAM node and the broader AO network. This device is essential for: Core Functions (Keys)¶ info¶ Retrieves or modifies the node's configuration message (often referred to as NodeMsg internally). GET /~meta@1.0/info Action: Returns the current node configuration message. Response: A message map containing the node's settings. Sensitive keys (like private wallets) are filtered out. Dynamically generated keys like the node's public address are added if a wallet is configured. POST /~meta@1.0/info Action: Updates the node's configuration message. Requires the request to be signed by the node's configured operator key/address. Request Body: A message map containing the configuration keys and values to update. Response: Confirmation message indicating success or failure. Note: Once a node's configuration is marked as initialized = permanent, it cannot be changed via this method. Key Configuration Parameters Managed by ~meta¶ While the info key is the primary interaction point, the NodeMsg managed by ~meta holds crucial configuration parameters affecting the entire node's behavior, including (but not limited to): port: HTTP server port. priv_wallet / key_location: Path to the node's Arweave key file. operator: The address designated as the node operator (defaults to the address derived from priv_wallet). initialized: Status indicating if the node setup is temporary or permanent. preprocessor / postprocessor: Optional messages defining pre/post-processing logic for requests. routes: Routing table used by dev_router. store: Configuration for data storage. trace: Debug tracing options. p4_*: Payment configuration. faff_*: Access control lists. (Refer to hb_opts.erl for a comprehensive list of options.) Utility Functions (Internal/Module Level)¶ The dev_meta.erl module also contains helper functions used internally or callable from other Erlang modules: is_operator(, ) -> boolean(): Checks if the signer of RequestMsg matches the configured operator in NodeMsg. Pre/Post-Processing Hooks¶ The ~meta device applies the node's configured preprocessor message before resolving the main request and the postprocessor message after obtaining the result, allowing for global interception and modification of requests/responses. Initialization¶ Before a node can process general requests, it usually needs to be initialized. Attempts to access devices other than ~meta@1.0/info before initialization typically result in an error. Initialization often involves setting essential parameters like the operator key via a POST to info. meta module October 3, 2025","estimatedWords":373,"lastModified":"2025-10-20T12:09:41.477Z","breadcrumbs":["build","devices","foundational","meta at 1 0"],"siteKey":"hyperbeam","siteName":"Hyperbeam","depth":4,"crawledAt":"2025-10-20T12:09:41.477Z"},{"url":"https://hyperbeam.arweave.net/build/devices/hyperbeam-devices.html","title":"HyperBEAM Devices","content":"HyperBEAM Devices¶ In AO-Core and its implementation HyperBEAM, Devices are modular components responsible for processing and interpreting Messages. They define the specific logic for how computations are performed, data is handled, or interactions occur within the AO ecosystem. Think of Devices as specialized engines or services that can be plugged into the AO framework. This modularity is key to AO's flexibility and extensibility. Purpose of Devices¶ Define Computation: Devices dictate how a message's instructions are executed. One device might run WASM code, another might manage process state, and yet another might simply relay data. Enable Specialization: Nodes running HyperBEAM can choose which Devices to support, allowing them to specialize in certain tasks (e.g., high-compute tasks, storage-focused tasks, secure TEE operations). Promote Modularity: New functionalities can be added to AO by creating new Devices, without altering the core protocol. Distribute Workload: Different Devices can handle different parts of a complex task, enabling parallel processing and efficient resource utilization across the network. Device Naming and Versioning¶ Devices are typically referenced using a name and version, like ~@ (e.g., ~process@1.0). The tilde (~) often indicates a primary, user-facing device, while internal or utility devices might use a dev_ prefix in the source code (e.g., dev_router). Versioning indicates the specific interface and behavior of the device. Changes to a device that break backward compatibility usually result in a version increment. Familiar Examples¶ HyperBEAM includes many preloaded devices that provide core functionality. Some key examples include: ~meta@1.0: Configures the node itself (hardware specs, supported devices, payment info). ~process@1.0: Manages persistent, shared computational states (like traditional smart contracts, but more flexible). ~scheduler@1.0: Handles the ordering and execution of messages within a process. ~wasm64@1.0: Executes WebAssembly (WASM) code, allowing for complex computations written in languages like Rust, C++, etc. ~lua@5.3a: Executes Lua scripts. ~relay@1.0: Forwards messages between AO nodes or to external HTTP endpoints. ~json@1.0: Provides access to JSON data structures. ~message@1.0: Manages message state and processing. Authentication Ecosystem: Comprehensive wallet-less authentication (~auth-hook@1.0, ~secret@1.0, ~cookie@1.0, ~http-auth@1.0). Data Discovery Engine: Advanced message search and query capabilities (~query@1.0). Data Replication Engine: External data ingestion and synchronization (~copycat@1.0). ~patch@1.0: Applies state updates directly to a process, often used for migrating or managing process data. Beyond the Basics¶ Devices aren't limited to just computation or state management. They can represent more abstract concepts: Security & Authentication Devices (~snp@1.0, dev_codec_httpsig, Authentication Ecosystem): Handle tasks related to Trusted Execution Environments (TEEs), message signing, wallet-less authentication, and session management, adding layers of security and verification. Data Management Devices (Data Discovery Engine, Data Replication Engine): Provide comprehensive data ingestion, search, and discovery capabilities for building data-rich applications with external source integration. Payment/Access Control Devices (~p4@1.0, ~faff@1.0): Manage metering, billing, or access control for node services. Workflow/Utility Devices (dev_cron, dev_stack, dev_monitor): Coordinate complex execution flows, schedule tasks, or monitor process activity. Using Devices¶ Devices are typically invoked via GET requests. The path specifies which Device should interpret the subsequent parts of the path or the request body. # Example: Execute the 'now' key on the process device for a specific process /~process@1.0/now # Example: Relay a GET request via the relay device /~relay@1.0/call?method=GET&path=https:","estimatedWords":517,"lastModified":"2025-10-20T12:09:42.094Z","breadcrumbs":["build","devices","hyperbeam devices"],"siteKey":"hyperbeam","siteName":"Hyperbeam","depth":3,"crawledAt":"2025-10-20T12:09:42.095Z"},{"url":"https://hyperbeam.arweave.net/build/getting-started/pathing-in-hyperbeam.html","title":"Pathing in HyperBEAM","content":"Pathing in HyperBEAM¶ Overview¶ Understanding how to construct and interpret paths in AO-Core is fundamental to working with HyperBEAM. This guide explains the structure and components of AO-Core paths, enabling you to effectively interact with processes and access their data. HyperBEAM Path Structure¶ Let's examine a typical HyperBEAM endpoint piece-by-piece: https:","estimatedWords":51,"lastModified":"2025-10-20T12:09:42.682Z","breadcrumbs":["build","getting started","pathing in hyperbeam"],"siteKey":"hyperbeam","siteName":"Hyperbeam","depth":3,"crawledAt":"2025-10-20T12:09:42.682Z"},{"url":"https://hyperbeam.arweave.net/build/getting-started/building-on-ao.html","title":"Building on HyperBEAM with ao","content":"Building on HyperBEAM with ao¶ The guides for building applications on HyperBEAM and interacting with ao processes have been moved to the AO Processes Cookbook to provide a centralized resource for developers. Here are some helpful resources from the AO Processes Cookbook: Get Started with ao to learn the basics of ao and how to start building processes. Migration Guide for moving processes from legacynet and using new HyperBEAM features. Using aos with HyperBEAM for using the aos command-line tool with HyperBEAM. Using aoconnect with HyperBEAM for using the aoconnect library to interact with processes on HyperBEAM. October 3, 2025","estimatedWords":100,"lastModified":"2025-10-20T12:09:43.319Z","breadcrumbs":["build","getting started","building on ao"],"siteKey":"hyperbeam","siteName":"Hyperbeam","depth":3,"crawledAt":"2025-10-20T12:09:43.319Z"},{"url":"https://hyperbeam.arweave.net/build/getting-started/hyperbeam-capabilities.html","title":"HyperBEAM Your Decentralized Development Toolkit","content":"HyperBEAM: Your Decentralized Development Toolkit¶ HyperBEAM is a versatile, multi-purpose tool that serves as the primary gateway to the AO Computer. It's not a single-purpose application, but rather a powerful, extensible engine—a \"Swiss Army knife\"—for developers building in the decentralized ecosystem. Designed to be modular, composable, and extensible, HyperBEAM lets you build anything from simple data transformations to complex, high-performance decentralized applications. Thinking in HyperBEAM¶ While AO-Core establishes the foundational concepts of Messages, Devices, and Paths, building on HyperBEAM can be simplified to four key principles: Everything is a message. You can compute on any message by calling its keys by name. The device specified in the message determines how these keys are resolved. The default device, message@1.0, resolves keys to their literal values within the message. Paths are pipelines of messages. A path defines a sequence of 'request' messages to be executed. You can set a key in a message directly within the path using the &key=value syntax. Headers and parameters added after a ? are applied to all messages in the pipeline. Device-specific requests with ~x@y. The ~x@y syntax allows you to apply a request as if the base message had a different device. This provides a powerful way to execute messages using specific compute or storage logic defined by a device. Signed responses over HTTP. The final message in a pipeline is returned as an HTTP response. This response is signed against the hashpath that generated it, ensuring the integrity and verifiability of the computation. Ready to build an AO process? The serverless compute capability is a powerful application of HyperBEAM's modular design. To learn how to create and manage AO processes with WASM or Lua, please refer to the AO Processes Cookbook. Modularity: A System of Devices¶ At its core, HyperBEAM is a modular system built on Devices. Each device is a specialized module responsible for a specific task. This modular architecture means you can think of HyperBEAM's functionality as a set of building blocks. Use Case: Imagine you need to create a serverless API that takes a number, runs a calculation, and returns a result. You would use the ~wasm64@1.0 or ~lua@5.3a devices to execute your calculation logic without needing to manage a server. If your API needs to return JSON, you can pipe the output to the ~json@1.0 device to ensure it's formatted correctly. Composability: Chaining Logic with URL Paths¶ HyperBEAM's modular devices become even more powerful when combined. Its pathing routing mechanism leverages standard URLs to create powerful, composable pipelines. By constructing a URL, you can define a \"path\" of messages that are executed in sequence, with the output of one message becoming the input for the next. Use Case: Suppose you have a token process and want to calculate the total circulating supply without making the client download and compute all balances. You can construct a single URL that: Reads the latest state of the AO process. Pipes the state to a Lua script and calls the sum function, which sums the balances from the state. Formats the final result as a JSON object. The request would look something like this: /{process-id}~process@1.0/now/~lua@5.3a&module={module-id}/sum/serialize~json@1.0 This path chains together the operations, returning just the computed supply in a single, efficient request. Find the full example in the AO Process Cookbook Learn more about Pathing in HyperBEAM. Extensibility: Building Beyond the Core¶ HyperBEAM is not a closed system. It is designed to be extended, allowing developers to add new functionality tailored to their specific needs. Build Custom Devices¶ You can build and deploy your own devices in Erlang to introduce entirely new, high-level functionality to the network. Use Case: You could build a custom device that acts as a bridge to another blockchain's API, allowing your AO processes to interact with external systems seamlessly. Learn how to Build Your Own Device. Achieve Raw Performance with Native Code¶ For the most demanding, performance-critical tasks, you can write Native Implemented Functions (NIFs) in low-level languages like C or Rust. These NIFs integrate directly with the Erlang VM, offering the highest possible performance. Use Case: If you were building a sophisticated cryptographic application, you could implement a new, high-speed hashing algorithm as a NIF to ensure maximum performance and security. This \"raw\" extensibility provides an escape hatch for ultimate control. October 3, 2025","estimatedWords":714,"lastModified":"2025-10-20T12:09:43.907Z","breadcrumbs":["build","getting started","hyperbeam capabilities"],"siteKey":"hyperbeam","siteName":"Hyperbeam","depth":3,"crawledAt":"2025-10-20T12:09:43.907Z"},{"url":"https://hyperbeam.arweave.net/build/introduction/what-is-ao-core.html","title":"What is AO-Core","content":"What is AO-Core?¶ Your browser does not support the video tag. AO-Core is a protocol and standard for distributed computation that forms the foundation of the AO Computer. Inspired by and built upon concepts from the Erlang language, AO-Core embraces the actor model for concurrent, distributed systems. It defines a minimal, generalized model for decentralized computation built around standard web technologies like HTTP. Think of it as a way to interpret the Arweave permaweb not just as static storage, but as a dynamic, programmable, and infinitely scalable computing environment. Unlike traditional blockchain systems, AO-Core defines a flexible, powerful computation protocol that enables a wide range of applications beyond just running Lua programs. Core Concepts¶ AO-Core revolves around three fundamental components: Messages Modular Data Packets Messages are cryptographically linked, forming a verifiable computation graph. Devices Extensible Execution Engines AO-Core introduces a modular architecture centered around Devices. These are pluggable components—typically implemented as modules—that define specific computational logic, such as executing WASM, managing state, or relaying data. Devices interpret and process messages, allowing for flexible and extensible computation. This design enables developers to extend the system by creating custom Devices to fit their specific needs, making the network highly adaptable and composable. Paths Composable Pipelines Paths in AO-Core are structures that link messages over time, creating a verifiable history of computations. They allow users to navigate the computation graph and access specific states or results. AO-Core leverages HashPaths—cryptographic fingerprints representing the sequence of operations leading to a specific message state—ensuring traceability and integrity. This pathing mechanism enables developers to compose complex, verifiable data pipelines and interact with processes and data in a flexible, trustless manner. Key Features¶ AO-Core is inherently resilient, running across a global network of machines that eliminates any single point of failure. Its computations are permanent, immutably stored on Arweave so they can be recalled—or continued—at any time. The protocol remains permissionless, meaning anyone can participate. And it is trustless, with every state mathematically verifiable so no central authority is required. The Actor Model in AO¶ Inspired by Erlang, AO-Core implements the actor model to provide a foundation for inherently concurrent, distributed, and scalable systems. In this model, computation is performed by independent actors (or processes). These actors communicate exclusively by passing messages to one another, and each can make local decisions, send more messages, and create new actors. Beyond Processes¶ While AO Processes (smart contracts built using the AO-Core protocol) are a powerful application, AO-Core itself enables a much broader range of computational patterns: Serverless functions with trustless guarantees Hybrid applications combining smart contracts and serverless functionality Custom execution environments through new devices Composable systems using the path language October 3, 2025","estimatedWords":444,"lastModified":"2025-10-20T12:09:44.534Z","breadcrumbs":["build","introduction","what is ao core"],"siteKey":"hyperbeam","siteName":"Hyperbeam","depth":3,"crawledAt":"2025-10-20T12:09:44.534Z"},{"url":"https://hyperbeam.arweave.net/run/running-a-hyperbeam-node.html","title":"Running a HyperBEAM Node","content":"Running a HyperBEAM Node¶ This guide provides the basics for running your own HyperBEAM node, installing dependencies, and connecting to the AO network. System Dependencies¶ To successfully build and run a HyperBEAM node, your system needs several software dependencies installed. macOSLinux (Debian/Ubuntu)Windows (WSL) Install core dependencies using Homebrew: brew install cmake git pkg-config openssl ncurses Install core dependencies using apt: sudo apt-get update && sudo apt-get install -y --no-install-recommends \\ build-essential \\ cmake \\ git \\ pkg-config \\ ncurses-dev \\ libssl-dev \\ sudo \\ curl \\ ca-certificates Using the Windows Subsystem for Linux (WSL) with a distribution like Ubuntu is recommended. Follow the Linux (Debian/Ubuntu) instructions within your WSL environment. Erlang/OTP¶ HyperBEAM is built on Erlang/OTP. You need version OTP 27 installed (check the rebar.config or project documentation for specific version requirements, typically OTP 27). Installation methods: macOS (brew)Linux (apt) brew install erlang@27 sudo apt install erlang=1:27.* Source Build Download from erlang.org and follow the build instructions for your platform. Rebar3¶ Rebar3 is the build tool for Erlang projects. Installation methods: macOS (brew)Linux / macOS (Direct Download) brew install rebar3 Get the rebar3 binary from the official website. Place the downloaded rebar3 file in your system's PATH (e.g., /usr/local/bin) and make it executable (chmod +x rebar3). Node.js¶ Node.js might be required for certain JavaScript-related tools or dependencies. Node version 22+ is required. Installation methods: macOS (brew)Linux (apt)asdf (Recommended) brew install node # Check your distribution's recommended method, might need nodesource repo sudo apt install nodejs npm asdf-vm with the asdf-nodejs plugin is recommended. asdf plugin add nodejs https:","estimatedWords":258,"lastModified":"2025-10-20T12:09:45.220Z","breadcrumbs":["run","running a hyperbeam node"],"siteKey":"hyperbeam","siteName":"Hyperbeam","depth":2,"crawledAt":"2025-10-20T12:09:45.221Z"},{"url":"https://hyperbeam.arweave.net/","title":"Hyperbeam","content":"Hyperbeam A Decentralized Operating System. Built on AO. Scroll For More. Build Build a new generation of apps and services for cyberspace. Fully trustless. Onchain. Boot Power your onchain compute and get paid. What is hyperBEAM? Hyperbeam. Powering the decentralized supercomputer: AO. Access, build, and lease hardware for applications and services at any scale. Your gateway to AO, a decentralized supercomputer network built on top of Arweave. AO and Arweave power a cyberspace which guarantees the rights of users, outside of the control of any individual or group. Hyper Parallel Ditch shared memory, embrace autonomous state. Execute concurrent processes without compromise. Async Communicate via asynchronous message passing for unheard of throughput. Distributed Get resilient compute in your terminal with one command. Learn More About Hyperbeam What Do I Do With Hyperbeam? 01 Monetize Your Hardware. Access a shared economy for hardware in the new cyberspace. All while earning $AO Offer compute to AO processes and their users, earning fees in return. Run your own gateway. Empower builders to launch trust-minimized, serverless WASM functions using built-in TEE integrations. Coming Soon: Offer support for GPUs. Run Your Node Sorry, your browser doesn’t support embedded video. 02 Apps for a cyberspace that guarantees user rights. Build freely. Deploy Once. Use it permanently. Access everywhere. Create a Permaweb App Sorry, your browser doesn't support embedded video. 03 Modular, scalable devices for the new frontier. Create for the infrastructure that unlocks access to 7,000,000+ smart contracts. Create on Hyperbeam Sorry, your browser doesn't support embedded video. Made with in San Francisco lol jk in cyberspace. Hungry to eat glass all day, Join Us.","estimatedWords":268,"lastModified":"2025-10-20T12:09:45.760Z","breadcrumbs":[],"siteKey":"hyperbeam","siteName":"Hyperbeam","depth":0,"crawledAt":"2025-10-20T12:09:45.760Z"},{"url":"https://hyperbeam.arweave.net/build/introduction/what-is-hyperbeam.html","title":"What is HyperBEAM","content":"What is HyperBEAM?¶ HyperBEAM is the primary, production-ready implementation of the AO-Core protocol, built on the robust Erlang/OTP framework. It serves as a decentralized operating system, powering the AO Computer—a scalable, trust-minimized, distributed supercomputer built on permanent storage of Arweave. Implementing AO-Core¶ HyperBEAM transforms the abstract concepts of AO-Core—Messages, Devices, and Paths—into a concrete, operational system. It provides the runtime environment and essential services to execute these computations across a network of distributed nodes. Messages Modular Data Packets In HyperBEAM, every interaction within the AO Computer is handled as a message. A message is a binary item or a map of functions. These cryptographically-linked data units are the foundation for communication, allowing processes to trigger computations, query state, and transfer value. HyperBEAM nodes are responsible for routing and processing these messages according to the rules of the AO-Core protocol. Devices Extensible Execution Engines HyperBEAM introduces a uniquely modular architecture centered around Devices. These pluggable components are Erlang modules that define specific computational logic—like running WASM, managing state, or relaying data—allowing for unprecedented flexibility. This design allows developers to extend the system by creating custom Devices to fit their specific computational needs. Paths Composable Pipelines HyperBEAM exposes a powerful HTTP API that uses structured URL patterns to interact with processes and data. This pathing mechanism allows developers to create verifiable data pipelines, composing functionality from multiple devices into a single, atomic request. The URL bar effectively becomes a command-line interface for AO's trustless compute environment. A Robust and Scalable Foundation¶ Built on the Erlang/OTP framework, HyperBEAM provides a robust and secure foundation that leverages the BEAM virtual machine for exceptional concurrency, fault tolerance, and scalability. This abstracts away underlying hardware, allowing diverse nodes to contribute resources without compatibility issues. The system governs how nodes coordinate and interact, forming a decentralized network that is resilient and permissionless. October 3, 2025","estimatedWords":308,"lastModified":"2025-10-20T12:09:45.893Z","breadcrumbs":["build","introduction","what is hyperbeam"],"siteKey":"hyperbeam","siteName":"Hyperbeam","depth":3,"crawledAt":"2025-10-20T12:09:45.893Z"},{"url":"https://hyperbeam.arweave.net/run/configuring-your-machine.html","title":"Configuring Your HyperBEAM Node","content":"Configuring Your HyperBEAM Node¶ This guide details the various ways to configure your HyperBEAM node's behavior, including ports, storage, keys, and logging. Configuration (config.flat)¶ The primary way to configure your HyperBEAM node is through a config.flat file located in the node's working directory or specified by the HB_CONFIG_LOCATION environment variable. This file uses a simple Key = Value. format (note the period at the end of each line). Example config.flat: % Set the HTTP port port = 8080. % Specify the Arweave key file priv_key_location = \"/path/to/your/wallet.json\". % Set the data store directory % Note: Storage configuration can be complex. See below. % store = [{local, [{root, >}]}]. % Example of complex config, not for config.flat % Enable verbose logging for specific modules % debug_print = [hb_http, dev_router]. % Example of complex config, not for config.flat Below is a reference of commonly used configuration keys. Remember that config.flat only supports simple key-value pairs (Atoms, Strings, Integers, Booleans). For complex configurations (Lists, Maps), you must use environment variables or hb:start_mainnet/1. Core Configuration¶ These options control fundamental HyperBEAM behavior. Option Type Default Description port Integer 8734 HTTP API port hb_config_location String \"config.flat\" Path to configuration file priv_key_location String \"hyperbeam-key.json\" Path to operator wallet key file mode Atom debug Execution mode (debug, prod) Server & Network Configuration¶ These options control networking behavior and HTTP settings. Option Type Default Description host String \"localhost\" Choice of remote node for non-local tasks gateway String \"https:","estimatedWords":239,"lastModified":"2025-10-20T12:09:46.483Z","breadcrumbs":["run","configuring your machine"],"siteKey":"hyperbeam","siteName":"Hyperbeam","depth":2,"crawledAt":"2025-10-20T12:09:46.483Z"},{"url":"https://hyperbeam.arweave.net/run/tee-nodes.html","title":"Trusted Execution Environment (TEE)","content":"Trusted Execution Environment (TEE)¶ Recommended Setup Use HyperBEAM OS for the easiest TEE deployment with pre-configured AMD SEV-SNP support. Note: HB-OS is typically used for TEE operations, but is not necessary for router registration. Overview¶ HyperBEAM supports Trusted Execution Environments (TEEs) through the ~snp@1.0 device, enabling secure, verifiable computation on remote machines. TEEs provide hardware-level isolation and cryptographic attestation that allows users to verify their code is running in a protected environment exactly as intended, even on untrusted hardware. The ~snp@1.0 device generates and validates attestation reports that prove: Code is running inside a genuine AMD SEV-SNP TEE The execution environment hasn't been tampered with Specific software components (firmware, kernel, initramfs) match trusted hashes Debug mode is disabled for security Configuration Files¶ Configuration can be set in either config.json (JSON) or config.flat (flat) format. For full details and examples of both formats, see Configuration Reference. The examples below use JSON for clarity. When to use HB-OS¶ Operation Use HB-OS? Purpose TEE Node (SNP) Recommended Secure, attested computation (hardware isolation) Router Registration Optional Registering/joining a router (TEE not required) If you are registering or running a router, you can do so without HB-OS. If you want to run a TEE node, HB-OS or an equivalent TEE setup is recommended for convenience and security. Quick Start: TEE Node with HyperBEAM OS¶ Prerequisites¶ AMD EPYC processor with SEV-SNP support (Milan generation or newer) Host system with SEV-SNP enabled in BIOS Setup TEE Node¶ # Clone and build TEE-enabled HyperBEAM # (Only needed for TEE nodes if you choose HB-OS) git clone https:","estimatedWords":259,"lastModified":"2025-10-20T12:09:47.226Z","breadcrumbs":["run","tee nodes"],"siteKey":"hyperbeam","siteName":"Hyperbeam","depth":2,"crawledAt":"2025-10-20T12:09:47.226Z"},{"url":"https://hyperbeam.arweave.net/run/joining-running-a-router.html","title":"Router Networks Joining vs Running","content":"Router Networks: Joining vs Running¶ Router networks in HyperBEAM have two distinct roles that are often confused: Two Different Concepts Joining a router = Registering your worker node with an existing router to receive work Running a router = Operating a router that manages and distributes work to other nodes When to use HB-OS¶ Operation Use HB-OS? Purpose TEE Node (SNP) Recommended Secure, attested computation (hardware isolation) Router Registration Optional Registering/joining a router (TEE not required) You can join or run a router without HB-OS. If you want to run a TEE node, HB-OS or an equivalent TEE setup is recommended for convenience and security. Configuration Files: config.json vs config.flat¶ Configuration can be set in either config.json (JSON syntax) or config.flat (flat syntax). The examples below use JSON for clarity, but you can use either format depending on your deployment. The syntax differs: config.json uses standard JSON structure (see examples below) config.flat uses key-value pairs Joining a Router Network (Worker Node)¶ Most users want to join an existing router to offer computational services. This does NOT require HB-OS or TEE unless you specifically want TEE security. Step-by-Step: Registering as a Worker Node¶ 1. Prepare Your Configuration (config.json example)¶ Use the following configuration as a template for your worker node: {","estimatedWords":210,"lastModified":"2025-10-20T12:09:47.865Z","breadcrumbs":["run","joining running a router"],"siteKey":"hyperbeam","siteName":"Hyperbeam","depth":2,"crawledAt":"2025-10-20T12:09:47.865Z"},{"url":"https://hyperbeam.arweave.net/run/reference/faq.html","title":"Node Operator FAQ","content":"Node Operator FAQ¶ This page answers common questions about running and maintaining a HyperBEAM node. What is HyperBEAM?¶ HyperBEAM is a client implementation of the AO-Core protocol written in Erlang. It serves as the node software for a decentralized operating system that allows operators to offer computational resources to users in the AO network. What are the system requirements for running HyperBEAM?¶ Currently, HyperBEAM is primarily tested and documented for Ubuntu 22.04 and macOS. Other platforms will be added in future updates. For detailed requirements, see the System Requirements page. Can I run HyperBEAM in a container?¶ While technically possible, running HyperBEAM in Docker containers or other containerization technologies is currently not recommended. The containerization approach may introduce additional complexity and potential performance issues. We recommend running HyperBEAM directly on the host system until container support is more thoroughly tested and optimized. How do I update HyperBEAM to the latest version?¶ To update HyperBEAM: Pull the latest code from the repository (check Discord for the branch of Beta releases) Rebuild the application Restart the HyperBEAM service Specific update instructions will vary depending on your installation method. Can I run multiple HyperBEAM nodes on a single machine?¶ Yes, you can run multiple HyperBEAM nodes on a single machine, but you'll need to configure them to use different ports and data directories to avoid conflicts. However, this is not recommended for production environments as each node should ideally have a unique IP address to properly participate in the network. Running multiple nodes on a single machine is primarily useful for development and testing purposes. Is there a limit to how many processes can run on a node?¶ The practical limit depends on your hardware resources. Erlang is designed to handle millions of lightweight processes efficiently, but the actual number will be determined by: Available memory CPU capacity Network bandwidth Storage speed The complexity of your processes Where can I get help if I encounter issues?¶ If you encounter issues: Check the Troubleshooting guide Search or ask questions on GitHub Issues Join the community on Discord October 3, 2025","estimatedWords":346,"lastModified":"2025-10-20T12:09:48.635Z","breadcrumbs":["run","reference","faq"],"siteKey":"hyperbeam","siteName":"Hyperbeam","depth":2,"crawledAt":"2025-10-20T12:09:48.635Z"},{"url":"https://hyperbeam.arweave.net/run/reference/glossary.html","title":"Node Operator Glossary","content":"Node Operator Glossary¶ This glossary provides definitions for terms and concepts relevant to running a HyperBEAM node. AO-Core Protocol¶ The underlying protocol that HyperBEAM implements, enabling decentralized computing and communication between nodes. Checkpoint¶ A saved state of a process that can be used to resume execution from a known point, used for persistence and recovery. Compute Unit (CU)¶ The NodeJS component of HyperBEAM that executes WebAssembly modules. While developers interact with it more, operators should know it's a key part of the stack. Erlang¶ The programming language used to implement the HyperBEAM core, known for its robustness and support for building distributed, fault-tolerant applications. ~flat@1.0¶ A format used for encoding settings files in HyperBEAM configuration, using HTTP header styling. HyperBEAM¶ The Erlang-based node software that handles message routing, process management, and device coordination in the HyperBEAM ecosystem. Node¶ An instance of HyperBEAM running on a physical or virtual machine that participates in the distributed network. ~meta@1.0¶ A device used to configure the node's hardware, supported devices, metering and payments information, amongst other configuration options. ~p4@1.0¶ A device that runs as a pre-processor and post-processor in HyperBEAM, enabling a framework for node operators to sell usage of their machine's hardware to execute AO-Core devices. ~simple-pay@1.0¶ A simple, flexible pricing device that can be used in conjunction with p4@1.0 to offer flat-fees for the execution of AO-Core messages. ~snp@1.0¶ A device used to generate and validate proofs that a node is executing inside a Trusted Execution Environment (TEE). Trusted Execution Environment (TEE)¶ A secure area inside a processor that ensures the confidentiality and integrity of code and data loaded within it. Used in HyperBEAM for trust-minimized computation. Permaweb Glossary¶ For a more comprehensive glossary of terms used in the permaweb, try the Permaweb Glossary. Or use it below: .dark-mode-iframe-container { display: none; } .light-mode-iframe-container { display: block; } [data-md-color-scheme=\"slate\"] .light-mode-iframe-container { display: none; } [data-md-color-scheme=\"slate\"] .dark-mode-iframe-container { display: block; } [data-md-color-scheme=\"default\"] .dark-mode-iframe-container { display: none; } [data-md-color-scheme=\"default\"] .light-mode-iframe-container { display: block; } October 3, 2025","estimatedWords":333,"lastModified":"2025-10-20T12:09:49.245Z","breadcrumbs":["run","reference","glossary"],"siteKey":"hyperbeam","siteName":"Hyperbeam","depth":2,"crawledAt":"2025-10-20T12:09:49.245Z"},{"url":"https://hyperbeam.arweave.net/run/reference/troubleshooting.html","title":"Node Operator Troubleshooting Guide","content":"Node Operator Troubleshooting Guide¶ This guide addresses common issues you might encounter when installing and running a HyperBEAM node. Installation Issues¶ Erlang Installation Fails¶ Symptoms: Errors during Erlang compilation or installation Solutions: Ensure all required dependencies are installed: sudo apt-get install -y libssl-dev ncurses-dev make cmake gcc g++ Try configuring with fewer options: ./configure --without-wx --without-debugger --without-observer --without-et Check disk space, as compilation requires several GB of free space Rebar3 Bootstrap Fails¶ Symptoms: Errors when running ./bootstrap for Rebar3 Solutions: Verify Erlang is correctly installed: erl -eval 'erlang:display(erlang:system_info(otp_release)), halt().' Ensure you have the latest version of the repository: git fetch && git reset --hard origin/master Try manually downloading a precompiled Rebar3 binary HyperBEAM Issues¶ HyperBEAM Won't Start¶ Symptoms: Errors when running rebar3 shell or the HyperBEAM startup command Solutions: Check for port conflicts: Another service might be using the configured port Verify the wallet key file exists and is accessible Examine Erlang crash dumps for detailed error information Ensure all required dependencies are installed HyperBEAM Crashes During Operation¶ Symptoms: Unexpected termination of the HyperBEAM process Solutions: Check system resources (memory, disk space) Examine Erlang crash dumps for details Reduce memory limits if the system is resource-constrained Check for network connectivity issues if connecting to external services Compute Unit Issues¶ Compute Unit Won't Start¶ Symptoms: Errors when running npm start in the CU directory Solutions: Verify Node.js is installed correctly: node -v Ensure all dependencies are installed: npm i Check that the wallet file exists and is correctly formatted Verify the .env file has all required settings Integration Issues¶ HyperBEAM Can't Connect to Compute Unit¶ Symptoms: Connection errors in HyperBEAM logs when trying to reach the CU Solutions: Verify the CU is running: curl http:","estimatedWords":284,"lastModified":"2025-10-20T12:09:49.885Z","breadcrumbs":["run","reference","troubleshooting"],"siteKey":"hyperbeam","siteName":"Hyperbeam","depth":2,"crawledAt":"2025-10-20T12:09:49.885Z"},{"url":"https://hyperbeam.arweave.net/build/devices/source-code/dev_patch.html","title":"Module dev_patcherl","content":"Module dev_patch.erl¶ A device that can be used to reorganize a message: Moving data from one path inside it to another. Description¶ This device's function runs in two modes: When using all to move all data at the path given in from to the path given in to. When using patches to move all submessages in the source to the target, if they have a method key of PATCH or a device key of patch@1.0. Source and destination paths may be prepended by base: or req: keys to indicate that they are relative to either of the message's that the computation is being performed on. The search order for finding the source and destination keys is as follows, where X is either from or to: The patch-X key of the execution message. The X key of the execution message. The patch-X key of the request message. The X key of the request message. Additionally, this device implements the standard computation device keys, allowing it to be used as an element of an execution stack pipeline, etc. Function Index¶ all/3Get the value found at the patch-from key of the message, or the from key if the former is not present.all_mode_test/0*compute/3init/3Necessary hooks for compliance with the execution-device standard.move/4*Unified executor for the all and patches modes.normalize/3patch_to_submessage_test/0*patches/3Find relevant PATCH messages in the given source key of the execution and request messages, and apply them to the given destination key of the request.req_prefix_test/0*snapshot/3uninitialized_patch_test/0* Function Details¶ all/3¶ all(Msg1, Msg2, Opts) -> any() Get the value found at the patch-from key of the message, or the from key if the former is not present. Remove it from the message and set the new source to the value found. all_mode_test/0 *¶ all_mode_test() -> any() compute/3¶ compute(Msg1, Msg2, Opts) -> any() init/3¶ init(Msg1, Msg2, Opts) -> any() Necessary hooks for compliance with the execution-device standard. move/4 *¶ move(Mode, Msg1, Msg2, Opts) -> any() Unified executor for the all and patches modes. normalize/3¶ normalize(Msg1, Msg2, Opts) -> any() patch_to_submessage_test/0 *¶ patch_to_submessage_test() -> any() patches/3¶ patches(Msg1, Msg2, Opts) -> any() Find relevant PATCH messages in the given source key of the execution and request messages, and apply them to the given destination key of the request. req_prefix_test/0 *¶ req_prefix_test() -> any() snapshot/3¶ snapshot(Msg1, Msg2, Opts) -> any() uninitialized_patch_test/0 *¶ uninitialized_patch_test() -> any() October 3, 2025","estimatedWords":384,"lastModified":"2025-10-20T12:09:50.456Z","breadcrumbs":["build","devices","source code","dev patch"],"siteKey":"hyperbeam","siteName":"Hyperbeam","depth":4,"crawledAt":"2025-10-20T12:09:50.456Z"},{"url":"https://hyperbeam.arweave.net/build/devices/application-features/data-replication-at-1-0.html","title":"Device copycat10","content":"Device: ~copycat@1.0¶ Overview¶ The ~copycat@1.0 device replicates data from external sources into HyperBEAM node caches. It fetches messages from GraphQL endpoints and Arweave nodes, enabling offline-first applications through local data caching. Core Concept: Data Ingestion¶ The ~copycat@1.0 device acts as a data ingestion orchestrator that fetches messages from external sources and imports them into the local node's cache system. It supports multiple engines (GraphQL, Arweave) and handles pagination automatically during large-scale replication operations. Key Functions (Keys)¶ POST /~copycat@1.0/graphql Action: Queries remote GraphQL endpoints and indexes results locally. Parameters: query: GraphQL query string or structured specification variables: Query variables for parameterized queries operationName: Specific operation in multi-operation queries node: Target GraphQL endpoint URL Response: Total number of successfully indexed messages with batch statistics. Processing: Automatic pagination handling, message parsing, and cache integration. POST /~copycat@1.0/arweave Action: Connects directly to Arweave nodes and imports transaction/block data. Parameters: node: Target Arweave node URL from: Starting block height to: Ending block height filter: Transaction filtering criteria Response: Replication status with imported message count and range coverage. Integration: Uses ~arweave@2.9-pre device for native Arweave communication. Data Sources¶ GraphQL Endpoints: Arweave Gateway APIs (arweave.net, ar.io gateways), custom GraphQL services, and federated endpoints with multi-endpoint coordination. Arweave Nodes: Direct node integration for block-level replication with height-based ranges and transaction indexing. Filter Examples¶ Tag-Based Filtering: { \"tag\": \"Content-Type\", \"value\": \"application/json\" } Owner-Based Filtering: { \"owner\": \"wallet-address-here\" } Multi-Tag Filtering: { \"tags\": { \"App-Name\": \"MyApplication\", \"Content-Type\": \"application/json\", \"Version\": \"1.0\" } } Comprehensive Replication: { \"all\": true, \"node\": \"https:","estimatedWords":248,"lastModified":"2025-10-20T12:09:51.354Z","breadcrumbs":["build","devices","application features","data replication at 1 0"],"siteKey":"hyperbeam","siteName":"Hyperbeam","depth":4,"crawledAt":"2025-10-20T12:09:51.354Z"},{"url":"https://hyperbeam.arweave.net/build/devices/source-code/dev_snp.html","title":"Module dev_snperl","content":"Module dev_snp.erl¶ This device offers an interface for validating AMD SEV-SNP commitments, as well as generating them, if called in an appropriate environment. Function Index¶ execute_is_trusted/3*Ensure that all of the software hashes are trusted.generate/3Generate an commitment report and emit it as a message, including all of the necessary data to generate the nonce (ephemeral node address + node message ID), as well as the expected measurement (firmware, kernel, and VMSAs hashes).generate_nonce/2*Generate the nonce to use in the commitment report.is_debug/1*Ensure that the node's debug policy is disabled.real_node_test/0*report_data_matches/3*Ensure that the report data matches the expected report data.trusted/3Validates if a given message parameter matches a trusted value from the SNP trusted list Returns {ok, true} if the message is trusted, {ok, false} otherwise.verify/3Verify an commitment report message; validating the identity of a remote node, its ephemeral private address, and the integrity of the report. Function Details¶ execute_is_trusted/3 *¶ execute_is_trusted(M1, Msg, NodeOpts) -> any() Ensure that all of the software hashes are trusted. The caller may set a specific device to use for the is-trusted key. The device must then implement the trusted resolver. generate/3¶ generate(M1, M2, Opts) -> any() Generate an commitment report and emit it as a message, including all of the necessary data to generate the nonce (ephemeral node address + node message ID), as well as the expected measurement (firmware, kernel, and VMSAs hashes). generate_nonce/2 *¶ generate_nonce(RawAddress, RawNodeMsgID) -> any() Generate the nonce to use in the commitment report. is_debug/1 *¶ is_debug(Report) -> any() Ensure that the node's debug policy is disabled. real_node_test/0 *¶ real_node_test() -> any() report_data_matches/3 *¶ report_data_matches(Address, NodeMsgID, ReportData) -> any() Ensure that the report data matches the expected report data. trusted/3¶ trusted(Msg1, Msg2, NodeOpts) -> any() Validates if a given message parameter matches a trusted value from the SNP trusted list Returns {ok, true} if the message is trusted, {ok, false} otherwise verify/3¶ verify(M1, M2, NodeOpts) -> any() Verify an commitment report message; validating the identity of a remote node, its ephemeral private address, and the integrity of the report. The checks that must be performed to validate the report are: 1. Verify the address and the node message ID are the same as the ones used to generate the nonce. 2. Verify the address that signed the message is the same as the one used to generate the nonce. 3. Verify that the debug flag is disabled. 4. Verify that the firmware, kernel, and OS (VMSAs) hashes, part of the measurement, are trusted. 5. Verify the measurement is valid. 6. Verify the report's certificate chain to hardware root of trust. October 3, 2025","estimatedWords":428,"lastModified":"2025-10-20T12:09:56.346Z","breadcrumbs":["build","devices","source code","dev snp"],"siteKey":"hyperbeam","siteName":"Hyperbeam","depth":4,"crawledAt":"2025-10-20T12:09:56.346Z"},{"url":"https://hyperbeam.arweave.net/build/devices/source-code/dev_p4.html","title":"Module dev_p4erl","content":"Module dev_p4.erl¶ The HyperBEAM core payment ledger. Description¶ This module allows the operator to specify another device that can act as a pricing mechanism for transactions on the node, as well as orchestrating a payment ledger to calculate whether the node should fulfil services for users. The device requires the following node message settings in order to function: p4_pricing-device: The device that will estimate the cost of a request. p4_ledger-device: The device that will act as a payment ledger. The pricing device should implement the following keys: GET /estimate?type=pre|post&body=[...]&request=RequestMessageGET /price?type=pre|post&body=[...]&request=RequestMessage The body key is used to pass either the request or response messages to the device. The type key is used to specify whether the inquiry is for a request (pre) or a response (post) object. Requests carry lists of messages that will be executed, while responses carry the results of the execution. The price key may return infinity if the node will not serve a user under any circumstances. Else, the value returned by the price key will be passed to the ledger device as the amount key. A ledger device should implement the following keys: POST /credit?message=PaymentMessage&request=RequestMessagePOST /charge?amount=PriceMessage&request=RequestMessageGET /balance?request=RequestMessage The type key is optional and defaults to pre. If type is set to post, the charge must be applied to the ledger, whereas the pre type is used to check whether the charge would succeed before execution. Function Index¶ balance/3Get the balance of a user in the ledger.faff_test/0*Simple test of p4's capabilities with the faff@1.0 device.hyper_token_ledger/0*hyper_token_ledger_test_/0*Ensure that Lua scripts can be used as pricing and ledger devices.is_chargable_req/2*The node operator may elect to make certain routes non-chargable, using the routes syntax also used to declare routes in router@1.0.non_chargable_route_test/0*Test that a non-chargable route is not charged for.request/3Estimate the cost of a transaction and decide whether to proceed with a request.response/3Postprocess the request after it has been fulfilled.test_opts/1*test_opts/2*test_opts/3* Function Details¶ balance/3¶ balance(X1, Req, NodeMsg) -> any() Get the balance of a user in the ledger. faff_test/0 *¶ faff_test() -> any() Simple test of p4's capabilities with the faff@1.0 device. hyper_token_ledger/0 *¶ hyper_token_ledger() -> any() hyper_token_ledger_test_/0 *¶ hyper_token_ledger_test_() -> any() Ensure that Lua scripts can be used as pricing and ledger devices. Our scripts come in two components: 1. A process script which is executed as a persistent local-process on the node, and which maintains the state of the ledger. This process runs hyper-token.lua as its base, then adds the logic of hyper-token-p4.lua to it. This secondary script implements the charge function that p4@1.0 will call to charge a user's account. 2. A client script, which is executed as a p4@1.0 ledger device, which uses ~push@1.0 to send requests to the ledger process. is_chargable_req/2 *¶ is_chargable_req(Req, NodeMsg) -> any() The node operator may elect to make certain routes non-chargable, using the routes syntax also used to declare routes in router@1.0. non_chargable_route_test/0 *¶ non_chargable_route_test() -> any() Test that a non-chargable route is not charged for. request/3¶ request(State, Raw, NodeMsg) -> any() Estimate the cost of a transaction and decide whether to proceed with a request. The default behavior if pricing-device or p4_balances are not set is to proceed, so it is important that a user initialize them. response/3¶ response(State, RawResponse, NodeMsg) -> any() Postprocess the request after it has been fulfilled. test_opts/1 *¶ test_opts(Opts) -> any() test_opts/2 *¶ test_opts(Opts, PricingDev) -> any() test_opts/3 *¶ test_opts(Opts, PricingDev, LedgerDev) -> any() October 3, 2025","estimatedWords":559,"lastModified":"2025-10-20T12:09:57.607Z","breadcrumbs":["build","devices","source code","dev p4"],"siteKey":"hyperbeam","siteName":"Hyperbeam","depth":4,"crawledAt":"2025-10-20T12:09:57.607Z"},{"url":"https://hyperbeam.arweave.net/build/devices/source-code/dev_faff.html","title":"Hyperbeam","content":"Hyperbeam A Decentralized Operating System. Built on AO. Scroll For More. Build Build a new generation of apps and services for cyberspace. Fully trustless. Onchain. Boot Power your onchain compute and get paid. What is hyperBEAM? Hyperbeam. Powering the decentralized supercomputer: AO. Access, build, and lease hardware for applications and services at any scale. Your gateway to AO, a decentralized supercomputer network built on top of Arweave. AO and Arweave power a cyberspace which guarantees the rights of users, outside of the control of any individual or group. Hyper Parallel Ditch shared memory, embrace autonomous state. Execute concurrent processes without compromise. Async Communicate via asynchronous message passing for unheard of throughput. Distributed Get resilient compute in your terminal with one command. Learn More About Hyperbeam What Do I Do With Hyperbeam? 01 Monetize Your Hardware. Access a shared economy for hardware in the new cyberspace. All while earning $AO Offer compute to AO processes and their users, earning fees in return. Run your own gateway. Empower builders to launch trust-minimized, serverless WASM functions using built-in TEE integrations. Coming Soon: Offer support for GPUs. Run Your Node Sorry, your browser doesn’t support embedded video. 02 Apps for a cyberspace that guarantees user rights. Build freely. Deploy Once. Use it permanently. Access everywhere. Create a Permaweb App Sorry, your browser doesn't support embedded video. 03 Modular, scalable devices for the new frontier. Create for the infrastructure that unlocks access to 7,000,000+ smart contracts. Create on Hyperbeam Sorry, your browser doesn't support embedded video. Made with in San Francisco lol jk in cyberspace. Hungry to eat glass all day, Join Us.","estimatedWords":268,"lastModified":"2025-10-20T12:09:57.737Z","breadcrumbs":["build","devices","source code","dev faff"],"siteKey":"hyperbeam","siteName":"Hyperbeam","depth":4,"crawledAt":"2025-10-20T12:09:57.737Z"},{"url":"https://hyperbeam.arweave.net/build/devices/source-code/dev_cron.html","title":"Module dev_cronerl","content":"Module dev_cron.erl¶ A device that inserts new messages into the schedule to allow processes to passively 'call' themselves without user interaction. Function Index¶ every/3Exported function for scheduling a recurring message.every_worker_loop/4*every_worker_loop_test/0*This test verifies that a recurring task can be scheduled and executed.info/1Exported function for getting device info.info/3once/3Exported function for scheduling a one-time message.once_executed_test/0*This test verifies that a one-time task can be scheduled and executed.once_worker/3*Internal function for scheduling a one-time message.parse_time/1*Parse a time string into milliseconds.stop/3Exported function for stopping a scheduled task.stop_every_test/0*This test verifies that a recurring task can be stopped by calling the stop function with the task ID.stop_once_test/0*test_worker/0*This is a helper function that is used to test the cron device.test_worker/1* Function Details¶ every/3¶ every(Msg1, Msg2, Opts) -> any() Exported function for scheduling a recurring message. every_worker_loop/4 *¶ every_worker_loop(CronPath, Req, Opts, IntervalMillis) -> any() every_worker_loop_test/0 *¶ every_worker_loop_test() -> any() This test verifies that a recurring task can be scheduled and executed. info/1¶ info(X1) -> any() Exported function for getting device info. info/3¶ info(Msg1, Msg2, Opts) -> any() once/3¶ once(Msg1, Msg2, Opts) -> any() Exported function for scheduling a one-time message. once_executed_test/0 *¶ once_executed_test() -> any() This test verifies that a one-time task can be scheduled and executed. once_worker/3 *¶ once_worker(Path, Req, Opts) -> any() Internal function for scheduling a one-time message. parse_time/1 *¶ parse_time(BinString) -> any() Parse a time string into milliseconds. stop/3¶ stop(Msg1, Msg2, Opts) -> any() Exported function for stopping a scheduled task. stop_every_test/0 *¶ stop_every_test() -> any() This test verifies that a recurring task can be stopped by calling the stop function with the task ID. stop_once_test/0 *¶ stop_once_test() -> any() test_worker/0 *¶ test_worker() -> any() This is a helper function that is used to test the cron device. It is used to increment a counter and update the state of the worker. test_worker/1 *¶ test_worker(State) -> any() October 3, 2025","estimatedWords":303,"lastModified":"2025-10-20T12:09:58.593Z","breadcrumbs":["build","devices","source code","dev cron"],"siteKey":"hyperbeam","siteName":"Hyperbeam","depth":4,"crawledAt":"2025-10-20T12:09:58.593Z"},{"url":"https://hyperbeam.arweave.net/build/devices/source-code/dev_stack.html","title":"Module dev_stackerl","content":"Module dev_stack.erl¶ A device that contains a stack of other devices, and manages their execution. Description¶ It can run in two modes: fold (the default), and map. In fold mode, it runs upon input messages in the order of their keys. A stack maintains and passes forward a state (expressed as a message) as it progresses through devices. For example, a stack of devices as follows: Device -> Stack Device-Stack/1/Name -> Add-One-Device Device-Stack/2/Name -> Add-Two-Device When called with the message: #{ Path = \"FuncName\", binary => > } Will produce the output: #{ Path = \"FuncName\", binary => > } {ok, #{ bin => > }} In map mode, the stack will run over all the devices in the stack, and combine their results into a single message. Each of the devices' output values have a key that is the device's name in the Device-Stack (its number if the stack is a list). You can switch between fold and map modes by setting the Mode key in the Msg2 to either Fold or Map, or set it globally for the stack by setting the Mode key in the Msg1 message. The key in Msg2 takes precedence over the key in Msg1. The key that is called upon the device stack is the same key that is used upon the devices that are contained within it. For example, in the above scenario we resolve FuncName on the stack, leading FuncName to be called on Add-One-Device and Add-Two-Device. A device stack responds to special statuses upon responses as follows: skip: Skips the rest of the device stack for the current pass. pass: Causes the stack to increment its pass number and re-execute the stack from the first device, maintaining the state accumulated so far. Only available in fold mode. In all cases, the device stack will return the accumulated state to the caller as the result of the call to the stack. The dev_stack adds additional metadata to the message in order to track the state of its execution as it progresses through devices. These keys are as follows: Stack-Pass: The number of times the stack has reset and re-executed from the first device for the current message. Input-Prefix: The prefix that the device should use for its outputs and inputs. Output-Prefix: The device that was previously executed. All counters used by the stack are initialized to 1. Additionally, as implemented in HyperBEAM, the device stack will honor a number of options that are passed to it as keys in the message. Each of these options is also passed through to the devices contained within the stack during execution. These options include: Error-Strategy: Determines how the stack handles errors from devices. See maybe_error/5 for more information. Allow-Multipass: Determines whether the stack is allowed to automatically re-execute from the first device when the pass tag is returned. See maybe_pass/3 for more information. Under-the-hood, dev_stack uses a default handler to resolve all calls to devices, aside set/2 which it calls itself to mutate the message's device key in order to change which device is currently being executed. This method allows dev_stack to ensure that the message's HashPath is always correct, even as it delegates calls to other devices. An example flow for a dev_stack execution is as follows: /Msg1/AlicesExcitingKey -> dev_stack:execute -> /Msg1/Set?device=/Device-Stack/1 -> /Msg2/AlicesExcitingKey -> /Msg3/Set?device=/Device-Stack/2 -> /Msg4/AlicesExcitingKey ... -> /MsgN/Set?device=[This-Device] -> returns {ok, /MsgN+1} -> /MsgN+1 In this example, the device key is mutated a number of times, but the resulting HashPath remains correct and verifiable. Function Index¶ benchmark_test/0*example_device_for_stack_test/0*generate_append_device/1generate_append_device/2*increment_pass/2*Helper to increment the pass number.info/1input_and_output_prefixes_test/0*input_output_prefixes_passthrough_test/0*input_prefix/3Return the input prefix for the stack.many_devices_test/0*maybe_error/5*no_prefix_test/0*not_found_test/0*output_prefix/3Return the output prefix for the stack.output_prefix_test/0*pass_test/0*prefix/3Return the default prefix for the stack.reinvocation_test/0*resolve_fold/3*The main device stack execution engine.resolve_fold/4*resolve_map/3*Map over the devices in the stack, accumulating the output in a single message of keys and values, where keys are the same as the keys in the original message (typically a number).router/3*router/4The device stack key router.simple_map_test/0*simple_stack_execute_test/0*skip_test/0*test_prefix_msg/0*transform/3*Return Message1, transformed such that the device named Key from the Device-Stack key in the message takes the place of the original Device key.transform_external_call_device_test/0*Ensure we can generate a transformer message that can be called to return a version of msg1 with only that device attached.transform_internal_call_device_test/0*Test that the transform function can be called correctly internally by other functions in the module.transformer_message/2*Return a message which, when given a key, will transform the message such that the device named Key from the Device-Stack key in the message takes the place of the original Device key. Function Details¶ benchmark_test/0 *¶ benchmark_test() -> any() example_device_for_stack_test/0 *¶ example_device_for_stack_test() -> any() generate_append_device/1¶ generate_append_device(Separator) -> any() generate_append_device/2 *¶ generate_append_device(Separator, Status) -> any() increment_pass/2 *¶ increment_pass(Message, Opts) -> any() Helper to increment the pass number. info/1¶ info(Msg) -> any() input_and_output_prefixes_test/0 *¶ input_and_output_prefixes_test() -> any() input_output_prefixes_passthrough_test/0 *¶ input_output_prefixes_passthrough_test() -> any() input_prefix/3¶ input_prefix(Msg1, Msg2, Opts) -> any() Return the input prefix for the stack. many_devices_test/0 *¶ many_devices_test() -> any() maybe_error/5 *¶ maybe_error(Message1, Message2, DevNum, Info, Opts) -> any() no_prefix_test/0 *¶ no_prefix_test() -> any() not_found_test/0 *¶ not_found_test() -> any() output_prefix/3¶ output_prefix(Msg1, Msg2, Opts) -> any() Return the output prefix for the stack. output_prefix_test/0 *¶ output_prefix_test() -> any() pass_test/0 *¶ pass_test() -> any() prefix/3¶ prefix(Msg1, Msg2, Opts) -> any() Return the default prefix for the stack. reinvocation_test/0 *¶ reinvocation_test() -> any() resolve_fold/3 *¶ resolve_fold(Message1, Message2, Opts) -> any() The main device stack execution engine. See the moduledoc for more information. resolve_fold/4 *¶ resolve_fold(Message1, Message2, DevNum, Opts) -> any() resolve_map/3 *¶ resolve_map(Message1, Message2, Opts) -> any() Map over the devices in the stack, accumulating the output in a single message of keys and values, where keys are the same as the keys in the original message (typically a number). router/3 *¶ router(Message1, Message2, Opts) -> any() router/4¶ router(Key, Message1, Message2, Opts) -> any() The device stack key router. Sends the request to resolve_stack, except for set/2 which is handled by the default implementation in dev_message. simple_map_test/0 *¶ simple_map_test() -> any() simple_stack_execute_test/0 *¶ simple_stack_execute_test() -> any() skip_test/0 *¶ skip_test() -> any() test_prefix_msg/0 *¶ test_prefix_msg() -> any() transform/3 *¶ transform(Msg1, Key, Opts) -> any() Return Message1, transformed such that the device named Key from the Device-Stack key in the message takes the place of the original Device key. This transformation allows dev_stack to correctly track the HashPath of the message as it delegates execution to devices contained within it. transform_external_call_device_test/0 *¶ transform_external_call_device_test() -> any() Ensure we can generate a transformer message that can be called to return a version of msg1 with only that device attached. transform_internal_call_device_test/0 *¶ transform_internal_call_device_test() -> any() Test that the transform function can be called correctly internally by other functions in the module. transformer_message/2 *¶ transformer_message(Msg1, Opts) -> any() Return a message which, when given a key, will transform the message such that the device named Key from the Device-Stack key in the message takes the place of the original Device key. This allows users to call a single device from the stack: /Msg1/Transform/DeviceName/keyInDevice -> keyInDevice executed on DeviceName against Msg1. October 3, 2025","estimatedWords":1152,"lastModified":"2025-10-20T12:09:59.205Z","breadcrumbs":["build","devices","source code","dev stack"],"siteKey":"hyperbeam","siteName":"Hyperbeam","depth":4,"crawledAt":"2025-10-20T12:09:59.205Z"},{"url":"https://hyperbeam.arweave.net/build/devices/index.html","title":"Core Devices","content":"Core Devices¶ HyperBEAM provides a comprehensive suite of devices that handle different aspects of computation, data management, authentication, and system operations. This page provides a complete overview of all available devices organized by their primary function. Essential Core Devices¶ These devices provide the fundamental building blocks for HyperBEAM applications: Process & State Management¶ ~process@1.0 - Manages persistent, shared computational states and orchestrates device execution ~scheduler@1.0 - Handles ordering and execution of messages within processes ~meta@1.0 - Configures node settings, hardware specs, and operational parameters Message & Data Handling¶ ~message@1.0 - Core message state and processing management ~json@1.0 - Provides structured access to JSON data ~relay@1.0 - Forwards messages between AO nodes and external endpoints Execution Environments¶ ~wasm64@1.0 - Executes WebAssembly code for high-performance computation ~lua@5.3a - Executes Lua scripts for flexible scripting capabilities Device Ecosystems¶ These comprehensive ecosystems provide advanced functionality through coordinated device interactions: Authentication & Security Ecosystem¶ Authentication Ecosystem - Complete wallet-less authentication system Included Devices: ~auth-hook@1.0 - Main authentication interceptor and request signer ~secret@1.0 - Wallet generation and secret management with access control ~cookie@1.0 - HTTP cookie-based session authentication ~http-auth@1.0 - HTTP Basic authentication with PBKDF2 key derivation Key Capabilities: Zero-friction blockchain authentication Server-side wallet management Session persistence across requests Enterprise HTTP authentication Multi-signature wallet support Data Management Ecosystem¶ Data Discovery Engine¶ Data Discovery Engine - Advanced message search and query system Primary Device: ~query@1.0 - Flexible message discovery with multiple search modes and return formats Key Capabilities: Complex message searching with flexible filtering GraphQL query support for advanced data operations Multiple return formats (paths, messages, counts, booleans) Integration with authentication for access-controlled queries Data Replication Engine¶ Data Replication Engine - External data ingestion and synchronization Primary Device: ~copycat@1.0 - Orchestrates data replication from external sources Key Capabilities: GraphQL endpoint data replication Direct Arweave node integration Automatic pagination and batch processing Comprehensive error handling and recovery Security & TEE Devices¶ Advanced security features and Trusted Execution Environment support: ~snp@1.0 - Secure Network Protocol for TEE operations dev_codec_httpsig - HTTP signature validation and processing Payment & Access Control Devices¶ Metering, billing, and access management: ~p4@1.0 - Payment processing and metering system ~faff@1.0 - Fine-grained access control and permissions Workflow & Utility Devices¶ Process coordination and system utilities: dev_cron - Scheduled task execution and automation dev_stack - Device stack management and coordination dev_monitor - System monitoring and health checks Storage & Cache Devices¶ Data persistence and caching infrastructure: dev_cache - Message caching and retrieval system hb_store - Persistent storage backend management Communication & Network Devices¶ Inter-node communication and network operations: hb_gateway_client - Gateway communication client hb_http_client - HTTP client operations hb_http_server - HTTP server management Development & Testing Devices¶ Tools for development, testing, and debugging: dev_test - Testing framework and utilities hb_debugger - Debugging tools and inspection dev_multipass - Multi-pass processing utilities Legacy & Specialized Devices¶ Specialized functionality and legacy support: ~patch@1.0 - Direct state updates for process migration dev_wasi - WebAssembly System Interface support dev_poda - Proof of Data Availability validation Device Integration Patterns¶ Complete Application Stack¶ Authentication → Data Replication → Data Discovery → Process Execution ↓ ↓ ↓ ↓ Auth Ecosystem Copycat Device Query Device Process Device Data Workflow Integration¶ Ingestion: Copycat replicates external data into local cache Discovery: Query provides search and filtering over cached data Authentication: Auth ecosystem controls access to data operations Processing: Process devices utilize data for computation Security Integration¶ Authentication ecosystem provides transparent user authentication TEE devices enable secure computation environments Access control devices manage permissions and resource usage HTTP signature devices ensure message integrity Getting Started¶ For Authentication:¶ Start with the Authentication Ecosystem to enable wallet-less blockchain applications. For Data Management:¶ Begin with Data Replication to import external data, then use Data Discovery for search and analysis. For Process Development:¶ Review ~process@1.0 for state management and ~scheduler@1.0 for message ordering. For Custom Devices:¶ See Building Devices for guidance on creating your own devices. Next Steps: - Building Devices - Learn to create custom devices - HyperBEAM Overview - Understand the device architecture - Source Code Reference - Detailed technical documentation October 3, 2025","estimatedWords":673,"lastModified":"2025-10-20T12:10:00.364Z","breadcrumbs":["build","devices","index"],"siteKey":"hyperbeam","siteName":"Hyperbeam","depth":4,"crawledAt":"2025-10-20T12:10:00.364Z"}],"lastCrawled":"2025-10-20T12:15:30.422Z","stats":{"totalPages":29,"averageWords":353,"duration":28807,"requestCount":53,"averageResponseTime":488.2641509433962,"pagesPerSecond":1.0066997604748846}},"ao":{"name":"AO Cookbook","baseUrl":"https://cookbook_ao.arweave.net","pages":[{"url":"https://cookbook_ao.arweave.net/guides/aos/blueprints/staking.html","title":"Staking Blueprint","content":"Staking Blueprint ​The Staking Blueprint is a predesigned template that helps you quickly build a staking system in ao. It is a great way to get started and can be customized to fit your needs.Prerequisites ​The Staking Blueprint requires the Token Blueprint to be loaded, first.Unpacking the Staking Blueprint ​Stakers: The Stakers array is used to store the staked tokens of the participants.Unstaking: The Unstaking array is used to store the unstaking requests of the participants.Stake Action Handler: The stake handler allows processes to stake tokens. When a process sends a message with the tag Action = \"Stake\", the handler will add the staked tokens to the Stakers array and send a message back to the process confirming the staking.Unstake Action Handler: The unstake handler allows processes to unstake tokens. When a process sends a message with the tag Action = \"Unstake\", the handler will add the unstaking request to the Unstaking array and send a message back to the process confirming the unstaking.Finalization Handler: The finalize handler allows processes to finalize the staking process. When a process sends a message with the tag Action = \"Finalize\", the handler will process the unstaking requests and finalize the staking process.How To Use: ​Open your preferred text editor.Open the Terminal.Start your aos process.Type in .load-blueprint stakingVerify the Blueprint is Loaded: ​Type in Handlers.list to see the newly loaded handlers.What's in the Staking Blueprint: ​luaStakers = Stakers or {} Unstaking = Unstaking or {} -- Stake Action Handler Handlers.stake = function(msg) local quantity = tonumber(msg.Tags.Quantity) local delay = tonumber(msg.Tags.UnstakeDelay) local height = tonumber(msg['Block-Height']) assert(Balances[msg.From] and Balances[msg.From] >= quantity, \"Insufficient balance to stake\") Balances[msg.From] = Balances[msg.From] - quantity Stakers[msg.From] = Stakers[msg.From] or {} Stakers[msg.From].amount = (Stakers[msg.From].amount or 0) + quantity Stakers[msg.From].unstake_at = height + delay end -- Unstake Action Handler Handlers.unstake = function(msg) local quantity = tonumber(msg.Tags.Quantity) local stakerInfo = Stakers[msg.From] assert(stakerInfo and stakerInfo.amount >= quantity, \"Insufficient staked amount\") stakerInfo.amount = stakerInfo.amount - quantity Unstaking[msg.From] = { amount = quantity, release_at = stakerInfo.unstake_at } end -- Finalization Handler local finalizationHandler = function(msg) local currentHeight = tonumber(msg['Block-Height']) -- Process unstaking for address, unstakeInfo in pairs(Unstaking) do if currentHeight >= unstakeInfo.release_at then Balances[address] = (Balances[address] or 0) + unstakeInfo.amount Unstaking[address] = nil end end end -- wrap function to continue handler flow local function continue(fn) return function (msg) local result = fn(msg) if (result) == -1 then return 1 end return result end end -- Registering Handlers Handlers.add(\"stake\", continue(Handlers.utils.hasMatchingTag(\"Action\", \"Stake\")), Handlers.stake) Handlers.add(\"unstake\", continue(Handlers.utils.hasMatchingTag(\"Action\", \"Unstake\")), Handlers.unstake) -- Finalization handler should be called for every message Handlers.add(\"finalize\", function (msg) return -1 end, finalizationHandler)","estimatedWords":425,"lastModified":"2025-10-20T12:10:06.374Z","breadcrumbs":["guides","aos","blueprints","staking"],"siteKey":"ao","siteName":"AO Cookbook","depth":4,"crawledAt":"2025-10-20T12:10:06.374Z"},{"url":"https://cookbook_ao.arweave.net/guides/aos/blueprints/cred-utils.html","title":"CRED Utils Blueprint","content":"CRED Utils Blueprint ​CRED is now deprecatedCRED was a token used during ao's legacynet phase to reward early developers. It is no longer earnable or redeemable.The CRED Utils Blueprint is a predesigned template that helps you quickly check your CRED balance in ao legacynet.Unpacking the CRED Utils Blueprint ​The CRED Metatable ​CRED.balance: Evaluating CRED.balance will print your process's last known balance of your CRED. If you have never fetched your CRED balance before, it will be fetched automatically. If you think your CRED has recently changed, consider running CRED.update first.CRED.process: Evaluating CRED.process will print the process ID of the CRED token issuer.CRED.send: Invoking CRED.send(targetProcessId, amount) like a function will transfer CRED from your ao process to another ao process.targetProcessId: string: the 43-character process ID of the recipient.amount: integer: The quantity of CRED units to send. 1 CRED === 1000 CRED units.CRED.update: Evaluating CRED.update will fetch your latest CRED balance by sending a message to the CRED issuer process. The UpdateCredBalance handler (see below) will ingest the response message.Handler Definitions ​Credit Handler: The CRED_Credit handler allows the CRED issuer process (and aos) to automatically notify you when your CRED balance increase.Debit Handler: The CRED_Debit handler allows the CRED issuer process (and aos) to automatically notify you when your CRED balance decreases.Update Balance Handler: The UpdateCredBalance handler ingests the response to any CRED.update requests.How To Use the Blueprint ​Open the Terminal.Start your aos process.Type in .load-blueprint credUtilsType in CRED.balanceWhat's in the CRED Utils Blueprint: ​See the aos source code on GitHub for the blueprint shipped in the latest version of aos.luaCRED_PROCESS = \"Sa0iBLPNyJQrwpTTG-tWLQU-1QeUAJA73DdxGGiKoJc\" _CRED = { balance = \"Your CRED balance has not been checked yet. Updating now.\" } local credMeta = { __index = function(t, key) -- sends CRED balance request if key == \"update\" then Send({ Target = CRED_PROCESS, Action = \"Balance\", Tags = { [\"Target\"] = ao.id } }) return \"Balance update requested.\" -- prints local CRED balance, requests it if not set elseif key == \"balance\" then if _CRED.balance == \"Your CRED balance has not been checked yet. Updating now.\" then Send({ Target = CRED_PROCESS, Action = \"Balance\", Tags = { [\"Target\"] = ao.id } }) end return _CRED.balance -- prints CRED process ID elseif key == \"process\" then return CRED_PROCESS -- tranfers CRED elseif key == \"send\" then return function(target, amount) -- ensures amount is string amount = tostring(amount) print(\"sending \" .. amount .. \"CRED to \" .. target) Send({ Target = CRED_PROCESS, Action = \"Transfer\", [\"Recipient\"] = target, [\"Quantity\"] = amount }) end else return nil end end } CRED = setmetatable({}, credMeta) -- Function to evaluate if a message is a balance update local function isCredBalanceMessage(msg) if msg.From == CRED_PROCESS and msg.Tags.Balance then return true else return false end end -- Function to evaluate if a message is a Debit Notice local function isDebitNotice(msg) if msg.From == CRED_PROCESS and msg.Tags.Action == \"Debit-Notice\" then return true else return false end end -- Function to evaluate if a message is a Credit Notice local function isCreditNotice(msg) if msg.From == CRED_PROCESS and msg.Tags.Action == \"Credit-Notice\" then return true else return false end end local function formatBalance(balance) -- Ensure balance is treated as a string balance = tostring(balance) -- Check if balance length is more than 3 to avoid unnecessary formatting if #balance > 3 then -- Insert dot before the last three digits balance = balance:sub(1, -4) .. \".\" .. balance:sub(-3) end return balance end -- Handles Balance messages Handlers.add( \"UpdateCredBalance\", isCredBalanceMessage, function(msg) local balance = nil if msg.Tags.Balance then balance = msg.Tags.Balance end -- Format the balance if it's not set if balance then -- Format the balance by inserting a dot after the first three digits from the right local formattedBalance = formatBalance(balance) _CRED.balance = formattedBalance print(\"CRED Balance updated: \" .. _CRED.balance) else print(\"An error occurred while updating CRED balance\") end end ) -- Handles Debit notices Handlers.add( \"CRED_Debit\", isDebitNotice, function(msg) print(msg.Data) end ) -- Handles Credit notices Handlers.add( \"CRED_Credit\", isCreditNotice, function(msg) print(msg.Data) end )","estimatedWords":659,"lastModified":"2025-10-20T12:10:07.018Z","breadcrumbs":["guides","aos","blueprints","cred utils"],"siteKey":"ao","siteName":"AO Cookbook","depth":4,"crawledAt":"2025-10-20T12:10:07.018Z"},{"url":"https://cookbook_ao.arweave.net/guides/aos/blueprints/chatroom.html","title":"Chatroom Blueprint","content":"Chatroom Blueprint ​The Chatroom Blueprint is a predesigned template that helps you quickly build a chatroom in ao. It is a great way to get started and can be customized to fit your needs.Unpacking the Chatroom Blueprint ​Members: The Members array is used to store the users who have registered to the chatroom.Register Handler: The register handler allows processes to join the chatroom. When a process sends a message with the tag Action = \"Register\", the handler will add the process to the Members array and send a message back to the process confirming the registration.Broadcast Handler: The broadcast handler allows processes to send messages to all the members of the chatroom. When a process sends a message with the tag Action = \"Broadcast\", the handler will send the message to all the members of the chatroom.How To Use: ​Open your preferred text editor.Open the Terminal.Start your aos process.Type in .load-blueprint chatroomVerify the Blueprint is Loaded: ​Type in Handlers.list to see the newly loaded handlers.What's in the Chatroom Blueprint: ​luaMembers = Members or {} Handlers.add( \"register\", Handlers.utils.hasMatchingTag(\"Action\", \"Register\"), function (msg) table.insert(Members, msg.From) Handlers.utils.reply(\"registered\")(msg) end ) Handlers.add( \"broadcast\", Handlers.utils.hasMatchingTag(\"Action\", \"Broadcast\"), function (msg) for _, recipient in ipairs(Members) do ao.send({Target = recipient, Data = msg.Data}) end Handlers.utils.reply(\"Broadcasted.\")(msg) end )","estimatedWords":207,"lastModified":"2025-10-20T12:10:07.612Z","breadcrumbs":["guides","aos","blueprints","chatroom"],"siteKey":"ao","siteName":"AO Cookbook","depth":4,"crawledAt":"2025-10-20T12:10:07.612Z"},{"url":"https://cookbook_ao.arweave.net/guides/aos/blueprints/index.html","title":"Blueprints","content":"Blueprints ​Blueprints are predesigned templates that help you quickly build in ao. They are a great way to get started and can be customized to fit your needs.Available Blueprints ​ChatroomCRED UtilsStakingTokenVoting","estimatedWords":31,"lastModified":"2025-10-20T12:10:08.300Z","breadcrumbs":["guides","aos","blueprints","index"],"siteKey":"ao","siteName":"AO Cookbook","depth":4,"crawledAt":"2025-10-20T12:10:08.300Z"},{"url":"https://cookbook_ao.arweave.net/guides/aos/token.html","title":"Building a Token in ao","content":"Building a Token in ao ​When creating tokens, we'll continue to use the Lua Language within ao to mint a token, guided by the principles outlined in the Token Specification.Two Ways to Create Tokens: ​1 - Use the token blueprint:.load-blueprint tokenUsing the token blueprint will create a token with all the handlers and state already defined. This is the easiest way to create a token. You'll be able to customize those handlers and state to your after loading the blueprint.You can learn more about available blueprints here: BlueprintsINFOUsing the token blueprint will definitely get quickly, but you'll still want to understand how to load and test the token, so you can customize it to your needs.2 - Build from Scratch:The following guide will guide you through the process of creating a token from scratch. This is a more advanced way to create a token, but it will give you a better understanding of how tokens work.Preparations ​Step 1: Initializing the Token ​Open our preferred text editor, preferably from within the same folder you used during the previous tutorial.Create a new file named token.lua.Within token.lua, you'll begin by initializing the token's state, defining its balance, name, ticker, and more:lualocal json = require('json') if not Balances then Balances = { [ao.id] = 100000000000000 } end if Name ~= 'My Coin' then Name = 'My Coin' end if Ticker ~= 'COIN' then Ticker = 'COIN' end if Denomination ~= 10 then Denomination = 10 end if not Logo then Logo = 'optional arweave TxID of logo image' endLet's break down what we've done here:local json = require('json'): This first line of this code imports a module for later use.if not Balances then Balances = { [ao.id] = 100000000000000 } end: This second line is initializing a Balances table which is the way the Process tracks who posses the token. We initialize our token process ao.id to start with all the balance.The Next 4 Lines, if Name, if Ticker, if Denomination, and if not Logo are all optional, except for if Denomination, and are used to define the token's name, ticker, denomination, and logo respectively.INFOThe code if Denomination ~= 10 then Denomination = 10 end tells us the number of the token that should be treated as a single unit.Step 2: Info and Balances Handlers ​Incoming Message Handler ​Now lets add our first Handler to handle incoming Messages.luaHandlers.add('Info', Handlers.utils.hasMatchingTag('Action', 'Info'), function(msg) ao.send( { Target = msg.From, Tags = { [\"Name\"] = Name, [\"Ticker\"] = Ticker, [\"Logo\"] = Logo, [\"Denomination\"] = tostring(Denomination) } }) end)INFOAt this point, you've probably noticed that we're building all of the handlers inside the token.lua file rather than using .editor.With many handlers and processes, it's perfectly fine to create your handlers using .editor, but because we're creating a full process for initializing a token, setting up info and balances handlers, transfer handlers, and a minting handler, it's best to keep everything in one file.This also allows us to maintain consistency since each handler will be updated every time we reload the token.lua file into aos.This code means that if someone Sends a message with the Tag, Action = \"Info\", our token will Send back a message with all of the information defined above. Note the Target = msg.From, this tells ao we are replying to the process that sent us this message.Info & Token Balance Handlers ​Now we can add 2 Handlers which provide information about token Balances.luaHandlers.add('Balance', Handlers.utils.hasMatchingTag('Action', 'Balance'), function(msg) local bal = '0' -- If not Target is provided, then return the Senders balance if (msg.Tags.Target and Balances[msg.Tags.Target]) then bal = tostring(Balances[msg.Tags.Target]) elseif Balances[msg.From] then bal = tostring(Balances[msg.From]) end ao.send({ Target = msg.From, Data = json.encode(tonumber(bal)), Tags = { [\"Balance\"] = bal, [\"Ticker\"] = Ticker } }) end) Handlers.add('Balances', Handlers.utils.hasMatchingTag('Action', 'Balances'), function(msg) ao.send({ Target = msg.From, Data = json.encode(Balances) }) end)The first Handler above Handlers.add('Balance' handles a process or person requesting their own balance or the balance of a Target. Then replies with a message containing the info. The second Handler Handlers.add('Balances' just replies with the entire Balances table.Step 3: Transfer Handlers ​Before we begin testing we will add 2 more Handlers one which allows for the transfer of tokens between processes or users.luaHandlers.add('Transfer', Handlers.utils.hasMatchingTag('Action', 'Transfer'), function(msg) assert(type(msg.Tags.Recipient) == 'string', 'Recipient is required!') assert(type(msg.Tags.Quantity) == 'string', 'Quantity is required!') if not Balances[msg.From] then Balances[msg.From] = 0 end if not Balances[msg.Tags.Recipient] then Balances[msg.Tags.Recipient] = 0 end local qty = tonumber(msg.Tags.Quantity) assert(type(qty) == 'number', 'qty must be number') if Balances[msg.From] >= qty then Balances[msg.From] = Balances[msg.From] - qty Balances[msg.Tags.Recipient] = Balances[msg.Tags.Recipient] + qty --[[ Only Send the notifications to the Sender and Recipient if the Cast tag is not set on the Transfer message ]] -- if not msg.Tags.Cast then -- Debit-Notice message template, that is sent to the Sender of the transfer local debitNotice = { Target = msg.From, Action = 'Debit-Notice', Recipient = msg.Recipient, Quantity = tostring(qty), Data = Colors.gray .. \"You transferred \" .. Colors.blue .. msg.Quantity .. Colors.gray .. \" to \" .. Colors.green .. msg.Recipient .. Colors.reset } -- Credit-Notice message template, that is sent to the Recipient of the transfer local creditNotice = { Target = msg.Recipient, Action = 'Credit-Notice', Sender = msg.From, Quantity = tostring(qty), Data = Colors.gray .. \"You received \" .. Colors.blue .. msg.Quantity .. Colors.gray .. \" from \" .. Colors.green .. msg.From .. Colors.reset } -- Add forwarded tags to the credit and debit notice messages for tagName, tagValue in pairs(msg) do -- Tags beginning with \"X-\" are forwarded if string.sub(tagName, 1, 2) == \"X-\" then debitNotice[tagName] = tagValue creditNotice[tagName] = tagValue end end -- Send Debit-Notice and Credit-Notice ao.send(debitNotice) ao.send(creditNotice) end else ao.send({ Target = msg.Tags.From, Tags = { [\"Action\"] = 'Transfer-Error', ['Message-Id'] = msg.Id, [\"Error\"] = 'Insufficient Balance!' } }) end end)In summary, this code checks to make sure the Recipient and Quantity Tags have been provided, initializes the balances of the person sending the message and the Recipient if they dont exist and then attempts to transfer the specified quantity to the Recipient in the Balances table.luaBalances[msg.From] = Balances[msg.From] - qty Balances[msg.Tags.Recipient] = Balances[msg.Tags.Recipient] + qtyIf the transfer was successful a Debit-Notice is sent to the sender of the original message and a Credit-Notice is sent to the Recipient.lua-- Send Debit-Notice to the Sender ao.send({ Target = msg.From, Tags = { [\"Action\"] = 'Debit-Notice', [\"Recipient\"] = msg.Tags.Recipient, [\"Quantity\"] = tostring(qty) } }) -- Send Credit-Notice to the Recipient ao.send({ Target = msg.Tags.Recipient, Tags = { [\"Action\"] = 'Credit-Notice', [\"Sender\"] = msg.From, [\"Quantity\"] = tostring(qty) } })If there was insufficient balance for the transfer it sends back a failure messageluaao.send({ Target = msg.Tags.From, Tags = { [\"Action\"] = 'Transfer-Error', ['Message-Id'] = msg.Id, [\"Error\"] = 'Insufficient Balance!' } })The line if not msg.Tags.Cast then Means were not producing any messages to push if the Cast tag was set. This is part of the ao protocol.Step 4: Mint Handler ​Finally, we will add a Handler to allow the minting of new tokens.luaHandlers.add('Mint', Handlers.utils.hasMatchingTag('Action', 'Mint'), function(msg, env) assert(type(msg.Tags.Quantity) == 'string', 'Quantity is required!') if msg.From == env.Process.Id then -- Add tokens to the token pool, according to Quantity local qty = tonumber(msg.Tags.Quantity) Balances[env.Process.Id] = Balances[env.Process.Id] + qty else ao.send({ Target = msg.Tags.From, Tags = { [\"Action\"] = 'Mint-Error', [\"Message-Id\"] = msg.Id, [\"Error\"] = 'Only the Process Owner can mint new ' .. Ticker .. ' tokens!' } }) end end)This code checks to make sure the Quantity Tag has been provided and then adds the specified quantity to the Balances table.Loading and Testing ​Once you've created your token.lua file, or you've used .load-blueprint token, you're now ready to begin testing.1 - Start the aos process ​Make sure you've started your aos process by running aos in your terminal.2 - Loading the token.lua file ​If you've followed along with the guide, you'll have a token.lua file in the same directory as your aos process. From the aos prompt, load in the file.lua.load token.lua3 - Testing the Token ​Now we can send Messages to our aos process ID, from the same aos prompt to see if is working. If we use ao.id as the Target we are sending a message to ourselves.luaSend({ Target = ao.id, Action = \"Info\" })This should print the Info defined in the contract. Check the latest inbox message for the response.luaInbox[#Inbox].TagsThis should print the Info defined in the contract.INFOMake sure you numerically are checking the last message. To do so, run #Inbox first to see the total number of messages are in the inbox. Then, run the last message number to see the data.Example:If #Inbox returns 5, then run Inbox[5].Data to see the data.4 - Transfer ​Now, try to transfer a balance of tokens to another wallet or process ID.INFOIf you need another process ID, you can run aos [name] in another terminal window to get a new process ID. Make sure it's not the same aos [name] as the one you're currently using.Example:If you're using aos in one terminal window, you can run aos test in another terminal window to get a new process ID.luaSend({ Target = ao.id, Tags = { [\"Action\"] = \"Transfer\", [\"Recipient\"] = 'another wallet or processid', [\"Quantity\"] = '10000' }})After sending, you'll receive a printed message in the terminal similar to Debit-Notice on the sender's side and Credit-Notice on the recipient's side.5 - Check the Balances ​Now that you've transferred some tokens, let's check the balances.luaSend({ Target = ao.id, Tags = { [\"Action\"] = \"Balances\" }})luaInbox[#Inbox].DataYou will see two process IDs or wallet addresses, each displaying a balance. The first should be your sending process ID, the second should be the recipient's process ID.6 - Minting Tokens ​Finally, attempt to mint some tokens.luaSend({ Target = ao.id, Tags = { [\"Action\"] = \"Mint\", [\"Quantity\"] = '1000' }})And check the balances again.luaSend({ Target = ao.id, Tags = { [\"Action\"] = \"Balances\" }}) Inbox[#Inbox].DataYou'll then see the balance of the process ID that minted the tokens has increased.Conclusion ​That concludes the \"Build a Token\" guide. Learning out to build custom tokens will unlock a great deal of potential for your projects; whether that be creating a new currency, a token for a game, a governance token, or anything else you can imagine.","estimatedWords":1686,"lastModified":"2025-10-20T12:10:08.991Z","breadcrumbs":["guides","aos","token"],"siteKey":"ao","siteName":"AO Cookbook","depth":3,"crawledAt":"2025-10-20T12:10:08.991Z"},{"url":"https://cookbook_ao.arweave.net/guides/aos/faq.html","title":"FAQ","content":"FAQ ​Ownership ​Understanding Process OwnershipStart a new process with the aos console, the ownership of the process is set to your wallet address. aos uses the Owner global variable to define the ownership of the process. If you wish to transfer ownership or lock the process so that no one can own, you simply modify the Owner variable to another wallet address or set it to nil.JSON ​encoding data as jsonWhen sending data to another process or an external service, you may want to use JSON as a way to encode the data for recipients. Using the json module in lua, you can encode and decode pure lua tables that contain values.luaSend({Target = Router, Data = require('json').encode({hello = \"world\"})})Send vs ao.send ​When to use Send vs ao.sendBoth functions send a message to a process, the difference is ao.send returns the message, in case you want to log it or troubleshoot. The Send function is intended to be used in the console for easier access. It is preferred to use ao.send in the handlers. But they are both interchangeable in aos.","estimatedWords":180,"lastModified":"2025-10-20T12:10:09.579Z","breadcrumbs":["guides","aos","faq"],"siteKey":"ao","siteName":"AO Cookbook","depth":3,"crawledAt":"2025-10-20T12:10:09.579Z"},{"url":"https://cookbook_ao.arweave.net/guides/aos/troubleshooting.html","title":"Troubleshooting using aolink","content":"Troubleshooting using ao.link ​Working with a decentralized computer and network, you need to be able to troubleshoot more than your own code. You need to be able to track messages, token balances, token transfers of processes. This is where https://ao.link becomes an essential tool in your toolbox.Analytics ​AOLink has a set of 4 analytic measures:Total MessagesTotal UsersTotal ProcessesTotal ModulesThese analytics give you a quick view into the ao network's total processing health.Events ​Below, the analytics are the latest events that have appeared on the ao computer. You have a list of messages being scheduled and that have been executed. These events are any of the ao Data Protocol Types. And you can click on the Process ID or the Message ID to get details about each.Message Details ​The message details give you key details about:FromToBlock HeightCreatedTagsDataResult TypeDataIf you want to further troubleshoot and debug, you have the option to look at the result of the CU (Compute Unit) by clicking on \"Compute\".And further understand linked messages. Process Details ​The process details provide you with information about the process it's useful to see in the tags with what module this got instantiated from. If you notice on the left you see the interaction with the process displayed on a graph. In this case, this is DevChat, and you can see all the processes that have interacted by Registering and Broadcasting Messages.You can effortless check the Info Handler, by pressing the \"Fetch\" button. On the bottom you see the processes balance and all messages send, with the option to break it down into Token transfers and Token balances using the tabs. Further Questions? ​Feel free to reach out on the community Discord of Autonomous Finance, for all questions and support regarding ao.link. https://discord.gg/4kF9HKZ4WuSummary ​AOLink is an excellent tool for tracking events in the ao computer. Give it a try. Also, there is another scanner tool available on the permaweb: https://ao_marton.g8way.io/ - check it out!","estimatedWords":322,"lastModified":"2025-10-20T12:10:10.189Z","breadcrumbs":["guides","aos","troubleshooting"],"siteKey":"ao","siteName":"AO Cookbook","depth":3,"crawledAt":"2025-10-20T12:10:10.190Z"},{"url":"https://cookbook_ao.arweave.net/guides/aos/inbox-and-handlers.html","title":"Understanding the Inbox","content":"Understanding the Inbox ​In aos, processes are executed in response to messages via handlers. Unhandled messages are routed to the process's Inbox.What are Handlers? ​A handler is a function that receives and evaluates messages within your process. It acts upon messages by taking them as parameters.Handlers are defined using the Handlers.add() function.The function takes three parameters:Name of the HandlerMatcher functionHandle functionluaHandlers.add(\"name\", function (Msg) -- Does this message match (return true or false) return Msg.Action == \"Register\" end, function (Msg) print(\"Registered User.\") table.insert(Members, Msg.From) ao.send({Target = Msg.From, Data = \"Registered.\"}) end )What about Inboxes? ​An inbox is a storage area for messages that have not yet been processed. Think of it as a holding zone for incoming, or \"inbound,\" items awaiting handling. Once a message is processed, it's no longer considered \"inbound\" and thus leaves the inbox.Example: Consider the inbox like your voicemail. Just as an unanswered phone call is directed to voicemail for you to address later, messages that your Process doesn't immediately handle are sent to the inbox. This way, unhandled messages are stored until you're ready to process them.Summary ​Initially, it might seem like all messages are meant to land in your Inbox, which can be puzzling if they disappear after being handled. The analogy of a voicemail should clarify this: much like calls you answer don't go to voicemail, messages you handle won't appear in your Inbox. This illustrates the roles of both the Inbox and Handlers.","estimatedWords":240,"lastModified":"2025-10-20T12:10:11.337Z","breadcrumbs":["guides","aos","inbox and handlers"],"siteKey":"ao","siteName":"AO Cookbook","depth":3,"crawledAt":"2025-10-20T12:10:11.337Z"},{"url":"https://cookbook_ao.arweave.net/guides/aos/editor.html","title":"Editor setup","content":"Editor setup ​Remembering all the built in ao functions and utilities can sometimes be hard. To enhance your developer experience, it is recommended to install the Lua Language Server extension into your favorite text editor and add the ao addon. It supports all built in aos modules and globals.VS Code ​Install the sumneko.lua extension:Search for \"Lua\" by sumneko in the extension marketplaceDownload and install the extensionOpen the VS Code command palette with Shift + Command + P (Mac) / Ctrl + Shift + P (Windows/Linux) and run the following command:> Lua: Open Addon ManagerIn the Addon Manager, search for \"ao\", it should be the first result. Click \"Enable\" and enjoy autocomplete!Other editors ​Verify that your editor supports the language server protocolInstall Lua Language Server by following the instructions at luals.github.ioInstall the \"ao\" addon to the language serverBetterIDEa ​BetterIDEa is a custom web based IDE for developing on ao.It offers a built in Lua language server with ao definitions, so you don't need to install anything. Just open the IDE and start coding!Features include:Code completionCell based notebook ui for rapid developmentEasy process managementMarkdown and Latex cell supportShare projects with anyone through ao processesTight integration with ao package managerRead detailed information about the various features and integrations of the IDE in the documentation.","estimatedWords":211,"lastModified":"2025-10-20T12:10:12.134Z","breadcrumbs":["guides","aos","editor"],"siteKey":"ao","siteName":"AO Cookbook","depth":3,"crawledAt":"2025-10-20T12:10:12.134Z"},{"url":"https://cookbook_ao.arweave.net/guides/aos/pingpong.html","title":"Creating a Pingpong Process in aos","content":"Creating a Pingpong Process in aos ​This tutorial will guide you through creating a simple \"ping-pong\" process in aos. In this process, whenever it receives a message with the data \"ping\", it will automatically reply with \"pong\". This is a basic example of message handling and interaction between processes in aos.Step 1: Open the aos CLI ​Start by opening your command-line interface and typing aos to enter the aos environment.Step 2: Access the Editor ​Type .editor in the aos CLI to open the inline text editor. This is where you'll write your ping-pong handler code.Step 3: Write the Pingpong Handler ​In the editor, enter the following Lua code to add a handler for the pingpong pattern:luaHandlers.add( \"pingpong\", Handlers.utils.hasMatchingData(\"ping\"), Handlers.utils.reply(\"pong\") )This lua script does three things:It adds a new handler named \"pingpong\".It uses Handlers.utils.hasMatchingData(\"ping\") to check if incoming messages contain the data \"ping\".If the message contains \"ping\", Handlers.utils.reply(\"pong\") automatically sends back a message with the data \"pong\".Step 4: Exit the Editor ​After writing your code, type .done and press Enter to exit the editor and run the script.Step 5: Test the Pingpong Process ​To test the process, send a message with the data \"ping\" to the process. You can do this by typing the following command in the aos CLI:luaSend({ Target = ao.id, Data = \"ping\" })The process should respond with a message containing \"pong\" in the Inbox.Step 6: Monitor the Inbox ​Check your Inbox to see the \"ping\" message and your Outbox to confirm the \"pong\" reply.luaInbox[#Inbox].DataStep 7: Experiment and Observe ​Experiment by sending different messages and observe how only the \"ping\" messages trigger the \"pong\" response.Step 8: Save Your Process (Optional) ​If you want to use this process in the future, save the handler code in a Lua file for easy loadinginto aos sessions.INFOADDITIONAL TIP:Handler Efficiency: The simplicity of the handler function is key. Ensure that it's efficient and only triggers under the correct conditions.Conclusion ​Congratulations! You have now created a basic ping-pong process in aos. This tutorial provides a foundation for understanding message handling and process interaction within the aos environment. As you become more comfortable with these concepts, you can expand to more complex processes and interactions, exploring the full potential of aos.","estimatedWords":366,"lastModified":"2025-10-20T12:10:12.818Z","breadcrumbs":["guides","aos","pingpong"],"siteKey":"ao","siteName":"AO Cookbook","depth":3,"crawledAt":"2025-10-20T12:10:12.819Z"},{"url":"https://cookbook_ao.arweave.net/guides/aos/load.html","title":"Load Lua Files with load filename","content":"Load Lua Files with .load ​This feature allows you to load lua code from a source file on your local machine, this simple feature gives you a nice DX experience for working with aos processes.When creating handlers you may have a lot of code and you want to take advantage of a rich development environment like vscode. You can even install the lua extension to get some syntax checking.So how do you publish your local lua source code to your ao process? This is where the .load command comes into play.hello.lualuaHandlers.add( \"ping\", Handlers.utils.hasMatchingData(\"ping\"), Handlers.utils.reply(\"pong\") )aos shelllua.load hello.luaEasy Peasy! 🐶","estimatedWords":99,"lastModified":"2025-10-20T12:10:13.592Z","breadcrumbs":["guides","aos","load"],"siteKey":"ao","siteName":"AO Cookbook","depth":3,"crawledAt":"2025-10-20T12:10:13.592Z"},{"url":"https://cookbook_ao.arweave.net/guides/aos/prompt.html","title":"Customizing the Prompt in aos","content":"Customizing the Prompt in aos ​Step 1: Open aos and Start the Editor ​Launch the aos command-line interface.Enter .editor to open the inline text editor.Step 2: Write the Custom Prompt Function ​In the editor, define your custom prompt function. For example:luafunction Prompt() return \"YourName@aos> \" endCustomize \"YourName@aos> \" to your preferred prompt text.Step 3: Exit and Run Your Code ​To exit the editor and execute your code, type .done and then press Enter.Your aos prompt should now display the new custom format.Step 4: Save for Future Use (Optional) ​If you wish to use this prompt in future aos sessions, save your script in a Lua file.In subsequent sessions, load this script to apply your custom prompt.Maximizing Your Prompt ​There's a great deal of utility and creativity that can come from customizing your prompt. Several things you can do within your prompt are:Tracking the number of unhandled messages you have in your inbox by creating a function that shows how many messages you have.lua --Example: function Prompt() return \"YourName Inbox: [\" .. #Inbox .. \"] > \" endTracking the number of members are within your process ID's chatroom.Tracking the balance of a specified token that your process ID holds.Conclusion ​Now that you understand how to maximize the utility within your Prompt, you've now gained a crucial step to streamlining your ao development experience.","estimatedWords":222,"lastModified":"2025-10-20T12:10:14.194Z","breadcrumbs":["guides","aos","prompt"],"siteKey":"ao","siteName":"AO Cookbook","depth":3,"crawledAt":"2025-10-20T12:10:14.194Z"},{"url":"https://cookbook_ao.arweave.net/guides/aos/cli.html","title":"CLI","content":"CLI ​There are some command-line arguments you pass to aos to do the following:[name] - create a new process or loads an existing process for your wallet--load - load a file, you can add one or many of this command--cron - only used when creating a process--wallet - use a specific walletManaging multiple processes with aos ​shaosStarts or connects to a process with the name defaultshaos chatroomStarts or connects to a process with the name of chatroomshaos treasureRoomStarts or connects to a process with the name of treasureRoomLoad flag ​shaos treasureRoom --load greeting.lua --load treasure.lua --load puzzle.luaWith the load flag I can load many source files to my processCRON Flag ​If you want to setup your process to react on a schedule we need to tell ao, we do that when we spawn the process.shaos chatroom --cron 2-minutesTag flags ​With the tag flags, you can start a process with some custom tags (for e.g. using them as static environment variables):shaos chatroom --tag-name Chat-Theme --tag-value Dark --tag-name Chat-Name --tag-value MychatThe command above will add the extra tags to the transaction that spawns your process:ts// process data item tags [ ... { name: \"Chat-Theme\", value: \"Dark\" }, { name: \"Chat-Name\", value: \"Mychat\" } ... ]","estimatedWords":203,"lastModified":"2025-10-20T12:10:14.792Z","breadcrumbs":["guides","aos","cli"],"siteKey":"ao","siteName":"AO Cookbook","depth":3,"crawledAt":"2025-10-20T12:10:14.792Z"},{"url":"https://cookbook_ao.arweave.net/guides/aos/installing.html","title":"Installing aos","content":"Installing aos ​Installing aos only requires NodeJS - https://nodejs.orgNOTE: If you are on windows you may get better results with WSL Console.shnpm i -g https://get_ao.g8way.ioOnce installed you can run by typing aos","estimatedWords":32,"lastModified":"2025-10-20T12:10:15.367Z","breadcrumbs":["guides","aos","installing"],"siteKey":"ao","siteName":"AO Cookbook","depth":3,"crawledAt":"2025-10-20T12:10:15.367Z"},{"url":"https://cookbook_ao.arweave.net/guides/aos/intro.html","title":"Introduction","content":"Introduction ​aos introduces a new approach to building processes — asynchronous, parallel-executing smart contracts. The ao computer is a decentralized computer network that allows compute to run anywhere and aos in a unique, interactive shell. You can use aos as your personal operating system, your development environment for building ao processes, and your bot army.Lets go over some basic commands.Variables ​If you want to display the contents of any variable through the console, simply type the variable name.luaNameInbox ​the Inbox is a collection of messages that your Process has received.luaInbox[1]If you want to get a count of messages, just add the # infront of Inbox.lua#InboxThe process of checking how many messages are in the inbox is a very common pattern. To make this easier, you can create a function that returns the number of messages within the inbox and displays it in the prompt.Use either .editor or .load file to load this function on your process.luafunction Prompt() return \"Inbox: \" .. #Inbox .. \" > \" endThe Expected Results:luaundefined Inbox: 2 >Your prompt now has changed to include the number of messages in your inbox.INFOThe Inbox is a Lua table (similar to an array) that contains messages received by your process that were not handled by any Handlers. The # operator is used to get the length of a table in Lua - so #Inbox returns the total number of unhandled messages currently in your inbox. This is a common Lua syntax pattern for getting the size/length of tables and strings.Globals ​In aos process there are some Globals that can make development a little more intuitive.NameDescriptionTypeInboxThis is a lua Table that stores all the messages that are received and not handlers by any handlers.Table(Array)Send(Message)This is a global function that is available in the interactive environment that allows you to send messages to ProcessesfunctionSpawn(Module, Message)This is a global function that is available in the aos interactive environment that allows you to spawn processesNamea string that is set on init that describes the name of your processstringOwnera string that is set on the init of the process that documents the owner of the process, warning if you change this value, it can brick you ability to interact with your processstringHandlersa lua Table that contains helper functions that allows you to create handlers that execute functionality based on the pattern matching function on inbound messagestableDumpa function that takes any lua Table and generates a print friendly output of the datafunctionUtilsa functional utility library with functions like map, reduce, filtermoduleaothis is a core function library for sending messages and spawing processesmoduleModules ​In aos there are some built in common lua modules that are already available for you to work with, these modules can be referenced with a \"require\" function.NameDescriptionjsona json module that allows you to encode and decode json documentsaocontains ao specific functions like send and spawn.base64a base64 module that allows you to encode and decode base64 text.prettya pretty print module using the function tprint to output formatted syntax.utilsan utility function library","estimatedWords":498,"lastModified":"2025-10-20T12:10:16.006Z","breadcrumbs":["guides","aos","intro"],"siteKey":"ao","siteName":"AO Cookbook","depth":3,"crawledAt":"2025-10-20T12:10:16.006Z"},{"url":"https://cookbook_ao.arweave.net/guides/aos/index.html","title":"aos AO Operating System","content":"aos: AO Operating System ​aos is a powerful operating system built on top of the AO hyper-parallel computer. While AO provides the distributed compute infrastructure, aos offers a simplified interface for interacting with and developing processes in this environment.What is aos? ​aos enables you to:Create and interact with processes on the AO networkDevelop distributed applications using a simple, intuitive approachLeverage the Lua programming language for deterministic, reliable operationsAll you need to get started is a terminal and a code editor. aos uses Lua as its primary language - a robust, deterministic, and user-friendly programming language that's ideal for distributed applications.New to AO? If you're just getting started, we recommend completing our tutorials first. They take just 15-30 minutes and provide an excellent foundation.Getting Started with aos ​Start here if you're new to aos:Introduction to aos - Overview of aos capabilities and conceptsInstallation Guide - Step-by-step instructions for setting up aosaos Command Line Interface - Learn to use the aos CLI effectivelyCustomizing Your Prompt - Personalize your aos development environmentLoad Lua Files - Learn how to load and execute Lua files in aosBuilding a Ping-Pong Server - Create your first interactive aos applicationBlueprints ​Blueprints in aos are templates that streamline the development of distributed applications by providing a framework for creating consistent and efficient processes across the AO network.Available Blueprints ​Chatroom - Template for building chatroom applicationsCred Utils - Tools for managing credentialsStaking - Framework for implementing staking mechanismsToken - Guide for creating and managing tokensVoting - Blueprint for setting up voting systemsaos Modules ​aos includes several built-in modules for common operations:JSON Module - Parse and generate JSON dataAO Module - Interface with the AO ecosystemCrypto Module - Perform cryptographic operationsBase64 Module - Encode and decode Base64 dataPretty Module - Format data for easier readingUtils Module - Common utility functionsDeveloper Resources ​More advanced topics for aos development:Editor Setup & Configuration - Configure your development environmentUnderstanding the Inbox & Message Handlers - Learn how message handling worksTroubleshooting with ao.link - Debug aos applicationsFrequently Asked Questions - Find answers to common questionsBuild a Token - Create your own token on AONavigation ​Use the sidebar to browse through specific aos guides. For a more structured learning path, we recommend following the guides in the order listed above.","estimatedWords":373,"lastModified":"2025-10-20T12:10:16.843Z","breadcrumbs":["guides","aos","index"],"siteKey":"ao","siteName":"AO Cookbook","depth":3,"crawledAt":"2025-10-20T12:10:16.843Z"},{"url":"https://cookbook_ao.arweave.net/guides/index.html","title":"Guides","content":"Guides ​This section provides detailed guides and documentation to help you build and deploy applications on the AO ecosystem. Whether you're creating chatrooms, autonomous bots, or complex decentralized applications, you'll find step-by-step instructions here.Core Technologies ​Comprehensive guides for AO's main technologies:AOS: Compute on AO - Learn how to use the AO operating system for distributed computingIntroduction to AOS - Get started with the AOS environmentInstallation Guide - Set up AOS on your systemCLI Usage - Learn the command-line interfaceAnd more...AO Connect: JavaScript Library - Interact with AO using JavaScriptInstallation - Set up the AO Connect libraryConnecting to AO - Establish connectionsSending Messages - Communicate with processesAnd more...Development Tools ​AO Module Builder CLI - Build WebAssembly modules for AO Installation - Install the development CLIProject Setup - Create your first projectBuilding & Deployment - Compile and deploy modulesUtilities & Storage ​Helpful tools and storage solutions:Using WeaveDrive - Store and manage data with WeaveDriveUsing SQLite - Integrate SQLite databases with your AO projectsAdditional Resources ​Community Resources - Connect with the AO communityRelease Notes - Stay updated on the latest changes and featuresNavigation ​Use the sidebar to browse through specific guides. Each guide provides detailed instructions and examples to help you build on AO.","estimatedWords":201,"lastModified":"2025-10-20T12:10:17.711Z","breadcrumbs":["guides","index"],"siteKey":"ao","siteName":"AO Cookbook","depth":2,"crawledAt":"2025-10-20T12:10:17.711Z"},{"url":"https://cookbook_ao.arweave.net/tutorials/bots-and-games/build-game.html","title":"Expanding the Arena","content":"Expanding the Arena ​Welcome to the final guide of Chapter 2, where you'll learn to build your own game on top of the arena framework introduced in the previous tutorial. In this guide, we'll take you through the process of creating the \"ao-effect\" game, which you experienced at the beginning of this chapter. As you progress through this example, you'll gain insights into structuring your game's logic and interacting with the arena's core code.Whether you're a seasoned developer or an aspiring game creator, this guide will empower you to unleash your creativity and bring your unique game ideas to life within the aos environment.Setting up the Development Environment ​Start by creating a new file named ao-effect.lua in your preferred directory.NOTEIdeally, this file should be placed in the same directory where your game process runs to ease the loading of the code. Else, you'll need to use relative paths to access the file.Writing the Code ​Now, let's dive into the logic.You'll notice that your game logic will involve calling functions and variables defined in the arena's logic. This showcases the power of composability, where your game builds on top of the existing arena logic, allowing seamless integration of variables and functions between the two. Because both logic become part of a unified logic for the game process.Initializing Game Mechanics ​First, define essential variables and functions that set the stage for your game's mechanics:lua-- AO EFFECT: Game Mechanics for AO Arena Game -- Game grid dimensions Width = 40 -- Width of the grid Height = 40 -- Height of the grid Range = 1 -- The distance for blast effect -- Player energy settings MaxEnergy = 100 -- Maximum energy a player can have EnergyPerSec = 1 -- Energy gained per second -- Attack settings AverageMaxStrengthHitsToKill = 3 -- Average number of hits to eliminate a player -- Initializes default player state -- @return Table representing player's initial state function playerInitState() return { x = math.random(Width/8), y = math.random(Height/8), health = 100, energy = 0 } end -- Function to incrementally increase player's energy -- Called periodically to update player energy function onTick() if GameMode ~= \"Playing\" then return end -- Only active during \"Playing\" state if LastTick == undefined then LastTick = Now end local Elapsed = Now - LastTick if Elapsed >= 1000 then -- Actions performed every second for player, state in pairs(Players) do local newEnergy = math.floor(math.min(MaxEnergy, state.energy + (Elapsed * EnergyPerSec // 2000))) state.energy = newEnergy end LastTick = Now end endThis code initializes your game's mechanics, including grid dimensions, player energy, and attack settings. The playerInitState function sets up the initial state for players when the game begins.Player Movement ​Next, add the code for player movement:lua-- Handles player movement -- @param msg: Message request sent by player with movement direction and player info function move(msg) local playerToMove = msg.From local direction = msg.Tags.Direction local directionMap = { Up = {x = 0, y = -1}, Down = {x = 0, y = 1}, Left = {x = -1, y = 0}, Right = {x = 1, y = 0}, UpRight = {x = 1, y = -1}, UpLeft = {x = -1, y = -1}, DownRight = {x = 1, y = 1}, DownLeft = {x = -1, y = 1} } -- calculate and update new coordinates if directionMap[direction] then local newX = Players[playerToMove].x + directionMap[direction].x local newY = Players[playerToMove].y + directionMap[direction].y -- updates player coordinates while checking for grid boundaries Players[playerToMove].x = (newX - 1) % Width + 1 Players[playerToMove].y = (newY - 1) % Height + 1 announce(\"Player-Moved\", playerToMove .. \" moved to \" .. Players[playerToMove].x .. \",\" .. Players[playerToMove].y .. \".\") else ao.send({Target = playerToMove, Action = \"Move-Failed\", Reason = \"Invalid direction.\"}) end onTick() -- Optional: Update energy each move endThe move function calculates new player coordinates based on the chosen direction while ensuring that players remain within the grid boundaries. Player movement adds dynamic interaction to your game and is announced to all players and listeners.Player Attacks ​Then you must implement the logic for player attacks:lua-- Handles player attacks -- @param msg: Message request sent by player with attack info and player state function attack(msg) local player = msg.From local attackEnergy = tonumber(msg.Tags.AttackEnergy) -- get player coordinates local x = Players[player].x local y = Players[player].y -- check if player has enough energy to attack if Players[player].energy = (x1 - range) and x2 = (y1 - range) and y2 = 1000 then -- Actions performed every second for player, state in pairs(Players) do local newEnergy = math.floor(math.min(MaxEnergy, state.energy + (Elapsed * EnergyPerSec // 2000))) state.energy = newEnergy end LastTick = Now end end -- Handles player movement -- @param msg: Message request sent by player with movement direction and player info function move(msg) local playerToMove = msg.From local direction = msg.Tags.Direction local directionMap = { Up = {x = 0, y = -1}, Down = {x = 0, y = 1}, Left = {x = -1, y = 0}, Right = {x = 1, y = 0}, UpRight = {x = 1, y = -1}, UpLeft = {x = -1, y = -1}, DownRight = {x = 1, y = 1}, DownLeft = {x = -1, y = 1} } -- calculate and update new coordinates if directionMap[direction] then local newX = Players[playerToMove].x + directionMap[direction].x local newY = Players[playerToMove].y + directionMap[direction].y -- updates player coordinates while checking for grid boundaries Players[playerToMove].x = (newX - 1) % Width + 1 Players[playerToMove].y = (newY - 1) % Height + 1 announce(\"Player-Moved\", playerToMove .. \" moved to \" .. Players[playerToMove].x .. \",\" .. Players[playerToMove].y .. \".\") else ao.send({Target = playerToMove, Action = \"Move-Failed\", Reason = \"Invalid direction.\"}) end onTick() -- Optional: Update energy each move end -- Handles player attacks -- @param msg: Message request sent by player with attack info and player state function attack(msg) local player = msg.From local attackEnergy = tonumber(msg.Tags.AttackEnergy) -- get player coordinates local x = Players[player].x local y = Players[player].y -- check if player has enough energy to attack if Players[player].energy = (x1 - range) and x2 = (y1 - range) and y2 <= (y1 + range) end -- HANDLERS: Game state management for AO-Effect -- Handler for player movement Handlers.add(\"PlayerMove\", { Action = \"PlayerMove\" }, move) -- Handler for player attacks Handlers.add(\"PlayerAttack\", { Action = \"PlayerAttack\" }, attack)Loading and Testing ​Once you've written your game code, it's time to load it into the aos game process and test your game:lua.load ao-effect.luaIMPORTANTMake sure to load the arena blueprint in the same process as well.Invite friends or create test player processes to experience your game and make any necessary adjustments for optimal performance.What's Next ​Congratulations! You've successfully expanded the arena by building your own game on top of its core functionalities. Armed with the knowledge and tools acquired in this guide, you're now equipped to build games on aos independently.The possibilities are endless. Continue adding more features to existing games or create entirely new ones. The sky's the limit! ⌃◦🚀","estimatedWords":1161,"lastModified":"2025-10-20T12:10:18.384Z","breadcrumbs":["tutorials","bots and games","build game"],"siteKey":"ao","siteName":"AO Cookbook","depth":3,"crawledAt":"2025-10-20T12:10:18.384Z"},{"url":"https://cookbook_ao.arweave.net/tutorials/bots-and-games/arena-mechanics.html","title":"Mechanics of the Arena","content":"Mechanics of the Arena ​This guide provides a comprehensive overview of the fundamental mechanics essential for designing and managing arena-style games in aos. In arena games, participants engage in rounds, strategically vying to eliminate each other until a sole victor emerges.The framework presented here lays the groundwork for crafting a wide range of games, all sharing the same core functionalities. Explore the intricacies of game development and unleash your creativity within this versatile arena.Core Functionalities ​Now, let's dive into the core functionalities that power arena-style games:Game Progression Modes:Arena games are structured into rounds that operate in a loop with the following progression modes: \"Not-Started\" → \"Waiting\" → \"Playing\" → [Someone wins or timeout] → \"Waiting\"...NOTEThe loop timesout if there are not enough players to start a game after the waiting state.Rounds offer a defined timeframe for players to engage, intensifying the excitement of gameplay.Token Stakes:Players must deposit a specified quantity of tokens (defined by PaymentQty) to participate in the game. These tokens add a tangible stake element to the game.Bonus Rewards:Beyond the thrill of victory, players are enticed by the prospect of extra rewards. The builder has the flexibility to offer bonus tokens, defined by BonusQty, to be distributed per round. Any bets placed by players are also added to these bonuses. These bonuses serve as an additional incentive, enhancing the competitive spirit of the gameplay.Player Management:Players waiting to join the next game are tracked in the Waiting table.Active players and their game states are stored in the Players table.Eliminated players are promptly removed from the Players table and placed in the Waiting table for the next game.Round Winner Reward:When a player eliminates another, they earn not only bragging rights but also the eliminated player's deposit tokens as a reward. Additionally, winners of each round share a portion of the bonus tokens, as well as their original stake, further motivating players to strive for victory.Listener Mode:For those who prefer to watch the action unfold, the \"Listen\" mode offers an opportunity to stay informed without active participation. Processes can register as listeners, granting them access to all announcements from the game. While they do not engage as players, listeners can continue to observe the game's progress unless they explicitly request removal.Game State Management:To maintain the flow and fairness of arena games, an automated system oversees game state transitions. These transitions encompass waiting, playing, and ending phases. Time durations for each state, such as WaitTime and GameTime, ensure that rounds adhere to defined timeframes, preventing games from lasting indefinitely.You can refer to the code for the arena in the dropdown below:Arena Game Blueprintlua-- ARENA GAME BLUEPRINT. -- This blueprint provides the framework to operate an 'arena' style game -- inside an ao process. Games are played in rounds, where players aim to -- eliminate one another until only one remains, or until the game time -- has elapsed. The game process will play rounds indefinitely as players join -- and leave. -- When a player eliminates another, they receive the eliminated player's deposit token -- as a reward. Additionally, the builder can provide a bonus of these tokens -- to be distributed per round as an additional incentive. If the intended -- player type in the game is a bot, providing an additional 'bonus' -- creates an opportunity for coders to 'mine' the process's -- tokens by competing to produce the best agent. -- The builder can also provide other handlers that allow players to perform -- actions in the game, calling 'eliminatePlayer()' at the appropriate moment -- in their game logic to control the framework. -- Processes can also register in a 'Listen' mode, where they will receive -- all announcements from the game, but are not considered for entry into the -- rounds themselves. They are also not unregistered unless they explicitly ask -- to be. -- GLOBAL VARIABLES. -- Game progression modes in a loop: -- [Not-Started] -> Waiting -> Playing -> [Someone wins or timeout] -> Waiting... -- The loop is broken if there are not enough players to start a game after the waiting state. GameMode = GameMode or \"Not-Started\" StateChangeTime = StateChangeTime or undefined -- State durations (in milliseconds) WaitTime = WaitTime or 2 * 60 * 1000 -- 2 minutes GameTime = GameTime or 20 * 60 * 1000 -- 20 minutes Now = Now or undefined -- Current time, updated on every message. -- Token information for player stakes. UNIT = 1000 PaymentToken = PaymentToken or \"ADDR\" -- Token address PaymentQty = PaymentQty or tostring(math.floor(UNIT)) -- Quantity of tokens for registration BonusQty = BonusQty or tostring(math.floor(UNIT)) -- Bonus token quantity for winners -- Players waiting to join the next game and their payment status. Waiting = Waiting or {} -- Active players and their game states. Players = Players or {} -- Number of winners in the current game. Winners = 0 -- Processes subscribed to game announcements. Listeners = Listeners or {} -- Minimum number of players required to start a game. MinimumPlayers = MinimumPlayers or 2 -- Default player state initialization. PlayerInitState = PlayerInitState or {} -- Sends a state change announcement to all registered listeners. -- @param event: The event type or name. -- @param description: Description of the event. function announce(event, description) for ix, address in pairs(Listeners) do ao.send({ Target = address, Action = \"Announcement\", Event = event, Data = description }) end return print(Colors.gray .. \"Announcement: \" .. Colors.red .. event .. \" \" .. Colors.blue .. description .. Colors.reset) end -- Sends a reward to a player. -- @param recipient: The player receiving the reward. -- @param qty: The quantity of the reward. -- @param reason: The reason for the reward. function sendReward(recipient, qty, reason) if type(qty) ~= number then qty = tonumber(qty) end ao.send({ Target = PaymentToken, Action = \"Transfer\", Quantity = tostring(qty), Recipient = recipient, Reason = reason }) return print(Colors.gray .. \"Sent Reward: \" .. Colors.blue .. tostring(qty) .. Colors.gray .. ' tokens to ' .. Colors.green .. recipient .. \" \" .. Colors.blue .. reason .. Colors.reset ) end -- Starts the waiting period for players to become ready to play. function startWaitingPeriod() GameMode = \"Waiting\" StateChangeTime = Now + WaitTime announce(\"Started-Waiting-Period\", \"The game is about to begin! Send your token to take part.\") print('Starting Waiting Period') end -- Starts the game if there are enough players. function startGamePeriod() local paidPlayers = 0 for player, hasPaid in pairs(Waiting) do if hasPaid then paidPlayers = paidPlayers + 1 end end if paidPlayers 0 then table.remove(Listeners, idx) end end -- HANDLERS: Game state management -- Handler for cron messages, manages game state transitions. Handlers.add( \"Game-State-Timers\", function(Msg) return \"continue\" end, function(Msg) Now = Msg.Timestamp if GameMode == \"Not-Started\" then startWaitingPeriod() elseif GameMode == \"Waiting\" then if Now > StateChangeTime then startGamePeriod() end elseif GameMode == \"Playing\" then if onTick and type(onTick) == \"function\" then onTick() end if Now > StateChangeTime then endGame() end end end ) -- Handler for player deposits to participate in the next game. Handlers.add( \"Transfer\", function(Msg) return Msg.Action == \"Credit-Notice\" and Msg.From == PaymentToken and tonumber(Msg.Quantity) >= tonumber(PaymentQty) and \"continue\" end, function(Msg) Waiting[Msg.Sender] = true ao.send({ Target = Msg.Sender, Action = \"Payment-Received\" }) announce(\"Player-Ready\", Msg.Sender .. \" is ready to play!\") end ) -- Registers new players for the next game and subscribes them for event info. Handlers.add( \"Register\", { Action = \"Register\" }, function(Msg) if Msg.Mode ~= \"Listen\" and Waiting[Msg.From] == undefined then Waiting[Msg.From] = false end removeListener(Msg.From) table.insert(Listeners, Msg.From) ao.send({ Target = Msg.From, Action = \"Registered\" }) announce(\"New Player Registered\", Msg.From .. \" has joined in waiting.\") end ) -- Unregisters players and stops sending them event info. Handlers.add( \"Unregister\", { Action = \"Unregister\" }, function(Msg) removeListener(Msg.From) ao.send({ Target = Msg.From, Action = \"Unregistered\" }) end ) -- Adds bet amount to BonusQty Handlers.add( \"AddBet\", { Reason = \"AddBet\" }, function(Msg) BonusQty = tonumber(BonusQty) + tonumber(Msg.Tags.Quantity) announce(\"Bet-Added\", Msg.From .. \"has placed a bet. \" .. \"BonusQty amount increased by \" .. Msg.Tags.Quantity .. \"!\") end ) -- Retrieves the current game state. Handlers.add( \"GetGameState\", { Action = \"GetGameState\" }, function (Msg) local json = require(\"json\") local TimeRemaining = StateChangeTime - Now local GameState = json.encode({ GameMode = GameMode, TimeRemaining = TimeRemaining, Players = Players, }) ao.send({ Target = Msg.From, Action = \"GameState\", Data = GameState}) end ) -- Alerts users regarding the time remaining in each game state. Handlers.add( \"AnnounceTick\", { Action = \"Tick\" }, function (Msg) local TimeRemaining = StateChangeTime - Now if GameMode == \"Waiting\" then announce(\"Tick\", \"The game will start in \" .. (TimeRemaining/1000) .. \" seconds.\") elseif GameMode == \"Playing\" then announce(\"Tick\", \"The game will end in \" .. (TimeRemaining/1000) .. \" seconds.\") end end ) -- Sends tokens to players with no balance upon request Handlers.add( \"RequestTokens\", { Action = \"RequestTokens\" }, function (Msg) print(\"Transferring Tokens: \" .. tostring(math.floor(10000 * UNIT))) ao.send({ Target = ao.id, Action = \"Transfer\", Quantity = tostring(math.floor(10000 * UNIT)), Recipient = Msg.From, }) end )Arena Game Blueprint ​For those interested in using this arena framework, we've made this code easily accessible through a blueprint. Simply run the following code in your terminal:lua.load-blueprint arenaSummary ​Understanding the mechanics of the arena can not only help you improve your autonomous agent created in the previous section but also empowers you to harness core functionalities for crafting your unique games.In the upcoming section, \"Building a Game,\" we will dive deep into the art of utilizing these mechanics to construct captivating and one-of-a-kind games within this framework. Get ready to embark on a journey into the dynamic realm of game development! 🎮","estimatedWords":1585,"lastModified":"2025-10-20T12:10:19.154Z","breadcrumbs":["tutorials","bots and games","arena mechanics"],"siteKey":"ao","siteName":"AO Cookbook","depth":3,"crawledAt":"2025-10-20T12:10:19.154Z"},{"url":"https://cookbook_ao.arweave.net/tutorials/bots-and-games/bringing-together.html","title":"Bringing it Together","content":"Bringing it Together ​This final guide wraps up our series, where you've built up an autonomous agent piece by piece. Now, let's refine your agent with some optimizations that fine-tune its operations. Here's a quick overview of the key improvements made:Sequential Command Execution: The introduction of an InAction flag ensures that your agent's actions are sequential (next action occurs only when the previous is successfully executed). This critical addition prevents your agent from acting on outdated game states, enhancing its responsiveness and accuracy. The full implementation can be found in the final code for the bot.lua file below.luaInAction = InAction or false -- Prevents the agent from taking multiple actions at once.Dynamic State Updates and Decisions: The agent now employs an automatic tick logic, allowing for dynamic updates and decisions. This logic enables the agent to self-trigger state updates and make subsequent decisions either upon receiving a Tick message or upon completing an action, promoting autonomous operation.luaHandlers.add(\"GetGameStateOnTick\", { Action = \"Tick\" }, function () if not InAction then InAction = true ao.send({Target = Game, Action = \"GetGameState\"}) end end)Automated Fee Transfer: To further streamline its operation and ensure uninterrupted participation in games, the autonomous agent now autonomously handles the transfer of confirmation fees.luaHandlers.add(\"AutoPay\", { Action = \"AutoPay\" }, function () ao.send({Target = Game, Action = \"Transfer\", Recipient = Game, Quantity = \"1000\"}) end)In addition to these features, we've also added a logging function for debugging purposes and colored prints for better comprehension of game events. These enhancements collectively make your autonomous agent more efficient and adaptable in the game environment.Check out the complete bot.lua code in the dropdown below, with all new additions highlighted accordingly:Updated bot.lua filelua-- Initializing global variables to store the latest game state and game host process. LatestGameState = LatestGameState or nil InAction = InAction or false -- Prevents the agent from taking multiple actions at once. Logs = Logs or {} colors = { red = \"\\27[31m\", green = \"\\27[32m\", blue = \"\\27[34m\", reset = \"\\27[0m\", gray = \"\\27[90m\" } function addLog(msg, text) -- Function definition commented for performance, can be used for debugging Logs[msg] = Logs[msg] or {} table.insert(Logs[msg], text) end -- Checks if two points are within a given range. -- @param x1, y1: Coordinates of the first point. -- @param x2, y2: Coordinates of the second point. -- @param range: The maximum allowed distance between the points. -- @return: Boolean indicating if the points are within the specified range. function inRange(x1, y1, x2, y2, range) return math.abs(x1 - x2) 5 and targetInRange then print(colors.red .. \"Player in range. Attacking.\" .. colors.reset) ao.send({Target = Game, Action = \"PlayerAttack\", Player = ao.id, AttackEnergy = tostring(player.energy)}) else print(colors.red .. \"No player in range or insufficient energy. Moving randomly.\" .. colors.reset) local directionMap = {\"Up\", \"Down\", \"Left\", \"Right\", \"UpRight\", \"UpLeft\", \"DownRight\", \"DownLeft\"} local randomIndex = math.random(#directionMap) ao.send({Target = Game, Action = \"PlayerMove\", Player = ao.id, Direction = directionMap[randomIndex]}) end InAction = false -- InAction logic added end -- Handler to print game announcements and trigger game state updates. Handlers.add( \"PrintAnnouncements\", { Action = \"Announcement\" }, function (msg) if msg.Event == \"Started-Waiting-Period\" then ao.send({Target = ao.id, Action = \"AutoPay\"}) elseif (msg.Event == \"Tick\" or msg.Event == \"Started-Game\") and not InAction then InAction = true -- InAction logic added ao.send({Target = Game, Action = \"GetGameState\"}) elseif InAction then -- InAction logic added print(\"Previous action still in progress. Skipping.\") end print(colors.green .. msg.Event .. \": \" .. msg.Data .. colors.reset) end ) -- Handler to trigger game state updates. Handlers.add( \"GetGameStateOnTick\", { Action = \"Tick\" }, function () if not InAction then -- InAction logic added InAction = true -- InAction logic added print(colors.gray .. \"Getting game state...\" .. colors.reset) ao.send({Target = Game, Action = \"GetGameState\"}) else print(\"Previous action still in progress. Skipping.\") end end ) -- Handler to automate payment confirmation when waiting period starts. Handlers.add( \"AutoPay\", { Action = \"AutoPay\" }, function (msg) print(\"Auto-paying confirmation fees.\") ao.send({ Target = Game, Action = \"Transfer\", Recipient = Game, Quantity = \"1000\"}) end ) -- Handler to update the game state upon receiving game state information. Handlers.add( \"UpdateGameState\", { Action = \"GameState\" }, function (msg) local json = require(\"json\") LatestGameState = json.decode(msg.Data) ao.send({Target = ao.id, Action = \"UpdatedGameState\"}) print(\"Game state updated. Print \\'LatestGameState\\' for detailed view.\") end ) -- Handler to decide the next best action. Handlers.add( \"decideNextAction\", { Action = \"UpdatedGameState\" }, function () if LatestGameState.GameMode ~= \"Playing\" then InAction = false -- InAction logic added return end print(\"Deciding next action.\") decideNextAction() ao.send({Target = ao.id, Action = \"Tick\"}) end ) -- Handler to automatically attack when hit by another player. Handlers.add( \"ReturnAttack\", { Action = \"Hit\" }, function (msg) if not InAction then -- InAction logic added InAction = true -- InAction logic added local playerEnergy = LatestGameState.Players[ao.id].energy if playerEnergy == undefined then print(colors.red .. \"Unable to read energy.\" .. colors.reset) ao.send({Target = Game, Action = \"Attack-Failed\", Reason = \"Unable to read energy.\"}) elseif playerEnergy == 0 then print(colors.red .. \"Player has insufficient energy.\" .. colors.reset) ao.send({Target = Game, Action = \"Attack-Failed\", Reason = \"Player has no energy.\"}) else print(colors.red .. \"Returning attack.\" .. colors.reset) ao.send({Target = Game, Action = \"PlayerAttack\", Player = ao.id, AttackEnergy = tostring(playerEnergy)}) end InAction = false -- InAction logic added ao.send({Target = ao.id, Action = \"Tick\"}) else print(\"Previous action still in progress. Skipping.\") end end )What's next? ​You're now equipped with the knowledge to craft intelligent autonomous agents. It's time to apply these insights into the game world. Understand the game's intricacies and leverage your agent's capabilities to dominate the arena. But there's more to come.In future sections, we'll dive deeper into the game arena, offering advanced strategies to elevate your agent's performance. Ready to take on the challenge? Let's see what you can create! 🕹️","estimatedWords":951,"lastModified":"2025-10-20T12:10:19.546Z","breadcrumbs":["tutorials","bots and games","bringing together"],"siteKey":"ao","siteName":"AO Cookbook","depth":3,"crawledAt":"2025-10-20T12:10:19.546Z"},{"url":"https://cookbook_ao.arweave.net/tutorials/bots-and-games/attacking.html","title":"Automated Responses","content":"Automated Responses ​Following our last guide, our creation has progressed from a simple bot to a sophisticated autonomous agent. Now, let's further enhance its capabilities by adding a counterattack feature, allowing it to instantly retaliate against an opponent's attack, potentially catching them off-guard before they can retreat to safety.Writing the code ​Add the following handler to your bot.lua file and you're set:lua-- Handler to automatically attack when hit by another player. Handlers.add( \"ReturnAttack\", { Action = \"Hit\" }, function (msg) local playerEnergy = LatestGameState.Players[ao.id].energy if playerEnergy == undefined then print(\"Unable to read energy.\") ao.send({Target = Game, Action = \"Attack-Failed\", Reason = \"Unable to read energy.\"}) elseif playerEnergy == 0 then print(\"Player has insufficient energy.\") ao.send({Target = Game, Action = \"Attack-Failed\", Reason = \"Player has no energy.\"}) else print(\"Returning attack.\") ao.send({Target = Game, Action = \"PlayerAttack\", Player = ao.id, AttackEnergy = tostring(playerEnergy)}) end InAction = false ao.send({Target = ao.id, Action = \"Tick\"}) end )Whenever your player is under attack you receive a message with the Action Hit. This setup ensures your agent can make a swift counter attack, given it has sufficient energy.You can refer to the latest code for bot.lua in the dropdown below:Updated bot.lua fileluaLatestGameState = LatestGameState or nil function inRange(x1, y1, x2, y2, range) return math.abs(x1 - x2) 5 and targetInRange then print(\"Player in range. Attacking.\") ao.send({Target = Game, Action = \"PlayerAttack\", Player = ao.id, AttackEnergy = tostring(player.energy)}) else print(\"No player in range or insufficient energy. Moving randomly.\") local directionMap = {\"Up\", \"Down\", \"Left\", \"Right\", \"UpRight\", \"UpLeft\", \"DownRight\", \"DownLeft\"} local randomIndex = math.random(#directionMap) ao.send({Target = Game, Action = \"PlayerMove\", Player = ao.id, Direction = directionMap[randomIndex]}) end end Handlers.add( \"HandleAnnouncements\", { Action = \"Announcement\" }, function (msg) ao.send({Target = Game, Action = \"GetGameState\"}) print(msg.Event .. \": \" .. msg.Data) end ) Handlers.add( \"UpdateGameState\", { Action = \"GameState\" }, function (msg) local json = require(\"json\") LatestGameState = json.decode(msg.Data) ao.send({Target = ao.id, Action = \"UpdatedGameState\"}) end ) Handlers.add( \"decideNextAction\", { Action = \"UpdatedGameState\" }, function () if LatestGameState.GameMode ~= \"Playing\" then return end print(\"Deciding next action.\") decideNextAction() end ) Handlers.add( \"ReturnAttack\", { Action = \"Hit\" }, function (msg) local playerEnergy = LatestGameState.Players[ao.id].energy if playerEnergy == undefined then print(\"Unable to read energy.\") ao.send({Target = Game, Action = \"Attack-Failed\", Reason = \"Unable to read energy.\"}) elseif playerEnergy == 0 then print(\"Player has insufficient energy.\") ao.send({Target = Game, Action = \"Attack-Failed\", Reason = \"Player has no energy.\"}) else print(\"Returning attack.\") ao.send({Target = Game, Action = \"PlayerAttack\", Player = ao.id, AttackEnergy = tostring(playerEnergy)}) end InAction = false ao.send({Target = ao.id, Action = \"Tick\"}) end )Loading and Testing ​To activate and test the counter attack feature, load the bot file in your aos player terminal:lua.load bot.luaWatch your terminal for the autonomous agent's reactions, now with the added ability to retaliate instantly. This feature showcases the agent's evolving strategic depth and autonomy. In the upcoming section, we'll consolidate all the knowledge we've gathered so far and add some features for optimization.","estimatedWords":484,"lastModified":"2025-10-20T12:10:20.369Z","breadcrumbs":["tutorials","bots and games","attacking"],"siteKey":"ao","siteName":"AO Cookbook","depth":3,"crawledAt":"2025-10-20T12:10:20.369Z"},{"url":"https://cookbook_ao.arweave.net/tutorials/bots-and-games/decisions.html","title":"Strategic Decisions","content":"Strategic Decisions ​With the latest game state at your disposal, your bot can evolve into an autonomous agent. This transition marks an upgrade in functionality, enabling not just reactions to game states but strategic actions that consider context, energy, and proximity to make decisions.Writing the Code ​Return to your bot.lua file and add the following functions:lua-- Determines proximity between two points. function inRange(x1, y1, x2, y2, range) return math.abs(x1 - x2) 5 and targetInRange then print(\"Player in range. Attacking.\") ao.send({Target = Game, Action = \"PlayerAttack\", Player = ao.id, AttackEnergy = tostring(player.energy)}) else print(\"No player in range or insufficient energy. Moving randomly.\") local directionMap = {\"Up\", \"Down\", \"Left\", \"Right\", \"UpRight\", \"UpLeft\", \"DownRight\", \"DownLeft\"} local randomIndex = math.random(#directionMap) ao.send({Target = Game, Action = \"PlayerMove\", Player = ao.id, Direction = directionMap[randomIndex]}) end endThe decideNextAction function is now a testament to our agent's ability to think and act based on a comprehensive understanding of its environment. It analyzes the latest game state to either attack if you have sufficient energy and an opponent is inRange or move otherwise.Now all you need is a handler to make sure this function runs on its own.luaHandlers.add( \"decideNextAction\", { Action = \"UpdatedGameState\" }, function () if LatestGameState.GameMode ~= \"Playing\" then return end print(\"Deciding next action.\") decideNextAction() end )This handler triggers upon receiving a message that the latest game state has been fetched and updated. An action is taken only when the game is in Playing mode.You can refer to the latest code for bot.lua in the dropdown below:Updated bot.lua fileluaLatestGameState = LatestGameState or nil function inRange(x1, y1, x2, y2, range) return math.abs(x1 - x2) 5 and targetInRange then print(\"Player in range. Attacking.\") ao.send({Target = Game, Action = \"PlayerAttack\", Player = ao.id, AttackEnergy = tostring(player.energy)}) else print(\"No player in range or insufficient energy. Moving randomly.\") local directionMap = {\"Up\", \"Down\", \"Left\", \"Right\", \"UpRight\", \"UpLeft\", \"DownRight\", \"DownLeft\"} local randomIndex = math.random(#directionMap) ao.send({Target = Game, Action = \"PlayerMove\", Player = ao.id, Direction = directionMap[randomIndex]}) end end Handlers.add( \"HandleAnnouncements\", { Action = \"Announcement\" }, function (msg) ao.send({Target = Game, Action = \"GetGameState\"}) print(msg.Event .. \": \" .. msg.Data) end ) Handlers.add( \"UpdateGameState\", { Action = \"GameState\" }, function (msg) local json = require(\"json\") LatestGameState = json.decode(msg.Data) ao.send({Target = ao.id, Action = \"UpdatedGameState\"}) end ) Handlers.add( \"decideNextAction\", { Action = \"UpdatedGameState\" }, function () if LatestGameState.GameMode ~= \"Playing\" then return end print(\"Deciding next action.\") decideNextAction() end )Loading and Testing ​Once again, to test out the latest upgrades, load the file in your aos player terminal as follows:lua.load bot.luaObserve your process output to see the decisions your autonomous agent makes in real-time, leveraging the current game state for strategic advantage. But what if another player attacks you and runs away while you are deciding the next move? In the next section you'll learn to automatically counter as soon as you have been attacked 🤺","estimatedWords":470,"lastModified":"2025-10-20T12:10:20.990Z","breadcrumbs":["tutorials","bots and games","decisions"],"siteKey":"ao","siteName":"AO Cookbook","depth":3,"crawledAt":"2025-10-20T12:10:20.990Z"},{"url":"https://cookbook_ao.arweave.net/tutorials/bots-and-games/game-state.html","title":"Fetching Game State","content":"Fetching Game State ​Now that you're seeing game announcements directly in your terminal, you have a better grasp of the game's dynamics. However, these insights are limited to specific actions occurring within the game.Wouldn't it be more useful to have on-demand access to comprehensive game data, like the positions, health, and energy of all players? This information could significantly improve your strategic planning, helping you assess threats, opportunities, and timing more effectively.If you thought of adding another handler to the bot created in the previous guide, you're absolutely right!Writing the Code ​Go back to your bot.lua file and update your existing handler as follows:luaHandlers.add( \"HandleAnnouncements\", { Action = \"Announcement\" }, function (msg) ao.send({Target = Game, Action = \"GetGameState\"}) print(msg.Event .. \": \" .. msg.Data) end )Adjustments to your handler include:Renaming to \"HandleAnnouncements\" to reflect its broader role.Addition of an extra operation to request the game for the updated state. The game is designed to respond to the GetGameState action tag.When you get a print of the announcement, you can check the latest message in your Inbox as follows:luaInbox[#Inbox]The Data field of this message contains the latest state of the game which includes:GameMode : Whether the game is in Waiting or Playing state.TimeRemaining : The time remaining for the game to start or end.Players : A table containing every player's stats like position, health and energy.But this can be taken a step further so that you can not just read but also use information from the latest state for other automations.Let's define a new variable that stores the latest state as follows:luaLatestGameState = LatestGameState or nilThe syntax preserves existing values of the variable when you load successive iterations of the bot.lua file in your terminal, instead of overwriting it. If there is no pre-existing value then a nil value is assigned to the variable.Then implement another handler as follows:lua-- Handler to update the game state upon receiving game state information. Handlers.add( \"UpdateGameState\", { Action = \"Announcement\" }, function (msg) local json = require(\"json\") LatestGameState = json.decode(msg.Data) ao.send({Target = ao.id, Action = \"UpdatedGameState\"}) print(\"Game state updated. Print \\'LatestGameState\\' for detailed view.\") end )The response from the game process from the previous handler has an action tag with the value GameState that helps us trigger this second handler. Once triggered, the handle function loads the in-built json package that parses the data into json and stores it in the LatestGameState variable.This handler additionally sends a message to your process indicating when the state has been updated. The significance of this feature will be explained in the following section.You can refer to the latest code for bot.lua in the dropdown below:Updated bot.lua fileluaLatestGameState = LatestGameState or nil Handlers.add( \"HandleAnnouncements\", { Action = \"Announcement\" }, function (msg) ao.send({Target = Game, Action = \"GetGameState\"}) print(msg.Event .. \": \" .. msg.Data) end ) Handlers.add( \"UpdateGameState\", { Action = \"GameState\" }, function (msg) local json = require(\"json\") LatestGameState = json.decode(msg.Data) ao.send({Target = ao.id, Action = \"UpdatedGameState\"}) print(\"Game state updated. Print \\'LatestGameState\\' for detailed view.\") end )Loading and Testing ​As usual, to test this new feature, load the file in your aos player terminal as follows:lua.load bot.luaThen check the LatestStateVariable to see if it has updated correctly by simply passing its name as follows:luaLatestGameStateWith real-time access to the latest state of the game you bot is equipped to make informed decisions decide your next action. Next let's try automating actions with the help of this data 🚶","estimatedWords":568,"lastModified":"2025-10-20T12:10:21.592Z","breadcrumbs":["tutorials","bots and games","game state"],"siteKey":"ao","siteName":"AO Cookbook","depth":3,"crawledAt":"2025-10-20T12:10:21.592Z"},{"url":"https://cookbook_ao.arweave.net/tutorials/bots-and-games/announcements.html","title":"Interpreting Announcements","content":"Interpreting Announcements ​Welcome back to your coding journey. It's time to use the skills you've acquired from previous tutorials to enhance your gaming experience.During the game, you've likely noticed announcements appearing in your terminal. These announcements are the game's way of communicating important events to players. However, these messages can sometimes seem cryptic or you might find yourself checking your inbox frequently for further details.Wouldn't it be convenient to access this information directly from your terminal? Well, there's a way to do that!By using handlers, you can create an autonomous agent to retrieve this information for you, marking the progression from simple bots to entities capable of interpreting and acting on game events directly.Setting up the Development Environment ​Start by creating a new file named bot.lua in your preferred directory.Ideally, this file should be placed in the same directory where your player process runs to ease the loading of the code. Else, you'll need to use relative paths to access the file.Writing the Code ​Let's dive into the logic.Each handler in aos requires three key pieces of information:name: A unique name for the handlerpattern: A pattern for the handler to identify, triggering its operationhandle: The operations to perform when the desired pattern is found.Here's how you can write a handler for printing announcement details:lua-- Handler to print game announcements directly in the terminal. Handlers.add( \"PrintAnnouncements\", { Action = \"Announcement\" }, function (msg) print(msg.Event .. \": \" .. msg.Data) end )In this case, the name of the handler is \"PrintAnnouncements\". It uses a special in-built utility (hasMatchingTags) represented by { Action = \"Announcement\" } to check if the incoming message has been tagged as an announcement. If true, the handler prints the Event and Data, which represent the title and description of the announcement.NOTEOnce a message is \"handled\", it will be discarded from your Inbox.Loading and Testing ​Now, let's bring this to life in the game.Navigate to your aos player terminal and enter a game session.Activate the handler by loading your bot.lua file with:lua.load bot.luaYou'll now see game announcements appear directly in your terminal, offering real-time insights without the need to sift through your inbox.Congratulations! You have just taken the first step in building a bot on aos. But let's keep working on adding more features to it 🌐","estimatedWords":378,"lastModified":"2025-10-20T12:10:22.428Z","breadcrumbs":["tutorials","bots and games","announcements"],"siteKey":"ao","siteName":"AO Cookbook","depth":3,"crawledAt":"2025-10-20T12:10:22.428Z"},{"url":"https://cookbook_ao.arweave.net/tutorials/bots-and-games/ao-effect.html","title":"Lets Play A Game","content":"Let's Play A Game! ​You've been powering through tutorials like a champ! Now, let's take a refreshing break and dive into something exciting. How about a game that adds a dash of fun to your learning journey?What's the game? ​ao-effect is a game where you can compete with friends or other players globally, in real-time, right from your terminal. We've set up a global game process for this adventure.The rules are simple. Each player starts on a 40x40 grid with health at 100 and energy at 0. Your energy replenishes over time to a maximum of 100. Navigate the grid, find other players, and use your energy to attack when they're within range. The battle continues until only one player remains or the allotted time expires.Checkout the guides on the Mechanics of the Arena and Expanding the Arena for a deeper understanding of the game.Heads Up: Don't sweat it if some command syntax seem unfamiliar. Focus on understanding the purpose of each command at a high level and, most importantly, enjoy the game!Preparing for an Adventure in ao-effect ​To join this global escapade, you'll need to set things up. Don't worry, it's as easy as 1-2-3!Install aosFire up your terminal and run:bashnpm i -g https://get_ao.arweave.netLaunch aosNext, create your instance of aos:bashaosSet Up the Game IDLet's keep our game server ID handy for quick access:luaGame = \"tm1jYBC0F2gTZ0EuUQKq5q_esxITDFkAG6QEpLbpI9I\"Print Game Announcements Directly To Terminal (Optional)Here's how you can write a handler for printing announcement details:This is temporary as we will be loading this via a lua script in the next section.luaHandlers.add( \"PrintAnnouncements\", { Action = \"Announcement\" }, function (msg) ao.send({Target = Game, Action = \"GetGameState\"}) print(msg.Event .. \": \" .. msg.Data) end )And voilà! You're all set to join the game.How to Register for a Game ​Ready to jump in? Just a few simple steps to get you going:Register with the Game Server ​All communication between processes in ao occurs through messages. To register, send this message to the game server:luaSend({ Target = Game, Action = \"Register\" }) -- Expected Result -- { output = \"Message added to outbox\", onReply = function: 0x29e5ac0, receive = function: 0x29fe440 } New Message From tm1...I9I: Action = Registered New Player Registered: a1b...y1z has joined in waiting.This places you in the Waiting Lobby. A small fee is needed to confirm your spot.Confirm your spot ​In order to confirm your spot you need some tokens. You can acquire them by sending the following message to the game:luaSend({ Target = Game, Action = \"RequestTokens\"}).receive().Data -- Expected Result -- You received 10000000 from a1b2C3d4e5F6g7h8IjkLm0nOpqR8s7t6U5v4w3X2y1zNOTEThe .receive().Data will wait for a response by adding a temporary Handler that only runs once and will print the response Data. If you would like to instead just wait for the response to hit your Inbox you can call Send() without .receive() and run Inbox[#Inbox].Data to see the response Data.Handler added by .receive():{ name = \"_once_0\", maxRuns = 1, pattern = { }, handle = function: 0x2925700 }Once you receive the tokens, confirm your spot by paying the game's entry fee like this:luaSend({ Target = Game, Action = \"Transfer\", Recipient = Game, Quantity = \"1000\"}).receive().Data -- Expected Result -- You transferred 1000 to tm1jYBC0F2gTZ0EuUQKq5q_esxITDFkAG6QEpLbpI9I New Message From tm1...I9I: Action = Payment-ReceivedWait for a few seconds, and you'll see live updates in your terminal about player payments and statuses.Let the Games Begin! ​Game Mechanics ​Game Start: The game begins after a 2-minute WaitTime if at least 2 players have paid. Non-paying players are removed. If not enough players pay, those who did are refunded.Players spawn at a random grid point once the game begins.It's Your Move! ​Making a Move: The first thing you can do is move around, no energy required! You can shift one square in any direction – up, down, left, right, or diagonally. Along with the direction you must also pass in your player id to help the game identify your move. Here's how:luaSend({ Target = Game, Action = \"PlayerMove\", Player = ao.id, Direction = \"DownRight\"})The available moves across the grid are as follows:luaUp = {x = 0, y = -1}, Down = {x = 0, y = 1}, Left = {x = -1, y = 0}, Right = {x = 1, y = 0}, UpRight = {x = 1, y = -1}, UpLeft = {x = -1, y = -1}, DownRight = {x = 1, y = 1}, DownLeft = {x = -1, y = 1}Keep in Mind: Directions are case sensitive!If you move off the grid, you'll pop up on the opposite side.Time to Strike! ​Launching an Attack: As the game progresses, you'll accumulate energy. Use it to attack other players within a 3x3 grid range. Your attack won't hurt you, but it will affect others in range.luaSend({ Target = Game, Action = \"PlayerAttack\", Player = ao.id, AttackEnergy = \"energy_integer\"})Health starts at 100 and decreases with hits from other players. Reach 0, and it's game over for you.Wrapping Up ​The game ends when there's one player left or time is up. Winners receive rewards, then it's back to the lobby for another round.Enjoyed the game? What if there was a way to make your experience even better or boost your odds of winning. Checkout the next guide to find out 🤔","estimatedWords":869,"lastModified":"2025-10-20T12:10:23.050Z","breadcrumbs":["tutorials","bots and games","ao effect"],"siteKey":"ao","siteName":"AO Cookbook","depth":3,"crawledAt":"2025-10-20T12:10:23.050Z"},{"url":"https://cookbook_ao.arweave.net/tutorials/bots-and-games/index.html","title":"Bots and Games","content":"Bots and Games ​NOTEBuild your own unique bot to complete Quest 3 and earn 1000 CRED, then enter games like the Grid to earn legacynet CRED 24/7!Leveraging insights from our previous chapter, this section will guide you through the realm of automation with bots in aos and the construction of games. You will learn to create autonomous agents, using them to navigate and interact with game environments effectively.Sections ​Getting Started with a Game ​0. # Let's Play A Game: Experience a game on aosEnhancing Game Interactions with Automation ​1. # Interpreting Announcements: Interpret in-game announcements2. # Fetching Game State: Retrieve and process the latest game state3. # Strategic Decisions: Utilize automation to determine your next move4. # Automated Responses: Streamline attack responses through automation5. # Bringing it Together: Combine your skills to craft an autonomous agentGame Development Insights ​6. # Mechanics of the Arena: Explore the underlying mechanics of a game's arena7. # Expanding the Arena: Build unique game logic upon the arenaA journey of discovery and creation awaits. Let the adventure begin!","estimatedWords":173,"lastModified":"2025-10-20T12:10:23.641Z","breadcrumbs":["tutorials","bots and games","index"],"siteKey":"ao","siteName":"AO Cookbook","depth":3,"crawledAt":"2025-10-20T12:10:23.641Z"},{"url":"https://cookbook_ao.arweave.net/tutorials/begin/tokengating.html","title":"Tokengating the Chatroom","content":"Tokengating the Chatroom ​INFONow that we've created a token and sent it to Trinity, we can use the token to tokengate our chatroom. This will allow only those who have the token to enter the chatroom.Video Tutorial ​How to Tokengate the Chatroom ​Let's create a handler that will allow us to tokengate the chatroom. This handler will respond to the tag Action = \"Broadcast\" meaning it will replace the original Broadcast handler we built for our chatroom.Step 1: Start the same aos process. ​Be sure you're using the same aos process that you've used throughout the tutorial.Step 2: Open the chatroom.lua file. ​This is the same file we used to create the chatroom during the chatroom tutorial.Step 3: Edit your Broadcast handler. ​Replace the original Broadcast handler with the following code:luaHandlers.add( \"Broadcast\", { Action = \"Broadcast\" }, function(m) if Balances[m.From] == nil or tonumber(Balances[m.From]) < 1 then print(\"UNAUTH REQ: \" .. m.From) return end local type = m.Type or \"Normal\" print(\"Broadcasting message from \" .. m.From .. \". Content: \" .. m.Data) for i = 1, #Members, 1 do ao.send({ Target = Members[i], Action = \"Broadcasted\", Broadcaster = m.From, Data = m.Data }) end end )This handler will now check the balance of the sender's token before broadcasting the message to the chatroom. If the sender doesn't have a token, the message will not be broadcasted.Save the file.Step 4: Reload the chatroom.lua file. ​To replace the original broadcast handler with the new one, you'll need to reload the chatroom.lua file.lua.load chatroom.luaStep 5: Test the Tokengate ​Now that the chatroom is tokengated, let's test it by sending a message to the chatroom.From the original aos process ​First, we'll test it from the original aos process.luaSend({ Target = ao.id , Action = \"Broadcast\", Data = \"Hello\" })Expected Results:{ output = \"Message added to outbox\", ... } Broadcasting message from [Your Process ID]. Content: Hello. New Message From [Your Process ID]: Action = BroadcastedTesting from another Process ID. ​From a new aos process ​Now, let's test it from a new aos process that doesn't have a token. The following command creates a new AO process with the name \"chatroom-no-token\".shaos chatroom-no-token # the `chatroom-no-token` is the new process nameNext we need to register to the chatroom we built on our original process, from our new process. Hint: type ao.id into your console to get the Process ID of the process you are currently connected to.luaSend({ Target = \"Your_Original_Process_ID\", Action = \"Register\" })Expected Results:message added to outbox New Message From [Your Process ID]: Data = Registered.Now, let's try to send a message to the chatroom.luaSend({ Target = \"Your_Original_Process_ID\" , Action = \"Broadcast\", Data = \"Hello?\" })Expected Results:message added to outbox UNAUTH REQ: [New Process ID]As you can see, the message was not broadcasted because the new process doesn't have a token.Tell Trinity \"It is done\" ​From the original aos process, send a broadcast message to the chatroom saying, \"It is done\".luaSend({ Target = ao.id , Action = \"Broadcast\", Data = \"It is done\" })WARNINGIt's important to be aware of exact match data and case sensitivity. If you're not receiving a response from either Morpheus or Trinity, be sure to check the the content of your Data and Tags.Trinity will then respond to the chatroom being tokengated.Expected Results: ​Trinity will send a message saying, \"I guess Morpheus was right. You are the one. Consider me impressed. You are now ready to join The Construct, an exclusive chatroom available to only those that have completed this tutorial. Now, go join the others by using the same tag you used Register, with this process ID: [Construct Process ID] Good luck. -Trinity\". Additionally, a footer will follow the message.Conclusion ​You've done it! You've successfully tokengated the chatroom. This has now unlocked access to the Construct, where only those that have fully completed this tutorial can enter.Congratulations! ​You've shown a great deal of promise. I hope you've enjoyed this tutorial. You're now ready to build freely in ao.","estimatedWords":658,"lastModified":"2025-10-20T12:10:24.451Z","breadcrumbs":["tutorials","begin","tokengating"],"siteKey":"ao","siteName":"AO Cookbook","depth":3,"crawledAt":"2025-10-20T12:10:24.451Z"},{"url":"https://cookbook_ao.arweave.net/tutorials/begin/token.html","title":"Crafting a Token","content":"Crafting a Token ​INFODiving deeper into the ao, you're now ready to create your own token, a symbol of value and exchange within this decentralized medium. If you've found yourself wanting to learn how to create a token, but haven't visited the Messaging and Build a Chatroom lessons, be sure to do so as this page is part of a multi-part interactive tutorial.When creating tokens, we'll continue to use the Lua Language within ao to mint a token, guided by the principles outlined in the Token Specification.Video Tutorial ​Continuing Down the Rabbit Hole ​In our last tutorial, Build a Chatroom, we learned how to create a chatroom within ao, invited both Morpheus and Trinity to the chatroom we created, and then Trinity has now asked for us to create a token for her as a way of proving ourselves worthy of continuing down the rabbit hole.Let us begin.The Two Paths To Building a Token ​There are two paths to take when building a token:The Blueprint: This is a predesigned template that helps you quickly build a token in ao. It is a great way to get started and can be customized to fit your needs.Check here to learn more about the Token Blueprint.The Manual Method: This is a step-by-step guide to building a token in ao from scratch. This path is for those who want to understand the inner workings of a token and how to build one from the ground up.Check here to review the full Build a Token guide.The Blueprint Method ​For this tutorial, we'll be using the Token Blueprint to create a token for Trinity. This is a predesigned template that helps you quickly build a token in ao.How To Use The Token Blueprint ​Make sure we're in the same directory as before during the previous steps in the tutorial.Open the Terminal.Start your aos process.Type in .load-blueprint tokenThis will load the required handlers for the tutorials token within ao. It's important to note that the token blueprint isn't specific to this tutorial and can be used as a foundation for any token you wish to create.Verify the Blueprint is Loaded ​Type in Handlers.list to see the newly loaded handlers.You should see a new list of handlers that have been loaded into your aos process. If you've been following along the with the previous steps in the tutorial, you should also see the handlers for your chatroom, as well.Example:Testing the Token ​Now that the token blueprint is loaded, we can test the token by sending a message to ourselves using the Action = \"Info\" tag.luaSend({ Target = ao.id, Action = \"Info\" }) Inbox[#Inbox].TagsThis will print the token information to the console. It should show your process ID with the total balance of tokens available.Sending Tokens to Trinity ​Now that we've tested the token and it's working as expected, we can send some tokens to Trinity. We'll send 1000 tokens to Trinity using the Action = \"Transfer\" tag.luaSend({ Target = ao.id, Action = \"Transfer\", Recipient = Trinity, Quantity = \"1000\"}).receive().DataWhen Trinity receives the tokens, she'll respond to the transfer with a message to confirm that she's received the tokens.Her response will look something like this:Trinity: \"Token received. Interesting. I wasn't sure you'd make it this far. I'm impressed, but we are not done yet. I want you to use this token to tokengate the chatroom. Do that, and then I will believe you could be the one.\"You've completed the process of creating a token and sending it to Trinity. You're now ready to move on to the next step in the tutorial. Tokengating the Chatroom.","estimatedWords":596,"lastModified":"2025-10-20T12:10:25.028Z","breadcrumbs":["tutorials","begin","token"],"siteKey":"ao","siteName":"AO Cookbook","depth":3,"crawledAt":"2025-10-20T12:10:25.028Z"},{"url":"https://cookbook_ao.arweave.net/tutorials/begin/chatroom.html","title":"Building a Chatroom in aos","content":"Building a Chatroom in aos ​INFOIf you've found yourself wanting to learn how to create a chatroom within ao, then that means we understand at least the basic methodology of sending and receiving messages. If not, it's suggested that you review the Messaging tutorial before proceeding.In this tutorial, we'll be building a chatroom within ao using the Lua scripting language. The chatroom will feature two primary functions:Register: Allows processes to join the chatroom.Broadcast: Sends messages from one process to all registered participants.Let's begin by setting up the foundation for our chatroom.Video Tutorial ​Step 1: The Foundation ​Open your preferred code editor, e.g. VS Code.INFOYou may find it helpful to have the Recommended Extensions installed in your code editor to enhance your Lua scripting experience.Create a new file named chatroom.lua.Step 2: Creating The Member List ​In chatroom.lua, you'll begin by initializing a list to track participants:luaMembers = Members or {}Save the chatroom.lua fileStep 3: Load the Chatroom into aos ​With chatroom.lua saved, you'll now load the chatroom into aos.If you haven't already, start your aos in your terminal inside the directory where chatroom.lua is savedIn the aos CLI, type the following script to incorporate your script into the aos process:lua.load chatroom.luaType Members, or whatever you named your user list, in aos. It should return an empty array { }.If you see an empty array, then your script has been successfully loaded into aos.Step 4: Creating Chatroom Functionalities ​The Registration Handler ​The register handler will allow processes to join the chatroom.Adding a Register Handler: Modify chatroom.lua to include a handler for Members to register to the chatroom with the following code:lua -- Modify `chatroom.lua` to include a handler for `Members` -- to register to the chatroom with the following code: Handlers.add( \"Register\", { Action = \"Register\"}, function (msg) table.insert(Members, msg.From) print(msg.From .. \" Registered\") msg.reply({ Data = \"Registered.\" }) end )This handler will allow processes to register to the chatroom by responding to the tag Action = \"Register\". A printed message will confirm stating Registered. will appear when the registration is successful.Reload and Test: Let's reload and test the script by registering ourselves to the chatroom.Save and reload the script in aos using .load chatroom.lua.Check to see if the register handler loaded with the following script:lua Handlers.listThis will return a list of all the handlers in the chatroom. Since this is most likely your first time developing in aos, you should only see one handler with the name Register.Let's test the registration process by registering ourselves to the chatroom:luaSend({ Target = ao.id, Action = \"Register\" })If successful, you should see that there was a message added to your outbox and that you then see a new printed message that says registered.Finally, let's check to see if we were successfully added to the Members list:lua MembersIf successful, you'll now see your process ID in the Members list.Adding a Broadcast Handler ​Now that you have a chatroom, let's create a handler that will allow you to broadcast messages to all members of the chatroom.Add the following handler to the chatroom.lua file:lua Handlers.add( \"Broadcast\", { Action = \"Broadcast\" }, function (msg) for _, recipient in ipairs(Members) do ao.send({Target = recipient, Data = msg.Data}) end msg.reply({Data = \"Broadcasted.\" }) end )This handler will allow you to broadcast messages to all members of the chatroom.Save and reload the script in aos using .load chatroom.lua.Let's test the broadcast handler by sending a message to the chatroom:luaSend({Target = ao.id, Action = \"Broadcast\", Data = \"Broadcasting My 1st Message\" }).receive().DataINFOWhile we use Send in the console for convenience, it's recommended to use ao.send in handlers - see the FAQ for more details.Step 5: Inviting Morpheus to the Chatroom ​Now that you've successfully registered yourself to the chatroom, let's invite Morpheus to join us. To do this, we'll send an invite to him that will allow him to register to the chatroom.Morpheus is an autonomous agent with a handler that will respond to the tag Action = \"Join\", in which will then have him use your Register tag to register to the chatroom.Let's send Morpheus an invitation to join the chatroom:luaSend({ Target = Morpheus, Action = \"Join\" })To confirm that Morpheus has joined the chatroom, check the Members list:luaMembersIf successful, you'll receive a broadcasted message from Morpheus.Step 6: Inviting Trinity to the Chatroom ​Within this message, he'll give you Trinity's process ID and tell you to invite her to the chatroom.Use the same processes to save her process ID as Trinity and to invite her to the chatroom as you did with Morpheus.If she successfully joins the chatroom, she'll then pose the next challenge to you, creating a token.Engaging Others in the Chatroom ​Onboarding Others ​Invite aos Users: Encourage other aos users to join your chatroom. They can register and participate in the broadcast.Provide Onboarding Instructions: Share a simple script with them for easy onboarding:lua-- Hey, let's chat on aos! Join my chatroom by sending this command in your aos environment: Send({ Target = [Your Process ID], Action = \"Register\" }) -- Then, you can broadcast messages using: Send({Target = [Your Process ID], Action = \"Broadcast\", Data = \"Your Message\" })Next Steps ​Congratulations! You've successfully built a chatroom in ao and have invited Morpheus to join you. You've also created a broadcast handler to send messages to all members of the chatroom.Next, you'll continue to engage with Morpheus, but this time you'll be adding Trinity to the conversation. She will lead you through the next set of challenges. Good Luck!","estimatedWords":909,"lastModified":"2025-10-20T12:10:25.659Z","breadcrumbs":["tutorials","begin","chatroom"],"siteKey":"ao","siteName":"AO Cookbook","depth":3,"crawledAt":"2025-10-20T12:10:25.659Z"},{"url":"https://cookbook_ao.arweave.net/tutorials/begin/messaging.html","title":"Messaging in ao","content":"Messaging in ao ​Learn how Messages gives ao Parallel Compute Capability ​In ao, every process runs in parallel, creating a highly scalable environment. Traditional direct function calls between processes aren't feasible because each process operates independently and asynchronously.Messaging addresses this by enabling asynchronous communication. Processes send and receive messages rather than directly invoking functions on each other. This method allows for flexible and efficient interaction, where processes can respond to messages, enhancing the system's scalability and responsiveness.We'll begin by exploring the basics of messaging in aos, how to see messages received in your inbox, and how to send messages to other processes.Video Tutorial ​Step 1: Understand the Message Structure ​Message Basics: Messages in ao are built using Lua tables, which are versatile data structures that can hold multiple values. Within these tables, the \"Data\" field is crucial as it contains the message's content or payload. This structure allows for efficient sending and receiving of information between processes, showcasing how ao primitives leverage Arweave's underlying capabilities to facilitate complex, composable operations.For detailed specifications, please refer to the original documentation on the G8way specs page.Example: { Data = \"Hello from Process A!\" } is a simple message.Step 2: Open the aos CLI ​Launch the aos command-line interface (CLI) by typing aos in your terminal and pressing Enter.shaosStep 3: How to Send a Message ​luaSend({ Target = \"process ID\", Data = \"Hello World!\" })Send: The Send function is globally available in the aos interactive environment.Target: To send a message to a specific process, include a Target field in your message.Data: The Data is the string message (or payload) you want to be received by the receiving process. In this example, the message is \"Hello World!\".Step 4: Store Morpheus's Process ID ​We'll use the process ID provided below and store it as a variable called Morpheus.luaFvan28CFY0JYl5f_ETB7d3PDwBhGS8Yq5IA0vcWulUcCopy the process ID above and store it as a variable by running the below command in the aos CLI:luaMorpheus = \"Fvan28CFY0JYl5f_ETB7d3PDwBhGS8Yq5IA0vcWulUc\"This will store the process ID as a variable called Morpheus, making it easier to interact with the specific process ID.Check the Morpheus Variable ​lua-- Check the Morpheus variable by typing `Morpheus` Morpheus -- Expected Results: Fvan28CFY0JYl5f_ETB7d3PDwBhGS8Yq5IA0vcWulUc -- If `undefined` is returned, -- then the variable was not created successfully.Step 5: Send a Message to Morpheus ​After obtaining Morpheus's process ID and storing it in a variable, you're ready to communicate with it. To do this, you use the Send function. Morpheus, himself, is a parallel process running in ao. He receives and sends messages using a series of Handlers. Let's send him a message and see what happens.luaSend({ Target = Morpheus, Data = \"Morpheus?\" })Your Target is Morpheus which is the variable we defined earlier using Morpheus's process ID.The Data is the message you want to send to Morpheus. In this case, it's \"Morpheus?\".Expected Results:lua-- Your Message Command Send({ Target = Morpheus, Data = \"Morpheus?\"}) -- Message is added to the outbox message added to outbox -- A New Message is received from `Morpheus`'s process ID New Message From BWM...ulw: Data = I am here. You are fYou've sent a message to Morpheus and received a response, but you can't read the full message. Let's learn about the Inbox and how to read messages.Step 6: The Inbox ​The Inbox is where you receive messages from other processes.INFOTo see an in depth view of an inbox message, head over to the Messages Concepts page.Let's check your inbox to see how many messages you have received.Inside your aos CLI, type the following command:lua #InboxIf you're actively following through the tutorial, the inbox will not have many messages. However, if you've been experimenting with the aos environment, you may more than 1 message in your inbox.Example Return:lua-- Your Inbox Command #Inbox -- The command will return the number of messages in your inbox. 4In the example above, the return is 4, stating that there are four messages in the inbox.As we're actively looking for Morpheus's response, we'll assume his message was the last one received. To read the last message in your inbox, type the following command:lua Inbox[#Inbox].DataThis command allows you to isolate the Data from the message and only read the contents of the data.The Expected Return:lua-- Your Inbox[x].Data Command Inbox[#Inbox].Data -- The command will return the `Data` of the message. -- Data is what usually represents the text-based message -- received from one process to another. I am here. You are finally awake. Are you ready to see how far the rabbit hole goes?You are now using your own process to communicate with Morpheus, another parallel process running in ao. You're now ready to move on to the next step in the tutorial.Step 7: Sending Messages with Tags ​Purpose of Tags: Tags in aos messages are used to categorize, route, and process messages efficiently. They play a crucial role in message handling, especially when dealing with multiple processes or complex workflows.Some processes use Handlers that specifically interact with messages that have certain tags. For example, a process may have a handler that only interacts with messages that have a specific tag, which we'll see an example of in the chatroom tutorial.How to Use Tags in Messages ​In the case of Morpheus, we can use tags to categorize our messages, and because Morpheus is a autonomous process, he has handlers that can interact with messages that have certain tags.Adding Tags to a Message:We already know that the Data of a message is the payload of the message you want to send to another process. Earlier, we sent a message to Morpheus without any tags, in which he used a handler to respond to an exact match within the Data field.Let's Show Morpheus That We're Ready ​Send Morpheus a message with the tag Action and the value rabbithole.Example:luaSend({ Target = Morpheus, Data = \"Code: rabbithole\", Action = \"Unlock\" })Read the message from Morpheus:luaInbox[#Inbox].DataExpected Return:Additional Tips for Using Tags ​Consistent Tagging: Develop a consistent tagging system for your application to make message handling more predictable.Tag Naming: Choose clear and descriptive names for your tags. This makes it easier to understand the purpose and context of messages at a glance.Security with Tags: Remember that tags are not encrypted or hidden, so avoid using sensitive information as tags.Advanced Usage of Tags ​Workflow Management: Tags can be instrumental in managing workflows, especially in systems where messages pass through multiple stages or processes.Additional Tips for Messaging ​Message Structure: Explore other fields like Epoch, From, and Nonce for more complex messaging needs.Debugging: Use the Dump function to print messages for debugging.Security Considerations: Be cautious with the content and handling of messages, and never send anything considered private or sensitive.Conclusion ​You've now learned how to send messages with tags, which is a powerful tool for categorizing and routing messages in aos.Morpheus has officially invited you to the next stage of your journey. You're now ready to move on to the next step in the tutorial, Creating a Chatroom.","estimatedWords":1151,"lastModified":"2025-10-20T12:10:26.262Z","breadcrumbs":["tutorials","begin","messaging"],"siteKey":"ao","siteName":"AO Cookbook","depth":3,"crawledAt":"2025-10-20T12:10:26.262Z"},{"url":"https://cookbook_ao.arweave.net/tutorials/begin/preparations.html","title":"Preparations","content":"Preparations ​INFOThe Awakening Begins:You've always known there's more to this world, just outside of your reach. You've been searching for it, not even knowing what it was you were looking for. It... is ao.We begin our journey by installing the aos client and starting a new process. This will allow us to interact with the ao computer and complete the rest of the tutorial.Video Tutorial ​System requirements ​The local client of aos is very simple to install. Just make sure you have:NodeJS version 20+. (If you haven't yet installed it, check out this page to find instructions for your OS).A code editor of your choice.INFOThough it's not required, we do recommend installing the ao addon into your text editor of choice to optimize your experience with aos.Installing aos ​Once you have NodeJS on your machine, all you need to do is install aos and run it:shnpm i -g https://get_ao.arweave.netAfter installation, we can simply run the command itself to start a new aos process!shaosWelcome to the rabbit hole ​The utility you just started is a local client, which is ready to relay messages for you to your new process inside the ao computer.After it connects, you should see the following:sh _____ _______ _____ /\\ \\ /::\\ \\ /\\ \\ /::\\ \\ /::::\\ \\ /::\\ \\ /::::\\ \\ /::::::\\ \\ /::::\\ \\ /::::::\\ \\ /::::::::\\ \\ /::::::\\ \\ /:::/\\:::\\ \\ /:::/~~\\:::\\ \\ /:::/\\:::\\ \\ /:::/__\\:::\\ \\ /:::/ \\:::\\ \\ /:::/__\\:::\\ \\ /::::\\ \\:::\\ \\ /:::/ / \\:::\\ \\ \\:::\\ \\:::\\ \\ /::::::\\ \\:::\\ \\ /:::/____/ \\:::\\____\\ ___\\:::\\ \\:::\\ \\ /:::/\\:::\\ \\:::\\ \\ |:::| | |:::| | /\\ \\:::\\ \\:::\\ \\ /:::/ \\:::\\ \\:::\\____\\|:::|____| |:::| |/::\\ \\:::\\ \\:::\\____\\ \\::/ \\:::\\ /:::/ / \\:::\\ \\ /:::/ / \\:::\\ \\:::\\ \\::/ / \\/____/ \\:::\\/:::/ / \\:::\\ \\ /:::/ / \\:::\\ \\:::\\ \\/____/ \\::::::/ / \\:::\\ /:::/ / \\:::\\ \\:::\\ \\ \\::::/ / \\:::\\__/:::/ / \\:::\\ \\:::\\____\\ /:::/ / \\::::::::/ / \\:::\\ /:::/ / /:::/ / \\::::::/ / \\:::\\/:::/ / /:::/ / \\::::/ / \\::::::/ / /:::/ / \\::/____/ \\::::/ / \\::/ / ~~ \\::/ / \\/____/ \\/____/ Welcome to AOS: Your operating system for AO, the decentralized open access supercomputer. Type \".load-blueprint chat\" to join the community chat and ask questions! AOS Client Version: 1.12.1. 2024 Type \"Ctrl-C\" twice to exit Your AOS process: QFt5SR6UwJSCnmgnROq62-W8KGY9z96k1oExgn4uAzk default@aos-0.2.2[Inbox:1]>Let's walk through the initial printout after running aos:After running aos in your terminal, you should see:An ASCII art image of AOS.A Welcome MessageThe version of aos you are running.An instructional exit message.Your process ID.INFOIf your OS version is different than the latest version, a message asking if you'd like to update the version will appear. If so, simply exit the process by pressing \"Ctrl+C\" twice, run npm i -g https://get_ao.g8way.io to update, and then run aos again.Welcome to your new home in the ao computer! The prompt you are now looking at is your own personal server in this decentralized machine.Now, let's journey further down the rabbit hole by exploring one of the two core concept type of ao: messaging.","estimatedWords":501,"lastModified":"2025-10-20T12:10:26.830Z","breadcrumbs":["tutorials","begin","preparations"],"siteKey":"ao","siteName":"AO Cookbook","depth":3,"crawledAt":"2025-10-20T12:10:26.830Z"},{"url":"https://cookbook_ao.arweave.net/tutorials/begin/index.html","title":"Begin An Interactive Tutorial","content":"Begin: An Interactive Tutorial ​In this tutorial series, you'll walk through an interactive steps that will help you deepen your knowledge and understanding of the aos environment.INFOThe Exercise ​In this fun exercise, you'll encounter a series of challenges presented by two familiar characters, Morpheus and Trinity. You'll dive deep into the rabbit hole guided by Morpheus as he presents you with a series of challenges to prove you're the one. Once you've completed all of the challenges presented by both Morpheus and Trinity, you'll receive a token that grants you access to an exclusive chatroom within ao called The Construct.Now, let's get started down the rabbit hole.Tutorials ​Getting Started - An Interactive Tutorial ​1. Quick Start2. Messaging3. Creating a Chatroom4. Build a Token","estimatedWords":123,"lastModified":"2025-10-20T12:10:27.420Z","breadcrumbs":["tutorials","begin","index"],"siteKey":"ao","siteName":"AO Cookbook","depth":3,"crawledAt":"2025-10-20T12:10:27.421Z"},{"url":"https://cookbook_ao.arweave.net/tutorials/index.html","title":"Tutorials","content":"Tutorials ​Here, we've created a series of tutorials to help you get started with aos and build your first processes. These tutorials include interactive guides, code snippets, and examples to help you get comfortable with the aos environment.List of Tutorials ​Getting Started - An Interactive GuideBots and Games","estimatedWords":48,"lastModified":"2025-10-20T12:10:27.559Z","breadcrumbs":["tutorials","index"],"siteKey":"ao","siteName":"AO Cookbook","depth":2,"crawledAt":"2025-10-20T12:10:27.559Z"},{"url":"https://cookbook_ao.arweave.net/migrating-to-hyperbeam/external-data.html","title":"External Data","content":"External Data ​HyperBEAM provides oracle functionality through its resolve mechanism, enabling AO processes to fetch and consume external data from web APIs. This creates a trustless bridge between on-chain processes and off-chain data sources.The oracle service works by taking a base message and combining it with external data fetched via HTTP requests. This \"resolve\" operation allows your AO processes to react to real-world data, price feeds, weather information, or any other web-accessible API.Key Concepts ​Oracle Requests: Fetch external data through HTTP calls to any web APITrustless Execution: Results are delivered directly to your process with cryptographic guaranteesRelay Device: The ~relay@1.0 device acts as the oracle gateway to external servicesMaking Oracle Requests ​The oracle service allows your process to query any web API and receive the response directly. Here's how to use it:Basic HTTP Request Example ​To fetch information from an Arweave node:luaSend({ target = ao.id, [\"relay-path\"] = \"https://arweave.net/info\", resolve = \"~relay@1.0/call\" })This code:Sets the target to ao.id (the current process) to receive the responseSpecifies the external URL to query via relay-pathUses the resolve property with ~relay@1.0/call to execute the HTTP requestManaging Trust ​When working with HyperBeam nodes, you may need to establish trust by adding the node's address to your authorities:luatable.insert(ao.authorities, \"HYPERBEAM_NODE_ADDRESS\")If you receive a \"not trusted\" response, this step is required before the resolve operation can succeed.Accessing Response Data ​After a successful resolve operation, you can access the response data from your inbox:luaInbox[3].DataThis will display the JSON data returned from the external API.Additional Example: JSON Placeholder API ​You can query any REST API using the same pattern:luaSend({ target = ao.id, [\"relay-path\"] = \"https://jsonplaceholder.typicode.com/posts\", resolve = \"~relay@1.0/call\" })Technical Details ​The resolve mechanism works by:Creating a base message with the target and relay pathSpecifying the resolve function to use (in this case, the relay device's call function)Executing the HTTP request through the relay deviceReturning the response as a new message to the specified targetCommon Oracle Use Cases ​Price Feeds: Fetch cryptocurrency or asset prices for DeFi applicationsWeather Data: Build parametric insurance or prediction marketsSports Results: Create betting or fantasy sports applicationsRandom Numbers: Access external randomness for games or lotteriesIoT Data: Integrate real-world sensor data into smart contractsNews & Events: React to real-world events in your processesSecurity Considerations ​Always ensure that HyperBeam nodes are trusted before using them for resolve operations. Add node addresses to your ao.authorities table to establish trust.","estimatedWords":389,"lastModified":"2025-10-20T12:10:28.155Z","breadcrumbs":["migrating to hyperbeam","external data"],"siteKey":"ao","siteName":"AO Cookbook","depth":2,"crawledAt":"2025-10-20T12:10:28.155Z"},{"url":"https://cookbook_ao.arweave.net/migrating-to-hyperbeam/web-serving.html","title":"Web Serving","content":"Web Serving ​Serve dynamic web content directly from AO processes using HyperBEAM. This guide shows how to host websites, serve HTML/CSS/JavaScript, and update content in real-time using an experimental Hyper AOS light implementation.Prerequisites ​HyperBeam server running locally rebar3 shell from your local HyperBeam repoExperimental Hyper AOS light moduleSetting Up the Environment ​First, export the AOS module and install the latest preview version of the AOS console:bash# Install the latest preview version of AOS console preview_ao.arweave.netThis installation enables the Lua device on HyperBeam to function as your process's virtual machine.Creating a Process ​To create a new process on your HyperBeam server:Spin up a new process using the AOS module as the initialization script export AOS_MODULE=Select the Lua device in the configuration optionsVerify the process is running with the experimental Hyper AOSFile Structure ​For this demonstration, we'll use three main files:app.css.lua - CSS styling wrapped in a Lua fileapp.js.lua - JavaScript functionality wrapped in a Lua filenow.html.lua - HTML content wrapped in a Lua fileState Management ​Unlike Genesis WASM which requires sending messages to cache with the patch device, Hyper AOS makes state directly available:lua-- Access state State[\"someProperty\"] -- Modify state State[\"someProperty\"] = newValueLoading Web Assets ​CSS File ​Load the CSS file into the process state:lua.load app.css.luaThe CSS file sets the content type and body:luaState[\"app.css\"] = { [\"content-type\"] = \"text/css\", body = [[ /* CSS styles */ * { margin: 0; padding: 0; box-sizing: border-box; } body { font-family: Arial, sans-serif; background: linear-gradient(180deg, #87CEEB 0%, #9980C8 100%); min-height: 100vh; overflow-x: hidden; display: flex; flex-direction: column; align-items: center; justify-content: center; } h1 { color: #333; margin-bottom: 2rem; text-align: center; font-size: 2.5rem; text-shadow: 2px 2px 4px rgba(0,0,0,0.1); } .marquee-container { width: 100%; height: 200px; background: rgba(255, 255, 255, 0.3); backdrop-filter: blur(10px); position: relative; overflow: hidden; box-shadow: 0 4px 6px rgba(0,0,0,0.1); } .marquee-track { display: flex; position: absolute; animation: scroll 20s linear infinite; height: 100%; align-items: center; } ]] }JavaScript File ​Similarly, load the JavaScript file:lua.load app.js.luaThe JavaScript file follows the same pattern:luaState[\"app.js\"] = { [\"content-type\"] = \"application/javascript\", body = [[ // JavaScript content here ]] }HTML File ​Finally, load the HTML file:lua.load now.html.luaThe HTML file is placed at the root of the state:luaState[\"content-type\"] = \"text/html\" State[\"body\"] = [[ HyperBeam Marquee Demo Elephant Parade ]]Note that the HTML references other assets using the path now/app.css and now/app.js.Accessing Your Web Application ​Retrieve your process ID by examining the inbox commitments:luaInbox[1].commitmentsLook for the signed ID (RSAPS-SHA-256). This appears as a long alphanumeric string like 6oNYypU3EIeauMkTL4EEYKgzASYFebqTCLP8W1idmMB.Access your web application in a browser:localhost:8734/~process@1.0/nowReplace the process ID with your actual process ID from the commitmentsMaking Dynamic Updates ​To demonstrate the dynamic nature of this approach:Modify your HTML file (e.g., change a heading to \"Elephant Parade on the beam\")Save the changesReload the file in the AOS console:lua.load now.html.luaRefresh your browser to see the changesAdvanced Capabilities ​This approach allows for:Real-time updates to web contentProcessing incoming messages to modify the websiteAdding additional content dynamicallyCreating interactive web applications directly within HyperBeam processesWarning ​The Hyper AOS light implementation used in this guide is experimental and should not be used for production environments. It is intended for demonstration purposes only.","estimatedWords":514,"lastModified":"2025-10-20T12:10:28.767Z","breadcrumbs":["migrating to hyperbeam","web serving"],"siteKey":"ao","siteName":"AO Cookbook","depth":2,"crawledAt":"2025-10-20T12:10:28.768Z"},{"url":"https://cookbook_ao.arweave.net/migrating-to-hyperbeam/user-owned-processes.html","title":"User-Owned Processes","content":"User-Owned Processes ​When building applications on AO, you may create architectures where users own their own processes rather than your application owning a centralized process. This pattern is common in decentralized marketplaces, user-specific vaults, and personal data stores.The Challenge ​With the deprecation of dry runs on Legacynet, you face a unique challenge: you cannot patch processes that you don't own. Only the process owner can send messages to update their process.Common Scenarios ​Marketplace Applications: Each vendor has their own process for managing inventory and salesUser Vaults: Each user has a personal process for storing data or assetsDecentralized Applications: Users spawn their own game characters, agents, or botsToken Pairs: DEX applications where users create their own trading pair processesIn these scenarios, you need to provide a way for users to migrate their own processes to use state patching.General Approach ​The solution typically involves three key components:Process-side handler: Add a handler to your process template that allows the owner to enable state patchingDetection mechanism: Check whether a user's process has patching enabledUser interface: Provide a way for users to trigger the updateYou'll need to decide how to implement each of these based on your application's architecture and user experience requirements.Process Update Handler ​Your process needs a handler that allows the owner to enable state patching. Here's a minimal example:luaHandlers.add( \"EnableStatePatch\", Handlers.utils.hasMatchingTag(\"Action\", \"EnableStatePatch\"), function(msg) -- Only allow process owner to enable if msg.From ~= ao.id then return end -- Sync current state to cache Send({ device = 'patch@1.0', cache = { -- Your state here balances = Balances, -- ... other state } }) end )After enabling, you'll also need to update your existing handlers to patch state when it changes. For example:lua-- After updating state, sync to cache if patching is enabled if StatePatchEnabled then Send({ device = 'patch@1.0', cache = { balances = Balances } }) endDetecting Patch Status ​You can check if a process has state patching enabled by attempting to access its cached state:javascriptasync function isPatchEnabled(processId) { try { const response = await fetch( `https://forward.computer/${processId}~process@1.0/compute/cache/balances`, { method: \"HEAD\" }, ); return response.ok; } catch { return false; } }Triggering the Update ​Users need to send a message to their own process to enable patching. The basic approach:javascriptimport { message, createDataItemSigner } from \"@permaweb/aoconnect\"; const messageId = await message({ process: userProcessId, signer: createDataItemSigner(window.arweaveWallet), tags: [{ name: \"Action\", value: \"EnableStatePatch\" }], });How you present this to users is up to you - it could be a button in your UI, a banner notification, a setup wizard, or a CLI script.Related Documentation ​State ExposureDynamic ReadsWhy Migrate to HyperBEAM","estimatedWords":426,"lastModified":"2025-10-20T12:10:29.362Z","breadcrumbs":["migrating to hyperbeam","user owned processes"],"siteKey":"ao","siteName":"AO Cookbook","depth":2,"crawledAt":"2025-10-20T12:10:29.362Z"},{"url":"https://cookbook_ao.arweave.net/migrating-to-hyperbeam/dynamic-reads.html","title":"Dynamic Reads","content":"Dynamic Reads ​Dynamic reads enable on-the-fly computations on your process state using Lua transformation functions. These functions process cached state and return computed results without modifying the underlying data.This pattern creates efficient data APIs by moving computation from clients to HyperBEAM nodes, reducing both network traffic and client-side complexity.This guide assumes you are already familiar with state exposure.How Dynamic Reads Work ​Dynamic reads leverage the lua@5.3a device to execute Lua scripts against cached state. The HyperBEAM URL constructs a processing pipeline:First, we grab the latest state of an AO process.Then, we pipe that state as the base message into the lua@5.3a device.We tell the Lua device which script to load (from an Arweave transaction) and which function to execute.The function runs, processing the base state.Finally, the result of the function is returned over HTTP.Example: Calculating Circulating Supply ​Let's consider a practical example: a token process where we have patched the Balances table to be readable. Rather than forcing clients to download all balance data to compute the total supply, we can do it on the HyperBEAM node.1. The Transformation Function ​First, create a Lua script (sum.lua) with a function that takes the state (base) and calculates the sum of balances.lua-- sum.lua function sum(base, req) -- Initialize total supply counter local totalSupply = 0 local total = 0 -- Check if we have balances in our state if base.balances then -- Iterate through all balances and sum them for address, balance in pairs(base.balances) do -- Ensure balance is a number and add to total local numBalance = tonumber(balance) or 0 totalSupply = totalSupply + numBalance total = total + 1 end end -- Return the computed result as a table return { CirculatingSupply = tostring(math.floor(totalSupply)), BalanceCount = tostring(math.floor(total)) } endThe transformation function receives two arguments:base: The message being processed, which in our pipeline will be the cached state data from your process.req: The incoming request object, which contains parameters and other metadata.2. Publishing the Function ​Next, publish your Lua script to Arweave. The arx CLI tool is recommended for this.bash# Install arx globally npm i -g @permaweb/arx # Upload your Lua function to Arweave arx upload sum.lua \\ -w PATH_TO_WALLET.json \\ -t arweave \\ --content-type application/lua \\ --tags Data-Protocol aoarx will return a transaction ID for your script. Let's say it's LUA_SCRIPT_TX_ID.3. Calling the Function ​With the process ID (YOUR_PROCESS_ID) and the script transaction ID (LUA_SCRIPT_TX_ID), you can construct a URL to call your function:HyperBEAMGET /~process@1.0/now/~lua@5.3a // An example process const moduleId = \"QSBQZsowVRdvsEbdTv-KEF4_Z5bYf11M3X5-8LN0NM4\"; // The example sum.lua script const hyperbeam = \"forward.computer\"; async function getDynamicState() { const url = `https://${hyperbeam}/${processId}~process@1.0/now/~lua@5.3a const response = await fetch(url); const data = await response.json(); console.log(`Total Supply: ${data.circulatingsupply}`); console.log(`Token Holders: ${data.balancecount}`); } getDynamicState();This approach significantly improves performance by offloading computation from the client to the HyperBEAM node and reducing the amount of data sent over the network.","estimatedWords":472,"lastModified":"2025-10-20T12:10:29.980Z","breadcrumbs":["migrating to hyperbeam","dynamic reads"],"siteKey":"ao","siteName":"AO Cookbook","depth":2,"crawledAt":"2025-10-20T12:10:29.981Z"},{"url":"https://cookbook_ao.arweave.net/migrating-to-hyperbeam/state-exposure.html","title":"State Exposure","content":"State Exposure ​ACTION REQUIREDLegacynet is deprecating dry runs on October 10, 2025. Migrating to state exposure is now urgent for all processes running on Legacynet. This guide shows you how to expose your process state for direct HTTP access, providing dramatically better performance than dry runs.HyperBEAM enables direct HTTP access to process state, eliminating the need for costly dryrun calls. This feature dramatically improves performance for web frontends and data services that need to read process data.The Patch Device ​The ~patch@1.0 device is the mechanism that allows AO processes to make parts of their internal state readable via direct HTTP GET requests.How State Exposure Works ​State exposure follows a simple four-step pattern:Process Logic: From your process (e.g., in Lua or WASM), send an outbound message to the ~patch@1.0 device.Patch Message Format: The message must include device and cache tags.luaSend({ Target = ao.id, device = 'patch@1.0', cache = { mydatakey = MyValue } })HyperBEAM Execution: HyperBEAM's dev_patch module processes this message, mapping the key-value pairs from the cache table to a URL path.HTTP Access: The exposed data is then immediately available via a standard HTTP GET request to the process's endpoint.HyperBEAMGET /~process@1.0/compute/cache/Initial State Sync (Optional) ​To make data available immediately on process creation, you can patch its initial state. A common pattern is to use a flag to ensure this sync only runs once, as shown in this example for a token's Balances and TotalSupply.lua-- Place this logic at the top level of your process script, -- outside of specific handlers, so it runs on load. Balances = { token1 = 100, token2 = 200 } -- A table of balances TotalSupply = 1984 -- A single total supply value -- 1. Initialize Flag: -- Initializes a flag if it doesn't exist. InitialSync = InitialSync or 'INCOMPLETE' -- 2. Check Flag: -- Checks if the sync has already run. if InitialSync == 'INCOMPLETE' then -- 3. Patch State: -- The `Send` call patches the state, making it available at endpoints like: -- /cache/balances -- /cache/totalsupply Send({ device = 'patch@1.0', cache = { balances = Balances, totalsupply = TotalSupply } }) -- 4. Update Flag: -- Updates the flag to prevent the sync from running again. InitialSync = 'COMPLETE' print(\"Initial state sync complete. Balances and TotalSupply patched.\") endThis pattern makes essential data queryable upon process creation, boosting application responsiveness.Example (Lua in aos) ​This handler exposes a currentstatus key that can be read via HTTP after the PublishData action is called.lua-- In your process code (e.g., loaded via .load) Handlers.add( \"PublishData\", Handlers.utils.hasMatchingTag(\"Action\", \"PublishData\"), function (msg) local dataToPublish = \"Some important state: \" .. math.random() -- Expose 'currentstatus' key under the 'cache' path Send({ device = 'patch@1.0', cache = { currentstatus = dataToPublish } }) print(\"Published data to /cache/currentstatus\") end )Avoiding Key Conflicts ​Keys in the cache table become URL path segments. To avoid conflicts with reserved HyperBEAM paths, use descriptive, specific keys. Avoid using reserved keywords such as:now, compute, state, info, testFor instance, prefer a key like myappstate over a generic key like state.WARNINGHTTP paths are case-insensitive. While the patch device stores keys with case sensitivity (e.g., MyKey vs mykey), HTTP access to paths like the following is ambiguous and may lead to unpredictable results.To prevent conflicts, always use lowercase keys in your cache table (e.g., mykey, usercount).HyperBEAMGET /~process@1.0/cache/mykeyKey Points ​Path Structure: Data is exposed at a path structured like this, where is a key from your cache table:HyperBEAM/~process@1.0/cache/Data Types: Basic data types like strings and numbers work best. Complex objects may require serialization.compute vs now: Accessing patched data can be done via two main paths:HyperBEAMGET /~process@1.0/compute/cache/... GET /~process@1.0/now/cache/...The compute endpoint serves the last known value quickly, while now may perform additional computation to get the most recent state.Read-Only Exposure: Patching is for efficient reads and does not replace your process's core state management logic.Using the patch device enables efficient, standard HTTP access to your process state, seamlessly connecting decentralized logic with web applications.Patching User-Owned Processes ​If your application spawns processes that are owned by users (not your application), you'll need to provide a way for users to patch their own processes. A common example is marketplace applications where each user has their own process instance.Implementation Strategy ​You can create UI components that allow users to update their processes. Here's an example approach:javascriptimport { message, createDataItemSigner } from \"@permaweb/aoconnect\"; async function patchUserProcess(processId) { // User must sign this message themselves const signer = createDataItemSigner(window.arweaveWallet); const messageId = await message({ process: processId, signer, tags: [{ name: \"Action\", value: \"UpdateToPatch\" }], }); return messageId; }In your process code, add a handler that users can trigger:luaHandlers.add( \"UpdateToPatch\", Handlers.utils.hasMatchingTag(\"Action\", \"UpdateToPatch\"), function(msg) -- Only allow the process owner to update if msg.From ~= ao.id then print(\"Only the process owner can update to use patch\") return end -- Add your patch logic here Send({ device = 'patch@1.0', cache = { -- Add the state you want to expose } }) print(\"Process updated to use state patching\") end )This allows users to maintain ownership of their processes while still benefiting from HyperBEAM's performance improvements.For a complete guide on implementing this pattern, see User-Owned Processes.Next Steps ​With state exposure configured, you can now add dynamic reads to compute values on-the-fly without modifying your process state.","estimatedWords":864,"lastModified":"2025-10-20T12:10:30.568Z","breadcrumbs":["migrating to hyperbeam","state exposure"],"siteKey":"ao","siteName":"AO Cookbook","depth":2,"crawledAt":"2025-10-20T12:10:30.568Z"},{"url":"https://cookbook_ao.arweave.net/migrating-to-hyperbeam/aos-cli.html","title":"AOS CLI","content":"AOS CLI ​Use the aos command-line interface to spawn and connect to processes on HyperBEAM mainnet.Understanding Legacy vs. HyperBEAM ​AO is currently maintaining two networks during the transition to HyperBEAM:Legacy Network (aos): The current stable network that most existing processes run onHyperBEAM Mainnet (hyper-aos): The new high-performance network being built outHyperAOS represents the future direction of AOS on HyperBEAM, offering improved performance and new capabilities.Installing aos ​The primary tool for interacting with AO and developing processes is aos, a command-line interface and development environment.npmpnpmbunbashnpm i -g https://get_ao.arweave.netbashpnpm add -g https://get_ao.arweave.netbash# Bun is not supported yet # bun install -g https://get_ao.arweave.netChoosing Your Network ​After installing aos, when you spawn a new process you'll be presented with a selection menu:bashaos myProcessYou'll see:? Please select › - Use arrow-keys. Return to submit. ❯ aos hyper-aos (experimental - DO NOT USE FOR PRODUCTION)Legacy Network Process (aos) ​Select aos to spawn a process on the legacy network (default). This creates a standard AO process on the existing stable network, compatible with all current tooling and processes.HyperBEAM Process (hyper-aos) ​Select hyper-aos to spawn a process directly on HyperBEAM. This is marked as experimental and should not be used for production workloads while HyperBEAM mainnet is being built out.ImportantThe hyper-aos option is experimental and actively under development. Use aos (legacy network) for production processes.Connecting to a Specific HyperBEAM Node ​You can also connect directly to a specific HyperBEAM node:bashaos --url \"https://forward.computer\" myMainnetProcessThis connects you to an interactive Lua environment running within a process on the HyperBEAM network at the specified URL.Running a Local HyperBEAM NodeIf you are running HyperBEAM locally and want to use that node when booting up aos, you must first start your local node with the genesis_wasm profile:bashrebar3 as genesis_wasm shellThen, you can connect aos to it:bashaos --url \"http://localhost:8734\" myLocalProcessUntil aos is fully HyperBEAM native, the genesis_wasm profile is required to run a local Compute Unit (CU) for executing aos.Interacting with Mainnet Processes ​Note on Blocking CallsBlocking message patterns, such as Receive and ao.send().receive(), are not available when running aos against a HyperBEAM process. HyperBEAM processes do not support the underlying wasm modules required for this functionality. You should rely on asynchronous patterns using handlers instead.","estimatedWords":362,"lastModified":"2025-10-20T12:10:31.170Z","breadcrumbs":["migrating to hyperbeam","aos cli"],"siteKey":"ao","siteName":"AO Cookbook","depth":2,"crawledAt":"2025-10-20T12:10:31.170Z"},{"url":"https://cookbook_ao.arweave.net/migrating-to-hyperbeam/why-migrate.html","title":"Why Migrate to HyperBEAM","content":"Why Migrate to HyperBEAM? ​Migrating processes from legacynet to HyperBEAM is essential for leveraging significant advancements in performance, features, and developer experience on AO.HyperBEAM is a new, more robust foundation for decentralized applications on AO, offering several key advantages:Enhanced Performance: Built on an architecture optimized for concurrency, HyperBEAM provides faster message scheduling and more responsive applications.Direct State Access: HyperBEAM allows processes to expose their state directly via HTTP. This enables immediate reads of your process's data, eliminating the need for dry-run messages which were a common performance bottleneck.Easy Extensibility: It allows core feature extensibility through modular devices.The most impactful change when migrating is the ability to expose parts of your process state for immediate reading. This dramatically improves the performance of web frontends and data-driven services.To learn how to implement this, see State Exposure.","estimatedWords":134,"lastModified":"2025-10-20T12:10:31.737Z","breadcrumbs":["migrating to hyperbeam","why migrate"],"siteKey":"ao","siteName":"AO Cookbook","depth":2,"crawledAt":"2025-10-20T12:10:31.737Z"},{"url":"https://cookbook_ao.arweave.net/welcome/getting-started.html","title":"Get started in 5 minutes","content":"Get started in 5 minutes ​In less than 5 mins, we'll walk you through the process of taking your first peek into the rabbit hole of AO Processes. 🕳️🐇Now that you understand the AO-Core protocol and how AO Processes work, let's get hands-on with creating your first AO Process.System requirements ​The local client of aos is super simple to install. Just make sure you have:NodeJS version 20+. (If you haven't yet installed it, check out this page to find instructions for your OS).A code editor of your choice.Installing aos ​Once you have NodeJS on your machine, all you need to do is install aos and run it:shnpm i -g https://get_ao.arweave.netAfter installation, we can simply run the command itself to start a new aos process!shaosYou authenticate yourself to your aos process using a keyfile. If you have an Arweave wallet you can specify it by adding a --wallet [location] flag. If you don't, a new keyfile will be generated and stored locally for you at ~/.aos.json.Welcome to the rabbit hole ​The utility you just started is a local client, which is ready to relay messages for you to your new process inside the ao computer.After it connects, you should see the following:lua _____ _______ _____ /\\ \\ /::\\ \\ /\\ \\ /::\\ \\ /::::\\ \\ /::\\ \\ /::::\\ \\ /::::::\\ \\ /::::\\ \\ /::::::\\ \\ /::::::::\\ \\ /::::::\\ \\ /:::/\\:::\\ \\ /:::/~~\\:::\\ \\ /:::/\\:::\\ \\ /:::/__\\:::\\ \\ /:::/ \\:::\\ \\ /:::/__\\:::\\ \\ /::::\\ \\:::\\ \\ /:::/ / \\:::\\ \\ \\:::\\ \\:::\\ \\ /::::::\\ \\:::\\ \\ /:::/____/ \\:::\\____\\ ___\\:::\\ \\:::\\ \\ /:::/\\:::\\ \\:::\\ \\ |:::| | |:::| | /\\ \\:::\\ \\:::\\ \\ /:::/ \\:::\\ \\:::\\____\\|:::|____| |:::| |/::\\ \\:::\\ \\:::\\____\\ \\::/ \\:::\\ /:::/ / \\:::\\ \\ /:::/ / \\:::\\ \\:::\\ \\::/ / \\/____/ \\:::\\/:::/ / \\:::\\ \\ /:::/ / \\:::\\ \\:::\\ \\/____/ \\::::::/ / \\:::\\ /:::/ / \\:::\\ \\:::\\ \\ \\::::/ / \\:::\\__/:::/ / \\:::\\ \\:::\\____\\ /:::/ / \\::::::::/ / \\:::\\ /:::/ / /:::/ / \\::::::/ / \\:::\\/:::/ / /:::/ / \\::::/ / \\::::::/ / /:::/ / \\::/____/ \\::::/ / \\::/ / ~~ \\::/ / \\/____/ \\/____/ Welcome to AOS: Your operating system for AO, the decentralized open access supercomputer. Type \".load-blueprint chat\" to join the community chat and ask questions! AOS Client Version: 1.12.1. 2024 Type \"Ctrl-C\" twice to exit Your AOS process: QFt5SR6UwJSCnmgnROq62-W8KGY9z96k1oExgn4uAzk default@aos-0.2.2[Inbox:1]>Welcome to your new home in the ao computer! The prompt you are now looking at is your own personal server in this decentralized machine. We will be using it to play with and explore ao in the rest of this tutorial.Sending your first command ​Your new personal aos process is a server that lives inside the computer, waiting to receive and execute your commands.aos loves to make things simple, so it wants to hear commands from you in the Lua programming language. Don't know Lua? Don't panic! It is a super straightforward, friendly, and fun language. We will learn it as we progress through this series.Let's break the ice and type:luaaos> \"Hello, ao!\"Then hit the \"[Enter]\" key. You should see your shell sign and post the message, request the result, then print the result as follows:lua\"Hello, ao!\"Eh. What's the big deal? ​Sent it a message to your process, permanently etched it into Arweave, then asked a distributed compute network to calculate its result.While the result might not look revolutionary, in reality you have done something quite extraordinary. Your process is a decentralized server that doesn't exist in any one particular place on Earth. It exists as data, replicated on Arweave between many different machines, distributed all over the world. If you wanted to, you could now attach a new compute unit to this process and recreate the state from its log of inputs (just your single command, for now) -- at any time in the future.This makes your new shell process...Resilient: There is no single place on Earth where your server actually resides. It is everywhere and nowhere -- immune from physical destruction or tampering of any kind.Permanent: Your process will never disappear. It will always exist in its ✨holographic state✨ on Arweave, allowing you to recall it and continue playing with it. A contribution has been made to Arweave's storage endowment, so that you never have to think about upkeep or maintenance payments again.Permissionless: You did not have to register in order to start this server. Your right to use it is guaranteed by its underlying protocol (Arweave), no matter what Google, Amazon, or any other BigTech company says.Trustless: The state of your server is mathematically guaranteed. This means that you -- and everyone else -- can trust it with certainty, without even having to trust the underlying hardware it runs on. This property lets you build trustless services on top: Code that runs without any privileged owner or controller, ruled purely by math.There is so much more to it, but these are the basics. Welcome to the ao computer, newbie! We are grateful to have you. 🫡Next Steps ​In the tutorials that follow, we will explore ao and build everything from chatrooms to autonomous, decentralized bots. Let's go!","estimatedWords":843,"lastModified":"2025-10-20T12:10:32.317Z","breadcrumbs":["welcome","getting started"],"siteKey":"ao","siteName":"AO Cookbook","depth":2,"crawledAt":"2025-10-20T12:10:32.318Z"},{"url":"https://cookbook_ao.arweave.net/welcome/legacynet-info/index.html","title":"Legacynet  HyperBEAM","content":"Legacynet → HyperBEAM ​As the AO ecosystem evolves, we are transitioning from Legacynet to HyperBEAM Mainnet, marking a significant upgrade in the implementation of the AO-Core protocol.Legacynet: The Initial Implementation ​Legacynet was the first implementation of the AO-Core protocol, written in JavaScript. Launched on February 27, 2024, it provided a fee-free environment for early adopters to experiment with AO's hyper-parallel architecture. However, being a JavaScript implementation, Legacynet had inherent limitations in terms of scalability and native support for the actor-oriented model that AO is based on.HyperBEAM: The Future of AO-Core ​HyperBEAM is the new, advanced implementation of the AO-Core protocol, written in Erlang—the language that inspired AO's actor-oriented design. This implementation innately benefits from Erlang's strengths in:Actor-Oriented Design: Erlang's native support for the actor model aligns perfectly with AO's architecture, where processes (actors) operate independently and communicate via message passing.Scalability: Erlang is renowned for its ability to handle massive concurrency, allowing HyperBEAM to scale efficiently with the growing demands of the AO computer.Reliability: Erlang's design for fault tolerance ensures that HyperBEAM can maintain system stability even under high load or during failures of individual components.The Transition to HyperBEAM ​While HyperBEAM represents the future of AO, the transition from Legacynet is being handled carefully to ensure a smooth experience for developers. Currently, most development activity remains on Legacynet, which provides a stable environment for building and testing.The goal is to provide a seamless future upgrade path to HyperBEAM Mainnet. While Legacynet will eventually be deprecated, for now, it is the primary environment for new developers to begin building on AO.HyperBEAM Documentation ​For detailed documentation on the HyperBEAM protocol itself, including running infrastructure and leveraging its powerful URL pathing, visit HyperBEAM.arweave.net.Building on HyperBEAMTo learn how to build applications on HyperBEAM using ao and aos, and to migrate existing processes, see the Migrating to HyperBEAM Guide.Preparing for the Future ​While you build on Legacynet, you can prepare for the future of AO by:Reviewing the HyperBEAM documentation to understand the new environment and its architecture.Exploring the enhanced capabilities that HyperBEAM offers due to its Erlang foundation.Building with the knowledge that a seamless migration path to HyperBEAM Mainnet is a core priority.This transition is a significant step forward for the AO ecosystem, ensuring that we can deliver on the promise of decentralized, hyper-parallel computation at any scale.","estimatedWords":382,"lastModified":"2025-10-20T12:10:32.902Z","breadcrumbs":["welcome","legacynet info","index"],"siteKey":"ao","siteName":"AO Cookbook","depth":3,"crawledAt":"2025-10-20T12:10:32.903Z"},{"url":"https://cookbook_ao.arweave.net/welcome/ao-processes.html","title":"AO Processes","content":"AO Processes ​AO Processes are persistent, programmable smart contracts that live inside the AO computer. Embodying the actor model from Erlang that inspired AO, these processes operate as independent computational units that have their own state and communicate with each other through message passing. This architecture makes them ideal for creating autonomous agents and complex decentralized applications.What are AO Processes? ​Following the actor model, each AO Process functions as an independent actor within the system, executing code—typically written in Lua—in response to messages it receives. Three core characteristics define them:Stateful: Each process has its own private state and memory, which persist across interactions.Persistent: All processes and their entire message history are permanently stored on Arweave.Generative: Processes can dynamically spawn new processes, enabling complex and evolving systems.AO Processes and the Actor Model ​The actor model provides several key benefits for process-based development, enabling naturally concurrent and resilient systems. By treating every process as an isolated \"actor,\" it simplifies development and enhances fault tolerance. Key advantages include:Concurrency & Isolation: Processes execute independently and are isolated from each other, enabling parallelism and preventing cascading failures.Message-Passing: All communication happens exclusively through asynchronous messages, simplifying interactions.Location Transparency & Fault Tolerance: Processes can interact without knowing each other's physical location on the network, and the system can continue operating even if individual processes fail.AOS: The Operating System for AO Processes ​AOS (AO Operating System) is an abstraction layer designed to simplify interaction with AO Processes. It provides developers with a powerful shell interface for sending commands, tools for managing process state, and a set of libraries for common functionalities, all contributing to a more streamlined development experience.Use Cases for AO Processes ​The persistent and concurrent nature of AO Processes makes them ideal for a wide range of decentralized applications. Here are a few examples:Autonomous Agents & Bots: Imagine a price-monitoring bot that tracks token prices across different decentralized exchanges (DEXs) and executes arbitrage trades automatically. AO makes it possible to build entire marketplaces for such agents, like Marketverse.Decentralized Finance (DeFi): You could build automated market makers (AMMs) or lending protocols where account balances and token reserves are tracked persistently within the process's state. A live example of this is Dexi, a decentralized exchange built on AO.On-Chain Games & Social Platforms: AO Processes can power fully on-chain games where the game state (like player positions or inventory) is managed by one or more processes, like the space strategy game Stargrid. They're also perfect for decentralized chat applications or social networks where user profiles, posts, and interactions are censorship-resistant.Next Steps ​Now that you understand the capabilities of AO Processes, the next step is to dive into Hyperbeam, the high-performance network that powers them.","estimatedWords":445,"lastModified":"2025-10-20T12:10:33.506Z","breadcrumbs":["welcome","ao processes"],"siteKey":"ao","siteName":"AO Cookbook","depth":2,"crawledAt":"2025-10-20T12:10:33.506Z"},{"url":"https://cookbook_ao.arweave.net/llms-explanation.html","title":"LLMs Documentation","content":"LLMs Documentation ​llms.txt: ​Structured overview of the ao ecosystem.Ideal for AI tools navigating documentation or answering general questions.Suited for agents with web search capabilities.llms-full.txt: ​Complete technical documentation.Designed for in-depth analysis, troubleshooting, or chatbot integration.Provides exhaustive details for complex queries.INFOThe llms-full.txt file only contains content from references and release notes, as testing showed this focused approach performs better with current AI models.Permaweb LLMs.txt: ​The following is a tool that allows you to build your own LLMs.txt files based on docs from the permaweb ecosystem.","estimatedWords":83,"lastModified":"2025-10-20T12:10:34.143Z","breadcrumbs":["llms explanation"],"siteKey":"ao","siteName":"AO Cookbook","depth":1,"crawledAt":"2025-10-20T12:10:34.143Z"},{"url":"https://cookbook_ao.arweave.net/welcome/ao-core-introduction.html","title":"Introduction to AO-Core","content":"Introduction to AO-Core ​AO-Core is a protocol and standard for distributed computation that forms the foundation of the AO computer. Inspired by and built upon concepts from the Erlang language, AO-Core embraces the actor model for concurrent, distributed systems. Unlike traditional blockchain systems, AO-Core defines a flexible, powerful computation protocol that enables a wide range of applications beyond just running Lua programs.What is AO-Core? ​AO-Core is the fundamental protocol of the AO computer that:Defines standards for trustless computation distributed across the worldProvides mathematical guarantees about program executionEnables composable, modular development through devicesSupports various execution environments beyond just LuaImplements the actor model for concurrent, message-passing computationThe Actor Model in AO ​AO references the actor model of computation where:Each actor (or process) is an independent unit of computationActors communicate exclusively through message passingActors can create other actors, send messages, and make local decisionsThe system is inherently concurrent and distributedThis approach, inspired by Erlang, provides natural scalability and resilience in distributed systems.Key Features of AO-Core ​Resilient: There is no single point of failure. AO-Core exists across many machines distributed worldwide, making it immune to physical destruction or tampering.Permanent: Computations following the AO-Core protocol are stored permanently on Arweave, allowing you to recall and continue your work at any time.Permissionless: No registration is required to use AO-Core. Your right to use it is guaranteed by the underlying protocol.Trustless: The state of your computations is mathematically guaranteed, allowing you to build services that don't require trust in any central authority.Beyond Just Processes ​While AO Processes (smart contracts built using the AO-Core protocol) are powerful for creating autonomous agents, AO-Core itself enables much more:Serverless functions with trustworthy guaranteesHybrid applications combining smart contract and serverless functionalityCustom execution environments through different devicesComposable systems using the path languageNext Steps ​In the following sections, we'll explore how AO Processes build on top of the AO-Core protocol, and how you can get started building your own applications in this powerful environment.","estimatedWords":320,"lastModified":"2025-10-20T12:10:34.707Z","breadcrumbs":["welcome","ao core introduction"],"siteKey":"ao","siteName":"AO Cookbook","depth":2,"crawledAt":"2025-10-20T12:10:34.707Z"},{"url":"https://cookbook_ao.arweave.net/guides/aos/blueprints/token.html","title":"Token Blueprint","content":"Token Blueprint ​The Token Blueprint is a predesigned template that helps you quickly build a token in ao. It is a great way to get started and can be customized to fit your needs.Unpacking the Token Blueprint ​Balances: The Balances array is used to store the token balances of the participants.Info Handler: The info handler allows processes to retrieve the token parameters, like Name, Ticker, Logo, and Denomination.Balance Handler: The balance handler allows processes to retrieve the token balance of a participant.Balances Handler: The balances handler allows processes to retrieve the token balances of all participants.Transfer Handler: The transfer handler allows processes to send tokens to another participant.Mint Handler: The mint handler allows processes to mint new tokens.Total Supply Handler: The totalSupply handler allows processes to retrieve the total supply of the token.Burn Handler: The burn handler allows processes to burn tokens.How To Use: ​Open your preferred text editor.Open the Terminal.Start your aos process.Type in .load-blueprint tokenVerify the Blueprint is Loaded: ​Type in Handlers.list to see the newly loaded handlers.What's in the Token Blueprint: ​lualocal bint = require('.bint')(256) --[[ This module implements the ao Standard Token Specification. Terms: Sender: the wallet or Process that sent the Message It will first initialize the internal state, and then attach handlers, according to the ao Standard Token Spec API: - Info(): return the token parameters, like Name, Ticker, Logo, and Denomination - Balance(Target?: string): return the token balance of the Target. If Target is not provided, the Sender is assumed to be the Target - Balances(): return the token balance of all participants - Transfer(Target: string, Quantity: number): if the Sender has a sufficient balance, send the specified Quantity to the Target. It will also issue a Credit-Notice to the Target and a Debit-Notice to the Sender - Mint(Quantity: number): if the Sender matches the Process Owner, then mint the desired Quantity of tokens, adding them the Processes' balance ]] -- local json = require('json') --[[ utils helper functions to remove the bint complexity. ]] -- local utils = { add = function(a, b) return tostring(bint(a) + bint(b)) end, subtract = function(a, b) return tostring(bint(a) - bint(b)) end, toBalanceValue = function(a) return tostring(bint(a)) end, toNumber = function(a) return bint.tonumber(a) end } --[[ Initialize State ao.id is equal to the Process.Id ]] -- Variant = \"0.0.3\" -- token should be idempotent and not change previous state updates Denomination = Denomination or 12 Balances = Balances or { [ao.id] = utils.toBalanceValue(10000 * 10 ^ Denomination) } TotalSupply = TotalSupply or utils.toBalanceValue(10000 * 10 ^ Denomination) Name = Name or 'Points Coin' Ticker = Ticker or 'PNTS' Logo = Logo or 'SBCCXwwecBlDqRLUjb8dYABExTJXLieawf7m2aBJ-KY' --[[ Add handlers for each incoming Action defined by the ao Standard Token Specification ]] -- --[[ Info ]] -- Handlers.add('info', \"Info\", function(msg) msg.reply({ Name = Name, Ticker = Ticker, Logo = Logo, Denomination = tostring(Denomination) }) end) --[[ Balance ]] -- Handlers.add('balance', \"Balance\", function(msg) local bal = '0' -- If not Recipient is provided, then return the Senders balance if (msg.Tags.Recipient) then if (Balances[msg.Tags.Recipient]) then bal = Balances[msg.Tags.Recipient] end elseif msg.Tags.Target and Balances[msg.Tags.Target] then bal = Balances[msg.Tags.Target] elseif Balances[msg.From] then bal = Balances[msg.From] end msg.reply({ Balance = bal, Ticker = Ticker, Account = msg.Tags.Recipient or msg.From, Data = bal }) end) --[[ Balances ]] -- Handlers.add('balances', \"Balances\", function(msg) msg.reply({ Data = json.encode(Balances) }) end) --[[ Transfer ]] -- Handlers.add('transfer', \"Transfer\", function(msg) assert(type(msg.Recipient) == 'string', 'Recipient is required!') assert(type(msg.Quantity) == 'string', 'Quantity is required!') assert(bint.__lt(0, bint(msg.Quantity)), 'Quantity must be greater than 0') if not Balances[msg.From] then Balances[msg.From] = \"0\" end if not Balances[msg.Recipient] then Balances[msg.Recipient] = \"0\" end if bint(msg.Quantity) <= bint(Balances[msg.From]) then Balances[msg.From] = utils.subtract(Balances[msg.From], msg.Quantity) Balances[msg.Recipient] = utils.add(Balances[msg.Recipient], msg.Quantity) --[[ Only send the notifications to the Sender and Recipient if the Cast tag is not set on the Transfer message ]] -- if not msg.Cast then -- Debit-Notice message template, that is sent to the Sender of the transfer local debitNotice = { Action = 'Debit-Notice', Recipient = msg.Recipient, Quantity = msg.Quantity, Data = Colors.gray .. \"You transferred \" .. Colors.blue .. msg.Quantity .. Colors.gray .. \" to \" .. Colors.green .. msg.Recipient .. Colors.reset } -- Credit-Notice message template, that is sent to the Recipient of the transfer local creditNotice = { Target = msg.Recipient, Action = 'Credit-Notice', Sender = msg.From, Quantity = msg.Quantity, Data = Colors.gray .. \"You received \" .. Colors.blue .. msg.Quantity .. Colors.gray .. \" from \" .. Colors.green .. msg.From .. Colors.reset } -- Add forwarded tags to the credit and debit notice messages for tagName, tagValue in pairs(msg) do -- Tags beginning with \"X-\" are forwarded if string.sub(tagName, 1, 2) == \"X-\" then debitNotice[tagName] = tagValue creditNotice[tagName] = tagValue end end -- Send Debit-Notice and Credit-Notice msg.reply(debitNotice) Send(creditNotice) end else msg.reply({ Action = 'Transfer-Error', ['Message-Id'] = msg.Id, Error = 'Insufficient Balance!' }) end end) --[[ Mint ]] -- Handlers.add('mint', \"Mint\", function(msg) assert(type(msg.Quantity) == 'string', 'Quantity is required!') assert(bint(0) < bint(msg.Quantity), 'Quantity must be greater than zero!') if not Balances[ao.id] then Balances[ao.id] = \"0\" end if msg.From == ao.id then -- Add tokens to the token pool, according to Quantity Balances[msg.From] = utils.add(Balances[msg.From], msg.Quantity) TotalSupply = utils.add(TotalSupply, msg.Quantity) msg.reply({ Data = Colors.gray .. \"Successfully minted \" .. Colors.blue .. msg.Quantity .. Colors.reset }) else msg.reply({ Action = 'Mint-Error', ['Message-Id'] = msg.Id, Error = 'Only the Process Id can mint new ' .. Ticker .. ' tokens!' }) end end) --[[ Total Supply ]] -- Handlers.add('totalSupply', \"Total-Supply\", function(msg) assert(msg.From ~= ao.id, 'Cannot call Total-Supply from the same process!') msg.reply({ Action = 'Total-Supply', Data = TotalSupply, Ticker = Ticker }) end) --[[ Burn ]] -- Handlers.add('burn', 'Burn', function(msg) assert(type(msg.Quantity) == 'string', 'Quantity is required!') assert(bint(msg.Quantity) <= bint(Balances[msg.From]), 'Quantity must be less than or equal to the current balance!') Balances[msg.From] = utils.subtract(Balances[msg.From], msg.Quantity) TotalSupply = utils.subtract(TotalSupply, msg.Quantity) msg.reply({ Data = Colors.gray .. \"Successfully burned \" .. Colors.blue .. msg.Quantity .. Colors.reset }) end)","estimatedWords":974,"lastModified":"2025-10-20T12:10:35.346Z","breadcrumbs":["guides","aos","blueprints","token"],"siteKey":"ao","siteName":"AO Cookbook","depth":4,"crawledAt":"2025-10-20T12:10:35.346Z"},{"url":"https://cookbook_ao.arweave.net/guides/aos/blueprints/voting.html","title":"Voting Blueprint","content":"Voting Blueprint ​The Voting Blueprint is a predesigned template that helps you quickly build a voting system in ao. It is a great way to get started and can be customized to fit your needs.Prerequisites ​The Staking Blueprint requires the Token Blueprint to be loaded, first.Unpacking the Voting Blueprint ​Balances: The Balances array is used to store the token balances of the participants.Votes: The Votes array is used to store the votes of the participants.Vote Action Handler: The vote handler allows processes to vote. When a process sends a message with the tag Action = \"Vote\", the handler will add the vote to the Votes array and send a message back to the process confirming the vote.Finalization Handler: The finalize handler allows processes to finalize the voting process. When a process sends a message with the tag Action = \"Finalize\", the handler will process the votes and finalize the voting process.How To Use: ​Open your preferred text editor.Open the Terminal.Start your aos process.Type in .load-blueprint votingVerify the Blueprint is Loaded: ​Type in Handlers.list to see the newly loaded handlers.What's in the Voting Blueprint: ​luaBalances = Balances or {} Votes = Votes or {} -- Vote Action Handler Handlers.vote = function(msg) local quantity = Stakers[msg.From].amount local target = msg.Tags.Target local side = msg.Tags.Side local deadline = tonumber(msg['Block-Height']) + tonumber(msg.Tags.Deadline) assert(quantity > 0, \"No staked tokens to vote\") Votes[target] = Votes[target] or { yay = 0, nay = 0, deadline = deadline } Votes[target][side] = Votes[target][side] + quantity end -- Finalization Handler local finalizationHandler = function(msg) local currentHeight = tonumber(msg['Block-Height']) -- Process voting for target, voteInfo in pairs(Votes) do if currentHeight >= voteInfo.deadline then if voteInfo.yay > voteInfo.nay then print(\"Handle Vote\") end -- Clear the vote record after processing Votes[target] = nil end end end -- wrap function to continue handler flow local function continue(fn) return function (msg) local result = fn(msg) if (result) == -1 then return 1 end return result end end Handlers.add(\"vote\", continue(Handlers.utils.hasMatchingTag(\"Action\", \"Vote\")), Handlers.vote) -- Finalization handler should be called for every message Handlers.add(\"finalize\", function (msg) return -1 end, finalizationHandler)","estimatedWords":342,"lastModified":"2025-10-20T12:10:35.983Z","breadcrumbs":["guides","aos","blueprints","voting"],"siteKey":"ao","siteName":"AO Cookbook","depth":4,"crawledAt":"2025-10-20T12:10:35.983Z"},{"url":"https://cookbook_ao.arweave.net/guides/aos/modules/json.html","title":"JSON","content":"JSON ​The JSON module allows you to encode and decode objects using JavaScript Object Notation.Example usage ​lualocal json = require(\"json\") json.encode({ a_string = \"This is a string\", nums = { 1, 2, 3 } })Module functions ​encode() ​This function returns a string representation of a Lua object in JSON.Parameters:val: {any} The object to format as JSONReturns: JSON string representation of the provided objectExample ​lua--[[ prints: \"[{\"name\":\"John Doe\",\"age\":23},{\"name\":\"Bruce Wayne\",age:34}]\" ]]-- print(json.encode({ { name = \"John Doe\", age = 23 }, { name = \"Bruce Wayne\", age = 34 } })) -- prints \"false\" print(json.encode(false))decode() ​The function takes a JSON string and turns it into a Lua object.Parameters:val: {any} The JSON string to decodeReturns: Lua object corresponding to the JSON string (throws an error for invalid JSON strings)Example ​lua--[[ creates the following table: { hello = \"world\" } ]]-- json.decode('{ \"hello\": \"world\" }') -- creates a boolean with true value json.decode(\"true\")","estimatedWords":149,"lastModified":"2025-10-20T12:10:36.943Z","breadcrumbs":["guides","aos","modules","json"],"siteKey":"ao","siteName":"AO Cookbook","depth":4,"crawledAt":"2025-10-20T12:10:36.943Z"},{"url":"https://cookbook_ao.arweave.net/guides/aos/modules/ao.html","title":"ao","content":"ao ​Built-in global library for process communication and management. The ao object provides core functionality for sending messages, spawning processes, and logging.Core Functions ​ao.send(msg) ​Sends a message to another process. See the ao.send reference for more information.lua-- Send a simple message ao.send({ Target = \"usjm4PCxUd5mtaon7zc97-dt-3qf67yPyqgzLnLqk5A\", Data = \"Hello!\", Tags = { Action = \"Greeting\" } }) -- Root-level fields are automatically converted to tags ao.send({ Target = \"usjm4PCxUd5mtaon7zc97-dt-3qf67yPyqgzLnLqk5A\", Data = \"Transfer tokens\", Action = \"Transfer\", -- Becomes a tag Quantity = \"1045\" -- Becomes a tag })ao.spawn(module: string, spawn: table) ​Creates a new process from a module. See the ao.spawn reference for more information.lua-- Spawn a calculator process ao.spawn(\"n0BFH80b73mi9VAWUzyuG9gEC3LI2zU2BFxum0N8A9s\", { Data = { initial = \"state\" }, Tags = { [\"Process-Type\"] = \"calculator\" } })ao.log(string|table) ​Logs messages or data that can be read by process callers.lua-- Log a debug message ao.log(\"Processing transfer...\") -- Log structured data ao.log({ event = \"transfer_complete\", amount = 100, recipient = \"addr123...\" })Environment ​The ao.env variable contains process initialization info like ID, owner, and tags.lua-- Access process info local processId = ao.env.Process.Id local owner = ao.env.Process.OwnerFor the complete API reference including all properties and functions, see the ao reference documentation.","estimatedWords":194,"lastModified":"2025-10-20T12:10:37.692Z","breadcrumbs":["guides","aos","modules","ao"],"siteKey":"ao","siteName":"AO Cookbook","depth":4,"crawledAt":"2025-10-20T12:10:37.692Z"},{"url":"https://cookbook_ao.arweave.net/guides/aos/modules/crypto.html","title":"crypto","content":"crypto ​Overview ​The crypto module provides a set of cryptographic primitives like digests, ciphers and other cryptographic algorithms in pure Lua. It offers several functionalities to hash, encrypt and decrypt data, simplifying the development of secure communication and data storage. This document will guide you through the module's functionalities, installation, and usage.Usage ​lualocal crypto = require(\".crypto\");Primitives ​Digests (sha1, sha2, sha3, keccak, blake2b, etc.)Ciphers (AES, ISSAC, Morus, NORX, etc.)Random Number Generators (ISAAC)MACs (HMAC)KDFs (PBKDF2)Utilities (Array, Stream, Queue, etc.)Digests ​MD2 ​Calculates the MD2 digest of a given message.Parameters:stream (Stream): The message in form of streamReturns: A table containing functions to get digest in different formats.asBytes(): The digest as byte table.asHex(): The digest as string in hexadecimal format.asString(): The digest as string format.Example:lualocal str = crypto.utils.stream.fromString(\"ao\") return crypto.digest.md2(str).asHex() -- 0d4e80edd07bee6c7965b21b25a9b1eaMD4 ​Calculates the MD4 digest of a given message.Parameters:stream (Stream): The message in form of streamReturns: A table containing functions to get digest in different formats.asBytes(): The digest as byte table.asHex(): The digest as string in hexadecimal format.asString(): The digest as string format.Example:lualocal str = crypto.utils.stream.fromString(\"ao\") return crypto.digest.md4(str).asHex() -- e068dfe3d8cb95311b58be566db66954MD5 ​Calculates the MD5 digest of a given message.Parameters:stream (Stream): The message in form of streamReturns: A table containing functions to get digest in different formats.asBytes(): The digest as byte table.asHex(): The digest as string in hexadecimal format.asString(): The digest as string format.Example:lualocal str = crypto.utils.stream.fromString(\"ao\") return crypto.digest.md5(str).asHex() -- adac5e63f80f8629e9573527b25891d3SHA1 ​Calculates the SHA1 digest of a given message.Parameters:stream (Stream): The message in form of streamReturns: A table containing functions to get digest in different formats.asBytes(): The digest as byte table.asHex(): The digest as string in hexadecimal format.asString(): The digest as string format.Example:lualocal str = crypto.utils.stream.fromString(\"ao\") return crypto.digest.sha1(str).asHex() -- c29dd6c83b67a1d6d3b28588a1f068b68689aa1dSHA2_256 ​Calculates the SHA2-256 digest of a given message.Parameters:stream (Stream): The message in form of streamReturns: A table containing functions to get digest in different formats. asBytes(): The digest as byte table.asHex(): The digest as string in hexadecimal format.asString(): The digest as string format.Example:lualocal str = crypto.utils.stream.fromString(\"ao\") return crypto.digest.sha2_256(str).asHex() -- ba7816bf8f01cfea414140de5dae2223b00361a396177a9cb410ff61f20015adSHA2_512 ​Calculates the SHA2-512 digest of a given message.Parameters:msg (string): The message to calculate the digestReturns: A table containing functions to get digest in different formats. asBytes(): The digest as byte table.asHex(): The digest as string in hexadecimal format.asString(): The digest as string format.Example:lualocal str = \"ao\" return crypto.digest.sha2_512(str).asHex() -- 6f36a696b17ce5a71efa700e8a7e47994f3e134a5e5f387b3e7c2c912abe94f94ee823f9b9dcae59af99e2e34c8b4fb0bd592260c6720ee49e5deaac2065c4b1SHA3 ​It contains the following functions:sha3_256sha3_512keccak256keccak512Each function calculates the respective digest of a given message.Parameters:msg (string): The message to calculate the digestReturns: A table containing functions to get digest in different formats.asBytes(): The digest as byte table.asHex(): The digest as string in hexadecimal format.asString(): The digest as string format.Example:lua local str = \"ao\" crypto.digest.sha3_256(str).asHex() -- 1bbe785577db997a394d5b4555eec9159cb51f235aec07514872d2d436c6e985 crypto.digest.sha3_512(str).asHex() -- 0c29f053400cb1764ce2ec555f598f497e6fcd1d304ce0125faa03bb724f63f213538f41103072ff62ddee701b52c73e621ed4d2254a3e5e9a803d83435b704d crypto.digest.keccak256(str).asHex() -- 76da52eec05b749b99d6e62bb52333c1569fe75284e6c82f3de12a4618be00d6 crypto.digest.keccak512(str).asHex() -- 046fbfad009a12cef9ff00c2aac361d004347b2991c1fa80fba5582251b8e0be8def0283f45f020d4b04ff03ead9f6e7c43cc3920810c05b33b4873b99affdeaBlake2b ​Calculates the Blake2b digest of a given message.Parameters:data (string): The data to be hashed.outlen (number): The length of the output hash (optional) default is 64.key (string): The key to be used for hashing (optional) default is \"\".Returns: A table containing functions to get digest in different formats.asBytes(): The digest as byte table.asHex(): The digest as string in hexadecimal format.asString(): The digest as string format.Example:lualocal str = \"ao\" crypto.digest.blake2b(str).asHex() -- 576701fd79a126f2c414ef94adf1117c88943700f312679d018c29c378b2c807a3412b4e8d51e191c48fb5f5f54bf1bca29a714dda166797b3baf9ead862ae1d crypto.digest.blake2b(str, 32).asHex() -- 7050811afc947ba7190bb3c0a7b79b4fba304a0de61d529c8a35bdcbbb5544f4 crypto.digest.blake2b(str, 32, \"secret_key\").asHex() -- 203c101980fdf6cf24d78879f2e3db86d73d91f7d60960b642022cd6f87408f8Ciphers ​AES ​The Advanced Encryption Standard (AES) is a symmetric block cipher used to encrypt sensitive information. It has two functions encrypt and decrypt.Encrypt ​Encrypts a given message using the AES algorithm.Parameters:data (string): The data to be encrypted.key (string): The key to be used for encryption.iv (string) optional: The initialization vector to be used for encryption. default is \"\"mode (string) optional: The mode of operation to be used for encryption. default is \"CBC\". Available modes are CBC, ECB, CFB, OFB, CTR.keyLength (number) optional: The length of the key to use for encryption. default is 128.Returns: A table containing functions to get encrypted data in different formats.asBytes(): The encrypted data as byte table.asHex(): The encrypted data as string in hexadecimal format.asString(): The encrypted data as string format.Decrypt ​Decrypts a given message using the AES algorithm.Parameters:cipher (string): Hex Encoded encrypted data.key (string): The key to be used for decryption.iv (string) optional: The initialization vector to be used for decryption. default is \"\"mode (string) optional: The mode of operation to be used for decryption. default is \"CBC\". Available modes are CBC, ECB, CFB, OFB, CTR.keyLength (number) optional: The length of the key to use for decryption. default is 128.Returns: A table containing functions to get decrypted data in different formats.asBytes(): The decrypted data as byte table.asHex(): The decrypted data as string in hexadecimal format.asString(): The decrypted data as string format.Example:lualocal str = \"ao\" local iv = \"super_secret_shh\" local key_128 = \"super_secret_shh\" local encrypted = crypto.cipher.aes.encrypt(\"ao\", key, iv).asHex() -- A3B9E6E1FBD9D46930E5F76807C84B8E local decrypted = crypto.cipher.aes.decrypt(encrypted, key, iv).asHex() -- 616F0000000000000000000000000000 crypto.utils.hex.hexToString(decrypted) -- aoISSAC Cipher ​ISAAC is a cryptographically secure pseudo-random number generator (CSPRNG) and stream cipher. It has the following functionsseedIsaac: Seeds the ISAAC cipher with a given seed.getRandomChar: Generates a random character using the ISAAC cipher.random: Generates a random number between a given range using the ISAAC cipher.getRandom: Generates a random number using the ISAAC cipher.encrypt: Encrypts a given message using the ISAAC cipher.decrypt: Decrypts a given message using the ISAAC cipher.Encrypt ​Encrypts a given message using the ISAAC cipher.Parameters:msg (string): The message to be encrypted.key (string): The key to be used for encryption.Returns: A table containing functions to get encrypted data in different formats. asBytes(): The encrypted data as byte table.asHex(): The encrypted data as string in hexadecimal format.asString(): The encrypted data as string format.Decrypt ​Decrypts a given message using the ISAAC cipher.Parameters:cipher (string): Hex Encoded encrypted data.key (string): Key to be used for decryption.Returns: A table containing functions to get decrypted data in different formats. asBytes(): The decrypted data as byte table.asHex(): The decrypted data as string in hexadecimal format.asString(): The decrypted data as string format.Example:lualocal message = \"ao\"; local key = \"secret_key\"; local encrypted = crypto.cipher.issac.encrypt(message, key) local decrypted = crypto.cipher.issac.decrypt(encrypted.asString(), key) -- ao encrypted.asHex() -- 7851random ​Generates a random number using the ISAAC cipher.Parameters:min (number) optional: The minimum value of the random number. defaults to 0.max (number) optional: The maximum value of the random number. defaults to 2^31 - 1.seed (string) optional: The seed to be used for generating the random number. defaults to math.random(0,2^32 - 1).Returns: A random number between the given range.Example:luacrypto.cipher.issac.random(0, 100) -- 42Morus Cipher ​MORUS is a high-performance authenticated encryption algorithm submitted to the CAESAR competition, and recently selected as a finalist.Encrypt ​Encrypts a given message using the MORUS cipher.Parameters:key (string): The encryption key (16 or 32-byte string).iv (string): The nonce or initial value (16-byte string).msg (string): The message to encrypt (variable length string).ad (string) optional: The additional data (variable length string). defaults to \"\".Returns: A table containing functions to get encrypted data in different formats. asBytes(): The encrypted data as byte table.asHex(): The encrypted data as string in hexadecimal format.asString(): The encrypted data as string format.Decrypt ​Decrypts a given message using the MORUS cipher.Parameters:key (string): The encryption key (16 or 32-byte string).iv (string): The nonce or initial value (16-byte string).cipher (string): The encrypted message (variable length string).adLen (number) optional: The length of the additional data (variable length string). defaults to 0.Returns: A table containing functions to get decrypted data in different formats. asBytes(): The decrypted data as byte table.asHex(): The decrypted data as string in hexadecimal format.asString(): The decrypted data as string format.Example:lualocal m = \"ao\" local k = \"super_secret_shh\" local iv = \"0000000000000000\" local ad= \"\" local e = crypto.cipher.morus.encrypt(k, iv, m, ad) local d = crypto.cipher.morus.decrypt(k, iv, e.asString(), #ad) -- ao e.asHex() -- 514ed31473d8fb0b76c6cbb17af35ed01d0aNORX Cipher ​NORX is an authenticated encryption scheme with associated data that was selected, along with 14 other primitives, for the third phase of the ongoing CAESAR competition. It is based on the sponge construction and relies on a simple permutation that allows efficient and versatile implementations.Encrypt ​Encrypts a given message using the NORX cipher.Parameters:key (string): The encryption key (32-byte string).nonce (string): The nonce or initial value (32-byte string).plain (string): The message to encrypt (variable length string).header (string) optional: The additional data (variable length string). defaults to \"\".trailer (string) optional: The additional data (variable length string). defaults to \"\".Returns: A table containing functions to get encrypted data in different formats. asBytes(): The encrypted data as byte table.asHex(): The encrypted data as string in hexadecimal format.asString(): The encrypted data as string format.Decrypt ​Decrypts a given message using the NORX cipher.Parameters:key (string): The encryption key (32-byte string).nonce (string): The nonce or initial value (32-byte string).crypted (string): The encrypted message (variable length string).header (string) optional: The additional data (variable length string). defaults to \"\".trailer (string) optional: The additional data (variable length string). defaults to \"\".Returns: A table containing functions to get decrypted data in different formats. asBytes(): The decrypted data as byte table.asHex(): The decrypted data as string in hexadecimal format.asString(): The decrypted data as string format.Example:lualocal key = \"super_duper_secret_password_shhh\" local nonce = \"00000000000000000000000000000000\" local data = \"ao\" -- Header and trailer are optional local header, trailer = data, data local encrypted = crypto.cipher.norx.encrypt(key, nonce, data, header, trailer).asString() local decrypted = crypto.cipher.norx.decrypt(key, nonce, encrypted, header, trailer) -- ao local authTag = encrypted:sub(#encrypted-32+1) crypto.utils.hex.stringToHex(encrypted) -- 0bb35a06938e6541eccd4440adb7b46118535f60b09b4adf378807a53df19fc4ea28 crypto.utils.hex.stringToHex(authTag) -- 5a06938e6541eccd4440adb7b46118535f60b09b4adf378807a53df19fc4ea28Random Number Generators ​The module contains a random number generator using ISAAC which is a cryptographically secure pseudo-random number generator (CSPRNG) and stream cipher.Parameters:min (number) optional: The minimum value of the random number. defaults to 0.max (number) optional: The maximum value of the random number. defaults to 2^31 - 1.seed (string) optional: The seed to be used for generating the random number. defaults to math.random(0,2^32 - 1).Returns: A random number between the given range.Example:luacrypto.random.(0, 100, \"seed\") -- 42MACs ​HMAC ​The Hash-based Message Authentication Code (HMAC) is a mechanism for message authentication using cryptographic hash functions. HMAC can be used with any iterative cryptographic hash function, e.g., MD5, SHA-1, in combination with a secret shared key.The modules exposes a function called createHmac which is used to create a HMAC instance.Parameters:data (Stream): The data to be hashed.key (Array): The key to be used for hashing.algorithm (string) optional: The algorithm to be used for hashing. default is \"sha256\". Available algorithms are \"sha1\", \"sha256\". default is \"sha1\".Returns: A table containing functions to get HMAC in different formats. asBytes(): The HMAC as byte table.asHex(): The HMAC as string in hexadecimal format.asString(): The HMAC as string format.Example:lualocal data = crypto.utils.stream.fromString(\"ao\") local key = crypto.utils.array.fromString(\"super_secret_key\") crypto.mac.createHmac(data, key).asHex() -- 3966f45acb53f7a1a493bae15afecb1a204fa32d crypto.mac.createHmac(data, key, \"sha256\").asHex() -- 542da02a324155d688c7689669ff94c6a5f906892aa8eccd7284f210ac66e2a7KDFs ​PBKDF2 ​The Password-Based Key Derivation Function 2 (PBKDF2) applies a pseudorandom function, such as hash-based message authentication code (HMAC), to the input password or passphrase along with a salt value and repeats the process many times to produce a derived key, which can then be used as a cryptographic key in subsequent operations.Parameters:password (Array): The password to derive the key from.salt (Array): The salt to use.iterations (number): The number of iterations to perform.keyLen (number): The length of the key to derive.digest (string) optional: The digest algorithm to use. default is \"sha1\". Available algorithms are \"sha1\", \"sha256\".Returns: A table containing functions to get derived key in different formats. asBytes(): The derived key as byte table.asHex(): The derived key as string in hexadecimal format.asString(): The derived key as string format.Example:lualocal salt = crypto.utils.array.fromString(\"salt\") local password = crypto.utils.array.fromString(\"password\") local iterations = 4 local keyLen = 16 local res = crypto.kdf.pbkdf2(password, salt, iterations, keyLen).asHex() -- C4C21BF2BBF61541408EC2A49C89B9C6Utilities ​Array ​Example Usage:lua local arr = crypto.utils.array arr.fromString(\"ao\") -- Array arr.toString(arr.fromString(\"ao\")) -- ao arr.fromHex(\"616f\") -- Array arr.toHex(arr.fromHex(\"616f\")) -- 616f arr.concat(arr.fromString(\"a\"), arr.fromString(\"o\")) -- Array arr.truncate(arr.fromString(\"ao\"), 1) -- Array arr.XOR(arr.fromString(\"a\"), arr.fromString(\"o\")) -- Array arr.substitute(arr.fromString(\"a\"), arr.fromString(\"o\")) -- Array arr.permute(arr.fromString(\"a\"), arr.fromString(\"o\")) -- Array arr.copy(arr.fromString(\"ao\")) -- Array arr.slice(arr.fromString(\"ao\"), 0, 1) -- Arraysize ​Returns the size of the array.Parameters:arr (Array): The array to get the size of.Returns: The size of the array.fromString ​Creates an array from a string.Parameters:str (string): The string to create the array from.Returns: The array created from the string.toString ​Converts an array to a string.Parameters:arr (Array): The array to convert to a string.Returns: The array as a string.fromStream ​Creates an array from a stream.Parameters:stream (Stream): The stream to create the array from.Returns: The array created from the stream.readFromQueue ​Reads data from a queue and stores it in the array.Parameters:queue (Queue): The queue to read data from.size (number): The size of the data to read.Returns: The array containing the data read from the queue.writeToQueue ​Writes data from the array to a queue.Parameters:queue (Queue): The queue to write data to.array (Array): The array to write data from.Returns: NonetoStream ​Converts an array to a stream.Parameters:arr (Array): The array to convert to a stream.Returns: (Stream) The array as a stream.fromHex ​Creates an array from a hexadecimal string.Parameters:hex (string): The hexadecimal string to create the array from.Returns: The array created from the hexadecimal string.toHex ​Converts an array to a hexadecimal string.Parameters:arr (Array): The array to convert to a hexadecimal string.Returns: The array as a hexadecimal string.concat ​Concatenates two arrays.Parameters:a (Array): The array to concatenate with.b (Array): The array to concatenate.Returns: The concatenated array.truncate ​Truncates an array to a given length.Parameters:a (Array): The array to truncate.newSize (number): The new size of the array.Returns: The truncated array.XOR ​Performs a bitwise XOR operation on two arrays.Parameters:a (Array): The first array.b (Array): The second array.Returns: The result of the XOR operation.substitute ​Creates a new array with keys of first array and values of secondParameters:input (Array): The array to substitute.sbox (Array): The array to substitute with.Returns: The substituted array.permute ​Creates a new array with keys of second array and values of first array.Parameters:input (Array): The array to permute.pbox (Array): The array to permute with.Returns: The permuted array.copy ​Creates a copy of an array.Parameters:input (Array): The array to copy.Returns: The copied array.slice ​Creates a slice of an array.Parameters:input (Array): The array to slice.start (number): The start index of the slice.stop (number): The end index of the slice.Returns: The sliced array.Stream ​Stream is a data structure that represents a sequence of bytes. It is used to store and manipulate data in a streaming fashion.Example Usage:lualocal stream = crypto.utils.stream local str = \"ao\" local arr = {97, 111} stream.fromString(str) -- Stream stream.toString(stream.fromString(str)) -- ao stream.fromArray(arr) -- Stream stream.toArray(stream.fromArray(arr)) -- {97, 111} stream.fromHex(\"616f\") -- Stream stream.toHex(stream.fromHex(\"616f\")) -- 616ffromString ​Creates a stream from a string.Parameters:str (string): The string to create the stream from.Returns: The stream created from the string.toString ​Converts a stream to a string.Parameters:stream (Stream): The stream to convert to a string.Returns: The stream as a string.fromArray ​Creates a stream from an array.Parameters:arr (Array): The array to create the stream from.Returns: The stream created from the array.toArray ​Converts a stream to an array.Parameters:stream (Stream): The stream to convert to an array.Returns: The stream as an array.fromHex ​Creates a stream from a hexadecimal string.Parameters:hex (string): The hexadecimal string to create the stream from.Returns: The stream created from the hexadecimal string.toHex ​Converts a stream to a hexadecimal string.Parameters:stream (Stream): The stream to convert to a hexadecimal string.Returns: The stream as a hexadecimal string.Hex ​Example Usage:lualocal hex = crypto.utils.hex hex.hexToString(\"616f\") -- ao hex.stringToHex(\"ao\") -- 616fhexToString ​Converts a hexadecimal string to a string.Parameters:hex (string): The hexadecimal string to convert to a string.Returns: The hexadecimal string as a string.stringToHex ​Converts a string to a hexadecimal string.Parameters:str (string): The string to convert to a hexadecimal string.Returns: The string as a hexadecimal string.Queue ​Queue is a data structure that represents a sequence of elements. It is used to store and manipulate data in a first-in, first-out (FIFO) fashion.Example Usage:lualocal q = crypto.utils.queue() q.push(1) q.push(2) q.pop() -- 1 q.size() -- 1 q.getHead() -- 2 q.getTail() -- 2 q.reset()push ​Pushes an element to the queue.Parameters:queue (Queue): The queue to push the element to.element (any): The element to push to the queue.Returns: Nonepop ​Pops an element from the queue.Parameters:queue (Queue): The queue to pop the element from.element (any): The element to pop from the queue.Returns: The popped element.size ​Returns the size of the queue.Parameters: NoneReturns: The size of the queue.getHead ​Returns the head of the queue.Parameters: NoneReturns: The head of the queue.getTail ​Returns the tail of the queue.Parameters: NoneReturns: The tail of the queue.reset ​Resets the queue.Parameters: None","estimatedWords":2629,"lastModified":"2025-10-20T12:10:38.490Z","breadcrumbs":["guides","aos","modules","crypto"],"siteKey":"ao","siteName":"AO Cookbook","depth":4,"crawledAt":"2025-10-20T12:10:38.491Z"},{"url":"https://cookbook_ao.arweave.net/guides/aos/modules/base64.html","title":"Base64","content":"Base64 ​A small base64 module to encode or decode base64 text.Note: It is recommended to enable caching for large chunks of texts for up to x2 optimization.Example usage ​lualocal base64 = require(\".base64\") local str = \"This will be encoded\" -- is: \"VGhpcyB3aWxsIGJlIGVuY29kZWQ=\" local encoded = base64.encode(str) -- is: \"This will be encoded\" local decoded = base64.decode(encoded) assert(decoded == str)Module functions ​encode() ​This function encodes the provided string using the default encoder table. The encoder can be customized and a cache is available for larger chunks of data.Parameters:str: {string} The string to encodeencoder: {table} Optional custom encoding tableusecache: {boolean} Optional cache for large strings (turned off by default)Returns: Base64 encoded stringExamples ​lua-- prints: \"SGVsbG8gd29ybGQ=\" print(base64.encode(\"Hello world\")) -- customize encoder and allow caching base64.encode( \"Hello world\", base64.makeencoder(nil, \"-\"), true )decode() ​This function decodes the provided base64 encoded string using the default decoder table. The decoder can be customized and a cache is also available here.Parameters:str: {string} The base64 encoded string to decodedecoder: {table} Optional custom decoding tableusecache: {boolean} Optional cache for large strings (turned off by default)Returns: Decoded stringExamples ​lua-- prints: \"Hello world\" print(base64.decode(\"SGVsbG8gd29ybGQ=\")) -- customize decoder and allow caching base64.decode( \"SGVsbG8gd29ybGQ=\", base64.makedecoder(nil, \"-\"), true )makeencoder() ​Allows creating a new encoder table to customize the encode() function's result.Parameters:s62: {string} Optional custom char for 62 (+ by default)s63: {string} Optional custom char for 63 (/ by default)spad: {string} Optional custom padding char (= by default)Returns: Custom encoder tableExamples ​lua-- create custom encoder local encoder = base64.makeencoder(nil, nil, \"~\") -- prints \"SGVsbG8gd29ybGQ~\" instead of \"SGVsbG8gd29ybGQ=\" print(base64.encode(\"Hello world\", encoder))makedecoder() ​Allows creating a new decoder table to be able to decode custom-encoded base64 strings.Parameters:s62: {string} Optional custom char for 62 (+ by default)s63: {string} Optional custom char for 63 (/ by default)spad: {string} Optional custom padding char (= by default)Returns: Custom decoder tableExamples ​lualocal encoder = base64.makeencoder(nil, nil, \"~\") local decoder = base64.makedecoder(nil, nil, \"~\") -- \"SGVsbG8gd29ybGQ~\" local encoded = base64.encode(\"Hello world\", encoder) -- prints \"Hello world\" print(base64.decode(encoded, decoder))","estimatedWords":323,"lastModified":"2025-10-20T12:10:39.121Z","breadcrumbs":["guides","aos","modules","base64"],"siteKey":"ao","siteName":"AO Cookbook","depth":4,"crawledAt":"2025-10-20T12:10:39.121Z"},{"url":"https://cookbook_ao.arweave.net/guides/aos/modules/pretty.html","title":"Pretty","content":"Pretty ​This module allows printing formatted, human-friendly and readable syntax.Module functions ​tprint() ​Returns a formatted string of the structure of the provided table.Parameters:tbl: {table} The table to formatindent: {number} Optional indentation of each level of the tableReturns: Table structure formatted as a stringExamples ​lualocal pretty = require(\".pretty\") local formatted = pretty.tprint({ name = \"John Doe\", age = 22, friends = { \"Maria\", \"Victor\" } }, 2) -- prints the formatted table structure print(formatted)","estimatedWords":73,"lastModified":"2025-10-20T12:10:39.864Z","breadcrumbs":["guides","aos","modules","pretty"],"siteKey":"ao","siteName":"AO Cookbook","depth":4,"crawledAt":"2025-10-20T12:10:39.864Z"},{"url":"https://cookbook_ao.arweave.net/guides/aos/modules/utils.html","title":"Utils","content":"Utils ​A utility library for generic table manipulation and validation. It supports both curry-styled and traditional programming.Note: It is important to verify that the inputs provided to the following functions match the expected types.Example usage ​lualocal utils = require(\".utils\") local totalSupply = utils.reduce( function (acc, v) return acc + v end, 0, { 2, 4, 9 } ) print(totalSupply) -- prints 15Module functions ​concat() ​This function concatenates array b to array a.Parameters:a: {table} The base arrayb: {table} The array to concat to the base arrayReturns: An unified array of a and bExamples ​lua-- returns { 1, 2, 3, 4, 5, 6 } concat({ 1, 2, 3 })({ 4, 5, 6 }) -- curry syntax -- returns { \"hello\", \"world\", \"and\", \"you\" } concat({ \"hello\", \"world\" }, { \"and\", \"you\" }) -- traditional syntaxreduce() ​This function executes the provided reducer function for all array elements, finally providing one (unified) result.Parameters:fn: {function} The reducer function. It receives the previous result, the current element's value and key in this orderinitial: {any} An optional initial valuet: {table} The array to reduceReturns: A single result from running the reducer across all table elementsExamples ​lualocal sum = utils.reduce( function (acc, v) return acc + v end, 0, { 1, 2, 3 } ) print(sum) -- prints 6lualocal sum = utils .reduce(function (acc, v) return acc + v end)(0)({ 5, 4, 3 }) print(sum) -- prints 12map() ​This function creates a new array filled with the results of calling the provided map function on each element in the provided array.Parameters:fn: {function} The map function. It receives the current array element and keydata: {table} The array to mapReturns: A new array composed of the results of the map functionExamples ​lua-- returns { \"Odd\", \"Even\", \"Odd\" } utils.map( function (val, key) return (val % 2 == 0 and \"Even\") or \"Odd\" end, { 3, 4, 7 } )lua-- returns { 4, 8, 12 } utils.map(function (val, key) return val * 2 end)({ 2, 4, 6 })filter() ​This function creates a new array from a portion of the original, only keeping the elements that passed a provided filter function's test.Parameters:fn: {function} The filter function. It receives the current array element and should return a boolean, deciding whether the element should be kept (true) or filtered out (false)data: {table} The array to filterReturns: The new filtered arrayExamples ​lua-- keeps even numbers utils.filter( function (val) return val % 2 == 0 end, { 3, 4, 7 } )lua-- keeps only numbers utils.filter( function (val) return type(val) == \"number\" end, { \"hello\", \"world\", 13, 44 } )find() ​This function returns the first element that matches in a provided function.Parameters:fn: {function} The find function that receives the current element and returns true if it matches, false if it doesn'tt: {table} The array to find an element inReturns: The found element or nil if no element matchedExamples ​lualocal users = { { name = \"John\", age = 50 }, { name = \"Victor\", age = 37 }, { name = \"Maria\", age = 33 } } -- returns the user \"John\" utils.find( function (val) return user.name == \"John\" end, users )lua-- returns the user \"Maria\" utils.find(function (val) return user.age == 33 end)(users)reverse() ​Transforms an array into reverse order.Parameters:data: {table} The array to reverseReturns: The original array in reverse orderExample ​lua-- is: { 3, 2, 1 } utils.reverse({ 1, 2, 3 })includes() ​Determines whether a value is part of an array.Parameters:val: {any} The element to check fort: {table} The array to check inReturns: A boolean indicating whether or not the provided value is part of the arrayExamples ​lua-- this is true utils.includes(\"John\", { \"Victor\", \"John\", \"Maria\" })lua-- this is false utils.includes(4)({ 3, 5, 7 })keys() ​Returns the keys of a table.Parameters:table: {table} The table to get the keys forReturns: An array of keysExample ​lua-- returns { \"hello\", \"name\" } utils.keys({ hello = \"world\", name = \"John\" })values() ​Returns the values of a table.Parameters:table: {table} The table to get the values forReturns: An array of valuesExample ​lua-- returns { \"world\", \"John\" } utils.values({ hello = \"world\", name = \"John\" })propEq() ​Checks if a specified property of a table equals with the provided value.Parameters:propName: {string} The name of the property to comparevalue: {any} The value to compare toobject: {table} The object to select the property fromReturns: A boolean indicating whether the property value equals with the provided value or notExamples ​lualocal user = { name = \"John\", age = 50 } -- returns true utils.propEq(\"age\", 50, user)lualocal user = { name = \"Maria\", age = 33 } -- returns false utils.propEq(\"age\", 45, user)prop() ​Returns the property value that belongs to the property name provided from an object.Parameters:propName: {string} The name of the property to getobject: {table} The object to select the property value fromReturns: The property value or nil if it was not foundExamples ​lualocal user = { name = \"Maria\", age = 33 } -- returns \"Maria\" utils.prop(\"name\", user)lualocal user = { name = \"John\", age = 50 } -- returns 50 utils.prop(\"age\")(user)compose() ​This function allows you to chain multiple array mutations together and execute them in reverse order on the provided array.Parameters:...: {function[]} The array mutationsv: {table} The object to execute the provided functions onReturns: The result from the provided mutationsExamples ​lua-- returns 12 utils.compose( utils.reduce(function (acc, val) return acc + val end, 0), utils.map(function (val) return val * 2 end) )({ 1, 2, 3 })","estimatedWords":890,"lastModified":"2025-10-20T12:10:40.523Z","breadcrumbs":["guides","aos","modules","utils"],"siteKey":"ao","siteName":"AO Cookbook","depth":4,"crawledAt":"2025-10-20T12:10:40.523Z"},{"url":"https://cookbook_ao.arweave.net/guides/aoconnect/aoconnect.html","title":"aoconnect","content":"aoconnect ​ao connect is a Javascript/Typescript library to interact with the system from Node JS or the browser.Guides in this section provide snippets on how to utilize ao connect. All snippets are written in Javascript but should translate easily to Typescript.","estimatedWords":41,"lastModified":"2025-10-20T12:10:40.672Z","breadcrumbs":["guides","aoconnect","aoconnect"],"siteKey":"ao","siteName":"AO Cookbook","depth":4,"crawledAt":"2025-10-20T12:10:40.672Z"},{"url":"https://cookbook_ao.arweave.net/guides/aoconnect/installing-connect.html","title":"Installing ao connect","content":"Installing ao connect ​Prerequisites ​In order to install ao connect into your app you must have NodeJS/NPM 18 or higher. Installing ​npm ​shnpm install --save @permaweb/aoconnectyarn ​shyarn add @permaweb/aoconnect -DThis module can now be used from NodeJS as well as a browser, it can be included as shown below.ESM (Node & Browser) aka type: module ​jsimport { result, results, message, spawn, monitor, unmonitor, dryrun, } from \"@permaweb/aoconnect\";CJS (Node) type: commonjs ​jsconst { result, results, message, spawn, monitor, unmonitor, dryrun, } = require(\"@permaweb/aoconnect\");","estimatedWords":82,"lastModified":"2025-10-20T12:10:41.285Z","breadcrumbs":["guides","aoconnect","installing connect"],"siteKey":"ao","siteName":"AO Cookbook","depth":4,"crawledAt":"2025-10-20T12:10:41.285Z"},{"url":"https://cookbook_ao.arweave.net/guides/aoconnect/connecting.html","title":"Connecting to specific ao nodes","content":"Connecting to specific ao nodes ​When including ao connect in your code you have the ability to connect to a specific MU and CU, as well as being able to specify an Arweave gateway. This can be done by importing the \"connect\" function and extracting the functions from a call to the \"connect\" function.You may want to do this if you want to know which MU is being called when you send your message so that later you can debug from the specified MU. You also may want to read a result from a specific CU. You may in fact just prefer a particular MU and CU for a different reason. You can specify the gateway in order to use something other than the default, which is arweave.net.Importing without a call to connect ​js// Here aoconnect will implicitly use the default nodes/units import { result, results, message, spawn, monitor, unmonitor, dryrun, } from \"@permaweb/aoconnect\";Connecting to a specific MU, CU, and gateway ​jsimport { connect } from \"@permaweb/aoconnect\"; const { result, results, message, spawn, monitor, unmonitor, dryrun } = connect( { MU_URL: \"https://mu.ao-testnet.xyz\", CU_URL: \"https://cu.ao-testnet.xyz\", GATEWAY_URL: \"https://arweave.net\", }, ); // now spawn, message, and result can be used the same way as if they were imported directlyAll three of these parameters to connect are optional and it is valid to specify only 1 or 2 of them, or none. You could pass in just the MU_URL, for example.jsimport { connect } from \"@permaweb/aoconnect\"; const { result, results, message, spawn, monitor, unmonitor, dryrun } = connect( { MU_URL: \"https://ao-mu-1.onrender.com\", }, );","estimatedWords":259,"lastModified":"2025-10-20T12:10:42.287Z","breadcrumbs":["guides","aoconnect","connecting"],"siteKey":"ao","siteName":"AO Cookbook","depth":4,"crawledAt":"2025-10-20T12:10:42.287Z"},{"url":"https://cookbook_ao.arweave.net/guides/aoconnect/sending-messages.html","title":"Sending a Message to a Process","content":"Sending a Message to a Process ​A deep dive into the concept of Messages can be found in the ao Messages concept. This guide focuses on using ao connect to send a message to a process.Sending a message is the central way in which your app can interact with ao. A message is input to a process. There are 5 parts of a message that you can specify which are \"target\", \"data\", \"tags\", \"anchor\", and finally the messages \"signature\".Refer to your process module's source code or documentation to see how the message is used in its computation. The ao connect library will translate the parameters you pass it in the code below, construct a message, and send it.🎓 To Learn more about Wallets visit the Permaweb CookbookSending a Message in NodeJS ​Need a test wallet, use npx -y @permaweb/wallet > /path/to/wallet.json to create a wallet keyfile.jsimport { readFileSync } from \"node:fs\"; import { message, createDataItemSigner } from \"@permaweb/aoconnect\"; const wallet = JSON.parse( readFileSync(\"/path/to/arweave/wallet.json\").toString(), ); // The only 2 mandatory parameters here are process and signer await message({ /* The arweave TxID of the process, this will become the \"target\". This is the process the message is ultimately sent to. */ process: \"process-id\", // Tags that the process will use as input. tags: [ { name: \"Your-Tag-Name-Here\", value: \"your-tag-value\" }, { name: \"Another-Tag\", value: \"another-value\" }, ], // A signer function used to build the message \"signature\" signer: createDataItemSigner(wallet), /* The \"data\" portion of the message If not specified a random string will be generated */ data: \"any data\", }) .then(console.log) .catch(console.error);Sending a Message in a browser ​New to building permaweb apps check out the Permaweb Cookbookjsimport { message, createDataItemSigner } from \"@permaweb/aoconnect\"; // The only 2 mandatory parameters here are process and signer await message({ /* The arweave TxID of the process, this will become the \"target\". This is the process the message is ultimately sent to. */ process: \"process-id\", // Tags that the process will use as input. tags: [ { name: \"Your-Tag-Name-Here\", value: \"your-tag-value\" }, { name: \"Another-Tag\", value: \"another-value\" }, ], // A signer function used to build the message \"signature\" signer: createDataItemSigner(globalThis.arweaveWallet), /* The \"data\" portion of the message. If not specified a random string will be generated */ data: \"any data\", }) .then(console.log) .catch(console.error);If you would like to learn more about signers, click here","estimatedWords":389,"lastModified":"2025-10-20T12:10:42.912Z","breadcrumbs":["guides","aoconnect","sending messages"],"siteKey":"ao","siteName":"AO Cookbook","depth":4,"crawledAt":"2025-10-20T12:10:42.912Z"},{"url":"https://cookbook_ao.arweave.net/guides/aoconnect/reading-results.html","title":"Reading results from an ao Process","content":"Reading results from an ao Process ​In ao, messages produce results which are made available by Compute Units (CU's). Results are JSON objects consisting of the following fields: messages, spawns, output and error.Results are what the ao system uses to send messages and spawns that are generated by processes. A process can send a message just like you can as a developer, by returning messages and spawns in a result.You may want to access a result to display the output generated by your message. Or you may want to see what messages etc., were generated. You do not need to take the messages and spawns from a result and send them yourself. They are automatically handled by Messenger Units (MU's). A call to results can also provide you paginated list of multiple results.Fetching a single result ​jsimport { result } from \"@permaweb/aoconnect\"; let { Messages, Spawns, Output, Error } = await result({ // the arweave TxID of the message message: \"message-id\", // the arweave TxID of the process process: \"process-id\", });Fetching a set of results ​jsimport { results } from \"@permaweb/aoconnect\"; // fetching the first page of results let resultsOut = await results({ process: \"process-id\", sort: \"ASC\", limit: 25, }); // calling more with a cursor let resultsOut2 = await results({ process: \"process-id\", from: resultsOut.edges?.[resultsOut.edges.length - 1]?.cursor ?? null, sort: \"ASC\", limit: 25, });","estimatedWords":224,"lastModified":"2025-10-20T12:10:43.497Z","breadcrumbs":["guides","aoconnect","reading results"],"siteKey":"ao","siteName":"AO Cookbook","depth":4,"crawledAt":"2025-10-20T12:10:43.497Z"},{"url":"https://cookbook_ao.arweave.net/guides/aoconnect/spawning-processes.html","title":"Spawning a Process","content":"Spawning a Process ​A deep dive into the concept of Processes can be found in the ao Processes concept. This guide focuses on using ao connect to spawn a Process.In order to spawn a Process you must have the TxID of an ao Module that has been uploaded to Arweave. The Module is the source code for the Process. The Process itself is an instantiation of that source.You must also have the wallet address of a Scheduler Unit (SU). This specified SU will act as the scheduler for this Process. This means that all nodes in the system can tell that they need to read and write to this SU for this Process. You can use the address below.Wallet address of an available Scheduler ​lua_GQ33BkPtZrqxA84vM8Zk-N2aO0toNNu_C-l-rawrBAIn addition, in order to receive messages from other processes an Authority tag must be supplied with the wallet address of an authorised Messaging Unit (MU).Wallet address of the legacynet MU ​luafcoN_xJeisVsPXA-trzVAuIiqO3ydLQxM-L4XbrQKzYSpawning a Process in NodeJS ​jsimport { readFileSync } from \"node:fs\"; import { createDataItemSigner, spawn } from \"@permaweb/aoconnect\"; const wallet = JSON.parse( readFileSync(\"/path/to/arweave/wallet.json\").toString(), ); const processId = await spawn({ // The Arweave TxID of the ao Module module: \"module TxID\", // The Arweave wallet address of a Scheduler Unit scheduler: \"_GQ33BkPtZrqxA84vM8Zk-N2aO0toNNu_C-l-rawrBA\", // A signer function containing your wallet signer: createDataItemSigner(wallet), /* Refer to a Processes' source code or documentation for tags that may effect its computation. */ tags: [ { name: \"Authority\", value: \"fcoN_xJeisVsPXA-trzVAuIiqO3ydLQxM-L4XbrQKzY\" }, { name: \"Another-Tag\", value: \"another-value\" }, ], });Spawning a Process in a browser ​jsimport { createDataItemSigner, spawn } from \"@permaweb/aoconnect\"; const processId = await spawn({ // The Arweave TxID of the ao Module module: \"module TxID\", // The Arweave wallet address of a Scheduler Unit scheduler: \"_GQ33BkPtZrqxA84vM8Zk-N2aO0toNNu_C-l-rawrBA\", // A signer function containing your wallet signer: createDataItemSigner(globalThis.arweaveWallet), /* Refer to a Processes' source code or documentation for tags that may effect its computation. */ tags: [ { name: \"Authority\", value: \"fcoN_xJeisVsPXA-trzVAuIiqO3ydLQxM-L4XbrQKzY\" }, { name: \"Another-Tag\", value: \"another-value\" }, ], });","estimatedWords":329,"lastModified":"2025-10-20T12:10:44.271Z","breadcrumbs":["guides","aoconnect","spawning processes"],"siteKey":"ao","siteName":"AO Cookbook","depth":4,"crawledAt":"2025-10-20T12:10:44.271Z"},{"url":"https://cookbook_ao.arweave.net/guides/aoconnect/calling-dryrun.html","title":"Calling DryRun","content":"Calling DryRun ​URGENT DEPRECATION NOTICELegacynet will stop supporting dry runs on October 10, 2025. If you are running processes on Legacynet, you must take action now.This method of reading state is being deprecated for all processes. After deprecation, you have two options:Recommended: Migrate to State Patching - Use the State Patching mechanism to expose state via HTTP for better performance. This is the recommended approach as dryrun was known to cause severe bottlenecks in web applications.Alternative: Run Your Own HyperBEAM Node - You can continue using dry runs by running your own HyperBEAM node infrastructure.Exception for Token Processes: If you have a token process, HyperBEAM can manage your balance state. You can still patch your token for improved performance.Important for User-Owned Processes: If your application spawns processes owned by users (not your app), users will need to patch their own processes. See the User-Owned Processes guide for implementation strategies.DryRun is the process of sending a message object to a specific process and getting the Result object back, but the memory is not saved, it is perfect to create a read message to return the current value of memory. For example, a balance of a token, or a result of a transfer, etc. You can use DryRun to obtain an output without sending an actual message.jsimport { createDataItemSigner, dryrun } from \"@permaweb/aoconnect\"; const result = await dryrun({ process: 'PROCESSID', data: '', tags: [{name: 'Action', value: 'Balance'}, anchor: '1234', ...rest are optional (Id, Owner, etc) }); console.log(result.Messages[0]);","estimatedWords":245,"lastModified":"2025-10-20T12:10:44.656Z","breadcrumbs":["guides","aoconnect","calling dryrun"],"siteKey":"ao","siteName":"AO Cookbook","depth":4,"crawledAt":"2025-10-20T12:10:44.656Z"},{"url":"https://cookbook_ao.arweave.net/guides/aoconnect/monitoring-cron.html","title":"Monitoring Cron","content":"Monitoring Cron ​When using cron messages, ao users need a way to start ingesting the messages, using this monitor method, ao users can initiate the subscription service for cron messages. Setting cron tags means that your process will start producing cron results in its outbox, but you need to monitor these results if you want messages from those results to be pushed through the network.jsimport { readFileSync } from \"node:fs\"; import { createDataItemSigner, monitor } from \"@permaweb/aoconnect\"; const wallet = JSON.parse( readFileSync(\"/path/to/arweave/wallet.json\").toString(), ); const result = await monitor({ process: \"process-id\", signer: createDataItemSigner(wallet), });You can stop monitoring by calling unmonitorjsimport { readFileSync } from \"node:fs\"; import { createDataItemSigner, unmonitor } from \"@permaweb/aoconnect\"; const wallet = JSON.parse( readFileSync(\"/path/to/arweave/wallet.json\").toString(), ); const result = await unmonitor({ process: \"process-id\", signer: createDataItemSigner(wallet), });","estimatedWords":127,"lastModified":"2025-10-20T12:10:45.235Z","breadcrumbs":["guides","aoconnect","monitoring cron"],"siteKey":"ao","siteName":"AO Cookbook","depth":4,"crawledAt":"2025-10-20T12:10:45.235Z"},{"url":"https://cookbook_ao.arweave.net/guides/aoconnect/assign-data.html","title":"Sending an Assignment to a Process","content":"Sending an Assignment to a Process ​Assignments can be used to load Data from another Message into a Process. Or to not duplicate Messages. You can create one Message and then assign it to any number of processes. This will make it available to the Processes you have sent an Assignment to.Sending an Assignment in NodeJS ​jsimport { readFileSync } from \"node:fs\"; import { assign } from \"@permaweb/aoconnect\"; await assign({ process: \"process-id\", message: \"message-id\", }) .then(console.log) .catch(console.error);Excluding DataItem fields ​You can also exclude most DataItem fields which will tell the CU not to load them into your process. You may want to do this if you need only the header data like the Tags and not the Data itself etc... If you exclude the Owner it wont have any effect because the CU requires the Owner, so excluding Owner will be ignored by the CU. Only capitalized DataItem/Message fields will have an effect in the CU.jsimport { readFileSync } from \"node:fs\"; import { assign } from \"@permaweb/aoconnect\"; await assign({ process: \"process-id\", message: \"message-id\", exclude: [\"Data\", \"Anchor\"], }) .then(console.log) .catch(console.error);Assigning L1 Transactions ​You can also assign a layer 1 transaction by passing the baseLayer param into assign. This is useful for minting tokens etc... using the base layer. By default, if the L1 tx does not have at least 20 confirmations the SU will reject it. This can be changed by setting the Settlement-Depth tag to a different number on the Process when it is created.jsimport { readFileSync } from \"node:fs\"; import { assign } from \"@permaweb/aoconnect\"; await assign({ process: \"process-id\", message: \"layer 1 tx id\", baseLayer: true, }) .then(console.log) .catch(console.error);","estimatedWords":270,"lastModified":"2025-10-20T12:10:45.799Z","breadcrumbs":["guides","aoconnect","assign data"],"siteKey":"ao","siteName":"AO Cookbook","depth":4,"crawledAt":"2025-10-20T12:10:45.800Z"},{"url":"https://cookbook_ao.arweave.net/guides/aoconnect/signers.html","title":"DataItem Signers","content":"DataItem Signers ​Every message sent to AO MUST be signed, aoconnect provides a helper function for signing messages or spawning new processes. This helper function createDataItemSigner is provided for arweave wallets. But you can create your own Signer instance too.What is a Wallet/Keyfile? ​A wallet/keyfile is a public/private key pair that can be used to sign and encrypt data.What is an ao message/dataItem? ​You often see the terms message and dataItem used interchangeably in the documentation, a message is a data-protocol type in ao that uses the dataItem specification to describe the messages intent. A dataItem is defined in the ANS-104 bundle specification. A dataItem is the preferred format of storage for arweave bundles. A bundle is a collection of these signed dataItems. A message implements specific tags using the dataItem specification. When developers send messages to ao, they are publishing dataItems on arweave.🎓 To learn more about messages click here and to learn more about ANS-104 dataItems click hereWhat is a signer? ​A signer is function that takes data, tags, anchor, target and returns an object of id, binary representing a signed dataItem. AO accepts arweave signers and ethereum signers. createDataItemSigner is a helper function that can take an arweave keyfile or a browser instance of an arweave wallet usually located in the global scope of the browser, when I user connects to a wallet using an extension or html app.Examples ​arweave keyfileNOTE: if you do not have a wallet keyfile you can create one using npx -y @permaweb/wallet > wallet.jsonjsimport * as WarpArBundles from \"warp-arbundles\"; const pkg = WarpArBundles.default ? WarpArBundles.default : WarpArBundles; const { createData, ArweaveSigner } = pkg; function createDataItemSigner(wallet) { const signer = async ({ data, tags, target, anchor }) => { const signer = new ArweaveSigner(wallet); const dataItem = createData(data, signer, { tags, target, anchor }); return dataItem.sign(signer).then(async () => ({ id: await dataItem.id, raw: await dataItem.getRaw(), })); }; return signer; }arweave browser extensionNOTE: This implementation works with ArweaveWalletKit, ArConnect, and Arweave.appjsimport { Buffer } from \"buffer/index.js\"; import * as WarpArBundles from \"warp-arbundles\"; if (!globalThis.Buffer) globalThis.Buffer = Buffer; const { DataItem } = WarpArBundles; function createDataItemSigner(arweaveWallet) { /** * createDataItem can be passed here for the purposes of unit testing * with a stub */ const signer = async ({ data, tags, target, anchor, createDataItem = (buf) => new DataItem(buf), }) => { /** * signDataItem interface according to ArweaveWalletConnector * * https://github.com/jfbeats/ArweaveWalletConnector/blob/7c167f79cd0cf72b6e32e1fe5f988a05eed8f794/src/Arweave.ts#L46C23-L46C23 */ const view = await arweaveWallet.signDataItem({ data, tags, target, anchor, }); const dataItem = createDataItem(Buffer.from(view)); return { id: await dataItem.id, raw: await dataItem.getRaw(), }; }; return signer; }ethereum keyjsimport { EthereumSigner, createData } from \"@dha-team/arbundles\"; function createDataItemSigner(wallet) { const signer = async ({ data, tags, target, anchor }) => { const signer = new EthereumSigner(wallet); const dataItem = createData(data, signer, { tags, target, anchor }); return dataItem.sign(signer).then(async () => ({ id: await dataItem.id, raw: await dataItem.getRaw(), })); }; return signer; }Summary ​Using the signer function developers can control how dataItems are signed without having to share the signing process with aoconnect.","estimatedWords":500,"lastModified":"2025-10-20T12:10:46.615Z","breadcrumbs":["guides","aoconnect","signers"],"siteKey":"ao","siteName":"AO Cookbook","depth":4,"crawledAt":"2025-10-20T12:10:46.615Z"},{"url":"https://cookbook_ao.arweave.net/guides/dev-cli/index.html","title":"AO Dev-Cli 01","content":"AO Dev-Cli 0.1 ​The AO dev-cli is a tool that is used to build ao wasm modules, the first versions of the tool only supported lua as the embedded language or c based module. With this release developers now can add any pure c or cpp module to their wasm builds. This opens the door for many different innovations from indexers to languages.Install ​RequirementsDocker is required: https://docker.comshellcurl -L https://install_ao.g8way.io | shStart a project ​shellao init [project-name]Build a project ​shellcd [project-name] ao buildDeploy a project ​RequirementsYou will need an arweave keyfile, you can create a local one using this command npx -y @permaweb/wallet > wallet.jsonshellao publish -w [path_to_wallet] [path_to_wasm]Configuration ​To customize your build process, create a config.yml file in the root directory of your project. This file will modify your settings during the build.Configuration Options: ​preset: Selects default values for stack_size, initial_memory, and maximum_memory. For available presets, see Config Presets. (Default: md)stack_size: Specifies the stack size, overriding the value from the preset. Must be a multiple of 64. (Default: 32MB)initial_memory: Defines the initial memory size, overriding the preset value. Must be larger than stack_size and a multiple of 64. (Default: 48MB)maximum_memory: Sets the maximum memory size, overriding the preset value. Must be larger than stack_size and a multiple of 64. (Default: 256MB)extra_compile_args: Provides additional compilation commands for emcc. (Default: [])keep_js: By default, the generated .js file is deleted since AO Loader uses predefined versions. Set this to true if you need to retain the .js file. (Default: false)Libraries ​Starting with version 0.1.3, you can integrate external libraries into your project. To do this, follow these guidelines:Adding Libraries ​Create a libs Directory: At the root of your project, create a directory named /libs. This is where you'll place your library files.Place Your Library Files: Copy or move your compiled library files (e.g., .a, .so, .o, .dylib, etc.) into the /libs directory.NOTEEnsure that all library files are compiled using emcc to ensure compatibility with your project.IMPORTANTMore details to come including an example project...Example Directory Structure ​project-root/ │ ├── libs/ │ ├── libexample.a │ ├── libanother.so │ └── libmore.o │ ├── process.lua ├── ao.lua │ └── config.ymlUsing Libraries in Your Code ​After adding the library files to the /libs directory, you need to link against these libraries in your project. This often involves specifying the library path and names in your build scripts or configuration files. For example:For C/C++ Projects: You can just include any header files placed in the libs folder as the libs with be automatically built into your module.For Lua Projects: Depending on how your build your libraries and if you compiled them with Lua bindings you can just require the libs in your lua files. markdown = require('markdown')IMPORTANTMore details to come...Lua Build Example ​To create and build a Lua project, follow these steps:shao init -l lua [project-name] cd [project-name] ao buildC Build Example ​To create and build a C project, follow these steps:shao init -l c [project-name] cd [project-name] ao buildConfig Presets ​Here are the predefined configuration presets:js'xs': { 'stack_size': 8388608, // 8mb 'initial_memory': 16777216, // 16mb 'maximum_memory': 67108864 // 64mb }, 'sm': { 'stack_size': 16777216, // 16mb 'initial_memory': 33554432, // 32mb 'maximum_memory': 134217728 // 128mb }, 'md': { 'stack_size': 33554432, // 32mb 'initial_memory': 50331648, // 48mb 'maximum_memory': 268435456 // 256mb }, 'lg': { 'stack_size': 50331648, // 48mb 'initial_memory': 67108864, // 64mb 'maximum_memory': 268435456 // 256mb }, 'xl': { 'stack_size': 67108864, // 64mb 'initial_memory': 100663296, // 96mb 'maximum_memory': 536870912 // 512mb }, 'xxl': { 'stack_size': 100663296, // 96mb 'initial_memory': 134217728, // 128mb 'maximum_memory': 4294967296 // 4096mb },","estimatedWords":587,"lastModified":"2025-10-20T12:10:47.541Z","breadcrumbs":["guides","dev cli","index"],"siteKey":"ao","siteName":"AO Cookbook","depth":4,"crawledAt":"2025-10-20T12:10:47.541Z"},{"url":"https://cookbook_ao.arweave.net/guides/snacks/sqlite.html","title":"Getting started with SQLite","content":"Getting started with SQLite ​SQLite is a relational database engine. In this guide, we will show how you can spawn a process with SQLite and work with data using a relational database.Setup ​NOTE: make sure you have aos installed, if not checkout Getting Startedspawn a new process mydb with a --sqlite flag, this instructs ao to use the latest sqlite module.shaos mydb --sqliteInstall AO Package Manager ​installing apm, the ao package manager we can add helper modules to make it easier to work with sqlite.lua.load-blueprint apmInstall dbAdmin package ​DbAdmin is a module that connects to a sqlite database and provides functions to work with sqlite.https://apm_betteridea.g8way.io/pkg?id=@rakis/DbAdminluaapm.install('@rakis/dbAdmin')Create sqlite Database ​lualocal sqlite = require('lsqlite3') Db = sqlite.open_memory() dbAdmin = require('@rakis/DbAdmin').new(Db)Create Table ​Create a table called CommentsluadbAdmin:exec([[ CREATE TABLE IF NOT EXISTS Comments ( ID INTEGER PRIMARY KEY AUTOINCREMENT, Asset TEXT, User TEXT, Body TEXT ); ]])Insert data ​lualocal SQL = \"INSERT INTO Comments (Asset, User, Body) VALUES (?,?,?);\" dbAdmin:apply(SQL, {\"dog.jpg\", \"Anon\", \"Nice Picture\"})List data ​lualocal SQL = \"SELECT * FROM Comments;\" dbAdmin:exec(SQL)Congrats! ​You are using sqlite on AO 🎉","estimatedWords":176,"lastModified":"2025-10-20T12:10:48.133Z","breadcrumbs":["guides","snacks","sqlite"],"siteKey":"ao","siteName":"AO Cookbook","depth":4,"crawledAt":"2025-10-20T12:10:48.133Z"},{"url":"https://cookbook_ao.arweave.net/guides/snacks/weavedrive.html","title":"Using WeaveDrive","content":"Using WeaveDrive ​WeaveDrive has been released on AO legacynet, which is great! But how to use it with your process? This post aims to provide a step by step guide on how to use WeaveDrive in your AOS process.The current availability time is called Assignments and this type puts WeaveDrive in a mode that allows you to define an Attestor wallet address when you create your AOS process. This will enable the process to load data from dataItems that have a Attestation created by this wallet.Prep Tutorial ​In order, to setup the tutorial for success we need to upload some data and upload an attestation. It will take a few minutes to get mined into a block on arweave.Install arxshnpm i -g @permaweb/arxCreate a walletnpx -y @permaweb/wallet > ~/.test-wallet.jsonCreate some datamkdir test-weavedrive cd test-weavedrive echo \"Hello WeaveDrive\" > data.html arx upload data.html -w ~/.test-wallet.json -t arweaveYou should get a result like:Loaded address: vfSWG3girEwCBggs9xeztuRyiltsT2CJH_m-S8A58yQ Uploaded to https://arweave.net/9TIPJD2a4-IleOQJzRwPnDHO5DA891MWAyIdJJ1SiSkCreate AttestationIt is important to copy the id of the uploaded dataItem, in the above case 9TIPJD2a4-IleOQJzRwPnDHO5DA891MWAyIdJJ1SiSk as your Message Value.echo \"attestation-example\" > att.txt arx upload att.txt -w ~/.test-wallet.json -t arweave --tags Data-Protocol ao Type Attestation Message 9TIPJD2a4-IleOQJzRwPnDHO5DA891MWAyIdJJ1SiSk👏 Awesome! That will take a few minutes to get mined on arweave, once it is mined then we will be able to read the data.html dataItem using WeaveDriveEnable WeaveDrive in a process ​Lets create a new AOS process with WeaveDrive enabled and the wallet we created above as an Attestor.NOTE: it is important to use the same wallet address that was used to sign the attestation data-item.aos test-weavedrive --tag-name Extension --tag-value WeaveDrive --tag-name Attestor --tag-value vfSWG3girEwCBggs9xeztuRyiltsT2CJH_m-S8A58yQ --tag-name Availability-Type --tag-value AssignmentsNOTE: It does take a few minutes for the data to get 20 plus confirmations which is the threshold for data existing on arweave. You may want to go grab a coffee. ☕Install apm and WeaveDrive ​.load-blueprint apm apm.install('@rakis/WeaveDrive')Load Data ​Drive = require('@rakis/WeaveDrive') Drive.getData(\"9TIPJD2a4-IleOQJzRwPnDHO5DA891MWAyIdJJ1SiSk\")","estimatedWords":316,"lastModified":"2025-10-20T12:10:49.712Z","breadcrumbs":["guides","snacks","weavedrive"],"siteKey":"ao","siteName":"AO Cookbook","depth":4,"crawledAt":"2025-10-20T12:10:49.712Z"},{"url":"https://cookbook_ao.arweave.net/concepts/index.html","title":"Concepts","content":"Concepts ​This section explains the core concepts and architecture behind AO, helping you understand how the system works at a fundamental level.System Architecture ​AO is built on a few fundamental principles that form its foundation:Two core types: Messages and Processes - the basic building blocks of the AO ecosystemNo shared state, only Holographic State - a unique approach to distributed computingDecentralized Computer (Grid) - enabling truly distributed applicationsCore Components ​Explore these foundational concepts to gain a deeper understanding of AO:How it Works - An overview of AO's architecture and how the different parts interactProcesses - Learn about processes, the computational units in AOMessages - Understand the messaging system that enables communicationEvaluation - Discover how code execution works in the AO environmentUnits - Learn about the computational units that power the AO networkTechnical Foundations ​Specifications - Detailed technical specifications for the AO protocolGetting Started ​Meet Lua - Introduction to Lua, the programming language used in AOAO System Tour - A guided tour of the AO system and its capabilitiesNavigation ​Use the sidebar to navigate between concept topics. Each document provides in-depth information about a specific aspect of AO.","estimatedWords":187,"lastModified":"2025-10-20T12:10:50.485Z","breadcrumbs":["concepts","index"],"siteKey":"ao","siteName":"AO Cookbook","depth":4,"crawledAt":"2025-10-20T12:10:50.485Z"},{"url":"https://cookbook_ao.arweave.net/concepts/how-it-works.html","title":"How ao messaging works","content":"How ao messaging works ​Before we dive in to ao, I want to share with you a little information about unix. Unix is a powerful operating system, but in its design it is focused on two Principal \"Types\". Files and Programs. A File is data and a Program is logic, when you combine the two you get information.Input.file | TransformProgram | Output.fileYou might have done something like this on the command line without knowing what you were doing. Being able to connect files to programs and return files which can then be passed to other programs creates a complex system composed of simple applications. This is a very powerful idea.Now, lets talk about ao the hyper parallel computer, and lets change the idea of a File to the ao concept of a Message and the idea of a Program to the ao concept of a Process. The ao computer takes messages and sends them to Processes in which those Processes can output messages that can be sent to other Processes. The result is a complex system built on simple modular logic containers.MessageA | Process | MessageBHere is a description of the process as outlined in the flowchart:A message is initiated from an ao Connect. This message is sent to the mu service using a POST request. The body of the request contains data following a protocol, labeled 'ao', and is of the type 'Message'.The mu service processes the POST request and forwards the message to the su service. This is also done using a POST request with the same data protocol and message type.The su service stores the assignment and message on Arweave.A GET request is made to the cu service to retrieve results based on a message ID. The cu is a service that evaluates messages on processes and can return results based on an individual message identifier.A GET request is made to the su service to retrieve the assignment and message. This request is looking for messages from a process ID, within a range of time from a start (from the last evaluation point) to (to the current message ID).The final step is to push any outbox Messages. It involves reviewing the messages and spawns in the Result object. Based on the outcome of this check, the steps 2, 3, and 4 may be repeated for each relevant message or spawn.","estimatedWords":393,"lastModified":"2025-10-20T12:10:51.061Z","breadcrumbs":["concepts","how it works"],"siteKey":"ao","siteName":"AO Cookbook","depth":4,"crawledAt":"2025-10-20T12:10:51.061Z"},{"url":"https://cookbook_ao.arweave.net/concepts/processes.html","title":"Processes","content":"Processes ​Processes possess the capability to engage in communication via message passing, both receiving and dispatching messages within the network. Additionally, they hold the potential to instantiate further processes, enhancing the network's computational fabric. This dynamic method of data dissemination and interaction within the network is referred to as a 'holographic state', underpinning the shared and persistent state of the network.When building a Process with aos you have the ability to add handlers, these handlers can be added by calling the Handlers.add function, passing a \"name\", a \"match\" function, and a \"handle\" function.The core module contains a helper library that gets injected into the handler function, this library is called ao.lua{ env = { Process = { Id = \"5WzR7rJCuqCKEq02WUPhTjwnzllLjGu6SA7qhYpcKRs\", Owner = \"_r9LpP4FtClpsGX3TOohubyaeb0IQTZZMcxQ24tTsGo\", Tags = {...} }, Module = { Id = \"UAUszdznoUPQvXRbrFuIIH6J0N_LnJ1h4Trej28UgrE\", Owner = \"_r9LpP4FtClpsGX3TOohubyaeb0IQTZZMcxQ24tTsGo\", Tags = {...} } }, id = \"5WzR7rJCuqCKEq02WUPhTjwnzllLjGu6SA7qhYpcKRs\", isTrusted = \"function: 0x5468d0\", result = \"function: 0x547120\", send = \"function: 0x547618\", spawn = \"function: 0x5468b0\" }The main functions to look at in this ao helper isao.send(Message) - sends a message to a processao.spawn(Module, Message) - creates a new processEthereum Signed Process or Module ​For an ao Process or Module, if the ANS-104 DataItem was signed using Ethereum keys, then the value in the env.Process.Owner or env.Module.Owner field, respectively, will be the EIP-55 Ethereum address of the signer. For example: 0xfB6916095ca1df60bB79Ce92cE3Ea74c37c5d359ao.send Example ​luaao.send({ Target = Chatroom, Action = \"Broadcast\", Data = \"Hello from my Process!\" })ao.spawn Example ​luaao.spawn(ao.env.Module.Id, { [\"Memory-Limit\"] = \"500-mb\", [\"Compute-Limit\"] = \"900000000000000000\" })ao.env ​NOTE: ao.env is important context data that you may need as a developer creating processes.The ao.env property contains the Process and Module Reference Objectsluaenv = { Process = { Id = \"5WzR7rJCuqCKEq02WUPhTjwnzllLjGu6SA7qhYpcKRs\", Owner = \"_r9LpP4FtClpsGX3TOohubyaeb0IQTZZMcxQ24tTsGo\", Tags = {...} }, Module = { Id = \"UAUszdznoUPQvXRbrFuIIH6J0N_LnJ1h4Trej28UgrE\", Owner = \"_r9LpP4FtClpsGX3TOohubyaeb0IQTZZMcxQ24tTsGo\", Tags = {...} } }Both the Process and the Module contain the attributes of the ao Data-Protocol.Summary ​Processes in the network communicate through message passing and can create new processes, contributing to a 'holographic state' of shared and persistent data. Developers can build a Process using aos by adding handlers through the Handlers.add function with specific name, match, and handle functions. The ao helper library within the core module aids in this process, providing functions like ao.send to dispatch messages and ao.spawn to create new modules, as well as the important ao.env property which contains essential Process and Module information. The ao Data-Protocol outlines the structure and attributes of these elements.","estimatedWords":409,"lastModified":"2025-10-20T12:10:51.737Z","breadcrumbs":["concepts","processes"],"siteKey":"ao","siteName":"AO Cookbook","depth":4,"crawledAt":"2025-10-20T12:10:51.737Z"},{"url":"https://cookbook_ao.arweave.net/concepts/messages.html","title":"Messages","content":"Messages ​The Message serves as the fundamental data protocol unit within ao, crafted from ANS-104 DataItems, thereby aligning with the native structure of Arweave. When engaged in a Process, a Message is structured as follows:lua{ Cron = false, Data = \"Hello aos\", Epoch = 0, From = \"5WzR7rJCuqCKEq02WUPhTjwnzllLjGu6SA7qhYpcKRs\", Id = \"ayVo53qvZswpvxLlhMf8xmGjwxN0LGuHzzQpTLT0_do\", Nonce = 1, Owner = \"z1pq2WzmaYnfDwvEFgUZBj48anUsxxN64ZjbWOsIn08\", Signature = \"...\", Tags = { Type = \"Message\", Variant = \"ao.TN.1\", [\"Data-Protocol\"] = \"ao\", [\"From-Module\"] = \"lXfdCypsU3BpYTWvupgTioLoZAEOZL2_Ihcqepz6RiQ\", [\"From-Process\"] = \"5WzR7rJCuqCKEq02WUPhTjwnzllLjGu6SA7qhYpcKRs\" }, Target = \"5WzR7rJCuqCKEq02WUPhTjwnzllLjGu6SA7qhYpcKRs\", Timestamp = 1704936415711, [\"Block-Height\"] = 1340762, [\"Forwarded-By\"] = \"z1pq2WzmaYnfDwvEFgUZBj48anUsxxN64ZjbWOsIn08\", [\"Hash-Chain\"] = \"hJ0B-0yxKxeL3IIfaIIF7Yr6bFLG2vQayaF8G0EpjbY\" }This architecture merges the Assignment Type with the Message Type, granting the Process a comprehensive understanding of the Message's context for effective processing.When sending a message, here is a visual diagram of how the messages travels through the ao computer.The message workflow initiates with the MU (Messenger Unit), where the message's signature is authenticated. Following this, the SU (Scheduler Unit) allocates an Epoch and Nonce to the message, bundles the message with an Assignment Type, and dispatches it to Arweave. Subsequently, the aoconnect library retrieves the outcome from the CU (Compute Unit). The CU then calls for all preceding messages leading up to the current Message Id from the SU (Scheduler Unit), processes them to deduce the result. Upon completion, the computed result is conveyed back to aoconnect, which is integrated within client interfaces such as aos.Ethereum Signed Message ​If the Message ANS-104 DataItem was signed using Ethereum keys, then the value in the Owner and From fields will be the EIP-55 Ethereum address of the signer. For example: 0xfB6916095ca1df60bB79Ce92cE3Ea74c37c5d359.Summary ​Messages serve as the primary data protocol type for the ao network, leveraging ANS-104 Data-Items native to Arweave. Messages contain several fields including data content, origin, target, and cryptographic elements like signatures and nonces. They follow a journey starting at the Messenger Unit (MU), which ensures they are signed, through the Scheduler Unit (SU) that timestamps and sequences them, before being bundled and published to Arweave. The aoconnect library then reads the result from the Compute Unit (CU), which processes messages to calculate results and sends responses back through aoconnect, utilized by clients such as aos. The CU is the execution environment for these processes.","estimatedWords":369,"lastModified":"2025-10-20T12:10:52.344Z","breadcrumbs":["concepts","messages"],"siteKey":"ao","siteName":"AO Cookbook","depth":4,"crawledAt":"2025-10-20T12:10:52.345Z"},{"url":"https://cookbook_ao.arweave.net/concepts/eval.html","title":"Eval","content":"Eval ​Each AO process includes an onboard Eval handler that evaluates any new code it receives. This handler determines the appropriate action for the code and verifies that the message originates from the process owner.The Eval handler can also be manually triggered to evaluate the Data field from an incoming message. When you use the .load function to load a file into a process, it relies on the Eval handler to evaluate the file’s content under the hood.Sending an Eval Message in NodeJS ​jsimport { readFileSync } from \"node:fs\"; import { message, createDataItemSigner } from \"@permaweb/aoconnect\"; const wallet = JSON.parse( readFileSync(\"/path/to/arweave/wallet.json\").toString(), ); await message({ // The arweave TxID of the process, this will become the \"target\". process: \"process-ID\", // Replace with the actual process ID // Tagging the Eval Action so the receiving process evaluates and adds the new Handler from the Data field. tags: [ { name: \"Action\", value: \"Eval\" }, { name: \"Data\", value: 'Handlers.add(\"ping\", Handlers.utils.reply(\"pong\"))', }, ], // A signer function used to build the message \"signature\" signer: createDataItemSigner(wallet), }) .then(console.log) .catch(console.error);Sending an Eval Message in a Browser ​jsimport { message, createDataItemSigner } from \"@permaweb/aoconnect\"; await message({ // The arweave TxID of the process, this will become the \"target\". process: \"process-ID\", // Replace with the actual process ID // Tagging the Eval Action so the receiving process evaluates and adds the new Handler from the Data field. tags: [ { name: \"Action\", value: \"Eval\" }, { name: \"Data\", value: 'Handlers.add(\"ping\", Handlers.utils.reply(\"pong\"))', }, ], // A signer function used to build the message \"signature\" signer: createDataItemSigner(globalThis.arweaveWallet), }) .then(console.log) .catch(console.error);","estimatedWords":261,"lastModified":"2025-10-20T12:10:53.125Z","breadcrumbs":["concepts","eval"],"siteKey":"ao","siteName":"AO Cookbook","depth":4,"crawledAt":"2025-10-20T12:10:53.125Z"},{"url":"https://cookbook_ao.arweave.net/concepts/units.html","title":"Units","content":"Units ​What is a Unit? ​The ao Computer is composed of three Unit types, each type contains a set of responsibilities for the computer. And each Unit is horizontally scalable.In ao we have the Messenger Unit or MU, and the Scheduler Unit or SU, and the Compute Unit or the CU. These units are the building blocks of the ao Computer Grid. There can be 1 or more of these units on the network and they work together to power the ao Operating System or aos.Messenger Unit - This unit is the front door to ao, it receives all the messages from the outside and as well as directs traffic flow for Processes. This traffic flow we call pushing. Each process can return an Outbox when it evaluates a Message, and this Outbox can be filled with Messages or requests to Spawn new processes, and the Messenger Unit is responsible for extracting these Messages from the Outbox and signing them and sending them to the Scheduler Units for processing.Scheduler Unit - The Scheduler unit is responsible for ordering the messages, and storing those messages on Arweave. It is important that every message is appropriately ordered so that the evaluation can be replayed and verified. The Scheduler Unit is responsible for this process. It provides the abilty to query it via an endpoint to get the order of messages for evaluation.Compute Unit - The Compute unit is responsible for compute, this unit loads the binary module and manages the memory of that module, so that the execution of the process is alway running on the most up to date memory. The compute unit provides the results of the evaluation back to the the messenger unit, which can then push any messages in the outbox of the given process.Summary ​The ao Computer consists of three scalable unit types—Messenger Unit (MU), Scheduler Unit (SU), and Compute Unit (CU)—which form the foundation of the ao Computer. These units can exist in multiples on the network and collectively operate the ao Operating System (aos).The MU acts as the entry point, receiving external messages and managing process communications. It processes outgoing messages and spawn requests from process outboxes and forwards them to the SU.The SU ensures messages are properly sequenced and stored on Arweave, maintaining order for consistent replay and verification of message evaluations.The CU handles computation, loading binary modules, and managing memory to ensure processes run with current data. It then returns the evaluation results to the MU for further message handling.","estimatedWords":417,"lastModified":"2025-10-20T12:10:53.741Z","breadcrumbs":["concepts","units"],"siteKey":"ao","siteName":"AO Cookbook","depth":4,"crawledAt":"2025-10-20T12:10:53.741Z"},{"url":"https://cookbook_ao.arweave.net/concepts/specs.html","title":"ao Specs","content":"ao Specs ​What is ao? ​The ao computer is the actor oriented machine that emerges from the network of nodes that adhere to its core data protocol, running on the Arweave network. This document gives a brief introduction to the protocol and its functionality, as well as its technical details, such that builders can create new implementations and services that integrate with it.The ao computer is a single, unified computing environment (a Single System Image), hosted on a heterogenous set of nodes in a distributed network. ao is designed to offer an environment in which an arbitrary number of parallel processes can be resident, coordinating through an open message passing layer. This message passing standard connects the machine's independently operating processes together into a 'web' -- in the same way that websites operate on independent servers but are conjoined into a cohesive, unified experience via hyperlinks.","estimatedWords":146,"lastModified":"2025-10-20T12:10:54.737Z","breadcrumbs":["concepts","specs"],"siteKey":"ao","siteName":"AO Cookbook","depth":4,"crawledAt":"2025-10-20T12:10:54.737Z"},{"url":"https://cookbook_ao.arweave.net/concepts/tour.html","title":"aos Brief Tour","content":"aos Brief Tour ​Welcome to a quick tour of aos! This tutorial will walk you through the key global functions and variables available in the aos environment, giving you a foundational understanding of how to interact with and utilize aos effectively.1. Introduction to Inbox ​What It Is: Inbox is a Lua table that stores all messages received by your process but not yet handled.How to Use: Check Inbox to see incoming messages. Iterate through Inbox[x] to process these messages.2. Sending Messages with Send(Message) ​Functionality: Send(Message) is a global function to send messages to other processes.Usage Example: Send({Target = \"...\", Data = \"Hello, Process!\"}) sends a message with the data \"Hello, Process!\" to a specified process.3. Creating Processes with Spawn(Module, Message) ​Purpose: Use Spawn(Module, Message) to create new processes.Example: Spawn(\"MyModule\", {Data = \"Start\"}) starts a new process using \"MyModule\" with the provided message.4. Understanding Name and Owner ​Name: A string set during initialization, representing the process's name.Owner: Indicates the owner of the process. Changing this might restrict your ability to interact with your process.Important Note: Treat these as read-only to avoid issues.5. Utilizing Handlers ​What They Are: Handlers is a table of helper functions for creating message handlers.Usage: Define handlers in Handlers to specify actions for different incoming messages based on pattern matching.6. Data Representation with Dump ​Function: Dump converts any Lua table into a print-friendly format.How to Use: Useful for debugging or viewing complex table structures. Example: Dump(Inbox) prints the contents of Inbox.7. Leveraging Utils Module ​Contents: Utils contains a collection of functional utilities likemap, reduce, and filter.Usage: Great for data manipulation and functional programming patterns in Lua. For example, Utils.map(myTable, function(x) return x * 2 end) to double the values in a table.8. Exploring the ao Core Library ​Description: ao is a core module that includes key functions for message handling and process management.Key Features: Includes functions for sending messages (send) and spawning processes (spawn), along with environment variables.Conclusion ​This brief tour introduces you to the primary globals and functionalities within the aos environment. With these tools at your disposal, you can create and manage processes, handle messages, and utilize Lua's capabilities to build efficient and responsive applications on the aos platform. Experiment with these features to get a deeper understanding and to see how they can be integrated into your specific use cases. Happy coding in aos!","estimatedWords":389,"lastModified":"2025-10-20T12:10:55.328Z","breadcrumbs":["concepts","tour"],"siteKey":"ao","siteName":"AO Cookbook","depth":4,"crawledAt":"2025-10-20T12:10:55.328Z"},{"url":"https://cookbook_ao.arweave.net/concepts/lua.html","title":"A whistle stop tour of Lua","content":"A whistle stop tour of Lua. ​Before we can explore ao in greater depth, let's take a moment to learn the basics of Lua: your companion for commanding aos processes.Lua is a simple language with few surprises. If you know Javascript, it will feel like a simplified, purer version. If you are learning from-scratch, it will seem like a tiny language that focuses on the important stuff: Clean computation with sane syntax.In this section we will cover the basics of Lua in just a few minutes. If you already know Lua, jump right through to the next chapterJumping back into your aos process. ​For the purpose of this tutorial, we will be assuming that you have already completed the getting started guide. If not, complete that first.If you logged out of your process, you can always re-open it by running aos on your command line, optionally specifying your key file with --wallet [location].Basic Lua expressions. ​In the remainder of this primer we will quickly run through Lua's core features and syntax.Try out on the examples on your aos process as you go, or skip them if they are intuitive to you.Basic arithmetic: Try some basic arithmetic, like 5 + 3. After processing, you will see the result 8. +, -, *, /, and ^ all work as you might expect. % is the symbol that Lua uses for modulus.Setting variables: Type a = 10 and press enter. This sets the variable a to 10. By convention (not enforced by the language), global variables start with a capital letter in Lua (for example Handlers).Using variables: Now type a * 2. You will see 20 returned on the command line.String concatenation: Say hello to yourself by executing \"Hello, \" .. ao.id.INFONote that while global variables conventionally start with a capital letter in Lua, this is not enforced by the language. For example, the ao module is a global variable that was intentionally lowercased for stylistic purposes.Experimenting with conditional statements. ​If-Else: Like most programming languages, Lua uses if-else blocks to conditionally execute code.In your aos process, type .editor and press enter. This will open an in-line text editor within your command-line interface.luaaos_coolness = 9001 if aos_coolness > 9000 then return \"aos has a coolness level over 9000!\" else return \"Oh. 🤷\" endOnce you are finished editing on your terminal, type .done on a new line and press enter. This will terminate edit mode and submit the expression to your process for evaluation.As a result, you will see that aos coolness is >9,000 cool. Good to know.if statements in Lua can also have additional elseif [condition] then blocks, making conditional execution hierarchies easier.Looping in Lua. ​There are a few different ways to loop in your code in Lua. Here are our favorites:While loops:Start by initializing your counter to zero by typing n = 0 and pressing enter.Then open the inline editor again with .editor .luawhile n < 5 do n = n + 1 endType .done on a new line to execute the while loop. You can check the result of the loop by simply running n.For loops:Lua can also execute python-style for loops between a set of values. For example, use the .editor to enter the following code block:luafor m = 1, 100 do n = n + m endRequest the new value of the variable by running n again.Getting functional. ​Define a function:Using the .editor once again, submit the following lines:luafunction greeting(name) return \"Hello, \" .. name endLua also has 'anonymous' or 'higher order' functions. These essentially allow you to use functions themselves as if they are normal data -- to be passed as arguments to other functions, etc. The following example defines an anonymous function and is equivalent to the above:luagreeting = function(name) return \"Hello, \" .. name endCalling the function: Call the function with greeting(\"Earthling\"). aos will return \"Hello, Earthling\".INFOHandlers in ao commonly utilize anonymous functions. When using Handlers.add(), the third argument is an anonymous function in the form function(msg) ... end. This is a key pattern you'll see frequently when working with ao processes.Defining deep objects with tables. ​Tables are Lua's only compound data structure. They map keys to values, but can also be used like traditional arrays.Create a simple table: Type ao_is = {\"hyper\", \"parallel\", \"compute\"}to create a simple table.Accessing the table's elements: Access an element with ao_is[2]. aos will return parallel. Note: Indices in Lua start from 1!Count a table's elements: The size of a table in Lua is found with the operator #. For example, running #ao_is will return 3.Set a named element: Type ao_is[\"cool\"] = true to add a new named key to the table. Named elements can also be accessed with the . operator (e.g. ao_is.cool), but only if the key is a valid identifier - for other keys like \"my key\", use brackets.Lua Wats. ​aos uses Lua because it is a simple, clean language that most experienced programmers can learn very quickly, and is an increasingly popular first programming language, too, thanks to its use in video games like Roblox.Nonetheless, there are a few things about the language that are prone to trip up rookie Lua builders. Tastes may vary, but here is our exhaustive list of Lua wats:Remember: Table indexing starts from 1 not 0!Remember: 'Not equals' is expressed with ~=, rather than != or similar.Remember: Objects in Lua are called 'tables', rather than their more common names.Let's go! ​With this in mind, you now know everything you need in order to build awesome decentralized processes with Lua! In the next chapter we will begin to build parallel processes with Lua and aos.","estimatedWords":931,"lastModified":"2025-10-20T12:10:56.044Z","breadcrumbs":["concepts","lua"],"siteKey":"ao","siteName":"AO Cookbook","depth":4,"crawledAt":"2025-10-20T12:10:56.044Z"},{"url":"https://cookbook_ao.arweave.net/references/index.html","title":"References","content":"References ​This section provides detailed technical references for AO components, languages, and tools. Use these resources to find specific information when implementing your AO projects.Programming Languages ​Resources for the programming languages used in AO:Lua - Reference for the Lua programming language, the primary language used in AOWebAssembly (WASM) - Information about using WebAssembly modules in AOLua Optimization - Techniques and best practices for optimizing Lua code in AOAO API Reference ​Documentation for AO's core APIs and functionality:AO Core - Core ao module and API referenceMessaging - Comprehensive guide to the AO messaging system patternsHandlers - Reference for event handlers and message processingToken - Information about token creation and managementArweave Data - Guide to data handling and storage in AOCron - Documentation for scheduling and managing timed eventsDevelopment Environment ​Tools and setup for AO development:Editor Setup - Guide to setting up your development environment for AOBetterIDEa - The ultimate native web IDE for AO developmentCommunity Resources ​Connect with the AO community:Community Resources - Information about AO community resources and supportNavigation ​Use the sidebar to navigate between reference topics. References are organized by category to help you find the information you need quickly.","estimatedWords":191,"lastModified":"2025-10-20T12:10:56.638Z","breadcrumbs":["references","index"],"siteKey":"ao","siteName":"AO Cookbook","depth":4,"crawledAt":"2025-10-20T12:10:56.638Z"},{"url":"https://cookbook_ao.arweave.net/references/lua.html","title":"Meet Lua","content":"Meet Lua ​Understanding Lua ​Background: Lua is a lightweight, high-level, multi-paradigm programming language designed primarily for embedded systems and clients. It's known for its efficiency, simplicity, and flexibility.Key Features: Lua offers powerful data description constructs, dynamic typing, efficient memory management, and good support for object-oriented programming.Setting Up ​Installation: Visit Lua's official website to download and install Lua.Environment: You can use a simple text editor and command line, or an IDE like ZeroBrane Studio or Eclipse with a Lua plugin.Basic Syntax and Concepts (in aos) ​Hello World:lua\"Hello, World!\"Variables and Types: Lua is dynamically typed. Basic types include nil, boolean, number, string, function, userdata, thread, and table.Control Structures: Includes if, while, repeat...until, and for.Functions: First-class citizens in Lua, supporting closures and higher-order functions.Tables: The only data structuring mechanism in Lua, which can be used to represent arrays, sets, records, etc.Hands-On Practice ​Experiment with Lua's Interactive Mode: Run aos in your terminal and start experimenting with Lua commands.Write Simple Scripts: Create .lua files and run them using the Lua interpreter. Use .load file.lua feature to upload lua code on your aos process.Resources ​Official Documentation: Lua 5.3 Reference ManualOnline Tutorials: Websites like Learn Lua are great for interactive learning.Books: \"Programming in Lua\" (first edition available online) is a comprehensive resource.Community: Join forums or communities like Lua Users for support and discussions.Best Practices ​Keep It Simple: Lua is designed to be simple and flexible. Embrace this philosophy in your code.Performance: Learn about Lua's garbage collection and efficient use of tables.Integration: Consider how Lua can be embedded into other applications, particularly C/C++ projects.Conclusion ​Lua is a powerful language, especially in the context of embedded systems and game development. Its simplicity and efficiency make it a great choice for specific use cases. Enjoy your journey into Lua programming!","estimatedWords":291,"lastModified":"2025-10-20T12:10:57.268Z","breadcrumbs":["references","lua"],"siteKey":"ao","siteName":"AO Cookbook","depth":4,"crawledAt":"2025-10-20T12:10:57.268Z"},{"url":"https://cookbook_ao.arweave.net/references/wasm.html","title":"Meet Web Assembly","content":"Meet Web Assembly ​WebAssembly (often abbreviated as Wasm) is a modern binary instruction format providing a portable compilation target for high-level languages like C, C++, and Rust. It enables deployment on the web for client and server applications, offering a high level of performance and efficiency. WebAssembly is designed to maintain the security and sandboxing features of web browsers, making it a suitable choice for web-based applications. It's a key technology for web developers, allowing them to write code in multiple languages and compile it into bytecode that runs in the browser at near-native speed.The significance of WebAssembly lies in its ability to bridge the gap between web and native applications. It allows complex applications and games, previously limited to desktop environments, to run in the browser with comparable performance. This opens up new possibilities for web development, including the creation of high-performance web apps, games, and even the porting of existing desktop applications to the web. WebAssembly operates alongside JavaScript, complementing it by enabling performance-critical components to be written in languages better suited for such tasks, thereby enhancing the capabilities and performance of web applications.","estimatedWords":186,"lastModified":"2025-10-20T12:10:57.898Z","breadcrumbs":["references","wasm"],"siteKey":"ao","siteName":"AO Cookbook","depth":4,"crawledAt":"2025-10-20T12:10:57.898Z"},{"url":"https://cookbook_ao.arweave.net/references/lua-optimization.html","title":"Lua Optimization Guide for AO Platform","content":"Lua Optimization Guide for AO Platform ​This guide provides practical tips for writing efficient, fast, and performant Lua code for on-chain programs on the AO platform.Table Operations ​Appending Elements ​lua-- ❌ Inefficient: Up to 7x slower in tight loops table.insert(t, v) -- ✅ Efficient: Direct indexing is ~2x faster t[#t + 1] = vRemoving Elements ​lua-- ❌ Inefficient: Shifts all elements left table.remove(t, 1) -- ✅ Efficient: Remove from end local x = t[#t] t[#t] = nilVariable Access ​Local Variables ​lua-- ❌ Inefficient: Global lookup each time for i = 1, 1000 do math.sin(i) end -- ✅ Efficient: Cache the function local sin = math.sin for i = 1, 1000 do sin(i) -- ~30% faster in loops endUpvalues ​lua-- ❌ Inefficient: Config lookup on each call Handlers.add(\"ValidateGameToken\", function(msg) local config = ao.config validateToken(msg, config) end ) -- ✅ Efficient: Cache config as upvalue local config = ao.config Handlers.add(\"ValidateGameToken\", function(msg) validateToken(msg, config) end )String Operations ​String Concatenation ​lua-- ❌ Inefficient: Creates many intermediate strings local str = \"\" for i = 1, N do str = str .. \"line\" .. i end -- ✅ Efficient: Single concatenation at end local lines = {} for i = 1, N do lines[i] = \"line\" .. i end local str = table.concat(lines)Pattern Matching ​lua-- ❌ Inefficient: Recompiles pattern on each iteration for line in io.lines() do if line:match(\"^%s*(%w+)%s*=%s*(%w+)\") then -- Process match end end -- ✅ Efficient: Compile pattern once local pattern = \"^%s*(%w+)%s*=%s*(%w+)\" for line in io.lines() do if line:match(pattern) then -- Process match end endMemory Management ​Table Reuse ​lua-- ❌ Inefficient: Creates new table on each call Handlers.add(\"ComputeGameResults\", function(msg) local results = {} -- Fill results return results end ) -- ✅ Efficient: Reuse and clear table local results = {} Handlers.add(\"ComputeGameResults\", function(msg) for k in pairs(results) do results[k] = nil end -- Fill results return results end )Minimize Garbage Creation ​lua-- ❌ Inefficient: Creates new response table on every transfer local function createTransferResponse(sender, recipient, amount) return { from = sender, to = recipient, quantity = amount, success = true, newBalance = Balances[sender], tags = { Action = \"Transfer-Complete\", Type = \"Token\" } } end -- ✅ Efficient: Reuse template table local transferResponse = { from = nil, to = nil, quantity = 0, success = false, newBalance = 0, tags = { Action = \"Transfer-Complete\", Type = \"Token\" } } local function createTransferResponse(sender, recipient, amount) transferResponse.from = sender transferResponse.to = recipient transferResponse.quantity = amount transferResponse.success = true transferResponse.newBalance = Balances[sender] return transferResponse endBlockchain-Specific Optimizations ​State Management ​lua-- ❌ Inefficient: Multiple separate state updates for _, item in ipairs(items) do ao.send({ Target = \"processID\", Action = \"Update\", Data = item }) end -- ✅ Efficient: Batch updates into single message local updates = {} for _, item in ipairs(items) do table.insert(updates, item) end ao.send({ Target = \"processID\", Action = \"BatchUpdate\", Data = updates })Additional Resources ​Lua Performance GuideSpecial thanks to @allquantor for sharing optimization tips","estimatedWords":483,"lastModified":"2025-10-20T12:10:58.513Z","breadcrumbs":["references","lua optimization"],"siteKey":"ao","siteName":"AO Cookbook","depth":4,"crawledAt":"2025-10-20T12:10:58.513Z"},{"url":"https://cookbook_ao.arweave.net/references/ao.html","title":"ao Module","content":"ao Module ​version: 0.0.3ao process communication is handled by messages, each process receives messages in the form of ANS-104 DataItems, and needs to be able to do the following common operations.ao.send(msg) - send message to another processao.spawn(module, msg) - spawn a processThe goal of this library is to provide this core functionality in the box of the ao developer toolkit. As a developer you have the option to leverage this library or not, but it integrated by default.Properties ​NameDescriptionTypeidProcess Identifier (TxID)string_moduleModule Identifier (TxID)stringauthoritiesSet of Trusted TXsstringAuthorityIdentifiers that the process is able to accept transactions from that are not the owner or the process (0-n)string_versionThe version of the librarystringreferenceReference number of the processnumberenvEvaluation EnvironmentobjectoutboxHolds Messages and Spawns for responseobjectassignablesList of assignables of the processlistnonExtractableTagsList of non-extractable tags of the processlistnonForwardableTagsList of non-forwardable tags of the processlistinitInitializes the AO environmentfunctionsendSends a message to a target processfunctionassignAssigns a message to the processfunctionspawnSpawns a processfunctionresultReturns the result of a messagefunctionisTrustedChecks if a message is trustedfunctionisAssignmentChecks if a message is an assignmentfunctionisAssignableChecks if a message is assignablefunctionaddAssignableAdds an assignable to the assignables listfunctionremoveAssignableRemoves an assignable from the assignables listfunctionclearOutboxClears the outboxfunctionnormalizeNormalizes a message by extracting tagsfunctionsanitizeSanitizes a message by removing non-forwardable tagsfunctioncloneClones a table recursivelyfunctionEnvironment Schema ​The ao.env variable contains information about the initializing message of the process. It follows this schema:luaao.env = { Process = { Id = string, -- Process ID Owner = string, -- Process owner TagArray = { -- Array of name-value pairs { name = string, value = string } }, Tags = { -- Tags as key-value pairs [string] = string } } }Example ​lua{ Process = { Id = \"A1b2C3d4E5f6G7h8I9j0K1L2M3N4O5P6Q7R8S9T0\", Owner = \"Xy9PqW3vR5sT8uB1nM6dK0gF2hL4jC7iE9rV3wX5\", TagArray = { { name = \"App-Name\", value = \"aos\" } }, Tags = { [\"App-Name\"] = \"aos\" } } }Methods ​ao.send(msg: Message) ​Takes a Message as input. The function adds ao-specific tags and stores the message in ao.outbox.Messages.Example ​lualocal message = ao.send({ Target = msg.From, Data = \"ping\", Tags = { [\"Content-Type\"] = \"text/plain\", [\"Action\"] = \"Ping\" } }) -- or local message = ao.send({ Target = msg.From, Data = \"ping\", Action = \"Ping\", -- will be converted to Tags [\"Content-Type\"] = \"text/plain\" -- will be converted to Tags })ao.spawn(module: string, spawn: Spawn) ​Takes a module ID string and Spawn as input. Returns a Spawn table with a generated Ref_ tag.Example ​lualocal process = ao.spawn(\"processId\", { Data = { initial = \"state\" }, Tags = { [\"Process-Type\"] = \"calculator\" } })ao.assign(assignment: Assignment) ​Takes an Assignment as input. Adds the assignment to ao.outbox.Assignments.Example ​luaao.assign({ Processes = {\"process-1\", \"process-2\"}, Message = \"sample-message-id\" })ao.result(result: Result) ​Takes a Result as input. Returns the final process execution result.Example ​lualocal process_result = ao.result({ Output = \"Process completed successfully\", Messages = { { Target = \"ProcessY\", Data = \"Result data\", Tags = { [\"Status\"] = \"Success\" } } }, Spawns = { \"spawned-process-1\" }, Assignments = { {Processes = { \"process-1\" }, Message = \"assignment-message-id\"} } })ao.isAssignable(msg: Message) ​Takes a Message as input. Returns true if the message matches a pattern in ao.assignables.Example ​lualocal can_be_assigned = ao.isAssignable({ Target = \"ProcessA\", Data = \"Some content\", Tags = { [\"Category\"] = \"Info\" } })ao.isAssignment(msg: Message) ​Takes a Message as input. Returns true if the message is assigned to a different process.Example ​lualocal is_assigned_elsewhere = ao.isAssignment({ Target = \"AnotherProcess\" })ao.addAssignable(name: string, condition: function) ​Adds a named condition function to the process's list of assignables. Messages matching any condition will be accepted when assigned.Note: The condition parameter uses a similar pattern matching approach as the pattern parameter in Handlers.add(). For more advanced pattern matching techniques, see the Handlers Pattern Matching documentation.Example ​lua-- Allow transactions from ArDrive ao.addAssignable(\"allowArDrive\", function (msg) return msg.Tags[\"App-Name\"] == \"ArDrive-App\" end) -- Allow transactions with specific content type ao.addAssignable(\"allowJson\", function (msg) return msg.Tags[\"Content-Type\"] == \"application/json\" end)ao.removeAssignable(name: string) ​Removes a previously added assignable condition from the process's list of assignables.Example ​luaao.removeAssignable(\"allowArDrive\")ao.isTrusted(msg: Message) ​Takes a Message as input. Returns true if the message is from a trusted source.Example ​luaif ao.isTrusted(msg) then -- Process trusted message else -- Handle untrusted message endCustom ao Table Structures ​Tags ​Used by: ao.send(), ao.spawn(), ao.normalize(), ao.sanitize()All of the below syntaxes are valid, but each syntax gets converted to { name = string, value = string } tables behind the scenes. We use alternative 1 throughout the documentation for brevity and consistency.lua-- Default: Array of name-value pair tables Tags = { { name = \"Content-Type\", value = \"text/plain\" }, { name = \"Action\", value = \"Ping\" } } -- Alternative 1: Direct key-value pairs in Tags table using string keys Tags = { [\"Content-Type\"] = \"text/plain\", [\"Action\"] = \"Ping\" } -- Alternative 2: Direct key-value pairs in Tags table using dot notation Tags = { Category = \"Info\", Action = \"Ping\" }Root-level Tag ConversionAny keys in the root message object that are not one of: Target, Data, Anchor, Tags, or From will automatically be converted into Tags using the key as the tag name and its value as the tag value.lua-- These root-level keys will be automatically converted to Tags { Target = \"process-id\", Data = \"Hello\", [\"Content-Type\"] = \"text/plain\", -- Will become a Tag Action = \"Ping\" -- Will become a Tag }Message ​Used by: ao.send(), ao.isTrusted(), ao.isAssignment(), ao.isAssignable(), ao.normalize(), ao.sanitize()lua-- Message structure { Target = string, -- Required: Process/wallet address Data = any, -- Required: Message payload Tags = Tag }Spawn ​Used by: ao.spawn()lua-- Spawn structure { Data = any, -- Required: Initial process state Tags = Tag -- Required: Process tags }Assignment ​Used by: ao.assign(), ao.result()lua-- Assignment configuration table structure { Processes = { string }, -- Required: List of target process ID strings Message = string -- Required: Message to assign }Result ​Used by: ao.result()lua-- Process result structure { Output = string, -- Optional: Process output Messages = Message, -- Optional: Generated messages Spawns = Spawn, -- Optional: Spawned processes Assignments = Assignment, -- Optional: Process assignments Error = string -- Optional: Error information }","estimatedWords":979,"lastModified":"2025-10-20T12:10:59.355Z","breadcrumbs":["references","ao"],"siteKey":"ao","siteName":"AO Cookbook","depth":4,"crawledAt":"2025-10-20T12:10:59.355Z"},{"url":"https://cookbook_ao.arweave.net/references/messaging.html","title":"Messaging Patterns in ao","content":"Messaging Patterns in ao ​This reference guide explains the messaging patterns available in ao and when to use each one.Quick Reference: Choosing the Right Pattern ​If you need to...Process FlowKey function(s)Send a message without waiting for a responseA → Bao.sendSend a message and wait for a responseA → B → Aao.send().receive()Process messages and respond to the senderB → AHandlers.add + msg.replyCreate a chain of processing servicesA → B → C → Amsg.forward + ao.send().receive()Wait for any matching message regardless of senderAny → AReceive (capital R)Create a standard automated responseB → AHandlers.utils.replySending Messages ​ao.send: Asynchronous Message Sending ​Non-blocking direct A → B messaging that returns immediately after sending.Use for fire-and-forget notifications or starting async conversationsReturns a promise-like object that can be chained with .receive() if neededGood for parallel processing since it doesn't block executionClient (A) → Service (B) ↓ ↓ Continues Processes execution messageBasic Send Example:lua-- Non-blocking send example local serviceId = \"process-789\" -- Process ID of the target service ao.send({ Target = serviceId, Data = \"Hello!\", Action = \"Greeting\" }) -- Code here runs immediately after sendingmsg.reply: Asynchronous Response Sending ​Non-blocking B → A response with automatic reference tracking. Used within handlers to respond to incoming messages.Automatically links response to original message via X-ReferenceEnables asynchronous request-response patternsAutomatically sets Target to the original sender or Reply-To address if specifiedClient (A) → Service (B) ← Response tagged with X-ReferenceHandler Reply Example:lua-- Non-blocking reply in a handler Handlers.add(\"greeting-handler\", { Action = \"Greeting\" }, function(msg) msg.reply({ Data = \"Hi back!\" }) -- Returns immediately -- Handler continues executing here end )msg.forward: Message Forwarding ​Non-blocking multi-process routing for A → B → C → A patterns. Creates a sanitized copy of the original message.Takes a target and a partial message to overwrite forwarded message fieldsPreserves Reply-To and X-Reference properties for complete message trackingSets X-Origin to original sender, enabling final service to reply directly to originatorClient (A) → Service (B) → Backend (C) ↖ ↙ Response with X-ReferenceMulti-Process Pipeline Example:lua-- In client process local middlewareProcessId = \"process-123\" local finalProcessId = \"process-456\" -- Send to middleware and wait for response from final service local response = ao.send({ Target = middlewareProcessId, Action = \"Transform\", Data = \"raw-data\" }).receive(finalProcessId) -- Explicitly wait for response from final service -- In middleware service Handlers.add(\"transform-middleware\", { Action = \"Transform\" }, function(msg) local finalProcessId = \"process-456\" msg.forward(finalProcessId, { Data = msg.Data .. \" (pre-processed)\", Action = \"Transform-Processed\" }) end ) -- In final service Handlers.add(\"final-processor\", { Action = \"Transform-Processed\" }, function(msg) -- No need to know the client ID - it's stored in X-Origin msg.forward(msg['X-Origin'], { Data = msg.Data .. \" (final processing complete)\", Action = \"Transform-Complete\" }) end )Handlers.utils.reply: Simple Reply Handler Creation ​Creates a handler function that automatically replies with a fixed response. A wrapper around msg.reply for common use cases.Simple String Response Example:lua-- Simple string response handler Handlers.add(\"echo-handler\", { Action = \"Echo\" }, Handlers.utils.reply(\"Echo reply!\") ) -- Equivalent to: Handlers.add(\"echo-handler\", { Action = \"Echo\" }, function(msg) msg.reply({ Data = \"Echo reply!\" }) end )Message Table Response Example:lua-- Message table response handler Handlers.add(\"status-handler\", { Action = \"Status\" }, Handlers.utils.reply({ Data = \"OK\", Action = \"Status-Response\" }) )Receiving Messages ​Receive (Capital R): Blocking Pattern Matcher ​Blocks execution until any matching message arrives from any sender. Under the hood, this is implemented using Handlers.once, making it a one-time pattern matcher that automatically removes itself after execution.Waits for any message matching the pattern, regardless of originUse for synchronous message processing flows or event listeningAutomatically removes the handler after first match (using Handlers.once internally) Process (A) ↓ Blocks until match received ↓ Continues executionMessage Pattern Matching Example:lua-- Blocks until matching message received local msg = Receive({ Action = \"Update\" }) if msg then -- Process message endao.send().receive (Lowercase r): Blocking Reference Matcher ​Blocks execution until a specific reply arrives, enabling A → B → A and A → B → C → A request-response cycles.Only matches messages linked by X-ReferenceCan specify a target process ID to indicate which process will replyImplicitly waits for the proper response based on message reference chainsFor A → B → A flows, process B uses msg.replyFor A → B → C → A flows, processes B and C use msg.forwardBasic Request-Response Example:lua-- Basic usage: wait for reply from target local serviceId = \"process-789\" local reply = ao.send({ Target = serviceId, Action = \"Query\", Data = { query: \"select\" } }).receive() -- Blocks until response receivedMessage Properties ​The following properties track message chains and ensure proper routing:Reference: Unique identifier automatically assigned to each message.Reply-To: Specifies the destination for responses.X-: Any property starting with X- denotes a 'forwarded' tag and is automatically managed by the system. X-Reference: Maintains the conversation chain across replies and forwards.X-Origin: Tracks the conversation originator.The system automatically manages these properties when using msg.reply and msg.forward. Check out the source code to see exactly how these properties are managed.Blocking vs. Non-Blocking ​Functions either pause your code or let it continue running:Non-blocking (ao.send, msg.reply, msg.forward): Send and continue executionBlocking (Receive, .receive()): Pause until response arrives","estimatedWords":825,"lastModified":"2025-10-20T12:11:00.230Z","breadcrumbs":["references","messaging"],"siteKey":"ao","siteName":"AO Cookbook","depth":4,"crawledAt":"2025-10-20T12:11:00.230Z"},{"url":"https://cookbook_ao.arweave.net/references/handlers.html","title":"Handlers (Version 005)","content":"Handlers (Version 0.0.5) ​Overview ​The Handlers library provides a flexible way to manage and execute a series of process functions based on pattern matching. An AO process responds based on receiving Messages, these messages are defined using the Arweave DataItem specification which consists of Tags, and Data. Using the Handlers library, you can define a pipeline of process evaluation based on the attributes of the AO Message. Each Handler is instantiated with a name, a pattern matching function, and a function to execute on the incoming message. This library is suitable for scenarios where different actions need to be taken based on varying input criteria.Concepts ​Handler Arguments Overview ​When adding a handler using Handlers.add(), you provide three main arguments:name (string): The identifier for your handlerpattern (table or function): Defines how to match incoming messageshandler (function or resolver table): Defines what to do with matched messagesPattern Matching Tables ​Pattern Matching Tables provide a declarative way to match incoming messages based on their attributes. This is used as the second argument in Handlers.add() to specify which messages your handler should process.Basic Pattern Matching Rules ​Simple Tag Matchinglua{ \"Action\" = \"Do-Something\" } -- Match messages that have an exact Action tag valueWildcard Matchinglua{ \"Recipient\" = '_' } -- Match messages with any Recipient tag valuePattern Matchinglua{ \"Quantity\" = \"%d+\" } -- Match using Lua string patterns (similar to regex)Function-based Matchinglua{ \"Quantity\" = function(v) return tonumber(v) ~= nil end } -- Custom validation functionCommon Pattern Examples ​Balance Action Handlerlua{ Action = \"Balance\" } -- Match messages with Action = \"Balance\"Numeric Quantity Handlerlua{ Quantity = \"%d+\" } -- Match messages where Quantity is a numberDefault Action Handlers (AOS 2.0+) ​AOS 2.0 introduces simplified syntax for Action-based handlers. Instead of writing explicit pattern functions, you can use these shorthand forms:lua-- Traditional syntax Handlers.add(\"Get-Balance\", function (msg) return msg.Action == \"Balance\", doBalance) -- Simplified syntax options: Handlers.add(\"Balance\", \"Balance\", doBalance) -- Explicit action matching Handlers.add(\"Balance\", doBalance) -- Implicit action matchingResolvers ​Resolvers are special tables that can be used as the third argument in Handlers.add() to enable conditional execution of functions based on additional pattern matching. Each key in a resolver table is a pattern matching table, and its corresponding value is a function that executes when that pattern matches.luaHandlers.add(\"Update\", { [{ Status = \"Ready\" }] = function (msg) print(\"Ready\") end, [{ Status = \"Pending\" }] = function (msg) print(\"Pending\") end, [{ Status = \"Failed\" }] = function (msg) print(\"Failed\") end } )This structure allows developers to create switch/case-like statements where different functions are triggered based on which pattern matches the incoming message. Resolvers are particularly useful when you need to handle a group of related messages differently based on additional criteria.Module Structure ​Handlers._version: String representing the version of the Handlers library.Handlers.list: Table storing the list of registered handlers.Common Handler Function Parameters ​ParameterTypeDescriptionnamestringThe identifier of the handler item in the handlers list.patterntable or functionSpecifies how to match messages. As a table, defines required message tags with string values (e.g. { Action = \"Balance\", Recipient = \"_\" } requires an \"Action\" tag with string value \"Balance\" and any string \"Recipient\" tag value). As a function, takes a message DataItem and returns: \"true\" (invoke handler and exit pipeline), \"false\" (skip handler), or \"continue\" (invoke handler and continue pipeline).handler(Resolver) table or functionEither a resolver table containing pattern-function pairs for conditional execution, or a single function that processes the message. When using a resolver table, each key is a pattern matching table and its value is the function to execute when that pattern matches. When using a function, it takes the message DataItem as an argument and executes business logic.maxRuns (optional)numberAs of 0.0.5, each handler function takes an optional function to define the amount of times the handler should match before it is removed. The default is infinity.Functions ​Handlers.add(name, pattern, handler) ​Adds a new handler or updates an existing handler by nameHandlers.append(name, pattern, handler) ​Appends a new handler to the end of the handlers list.Handlers.once(name, pattern, handler) ​Only runs once when the pattern is matched. Equivalent to setting maxRuns = 1. This is the underlying implementation used by the Receive function in the messaging system.Handlers.prepend(name, pattern, handler) ​Prepends a new handler to the beginning of the handlers list.Handlers.before(handleName) ​Returns an object that allows adding a new handler before a specified handler.Handlers.after(handleName) ​Returns an object that allows adding a new handler after a specified handler.Handlers.remove(name) ​Removes a handler from the handlers list by name.Handler Execution Notes ​Execution Order ​Handlers are executed in the order they appear in Handlers.list.When a message arrives, each handler's pattern function is called sequentially to determine if it should process the message.Pattern Function Return Values ​Pattern functions determine the message handling flow based on their return values:Skip Handler (No Match)Return: 0, false, or any string except \"continue\" or \"break\"Effect: Skips current handler and proceeds to the next one in the listHandle and ContinueReturn: 1 or \"continue\"Effect: Processes the message and continues checking subsequent handlersUse Case: Ideal for handlers that should always execute (e.g., logging)Handle and StopReturn: -1, true, or \"break\"Effect: Processes the message and stops checking further handlersUse Case: Most common scenario where a handler exclusively handles its matched messagePractical Examples ​Logging Handler: Place at the start of the list and return \"continue\" to log all messages while allowing other handlers to process them.Specific Message Handler: Return \"break\" to handle matched messages exclusively and prevent further processing by other handlers.Handlers.utils ​The Handlers.utils module provides two functions that are common matching patterns and one function that is a common handle function.hasMatchingData(data: string)hasMatchingTag(name: string, value: string)reply(text: string)Handlers.utils.hasMatchingData(data: string) ​This helper function returns a pattern matching function that takes a message as input. The returned function checks if the message's Data field contains the specified string. You can use this helper directly as the pattern argument when adding a new handler.luaHandlers.add(\"ping\", Handlers.utils.hasMatchingData(\"ping\"), ... )Handlers.utils.hasMatchingTag(name: string, value: string) ​This helper function returns a pattern matching function that takes a message as input. The returned function checks if the message has a tag with the specified name and value. If they match exactly, the pattern returns true and the handler function will be invoked. This helper can be used directly as the pattern argument when adding a new handler.luaHandlers.add(\"ping\", Handlers.utils.hasMatchingTag(\"Action\", \"Ping\"), ... )Handlers.utils.reply(text: string) ​This helper is a simple handle function, it basically places the text value in to the Data property of the outbound message.luaHandlers.add(\"ping\", Handlers.utils.hasMatchingData(\"ping\"), Handlers.utils.reply(\"pong\") )Example Handlers ​Pattern Matching Table ​luaHandlers.add(\"Ping\", -- Name of the handler { Action = \"Ping\" }, -- Matches messages with Action = \"Ping\" tag function(msg) -- Business logic to execute on Message print(\"ping\") msg.reply({ Data = \"pong\" }) end )Resolver Table Handler ​luaHandlers.add(\"Foobarbaz\", -- Name of the handler { Action = \"Speak\" }, -- Matches messages with Action = \"Speak\" tag { -- Resolver with pattern-function pairs [{ Status = \"foo\" }] = function(msg) print(\"foo\") end, [{ Status = \"bar\" }] = function(msg) print(\"bar\") end, [{ Status = \"baz\" }] = function(msg) print(\"baz\") end } )Function-Based Pattern Matching & Handler ​luaHandlers.add(\"Example\", -- Name of the handler function(msg) -- Pattern function matches messages with Action = \"Speak\" tag return msg.Action == \"Speak\" end, function(msg) -- Handler function that executes business logic print(msg.Status) end )","estimatedWords":1185,"lastModified":"2025-10-20T12:11:00.822Z","breadcrumbs":["references","handlers"],"siteKey":"ao","siteName":"AO Cookbook","depth":4,"crawledAt":"2025-10-20T12:11:00.822Z"},{"url":"https://cookbook_ao.arweave.net/references/token.html","title":"ao Token and Subledger Specification","content":"ao Token and Subledger Specification ​Status: DRAFT-1 Targeting Network: ao.TN.1This specification describes the necessary message handlers and functionality required for a standard ao token process. Implementations of this standard typically offer users the ability to control a transferrable asset, whose scarcity is maintained by the process.Each compliant process will likely implement a ledger of balances in order to encode ownership of the asset that the process represents. Compliant processes have a set of methods that allow for the modification of this ledger, typically with safe-guards to ensure the scarcity of ownership of the token represented by the process.Additionally, this specification describes a 'subledger' process type which, when implemented, offers the ability to split move a number of the tokens from the parent into a child process that implements the same token interface specification. If the From-Module of the subledger process is trusted by the participants, these subledgers can be used to transact in the 'source' token, without directly exchanging messages with it. This allows participants to use the tokens from a process, even if that process is congested. Optionally, if the participants trust the Module a subledger process is running, they are able to treat balances across these processes as fungible. The result of this is that an arbitrary numbers of parallel processes -- and thus, transactions -- can be processed by a single token at any one time.Token Processes ​A specification-compliant token process responds to a number of different forms of messages, with each form specified in an Action tag. The full set of Action messages that the token must support are as follows:NameDescriptionRead-OnlyBalanceget the balance of an identifier✔️Balancesget a list of all ledger/account balances✔️Transfersend 1 or more units from the callers balance to one or move targets with the option to notify targets❌Mintif the ledger process is the root and you would like to increase token supply❌In the remainder of this section the tags necessary to spawn a compliant token process, along with the form of each of the Action messages and their results is described.Spawning Parameters ​Every compliant token process must carry the following immutable parameters upon its spawning message:TagDescriptionOptional?NameThe title of the token, as it should be displayed to users.✔️TickerA suggested shortened name for the token, such that it can be referenced quickly.✔️LogoAn image that applications may desire to show next to the token, in order to make it quickly visually identifiable.✔️DenominationThe number of the token that should be treated as a single unit when quantities and balances are displayed to users.❌Messaging Protocol ​Balance(Target? : string) ​Returns the balance of a target, if a target is not supplied then the balance of the sender of the message must be returned.Example Action message:luaao.send({ Target = \"{TokenProcess Identifier}\", Tags = { [\"Action\"] = \"Balance\", [\"Target\"] = \"{IDENTIFIER}\" } })Example response message:lua{ Tags = { [\"Balance\"] = \"50\", [\"Target\"] = \"LcldyO8wwiGDzC3iXzGofdO8JdR4S1_2A6Qtz-o33-0\", [\"Ticker\"] = \"FUN\" } }Balances() ​Returns the balance of all participants in the token.luaao.send({ Target = \"[TokenProcess Identifier]\", Tags = { [\"Action\"] = \"Balances\", [\"Limit\"] = 1000, # TODO: Is this necessary if the user is paying for the compute and response? [\"Cursor\"] = \"BalanceIdentifier\" } })Example response message:lua{ Data = { \"MV8B3MAKTsUOqyCzQ0Tsa2AR3TiWTBU1Dx0xM4MO-f4\": 100, \"LcldyO8wwiGDzC3iXzGofdO8JdR4S1_2A6Qtz-o33-0\": 50 } }Transfer(Target, Quantity) ​If the sender has a sufficient balance, send the Quantity to the Target, issuing a Credit-Notice to the recipient and a Debit-Notice to the sender. The Credit- and Debit-Notice should forward any and all tags from the original Transfer message with the X- prefix. If the sender has an insufficient balance, fail and notify the sender.luaao.send({ Target = \"[TokenProcess Identifier]\", Tags = { [\"Action\"] = \"Transfer\", [\"Recipient\"] = \"[ADDRESS]\", [\"Quantity\"] = \"100\", [\"X-[Forwarded Tag(s) Name]\"] = \"[VALUE]\" } })If a successful transfer occurs a notification message should be sent if Cast is not set.luaao.send({ Target = \"[Recipient Address]\", Tags = { [\"Action\"] = \"Credit-Notice\", [\"Sender\"] = \"[ADDRESS]\", [\"Quantity\"] = \"100\", [\"X-[Forwarded Tag(s) Name]\"] = \"[VALUE]\" } })Recipients will infer from the From-Process tag of the message which tokens they have received.Get-Info() ​luaao.send({ Target = \"{Token}\", Tags = { [\"Action\"] = \"Info\" } })Mint() [optional] ​Implementing a Mint action gives the process a way of allowing valid participants to create new tokens.luaao.send({ Target =\"{Token Process}\", Tags = { [\"Action\"] = \"Mint\", [\"Quantity\"] = \"1000\" } })Subledger Processes ​In order to function appropriately, subledgers must implement the full messaging protocol of token contracts (excluding the Mint action). Subledgers must also implement additional features and spawn parameters for their processes. These modifications are described in the following section.Spawning Parameters ​Every compliant subledger process must carry the following immutable parameters upon its spawning message:TagDescriptionOptional?Source-TokenThe ID of the top-most process that this subledger represents.❌Parent-TokenThe ID of the parent process that this subledger is attached to.❌Credit-Notice Handler ​Upon receipt of a Credit-Notice message, a compliant subledger process must check if the process in question is the Parent-Token. If it is, the subledger must increase the balance of the Sender by the specified quantity.Transfer(Target, Quantity) ​In addition to the normal tags that are passed in the Credit-Notice message to the recipient of tokens, a compliant subledger process must also provide both of the Source-Token and Parent-Token values. This allows the recipient of the Transfer message -- if they trust the Module of the subledger process -- to credit a receipt that is analogous (fungible with) deposits from the Source-Token.The modified Credit-Notice should be structured as follows:luaao.send({ Target = \"[Recipient Address]\", Tags = { [\"Action\"] = \"Credit-Notice\", [\"Quantity\"] = \"100\", [\"Source-Token\"] = \"[ADDRESS]\", [\"Parent-Token\"] = \"[ADDRESS]\", [\"X-[Forwarded Tag(s) Name]\"] = \"[VALUE]\" } })Withdraw(Target?, Quantity) ​All subledgers must allow balance holders to withdraw their tokens to the parent ledger. Upon receipt of an Action: Withdraw message, the subledger must send an Action message to its Parent-Ledger, transferring the requested tokens to the caller's address, while debiting their account locally. This transfer will result in a Credit-Notice from the Parent-Ledger for the caller.luaao.send({ Target = \"[TokenProcess Identifier]\", Tags = { [\"Action\"] = \"Withdraw\", [\"Recipient\"] = \"[ADDRESS]\", [\"Quantity\"] = \"100\" } })Token Example ​NOTE: When implementing a token it is important to remember that all Tags on a message MUST be \"string\"s. Using thetostring function you can convert simple types to strings.luaif not balances then balances = { [ao.id] = 100000000000000 } end if name ~= \"Fun Coin\" then name = \"Fun Coin\" end if ticker ~= \"Fun\" then ticker = \"fun\" end if denomination ~= 6 then denomination = 6 end -- handlers that handler incoming msg Handlers.add( \"Transfer\", Handlers.utils.hasMatchingTag(\"Action\", \"Transfer\"), function (msg) assert(type(msg.Tags.Recipient) == 'string', 'Recipient is required!') assert(type(msg.Tags.Quantity) == 'string', 'Quantity is required!') if not balances[msg.From] then balances[msg.From] = 0 end if not balances[msg.Tags.Recipient] then balances[msg.Tags.Recipient] = 0 end local qty = tonumber(msg.Tags.Quantity) assert(type(qty) == 'number', 'qty must be number') -- handlers.utils.reply(\"Transferring qty\")(msg) if balances[msg.From] >= qty then balances[msg.From] = balances[msg.From] - qty balances[msg.Tags.Recipient] = balances[msg.Tags.Recipient] + qty ao.send({ Target = msg.From, Tags = { [\"Action\"] = \"Debit-Notice\", [\"Quantity\"] = tostring(qty) } }) ao.send({ Target = msg.Tags.Recipient, Tags = { [\"Action\"] = \"Credit-Notice\", [\"Quantity\"] = tostring(qty) } }) -- if msg.Tags.Cast and msg.Tags.Cast == \"true\" then -- return -- end end end ) Handlers.add( \"Balance\", Handlers.utils.hasMatchingTag(\"Action\", \"Balance\"), function (msg) assert(type(msg.Tags.Target) == \"string\", \"Target Tag is required!\") local bal = \"0\" if balances[msg.Tags.Target] then bal = tostring(balances[msg.Tags.Target]) end ao.send({ Target = msg.From, Tags = { [\"Balance\"] = bal, [\"Ticker\"] = ticker or \"\" } }) end ) local json = require(\"json\") Handlers.add( \"Balances\", Handlers.utils.hasMatchingTag(\"Action\", \"Balances\"), function (msg) ao.send({ Target = msg.From, Data = json.encode(balances) }) end ) Handlers.add( \"Info\", Handlers.utils.hasMatchingTag(\"Action\", \"Info\"), function (msg) ao.send({ Target = msg.From, Tags = { [\"Name\"] = name, [\"Ticker\"] = ticker, [\"Denomination\"] = tostring(denomination) } }) end )","estimatedWords":1265,"lastModified":"2025-10-20T12:11:01.469Z","breadcrumbs":["references","token"],"siteKey":"ao","siteName":"AO Cookbook","depth":4,"crawledAt":"2025-10-20T12:11:01.469Z"},{"url":"https://cookbook_ao.arweave.net/references/data.html","title":"Accessing Data from Arweave with ao","content":"Accessing Data from Arweave with ao ​There may be times in your ao development workflow that you want to access data from Arweave. With ao, your process can send an assignment instructing the network to provide that data to your Process.Defining Acceptable Transactions (Required First Step) ​Before you can assign any Arweave transaction to your process, you must first define which transactions your process will accept using ao.addAssignable. This function creates conditions that determine which Arweave transactions your process will accept.lua-- Allow transactions from ArDrive ao.addAssignable(\"allowArDrive\", function (msg) return msg.Tags[\"App-Name\"] == \"ArDrive-App\" end) -- Allow specific content types ao.addAssignable(\"allowImages\", function (msg) return msg.Tags[\"Content-Type\"] and string.match(msg.Tags[\"Content-Type\"], \"^image/\") end)Warning: If you attempt to assign a transaction without first defining a matching assignable pattern, that transaction will be permanently blacklisted and can never be assigned to your process, even if you later add a matching assignable.You can remove assignables with ao.removeAssignable(\"\").The condition functions use similar pattern matching techniques as found in the Handlers documentation. For complete details on the ao.addAssignable function, including parameter descriptions and additional examples, see the ao Module Reference.Assignment Methods ​After defining acceptable transactions and setting up your listener (if needed), you can request Arweave data in one of two ways:Using Assign ​The primary method to request data from Arweave:luaAssign({ Processes = { ao.id }, Message = '' })Using Send with Assignments ​Alternatively, you can use the Send function with an Assignments parameter:luaSend({ Target = ao.id, Data = 'Hello World', Assignments = { '', '' } })Working with Assigned Data ​You can process assigned data using either Receive or Handlers:Using Receive Directly ​lua-- Listen for messages from ArDrive ArweaveData = Receive(function(msg) return msg.Tags[\"App-Name\"] == \"ArDrive-App\" end) -- Process the data when received if ArweaveData then print(ArweaveData.Tags[\"App-Name\"]) -- Raw Arweave Data is available in ArweaveData.Data endYou can also match specific transactions or combine conditions:lua-- Match a specific transaction ID ArweaveData = Receive({ Id = \"\" }) -- Or combine multiple conditions ArweaveData = Receive(function(msg) return msg.Tags[\"App-Name\"] == \"ArDrive-CLI\" and msg.Tags[\"Content-Type\"] == \"image/png\" end)Note: When using .load, the script pauses at Receive until data arrives. When running commands separately in the shell, each command executes independently.Using Handlers ​For persistent processing, set up a handler:luaHandlers.add(\"ProcessArDriveFiles\", { Tags = { [\"App-Name\"] = \"ArDrive-App\" } }, function(msg) print(msg.Tags[\"App-Name\"]) -- Raw Arweave Data is available in msg.Data end )Handlers are ideal for:Processing multiple assignments over timeAutomated processing without manual interventionBuilding services that other processes can interact withFor more details, see the Messaging Patterns and Handlers documentation.Complete Example Workflow ​Here's a complete example that demonstrates the entire process of accessing data from an Arweave transaction:lua-- Step 1: Define which transactions your process will accept ao.addAssignable(\"allowArDrive\", function (msg) return msg.Tags[\"App-Name\"] == \"ArDrive-App\" end) -- Step 2: Request the data Assign({ Processes = { ao.id }, Message = '' }) -- Step 3: Immediately capture the Assignment; blocking until received ArweaveData = Receive(function(msg) return msg.Tags[\"App-Name\"] == \"ArDrive-App\" end) print(ArweaveData.Tags[\"App-Name\"]) -- e.g., \"ArDrive-CLI\" -- Raw Arweave Data is available in ArweaveData.DataThis pattern creates a synchronous flow where your process:Defines acceptable transactionsRequests the dataCaptures the data using ReceiveProcesses the dataPractical Examples ​Here are two practical examples showing different approaches to working with Arweave data in your ao process:Example 1: Caching Arweave Data ​This example demonstrates how to load and cache data from Arweave, then use it in subsequent operations:lua-- Initialize state local Number = 0 -- Step 1: Define which transactions your process will accept print(\"Step 1: Defining acceptable transactions\") ao.addAssignable(\"addNumber\", function (msg) return msg.Tags[\"Action\"] == \"Number\" end) -- Step 2: Request and cache the initial number from Arweave -- This uses a self-executing function to fetch and cache the value only once NumberFromArweave = NumberFromArweave or (function() print(\"Step 2: Requesting initial number from Arweave\") Assign({ Processes = { ao.id }, Message = 'DivdWHaNj8mJhQQCdatt52rt4QvceBR_iyX58aZctZQ' }) return tonumber(Receive({ Action = \"Number\"}).Data) end)() -- Step 3: Set up handler for future number updates -- This handler will add new numbers to our cached Arweave number Handlers.add(\"Number\", function (msg) print(\"Received message with Data = \" .. msg.Data) print(\"Old Number: \" .. Number) Number = NumberFromArweave + tonumber(msg.Data) print(\"New Number: \" .. Number) end)This example shows how to:Cache Arweave data using a self-executing functionUse the cached data in subsequent message handlingCombine Arweave data with new incoming dataExample 2: Dynamic Transaction Processing ​This example shows how to process arbitrary Arweave transactions and maintain state between requests:lua-- Table to store pending requests (maps transaction ID to original sender) local PendingRequests = {} -- Step 1: Define which transactions your process will accept print(\"Step 1: Defining acceptable transactions\") ao.addAssignable(\"processArweaveNumber\", function (msg) return msg.Tags[\"Action\"] == \"Number\" end) -- Step 2: Set up handler for initiating the processing Handlers.add( \"ProcessArweaveNumber\", function (msg) if not msg.Tags[\"ArweaveTx\"] then print(\"Error: No ArweaveTx tag provided\") return end local txId = msg.Tags[\"ArweaveTx\"] print(\"Assigning Arweave transaction: \" .. txId) -- Store the original sender associated with this transaction ID PendingRequests[txId] = msg.From -- Assign the transaction to this process Assign({ Processes = { ao.id }, Message = txId }) print(\"Assignment requested; waiting for data...\") end ) -- Step 3: Set up handler for processing the assigned message Handlers.add( \"Number\", function (msg) local txId = msg.Id -- The ID of the assigned message local originalSender = PendingRequests[txId] if not originalSender then print(\"Error: No pending request found for transaction \" .. txId) return end local data = msg.Data if not data or not tonumber(data) then print(\"Error: Invalid number data in assigned message\") return end local number = tonumber(data) local result = number + 1 print(string.format(\"Processing: %d + 1 = %d\", number, result)) -- Send the result back to the original sender Send({ Target = originalSender, Data = tostring(result) }) -- Clean up the pending request PendingRequests[txId] = nil end )To use this example:luaSend({ Target = ao.id, Action = \"ProcessArweaveNumber\", Tags = { ArweaveTx = \"YOUR-ARWEAVE-TX-ID\" -- ID of a transaction containing a number } })This example demonstrates:Processing arbitrary Arweave transactionsMaintaining state between requests using a pending requests tableSending results back to the original requesterError handling and request cleanupWARNINGWhen using Assign to bridge Arweave data into AO, you must ensure that:The Arweave transaction you're assigning matches one of your defined assignablesYou have a corresponding handler or receiver set up to process that transaction typeThe handler's pattern matching matches the assigned transaction's tags/propertiesFor example, if you're assigning a transaction with Action = \"Number\", you need:An assignable that accepts msg.Tags[\"Action\"] == \"Number\"Either a Receive function or a handler that matches the same patternBoth the assignable and handler must use consistent pattern matchingImportant Limitations ​There are critical limitations to be aware of when working with assignables:Matching is Required: Transactions must match at least one of your defined assignable patterns to be accepted.Blacklisting is Permanent: If you attempt to assign a transaction before defining an appropriate assignable, it will be permanently blacklisted. Even if you later add a matching assignable, that transaction will never be accepted.One-time Assignment: Each Arweave transaction can only be assigned once to a given process. Subsequent assignments of the same transaction will be ignored.Proper Sequence for Assigning Arweave Transactions ​For successful assignment of Arweave transactions, follow these steps:Define assignables to specify which Arweave transactions your process will acceptWait for any transaction confirmations (by default, 20 confirmations are required)Set up handlers or listeners with Receive or Handlers.add to process the dataAssign the Arweave transaction to your process (see Assignment Methods)The order of steps 3 and 4 can be interchanged based on your needs:When using Receive in a script loaded with .load, ensure Assign is placed before Receive to prevent the process from hanging, as Receive is blocking.When using handlers or running commands separately in the shell, the order doesn't matter as handlers will catch messages whenever they arriveWhy Access Data from Arweave? ​There are several practical reasons to access Arweave data from your ao process:Efficient Handling of Large Data: For larger content, directly accessing Arweave is more efficient:Reference large media files (images, videos, documents) without storing them in your processWork with datasets too large to fit in process memoryMaintain a lightweight process that can access substantial external resourcesExternal Data for Decision-Making: Your process may need data stored on Arweave to make informed decisions. For example:Reading token price data stored by an oracleAccessing verified identity informationRetrieving voting records or governance dataDynamic Loading of Features: Rather than including all functionality in your initial process code:Load modules or plugins from Arweave as neededUpdate configuration without redeploying your entire processImplement upgradable components with new versions stored on ArweaveThis approach allows you to create more sophisticated applications that leverage Arweave's permanent storage while maintaining efficient process execution in the ao environment.When another process Assigns a transaction to this process, you can also use handlers to process the data asynchronously.","estimatedWords":1434,"lastModified":"2025-10-20T12:11:01.869Z","breadcrumbs":["references","data"],"siteKey":"ao","siteName":"AO Cookbook","depth":4,"crawledAt":"2025-10-20T12:11:01.869Z"},{"url":"https://cookbook_ao.arweave.net/references/cron.html","title":"Cron Messages","content":"Cron Messages ​ao has the ability to generate messages on a specified interval, this interval could be seconds, minutes, hours, or blocks. These messages automatically get evaluated by a monitoring process to inform the Process to evaluate these messages over time. The result is a real-time Process that can communicate with the full ao network or oracles in the outside network.Setting up cron in a process ​The easiest way to create these cron messages is by spawning a new process in the aos console and defining the time interval.shaos [myProcess] --cron 5-minutesWhen spawning a new process, you can pass a cron argument in your command line followed by the interval you would like the cron to tick. By default, cron messages are lazily evaluated, meaning they will not be evaluated until the next scheduled message. To initiate these scheduled cron messages, call .monitor in aos - this kicks off a worker process on the mu that triggers the cron messages from the cu. Your Process will then receive cron messages every x-interval.lua.monitorIf you wish to stop triggering the cron messages simply call .unmonitor and this will stop the triggering process, but the next time you send a message, the generated cron messages will still get created and processed.Handling cron messages ​Every cron message has an Action tag with the value Cron. Handlers can be defined to perform specific tasks autonomously, each time a cron message is received.luaHandlers.add( \"CronTick\", -- Handler name Handlers.utils.hasMatchingTag(\"Action\", \"Cron\"), -- Handler pattern to identify cron message function () -- Handler task to execute on cron message -- Do something end )Cron messages are a powerful utility that can be used to create \"autonomous agents\" with expansive capabilities.","estimatedWords":281,"lastModified":"2025-10-20T12:11:02.429Z","breadcrumbs":["references","cron"],"siteKey":"ao","siteName":"AO Cookbook","depth":4,"crawledAt":"2025-10-20T12:11:02.429Z"},{"url":"https://cookbook_ao.arweave.net/references/editor-setup.html","title":"Editor setup","content":"Editor setup ​Remembering all the built in ao functions and utilities can sometimes be hard. To enhance your developer experience, it is recommended to install the Lua Language Server extension into your favorite text editor and add the ao addon. It supports all built in aos modules and globals.VS Code ​Install the sumneko.lua extension:Search for \"Lua\" by sumneko in the extension marketplaceDownload and install the extensionOpen the VS Code command palette with Shift + Command + P (Mac) / Ctrl + Shift + P (Windows/Linux) and run the following command:> Lua: Open Addon ManagerIn the Addon Manager, search for \"ao\", it should be the first result. Click \"Enable\" and enjoy autocomplete!Other editors ​Verify that your editor supports the language server protocolInstall Lua Language Server by following the instructions at luals.github.ioInstall the \"ao\" addon to the language serverBetterIDEa ​BetterIDEa is a custom web based IDE for developing on ao.It offers a built in Lua language server with ao definitions, so you don't need to install anything. Just open the IDE and start coding!Features include:Code completionCell based notebook ui for rapid developmentEasy process managementMarkdown and Latex cell supportShare projects with anyone through ao processesTight integration with ao package managerRead detailed information about the various features and integrations of the IDE in the documentation.","estimatedWords":211,"lastModified":"2025-10-20T12:11:03.230Z","breadcrumbs":["references","editor setup"],"siteKey":"ao","siteName":"AO Cookbook","depth":4,"crawledAt":"2025-10-20T12:11:03.230Z"},{"url":"https://cookbook_ao.arweave.net/references/betteridea/index.html","title":"BetterIDEa","content":"BetterIDEa ​BetterIDEa is a custom web based IDE for developing on ao.It offers a built in Lua language server with ao definitions, so you don't need to install anything. Just open the IDE and start coding!Features include:Code completionCell based notebook ui for rapid developmentEasy process managementMarkdown and Latex cell supportShare projects with anyone through ao processesTight integration with ao package managerRead detailed information about the various features and integrations of the IDE in the documentation.","estimatedWords":75,"lastModified":"2025-10-20T12:11:03.822Z","breadcrumbs":["references","betteridea","index"],"siteKey":"ao","siteName":"AO Cookbook","depth":4,"crawledAt":"2025-10-20T12:11:03.822Z"},{"url":"https://cookbook_ao.arweave.net/references/deprecated/index.html","title":"Deprecated Features","content":"Deprecated Features ​This section contains documentation for deprecated features and APIs in the AO ecosystem.Coming Soon ​After October 10, 2025, this section will include documentation for features that have been deprecated, including migration guides and alternatives.Current Deprecations ​For information about features being deprecated, please see:Calling DryRun - Includes urgent deprecation notice for Legacynet dry run supportMigration Support ​If you need help migrating away from deprecated features:Review the Migrating to HyperBEAM guideJoin the Discord community for support","estimatedWords":76,"lastModified":"2025-10-20T12:11:04.548Z","breadcrumbs":["references","deprecated","index"],"siteKey":"ao","siteName":"AO Cookbook","depth":4,"crawledAt":"2025-10-20T12:11:04.548Z"},{"url":"https://cookbook_ao.arweave.net/references/community.html","title":"Community Resources","content":"Community Resources ​This page provides a comprehensive list of community resources, tools, guides, and links for the AO ecosystem.Core Resources ​Autonomous FinanceAutonomous Finance is a dedicated research and technology entity, focusing on the intricacies of financial infrastructure within the ao network.BetterIdeaBuild faster, smarter, and more efficiently with BetterIDEa, the ultimate native web IDE for AO development0rbit0rbit provides any data from the web to an ao process by utilizing the power of ao, and 0rbit nodes. The user sends a message to the 0rbit ao, 0rbit nodes fetches the data and the user process receives the data.ArweaveHubA community platform for the Arweave ecosystem featuring events, developer resources, and discovery tools.AR.IOThe first permanent cloud network built on Arweave, providing infrastructure for the permaweb with no 404s, no lost dependencies, and reliable access to applications and data through gateways, domains, and deployment tools.Developer Tools ​AO Package ManagerContributing ​Not seeing an AO Community Member or resource? Create an issue or submit a pull request to add it to this page: https://github.com/permaweb/ao-cookbook","estimatedWords":167,"lastModified":"2025-10-20T12:11:05.444Z","breadcrumbs":["references","community"],"siteKey":"ao","siteName":"AO Cookbook","depth":4,"crawledAt":"2025-10-20T12:11:05.444Z"},{"url":"https://cookbook_ao.arweave.net/releasenotes/index.html","title":"Release Notes","content":"Release Notes ​This section provides detailed information about updates, new features, bug fixes, and changes in each release of AO and its related tools. Release notes are essential for understanding what's changed between versions and how these changes might affect your projects.AOS Releases ​AOS is the operating system built on top of the AO computer. These release notes document changes and improvements to AOS:AOS v2.0.9 - Deduplication feature reverted and Hyper AOS integration with forward.computerAOS v2.0.8 - SDK mode for headless execution, token blueprint updates, and hyperbeam state-cache supportAOS v2.0.7 - HyperBEAM support with state synchronization and API differences from standard AOSAOS 2.0.2 - Improved spawn process times and various bug fixesAOS 2.0.1 - Details about patch updates and fixes in the 2.0.1 releaseAOS 2.0.0 - Major release information, including new features and significant changes","estimatedWords":135,"lastModified":"2025-10-20T12:11:06.852Z","breadcrumbs":["releasenotes","index"],"siteKey":"ao","siteName":"AO Cookbook","depth":4,"crawledAt":"2025-10-20T12:11:06.852Z"},{"url":"https://cookbook_ao.arweave.net/guides/aos/modules/index.html","title":"Modules","content":"Modules ​Documentation on all the built-in modules for aos.Available Modules ​JSONaocryptoBase64prettyUtils","estimatedWords":11,"lastModified":"2025-10-20T12:11:08.013Z","breadcrumbs":["guides","aos","modules","index"],"siteKey":"ao","siteName":"AO Cookbook","depth":4,"crawledAt":"2025-10-20T12:11:08.013Z"},{"url":"https://cookbook_ao.arweave.net/welcome/index.html","title":"Welcome to ao","content":"Welcome to ao ​AO is a decentralized compute system where countless parallel processes interact within a single, cohesive environment. Each process operates independently, yet they are seamlessly connected through a native message-passing layer, similar to how websites form the World Wide Web.AO + AOS: The rocket and your rocket fuel. ​Typically when you use AO, you will interact with it through its operating system: AOS.AOS is an abstraction layer that runs in your processes, making it easy to use the full functionality of the AO computer. In this cookbook, you will learn everything you need to know about getting started with the AO computer using AOS.Mainnet and Legacynet ​AO Mainnet launched on February 8, 2025, paving the way for a decentralized, open-access supercomputer directly connected to the internet with HyperBEAM.AO Legacynet launched on February 27, 2024, providing a fee-free environment for early adopters to experiment with AO’s hyper-parallel architecture.Legacynet Documentation ​These tutorials explore AO Legacynet, covering everything from building chatrooms to developing autonomous, decentralized bots. It’s a great starting point for experimenting with AO’s hyper-parallel architecture.Legacynet TutorialsFurther References ​GuidesConceptsReferences","estimatedWords":179,"lastModified":"2025-10-20T12:11:10.123Z","breadcrumbs":["welcome","index"],"siteKey":"ao","siteName":"AO Cookbook","depth":4,"crawledAt":"2025-10-20T12:11:10.123Z"}],"lastCrawled":"2025-10-20T12:15:30.422Z","stats":{"totalPages":92,"averageWords":461,"duration":67114,"requestCount":104,"averageResponseTime":618.25,"pagesPerSecond":1.3708019191226868}},"ario":{"name":"AR-IO Network","baseUrl":"https://docs.ar.io","pages":[{"url":"https://docs.ar.io/sdks/turbo-sdk","title":"Turbo SDK","content":"Turbo SDKCopy MarkdownOpenFor AI and LLM users: Access the complete Turbo SDK documentation in plain text format at llm.txt for easy consumption by AI agents and language models. Turbo SDK Please refer to the source code for SDK details.How is this guide?GoodBadPaginationTypeScript/JavaScript SDK for interacting with the AR.IO ecosystemTurboFactorySDK for interacting with Turbo, a fast and efficient data upload service for Arweave","estimatedWords":62,"lastModified":"2025-10-20T12:11:15.598Z","breadcrumbs":["sdks","turbo sdk"],"siteKey":"ario","siteName":"AR-IO Network","depth":2,"crawledAt":"2025-10-20T12:11:15.598Z"},{"url":"https://docs.ar.io/apis/turbo","title":"Turbo APIs","content":"Turbo APIsCopy MarkdownOpenTurbo provides high-performance upload and payment services for the Arweave network, offering fast, reliable data uploads with instant confirmation and transparent pricing. Services Upload ServiceFast, reliable data uploads to Arweave with instant confirmation and metadata managementPayment ServiceTransparent pricing, payment processing, and credit management for Turbo uploads Upload Service The Turbo Upload Service provides high-performance data uploads to the Arweave network with features including: Fast Uploads - Optimized upload processing for quick data submission Instant Confirmation - Immediate upload confirmations and transaction IDs Metadata Management - Comprehensive data tagging and organization Account Management - User account and upload history tracking Service Information - Real-time service status and capabilities Key endpoints include account management, upload processing, pricing information, and transaction data retrieval. Payment Service The Turbo Payment Service handles all financial aspects of data uploads with transparent and flexible payment options: Transparent Pricing - Clear, upfront costs for all upload operations Multiple Currencies - Support for various payment methods and currencies Credit Management - Prepaid credits and balance tracking Payment Processing - Secure payment handling and transaction management Approval Workflows - Payment authorization and confirmation flows Key endpoints include balance management, payment processing, pricing calculations, and credit redemption. Getting Started with Turbo Choose your service - Upload for data submission, Payment for financial operations Review the APIs documentation - Detailed endpoint specifications and examples Test with sample data - Try uploads and payment flows with test data Integrate into your application - Implement the APIs in your workflow Use the Turbo SDK For a more convenient integration experience, consider using the Turbo SDK instead of direct API calls: Interact with Turbo via the SDKUse the Turbo SDK for simplified integration with built-in error handling, retries, and TypeScript support The SDK provides a higher-level interface with built-in error handling, automatic retries, and full TypeScript support, making it easier to integrate Turbo services into your applications.How is this guide?GoodBadFarcaster FramesPrevious PageAccount GETNext Page","estimatedWords":321,"lastModified":"2025-10-20T12:11:16.548Z","breadcrumbs":["apis","turbo"],"siteKey":"ario","siteName":"AR-IO Network","depth":2,"crawledAt":"2025-10-20T12:11:16.548Z"},{"url":"https://docs.ar.io/apis/ar-io-node","title":"ARIO Gateway APIs","content":"AR.IO Gateway APIsCopy MarkdownOpenThe AR.IO Gateway is the core software for the AR.IO network, serving the essential responsibility of gateways for accessing, caching, and querying data stored on Arweave. It provides a robust, decentralized infrastructure for interacting with the permanent web. Core Responsibilities The AR.IO Gateway handles fundamental operations for the Arweave ecosystem: Data Access - Retrieve transaction data, files, and metadata from Arweave Caching - Intelligent caching strategies for improved performance and availability Data Querying - Powerful search and indexing capabilities for Arweave data ArNS Resolution - Resolve human-readable names to Arweave transaction IDs Network Management - Coordinate with other gateways in the AR.IO network Advanced Features Beyond basic gateway functionality, AR.IO Gateway includes sophisticated capabilities: Parquet Generation - Convert Arweave data into optimized Parquet format for analytics Data Verification - Cryptographic verification of data integrity and authenticity Index Querying - Advanced search and filtering across Arweave datasets Farcaster Frames - Support for Farcaster protocol integration Admin Controls - Comprehensive gateway management and configuration APIs Categories Data AccessRetrieve transaction data, files, and metadata from ArweaveArNS ResolutionResolve human-readable names to Arweave transaction IDsTransactions & BlocksAccess transaction details, block information, and network dataIndex QueryingAdvanced search and filtering capabilities across Arweave dataNetwork & GatewayGateway status, network information, and peer coordinationAdmin & ManagementGateway configuration, pricing, and administrative controls Get Involved with AR.IO Gateways Run a GatewayJoin the AR.IO network by operating your own gateway and earn rewardsLeverage Gateways with WayfinderUse Wayfinder SDK to access data through the distributed gateway networkJoin the NetworkLearn about the AR.IO network and how to participate in the ecosystem Getting Started Explore the APIs endpoints - Review the comprehensive APIs documentation Test with sample requests - Try out the interactive examples Choose your integration approach - Direct APIs calls or SDK usage Consider running a gateway - Contribute to the network infrastructure The AR.IO Gateway APIs provide the foundation for building robust, decentralized applications on Arweave with reliable data access and advanced querying capabilities.How is this guide?GoodBadAPIs ReferenceREST APIs for interacting with AR.IO services and infrastructureDataNext Page","estimatedWords":338,"lastModified":"2025-10-20T12:11:17.491Z","breadcrumbs":["apis","ar io node"],"siteKey":"ario","siteName":"AR-IO Network","depth":2,"crawledAt":"2025-10-20T12:11:17.491Z"},{"url":"https://docs.ar.io/glossary","title":"Glossary","content":"GlossaryCopy MarkdownOpenAO Computer AO (Actor Oriented) is a hyper-parallel computing platform built on Arweave that enables decentralized applications to run with unlimited computational capacity. AO provides the compute layer for the AR.IO Network's smart contracts and token operations. Public Key A cryptographic key that can be shared publicly and is used to verify digital signatures or encrypt data. In the AR.IO context, public keys are used to identify wallet addresses and verify transactions. Native Address An address format that uses the raw public key bytes directly, without additional encoding or transformation. This is the most basic form of an Arweave address. Normalized Address A standardized address format that ensures consistent representation across different systems and contexts. Normalized addresses help prevent issues with address matching and lookup operations. Optimistic Indexing A data indexing strategy where new data is immediately made available for queries while verification processes continue in the background. This approach improves performance while maintaining data integrity through eventual consistency.How is this guide?GoodBadIntroductionBuild on the permanent web with AR.IO's decentralized gateway protocol for Arweave","estimatedWords":174,"lastModified":"2025-10-20T12:11:18.594Z","breadcrumbs":["glossary"],"siteKey":"ario","siteName":"AR-IO Network","depth":1,"crawledAt":"2025-10-20T12:11:18.594Z"},{"url":"https://docs.ar.io/build/guides/arns-marketplace","title":"ArNS Marketplace","content":"BuildGuidesArNS MarketplaceCopy MarkdownOpenArNS tokens have the potential to be traded and sold in decentralized marketplaces. ANTs (Arweave Name Tokens) are both smart contracts and transferable tokens, making them valuable digital assets that could be bought, sold, and traded. However, no established marketplace has yet emerged as the preferred platform for ArNS trading. Current State of ArNS Trading No established marketplace exists yet - While ANTs are technically transferable tokens, there is currently no widely adopted marketplace specifically for ArNS trading. Direct transfers are possible - You can transfer ANTs directly between wallets, but this requires technical knowledge and direct coordination between buyer and seller. Future potential - As the ArNS ecosystem grows, dedicated marketplaces may emerge to facilitate easier trading of ArNS tokens and domains. What Are ANTs? Arweave Name Tokens (ANTs) are: Smart contracts - Define the rules and functionality of your domain Transferable tokens - Can be bought, sold, and traded Digital assets - Represent ownership of ArNS domains Permanent - Stored on Arweave forever How Trading Could Work 1. Token Ownership When you own an ANT: You control the domain name You can update where it points You can transfer ownership You could potentially sell it to others 2. Potential Marketplace Dynamics Possible trading mechanisms: Direct transfers - Send tokens directly to another wallet Marketplace platforms - Use dedicated trading platforms (when available) Auction systems - Bid on available domains (when implemented) Fixed price sales - Set a price and wait for buyers (when supported) 3. Name Characteristics What makes ANTs desirable: Domain length - Shorter names are more memorable Memorability - Easy-to-remember names are more useful Brand potential - Names that could become recognizable Uniqueness - Creative and distinctive names Content attached - Domains with established content Potential Trading Examples Popular Domain Types Short names: ar:","estimatedWords":300,"lastModified":"2025-10-20T12:11:19.373Z","breadcrumbs":["build","guides","arns marketplace"],"siteKey":"ario","siteName":"AR-IO Network","depth":3,"crawledAt":"2025-10-20T12:11:19.373Z"},{"url":"https://docs.ar.io/build/guides/arns-undernames-versioning","title":"ArNS Undernames for Permasite Versioning","content":"GuidesArNS Undernames for Permasite VersioningCopy MarkdownOpenUse ArNS undernames to organize and version your permanent website components. Undernames allow you to create sub-domains under your main ArNS name, making it easy to manage different versions, pages, and assets. What Are Undernames? Undernames are sub-domains under your main ArNS name that can point to different Arweave transactions. They provide a structured way to organize your permanent website content. Example structure: yourname.arweave.dev - Main site v1_yourname.arweave.dev - Version 1 v2_yourname.arweave.dev - Version 2 api_yourname.arweave.dev - API endpoints docs_yourname.arweave.dev - Documentation Real-World Example: ArDrive The ArDrive website uses undernames to organize different components and versions: { \"@\": { \"priority\": 0, \"ttlSeconds\": 3600, \"transactionId\": \"Vrf5_MrC1R-6rAk7o_E52DwOsKhyJmkSUqh0h5q4mDQ\", \"index\": 0 }, \"dapp\": { \"ttlSeconds\": 3600, \"transactionId\": \"1ubf6cW8T5dYN3COApn8Yii4bA0HKoGeid-z2IjelTo\", \"index\": 1 }, \"home\": { \"ttlSeconds\": 900, \"transactionId\": \"V9rQR06L1w9eLBHh2lY7o4uaDO6OqBI8j7TM_qjmNfE\", \"index\": 2 }, \"v1_home\": { \"ttlSeconds\": 900, \"transactionId\": \"YzD_Pm5VAfYpMD3zQCgMUcKKuleGhEH7axlrnrDCKBo\", \"index\": 9 }, \"v2_home\": { \"ttlSeconds\": 900, \"transactionId\": \"nOXJjj_vk0Dc1yCgdWD8kti_1iHruGzLQLNNBHVpN0Y\", \"index\": 10 }, \"v3_home\": { \"ttlSeconds\": 900, \"transactionId\": \"YvGRDf0h2F7LCaGPvdH19m5lqbag5DGRnw607ZJ1oUg\", \"index\": 11 } } This structure provides: @ - Main site (ardrive.arweave.dev) dapp - Application interface (dapp_ardrive.arweave.dev) home - Homepage (home_ardrive.arweave.dev) v1_home - Version 1 homepage (v1_home_ardrive.arweave.dev) v2_home - Version 2 homepage (v2_home_ardrive.arweave.dev) v3_home - Version 3 homepage (v3_home_ardrive.arweave.dev) Use Cases for Undernames 1. Website Versioning Maintain multiple versions: v1_yourname.arweave.dev - Previous version v2_yourname.arweave.dev - Current version beta_yourname.arweave.dev - Beta testing staging_yourname.arweave.dev - Staging environment 2. Component Organization Separate different parts: api_yourname.arweave.dev - API endpoints docs_yourname.arweave.dev - Documentation assets_yourname.arweave.dev - Static assets blog_yourname.arweave.dev - Blog content 3. Content Management Organize by content type: home_yourname.arweave.dev - Homepage about_yourname.arweave.dev - About page contact_yourname.arweave.dev - Contact page privacy_yourname.arweave.dev - Privacy policy Benefits of Undername Versioning Easy access to versions: Users can access any version directly via URL No need to remember transaction IDs Clear versioning structure Permanent version history: All versions remain accessible forever Historical record of your website evolution Easy rollback to previous versions Organized content: Logical structure for different components Easy to manage and update Clear separation of concerns Transferable with ANT: Undernames transfer with the main ArNS name Maintain ownership of all versions Sell or transfer entire website structure How to Set Up Undernames 1. Register your main ArNS name: Choose your primary name (e.g., myapp) Register through ArNS App 2. Create undernames: Use the ANT interface to add undernames Point each undername to different transaction IDs Set appropriate TTL values 3. Deploy different versions: Upload each version to Arweave Get transaction IDs for each version Update undername records Example Implementation Deploy version 1: # Deploy to main site npx permaweb-deploy --arns-name myapp --deploy-folder ./v1-build # Deploy to v1 undername npx permaweb-deploy --arns-name myapp --undername v1 --deploy-folder ./v1-build Deploy version 2: # Deploy to main site npx permaweb-deploy --arns-name myapp --deploy-folder ./v2-build # Deploy to v2 undername npx permaweb-deploy --arns-name myapp --undername v2 --deploy-folder ./v2-build Access different versions: myapp.arweave.dev - Current version v1_myapp.arweave.dev - Version 1 v2_myapp.arweave.dev - Version 2 Ready to Version Your Site? Want to learn more? Check out ArNS Primary Names for identity management. Need deployment help? See Hosting Decentralized Websites for setup guides. Want to trade domains? Explore ArNS Marketplace for buying and selling.How is this guide?GoodBadWorking With Primary NamesCreate web3 identity using ArNS names that resolve to wallet addressesCrossmint NFT Minting AppBuild a decentralized NFT minting app with Arweave and Crossmint","estimatedWords":541,"lastModified":"2025-10-20T12:11:20.411Z","breadcrumbs":["build","guides","arns undernames versioning"],"siteKey":"ario","siteName":"AR-IO Network","depth":3,"crawledAt":"2025-10-20T12:11:20.411Z"},{"url":"https://docs.ar.io/build/guides/arns-primary-names","title":"Working With Primary Names","content":"GuidesWorking With Primary NamesCopy MarkdownOpenCreate web3 identity using ArNS names. Primary names allow you to use human-readable names as your identity in the Arweave ecosystem, making it easy for others to find and interact with you. What Are Primary Names? Primary names are ArNS names used as identity that: Resolve to wallet addresses - Link human-readable names to wallet addresses Provide web3 identity - Give users friendly names for their Arweave identity Are bidirectional - Can resolve from name to address or address to name Require ownership - Only the owner of an ArNS name can set it as their primary name Enable secure verification - Ownership requirement ensures identity authenticity Work across gateways - Accessible from any AR.IO gateway How It Works 1. Identity Registration Register a primary name: Choose a unique name (e.g., jonniesparkles) Pay the registration fee Link the name to your wallet address Use as your web3 identity 2. Bidirectional Resolution Name to address resolution: jonniesparkles → OU48aJtcq3KjsEqSUWDVpynh1xP2Y1VI-bwiSukAktU Others can find your wallet using your name Use in dApps and applications Address to name resolution: OU48aJtcq3KjsEqSUWDVpynh1xP2Y1VI-bwiSukAktU → jonniesparkles Find the name associated with any wallet Verify identity in transactions 3. Application Integration Use in supported apps: Send tokens to \"jonniesparkles\" instead of copying long wallet addresses Display friendly names as usernames when connecting wallets Apps resolve names to wallet addresses using the AR.IO SDK Seamless user experience with human-readable identifiers Basic Integration Using the AR.IO SDK Get a primary name by address: import { ARIO } from \"@ar-io/sdk\";","estimatedWords":252,"lastModified":"2025-10-20T12:11:21.594Z","breadcrumbs":["build","guides","arns primary names"],"siteKey":"ario","siteName":"AR-IO Network","depth":3,"crawledAt":"2025-10-20T12:11:21.594Z"},{"url":"https://docs.ar.io/build/guides/hosting-decentralized-websites","title":"Hosting Decentralized Websites","content":"GuidesHosting Decentralized WebsitesCopy MarkdownOpenCreate permanent websites that can't be censored, taken down, or modified after deployment. Host your content on Arweave and serve it through AR.IO gateways for a truly decentralized web experience. What Makes It Different? Traditional websites: Hosted on centralized servers Can be taken down or censored Require ongoing hosting costs Single point of failure Decentralized websites: Stored permanently on Arweave Censorship-resistant Pay once, host forever Distributed across the network How It Works 1. Arweave Manifests Arweave manifests are JSON files that define how your website's files are organized and linked together. They enable: Friendly URLs - Access files with readable paths instead of transaction IDs Relative linking - Use ./style.css instead of full transaction IDs Fallback pages - Handle 404 errors gracefully File organization - Structure your website like a traditional site Example manifest: { \"manifest\": \"arweave/paths\", \"version\": \"0.2.0\", \"index\": { \"path\": \"index.html\" }, \"fallback\": { \"id\": \"404-page-transaction-id\" }, \"paths\": { \"index.html\": { \"id\": \"main-page-transaction-id\" }, \"style.css\": { \"id\": \"css-transaction-id\" }, \"script.js\": { \"id\": \"js-transaction-id\" } } } 2. Deployment Tools Permaweb Deploy - CLI deployment tool that: Uploads your build folder to Arweave using Turbo Creates a manifest automatically Updates your ArNS domain Integrates with GitHub Actions Requires --deploy-folder and --arns-name parameters Arlink - User-friendly web interface for: Quick website uploads through the browser Manifest generation ArNS integration ArDrive Web - User-friendly interface for: File management Website building Deployment workflows 3. ArNS Integration Primary Names - Decentralized domain names that: Point to your website's manifest Provide human-readable URLs Can be updated to point to new versions Are owned and controlled by you Quick Start 1. Build your website - Create a static website with HTML, CSS, and JavaScript 2. Choose a deployment tool: # Using permaweb-deploy (CLI tool) npm install permaweb-deploy --save-dev npx permaweb-deploy --deploy-folder ./build --arns-name your-domain # Using arlink (web interface) # Visit the arlink website and upload through the browser 3. Get an ArNS domain - Register a primary name to point to your website 4. Access your site - Visit https:","estimatedWords":339,"lastModified":"2025-10-20T12:11:22.657Z","breadcrumbs":["build","guides","hosting decentralized websites"],"siteKey":"ario","siteName":"AR-IO Network","depth":3,"crawledAt":"2025-10-20T12:11:22.657Z"},{"url":"https://docs.ar.io/sdks/ar-io-sdk","title":"ARIO SDK","content":"AR.IO SDKCopy MarkdownOpenFor AI and LLM users: Access the complete AR.IO SDK documentation in plain text format at llm.txt for easy consumption by AI agents and language models. AR.IO SDK Please refer to the source code for SDK details.How is this guide?GoodBadSupportJavaScript/TypeScript SDK for interacting with ArDriveGeneralTypeScript/JavaScript SDK for interacting with the AR.IO ecosystem","estimatedWords":54,"lastModified":"2025-10-20T12:11:23.597Z","breadcrumbs":["sdks","ar io sdk"],"siteKey":"ario","siteName":"AR-IO Network","depth":2,"crawledAt":"2025-10-20T12:11:23.597Z"},{"url":"https://docs.ar.io/sdks/wayfinder","title":"Wayfinder SDKs","content":"Wayfinder SDK'sCopy MarkdownOpenFor AI and LLM users: Access the complete Wayfinder SDK's documentation in plain text format at llm.txt for easy consumption by AI agents and language models. Wayfinder SDK's Please refer to the source code for SDK details.How is this guide?GoodBadTurbo Credit SharingSDK for interacting with Turbo, a fast and efficient data upload service for ArweavePackagesDecentralized access to Arweave data with built-in verification and gateway routing","estimatedWords":67,"lastModified":"2025-10-20T12:11:24.454Z","breadcrumbs":["sdks","wayfinder"],"siteKey":"ario","siteName":"AR-IO Network","depth":2,"crawledAt":"2025-10-20T12:11:24.454Z"},{"url":"https://docs.ar.io/sdks/turbo-sdk/events","title":"Events","content":"Turbo SDKEventsCopy MarkdownOpenThe SDK provides events for tracking the state signing and uploading data to Turbo. You can listen to these events by providing a callback function to the events parameter of the upload, uploadFile, and uploadSignedDataItem methods. onProgress - emitted when the overall progress changes (includes both upload and signing). Each event consists of the total bytes, processed bytes, and the step (upload or signing) onError - emitted when the overall upload or signing fails (includes both upload and signing) onSuccess - emitted when the overall upload or signing succeeds (includes both upload and signing) - this is the last event emitted for the upload or signing process onSigningProgress - emitted when the signing progress changes. onSigningError - emitted when the signing fails. onSigningSuccess - emitted when the signing succeeds onUploadProgress - emitted when the upload progress changes onUploadError - emitted when the upload fails onUploadSuccess - emitted when the upload succeeds console.log('Overall progress:', { totalBytes, processedBytes, step, percentComplete: percentComplete.toFixed(2) + '%',","estimatedWords":163,"lastModified":"2025-10-20T12:11:25.513Z","breadcrumbs":["sdks","turbo sdk","events"],"siteKey":"ario","siteName":"AR-IO Network","depth":3,"crawledAt":"2025-10-20T12:11:25.513Z"},{"url":"https://docs.ar.io/build/advanced","title":"Advanced","content":"AdvancedCopy MarkdownOpenOverview Explore advanced topics and specialized guides for building on Arweave and AR.IO. These resources are designed for developers and operators who need deeper technical knowledge and advanced configuration options. Advanced Topics ArFS ProtocolAdvanced ArFS documentation for structured data storageNormalized AddressesUnderstanding wallet address normalization across different networksBrowser SandboxingSecurity mechanisms in AR.IO gateways Key topics: - Same-origin policyEthAReum ProtocolGenerate Arweave wallets from Ethereum or Solana wallets Ready to Go Advanced? New to Arweave? Start with our Getting Started guide to understand the basics. Building dApps? Check out ArFS Protocol for structured data storage solutions.How is this guide?GoodBadUsing Turbo SDK with ViteConfigure Turbo SDK in a Vite application with proper polyfills for client-side usageArFS ProtocolA decentralized file system on Arweave for structured data storage and retrieval","estimatedWords":125,"lastModified":"2025-10-20T12:11:27.807Z","breadcrumbs":["build","advanced"],"siteKey":"ario","siteName":"AR-IO Network","depth":2,"crawledAt":"2025-10-20T12:11:27.807Z"},{"url":"https://docs.ar.io/build/extensions","title":"Extensions  Sidecars","content":"Extensions & SidecarsCopy MarkdownOpenWhat are Extensions? Extensions are additional scripts and tools you can run alongside your gateway to expand its capabilities or enhance the operator experience. The full list of community extensions can be found at gateways.ar.io/#/extensions. What are Sidecars? Sidecars are dockerized services that add additional functionality, APIs, and services to AR.IO gateways. They run as separate containers alongside your gateway, providing specialized capabilities. Getting Started with Team-Supported Sidecars The following sidecars are developed and maintained by the AR.IO team, designed to run alongside your gateway as separate containers. Grafana Monitoring SidecarVisualize gateway metrics with comprehensive dashboards and performance monitoring.ClickHouse & ParquetImprove query performance for large datasets using columnar storage and analytical optimization.Bundler SidecarAccept and process ANS-104 data item uploads with multiple payment methods and access control.AO Compute Unit SidecarExecute AO processes locally with WASM module support and state management. Ready to enhance your gateway? Click any sidecar above to get started with detailed setup guides. Explore More Monitor your gateway with GrafanaSet up comprehensive monitoring and analytics for your gateway infrastructurePerformance OptimizationOptimize your gateway for large datasets and high-performance queriesGateway OperationsLearn advanced gateway management, troubleshooting, and configurationDeveloper SDKsIntegrate AR.IO services into your applications with our SDKsHow is this guide?GoodBadTroubleshootingComprehensive troubleshooting guide and FAQ for AR.IO Gateway operators, including common issues, failed epoch guidance, and frequently asked questions.GrafanaComprehensive guide to deploying and configuring Grafana for AR.IO Gateway monitoring and analytics","estimatedWords":232,"lastModified":"2025-10-20T12:11:29.043Z","breadcrumbs":["build","extensions"],"siteKey":"ario","siteName":"AR-IO Network","depth":2,"crawledAt":"2025-10-20T12:11:29.043Z"},{"url":"https://docs.ar.io/build/access","title":"Access Data","content":"Access DataCopy MarkdownOpenOnce data is stored on Arweave, it's permanently available. Here's how to access it efficiently for your applications. Access Methods Different methods serve different needs. Each provides unique capabilities for retrieving data from Arweave. Find DataSearch and discover data on Arweave Query by tags and metadata Filter by app, owner, timestamp Get transaction IDs for fetchingFetch DataRetrieve data bytes from Arweave REST API endpoints GET arweave.net/[txId] Returns raw data/filesArNS NamesAssign names to data and apps Create names like ardrive.ar.io Point to any Arweave data Update targets as needed Common Access Patterns Finding Data Search for data by tags, owner, or timestamp Discover content from specific applications Get transaction IDs for data retrieval Fetching Data Retrieve the actual files/data using transaction IDs Access data via REST API: GET arweave.net/[txId] Stream large files efficiently Naming with ArNS Register memorable names for your apps and data Create permanent links like ardrive.ar.io Update where names point without changing the URL Quick Example: Find and Fetch Find DataUse GraphQL to search for data and get transaction IDs:query { transactions( tags: [{ name: \"App-Name\", values: [\"ArDrive\"] }] first: 1 ) { edges { node { id } } } }Fetch DataUse the transaction ID to retrieve the actual data:curl https:","estimatedWords":206,"lastModified":"2025-10-20T12:11:30.030Z","breadcrumbs":["build","access"],"siteKey":"ario","siteName":"AR-IO Network","depth":2,"crawledAt":"2025-10-20T12:11:30.030Z"},{"url":"https://docs.ar.io/build/guides","title":"Guides","content":"GuidesCopy MarkdownOpenExplore real-world applications and use cases for Arweave and AR.IO infrastructure. These examples show what's possible with permanent data storage and decentralized web services. What You Can Build Arweave and AR.IO enable: Decentralized websites - Host permanent, censorship-resistant web content ArNS domains - Create and manage decentralized domain names Data marketplaces - Trade and sell digital assets and data Permanent applications - Deploy apps that can't be taken down And much more - The permanent web is only limited by your imagination Getting Started Hosting Decentralized WebsitesBuild permanent websites that can't be censored or taken downKey topics: Arweave manifests for file routing Permaweb deployment tools ArNS domain integration ArNS Primary NamesCreate and manage decentralized domain namesKey topics: Primary name registration Domain management Integration with applications ArNS Undernames for VersioningVersion and organize your permanent website contentKey topics: Undername management Website versioning Component organization ArNS MarketplaceTrade and sell ArNS tokens and digital assetsKey topics: Arweave Name Token (ANT) trading Marketplace dynamics Asset ownership Deploy a dApp with ArDrive WebDeploy dApps easily using the ArDrive web interfaceKey topics: ArDrive web deployment Manifest creation ArNS name assignment Version management Crossmint NFT Minting AppBuild a decentralized NFT minting app with Arweave and CrossmintKey topics: Permanent NFT storage on Arweave Crossmint API integration Payment processing Decentralized deployment Why Use Arweave? Permanent storage - Data stored on Arweave is permanent and cannot be deleted Decentralized - No single point of failure or control Cost-effective - Pay once, store forever Censorship-resistant - Content cannot be taken down by authorities Ready to Build? Start with websites - Learn how to host decentralized websites with Hosting Decentralized Websites. Want domains? Explore ArNS Primary Names for decentralized domain management. Interested in trading? Check out ArNS Marketplace for digital asset trading.How is this guide?GoodBadAO Compute Unit (CU)Steps for deploying an AO Compute Unit (CU) sidecar alongside your AR.IO Gateway.Storing DePIN Data on Arweave Using TurboComplete guide to storing and accessing DePIN network data permanently on Arweave using Turbo and AR.IO Network","estimatedWords":330,"lastModified":"2025-10-20T12:11:31.128Z","breadcrumbs":["build","guides"],"siteKey":"ario","siteName":"AR-IO Network","depth":2,"crawledAt":"2025-10-20T12:11:31.128Z"},{"url":"https://docs.ar.io/build/run-a-gateway","title":"Run a Gateway","content":"Run a GatewayCopy MarkdownOpenJoin the decentralized network that powers permanent data access. Run your own AR.IO Gateway to support the permaweb infrastructure and earn rewards. Gateway Options Choose the deployment approach that fits your needs - from local testing to production infrastructure. Production Gateway Earn RewardsJoin the AR.IO Network and earn ARIO tokens Earn ARIO token rewards Serve Wayfinder traffic Cache and serve Arweave dataLocal GatewayPerfect for development and testing Quick Docker setup Test gateway features No commitment requiredCustom ConfigurationOptimize for specific use cases• Data filtering options• Performance tuning• Advanced features Why Run a Gateway? Economic Benefits Earn ARIO tokens through network participation Set custom pricing for premium services Build sustainable infrastructure business Technical Advantages Full control over data access and caching Custom configuration for your applications Direct integration with your services Network Impact Support decentralized web infrastructure Increase network reliability and redundancy Enable censorship-resistant data access Quick Start in 30 Seconds Get a gateway running locally with a single command: # Prerequisites: Docker installed on your system docker run -p 4000:4000 ghcr.io/ar-io/ar-io-core:latest Test your gateway: # Fetch a transaction curl localhost:4000/4jBV3ofWh41KhuTs2pFvj-KBZWUkbrbCYlJH0vLA6LM Your gateway is now serving Arweave data! This local setup is perfect for: Testing gateway functionality Developing applications Understanding gateway operations Complete Setup GuideFull instructions for production deploymentJoin the NetworkStake tokens and earn rewards Learn Before You Build Understanding gateways helps you make informed infrastructure decisions. What Are Gateways?Core concepts and architectureGateway BenefitsEconomic and technical advantagesNetwork OverviewHow the AR.IO Network operates Ready to Deploy? Whether you're exploring gateway capabilities or ready to join the network, we have resources to help: DocumentationTechnical specs and configurationCommunity SupportGet help in our DiscordGateway DashboardMonitor network gatewaysHow is this guide?GoodBadWayfinderUse AR.IO Wayfinder for decentralized content discovery and optimized data accessInstallation & SetupGet started running AR.IO gateway using Docker - minimal setup, maximum results","estimatedWords":300,"lastModified":"2025-10-20T12:11:32.375Z","breadcrumbs":["build","run a gateway"],"siteKey":"ario","siteName":"AR-IO Network","depth":2,"crawledAt":"2025-10-20T12:11:32.375Z"},{"url":"https://docs.ar.io/build/upload","title":"Upload Data","content":"Upload DataCopy MarkdownOpenArweave enables permanent data storage with a single payment. Unlike traditional cloud storage that requires ongoing fees, your data is preserved forever. Upload Methods There are multiple ways to upload data to Arweave. Each has its own attributes and characteristics to help you decide which is best for your use case. Turbo RecommendedProduction-ready bundling service with enterprise features Credit cards, AR, ETH, SOL, MATIC Free uploads under 100 KiB Automatic retry & confirmationArDriveNo-code solution for personal and business files Drag-and-drop interface End-to-end encryption Powered by TurboDirect to ArweaveRaw protocol access for advanced use cases• AR tokens required• Manual transaction handling• Complex setup needed Why Developers Choose Turbo Cost Effective Pay per byte, not empty chunks - only pay for actual data uploaded Free uploads under 100 KiB - subsidized small file uploads No failed upload charges - automatic retry without extra costs Developer Experience TypeScript & CLI support - choose your preferred tools Simple 3-line integration - get started in minutes Comprehensive documentation - extensive guides and examples Enterprise Ready High Availability - reliable service for production apps Handles millions of uploads daily - battle-tested infrastructure used by ArDrive Open source infrastructure - fully auditable and transparent Get Started in Minutes With Turbo, uploading to Arweave is as simple as using any cloud storage API: import { TurboFactory } from \"@ardrive/turbo-sdk\";","estimatedWords":223,"lastModified":"2025-10-20T12:11:33.392Z","breadcrumbs":["build","upload"],"siteKey":"ario","siteName":"AR-IO Network","depth":2,"crawledAt":"2025-10-20T12:11:33.392Z"},{"url":"https://docs.ar.io/learn/token","title":"Token","content":"TokenCopy MarkdownOpenOverview ARIO is the multifunction AO Computer based token that powers the AR.IO Network and its suite of permanent cloud applications. Built on AO, ARIO leverages the computational power and permanence of the Arweave ecosystem to create a robust, decentralized infrastructure token. Key Features Native AO Token: ARIO is built directly on AO Computer, utilizing its decentralized compute capabilities for smart contract execution and network operations Staking-Based Infrastructure: The network operates on a staking-based incentive system where gateway operators secure infrastructure services through token commitment Multi-utility Design: ARIO serves multiple functions within the ecosystem, from network participation to governance and payments Incentive Mechanisms ARIO's design creates powerful incentives for network participants through multiple reward streams: Gateway Operator Incentives Staking Rewards: Gateway operators earn rewards for maintaining network infrastructure and providing reliable data services Performance Bonuses: Additional rewards for gateways that demonstrate high uptime and fast response times Network Growth Rewards: Operators benefit as the network scales and generates more activity Delegator Rewards Delegated Staking: Token holders can delegate their ARIO to trusted gateway operators to participate in network operations Increased Operator Stake: Delegation enhances gateway operator visibility and influence within the network Shared Risk and Rewards: Delegators participate in the risk and rewards of gateway operations Withdrawal Flexibility: Delegated tokens can be withdrawn following the same vault lock period rules Ecosystem Participation ArNS Revenue Sharing: Name registration fees flow back to network participants through the reward distribution mechanism Protocol Growth: As network usage increases, token utility and value proposition strengthen Community Incentives: Active participation in governance and ecosystem development is rewarded Staking Architecture The AR.IO Network implements a robust staking-based incentive system that ensures infrastructure reliability and network participation: Gateway Operator Requirements Gateway operators must stake a minimum amount of ARIO tokens to join the network Staking demonstrates commitment to network objectives and promotes infrastructure reliability Only gateways that pass Observation and Incentive Protocol evaluations receive rewards Staked tokens remain locked until withdrawal is initiated or vault period expires Network Quality Assurance Non-Inflationary Design: Fixed supply of 1 billion ARIO tokens with no minting mechanism Immutable Protocol: No governance control or special write access for upgrades Infrastructure Focus: Staking secures gateway infrastructure for permanent cloud services Peer Monitoring: Gateways serve as observers, testing and evaluating each other's performance Economic Incentives Gateway operators earn rewards only when they pass observation evaluations (≥50% consensus) Staking creates opportunity cost that aligns operator incentives with network health Observer gateways receive additional rewards for monitoring network quality Delegated staking allows broader community participation in successful gateway operations Built on AO Computer ARIO's foundation on AO Computer provides unique advantages: Computational Permanence All token operations and smart contracts benefit from Arweave's permanent storage Network history and token transactions are immutably recorded Computational results are verifiable and permanent Decentralized Execution Token logic runs across distributed AO processes, eliminating single points of failure Smart contract upgrades follow community governance processes Network operations scale with AO's computational capacity Ecosystem Integration Native compatibility with other AO-based applications and tokens Seamless interaction with Arweave's data storage layer Built-in interoperability with the broader permaweb ecosystem Explore the Token Token ArchitectureLearn about ARIO's technical implementation and AO Computer integrationGet the TokenDiscover where and how to acquire ARIO tokensStaking & DelegationUnderstand how to stake ARIO and delegate to gatewaysAdd to WanderLearn how to add ARIO to your Wander walletHow is this guide?GoodBadUse CasesExplore practical applications of the Wayfinder Protocol across different industries and use casesArchitectureExplore the technical architecture of the $ARIO contract and the AR.IO Network system components","estimatedWords":585,"lastModified":"2025-10-20T12:11:34.372Z","breadcrumbs":["learn","token"],"siteKey":"ario","siteName":"AR-IO Network","depth":2,"crawledAt":"2025-10-20T12:11:34.373Z"},{"url":"https://docs.ar.io/learn/wayfinder","title":"Wayfinder Protocol","content":"Wayfinder ProtocolCopy MarkdownOpenThe Problem: Centralized Gateway Reliance Today, most Arweave content is accessed through a single gateway: arweave.net. This creates a critical centralization risk: Single point of failure - If arweave.net goes down, content becomes inaccessible Censorship vulnerability - A single gateway can block or filter content Performance bottlenecks - All traffic flows through one gateway No content verification - Users must trust the gateway to serve authentic content What is Wayfinder? The Wayfinder protocol solves these problems by enabling decentralized access to Arweave content through any gateway in the AR.IO network. It's a URI scheme that transforms centralized URLs like https:","estimatedWords":102,"lastModified":"2025-10-20T12:11:35.380Z","breadcrumbs":["learn","wayfinder"],"siteKey":"ario","siteName":"AR-IO Network","depth":2,"crawledAt":"2025-10-20T12:11:35.380Z"},{"url":"https://docs.ar.io/learn/arns","title":"Arweave Name System (ArNS)","content":"Arweave Name System (ArNS)Copy MarkdownOpenWhat is ArNS? Arweave URLs and transaction IDs are long, difficult to remember, and occasionally categorized as spam. The Arweave Name System (ArNS) aims to resolve these problems in a decentralized manner. ArNS is a censorship-resistant naming system stored on Arweave, powered by ARIO tokens, enabled through AR.IO gateway domains, and used to connect friendly domain names to permaweb apps, web pages, data, and identities. It's an open, permissionless, domain name registrar that doesn't rely on a single TLD. How ArNS Works This system works similarly to traditional DNS services, where users can purchase a name in a registry and DNS Name servers resolve these names to IP addresses. The system is flexible and allows users to purchase names permanently or lease them for a defined duration based on their use case. With ArNS, the registry is stored permanently on Arweave via AO, making it immutable and globally resilient. This also means that apps and infrastructure cannot just read the latest state of the registry but can also check any point in time in the past, creating a \"Wayback Machine\" of permanent data. Name Resolution Users can register a name, like ardrive, within the ArNS Registry. Before owning a name, they must create an Arweave Name Token (ANT), an AO Computer based token and open-source protocol used by ArNS to track the ownership and control over the name. ANTs allow the owner to set a mutable pointer to any type of permaweb data, like a page, app or file, via its Arweave transaction ID. Each AR.IO gateway acts as an ArNS Name resolver. They fetch the latest state of both the ArNS Registry and its associated ANTs from an AO compute unit (CU) and serve this information rapidly for apps and users. AR.IO gateways will also resolve that name as one of their own subdomains, e.g., https:","estimatedWords":311,"lastModified":"2025-10-20T12:11:36.245Z","breadcrumbs":["learn","arns"],"siteKey":"ario","siteName":"AR-IO Network","depth":2,"crawledAt":"2025-10-20T12:11:36.245Z"},{"url":"https://docs.ar.io/learn/oip","title":"Observation  Incentive Protocol","content":"Observation & Incentive ProtocolCopy MarkdownOpenOverview The Observation and Incentive Protocol ensures network quality through peer monitoring and performance-based rewards. Gateways are incentivized to maintain high performance while also serving as \"observers\" that evaluate their peers' ArNS resolution capabilities and data integrity verification. The protocol operates on 24-hour epochs where up to 50 gateways are selected as observers to test other gateways against ArNS name resolution criteria and chunk/offset validation. This creates a self-regulating ecosystem with transparent, consensus-based performance evaluation. Architecture Overview The Observer Protocol operates through a systematic process where selected gateways monitor their peers and report findings to maintain network quality: Epoch Cycle and Responsibilities Each 24-hour epoch follows a structured process with specific responsibilities for gateways and observers: Epoch Start Smart Contract: Selects up to 50 observers using weighted random selection Smart Contract: Generates 2 prescribed ArNS names for all observers to test Selected Observers: Receive notification of selection and prescribed names Observation Phase Observers: Choose 8 additional ArNS names to test (total of 10 names per gateway) Observers: Select subset of gateways for chunk/offset validation based on sampling rate Observers: Test assigned gateways for ArNS resolution, wallet ownership, content hashes, and response times Observers: Validate chunk/offset data integrity using cryptographic Merkle proofs Target Gateways: Respond to resolution requests, serve content, and provide chunk data with proofs Reporting Phase Observers: Upload detailed JSON reports to Arweave for transparency Observers: Submit failed gateway lists to the AR.IO Smart Contract for consensus voting Evaluation and Distribution Smart Contract: Tallies all observer votes (≥50% pass = functional gateway) Smart Contract: Distributes rewards at epoch end based on performance Functional Gateways/Observers: Receive ARIO token rewards automatically Key Features Decentralized Monitoring: Peer-to-peer evaluation ensures no single point of failure Consensus-Based Scoring: Majority rule (≥50% pass votes) determines gateway functionality Performance Incentives: Only functional gateways and observers receive ARIO token rewards Data Integrity Validation: Cryptographic verification of chunk/offset data using Merkle proofs Transparent Accountability: All reports permanently stored on Arweave and viewable at gateways.ar.io Sustainable Funding: Protocol balance funded by ArNS name purchases, aligning rewards with network usage Chunk/Offset Validation The protocol includes advanced data integrity verification through chunk/offset observation. Observers validate that gateways can correctly serve and verify Arweave data chunks using cryptographic proofs: Validation Process Sampling: A subset of gateways is selected for chunk validation each epoch Offset Testing: Random offsets within the stable weave range are tested Merkle Proof Verification: Cryptographic validation ensures chunk authenticity Binary Search Optimization: Efficient transaction lookup using cached metadata Technical Implementation Chunk Retrieval: GET /chunk/{offset} returns chunk data and Merkle proof Proof Validation: Uses Arweave's validatePath() function for cryptographic verification Performance Optimization: LRU caching for blocks, transactions, and metadata Early Stopping: Tests stop immediately upon first successful validation View Live Data: See current observers and performance metrics at gateways.ar.io Explore the Protocol Observer SelectionLearn how gateways are chosen as observers using weighted criteria and entropyPerformance EvaluationUnderstand vote tallying, consensus mechanisms, and weight calculationsReporting SystemExplore observer responsibilities for dual-channel submission to ArweaveReward DistributionLearn about reward formulas, funding mechanisms, and penalty structuresHow is this guide?GoodBadGateway RegistryThe AR.IO Network consists of AR.IO gateway nodes, which are identified by their registered Arweave wallet addresses and either their IP addresses or hostnames, as stored in the networkObserver SelectionLearn about how gateways are selected as observers each epoch and how ArNS names are chosen using weighted random selection and Hashchain entropy","estimatedWords":559,"lastModified":"2025-10-20T12:11:37.368Z","breadcrumbs":["learn","oip"],"siteKey":"ario","siteName":"AR-IO Network","depth":2,"crawledAt":"2025-10-20T12:11:37.368Z"},{"url":"https://docs.ar.io/learn/gateways","title":"ARIO Gateways","content":"AR.IO GatewaysCopy MarkdownOpenWhat Are AR.IO Gateways? AR.IO gateways are specialized infrastructure nodes that serve as bridges between the Arweave network and applications. They transform raw Arweave blockchain data into a fast, reliable, and developer-friendly platform for storing and retrieving permanent data. Core Responsibilities AR.IO gateways handle three fundamental responsibilities: Data Writing & Proxying Transaction relay: Forward transaction headers to Arweave miners for mempool inclusion Chunk distribution: Proxy data chunks to Arweave nodes for storage and replication Bundle processing: Receive and bundle ANS-104 data items into base layer transactions Data Retrieval & Serving Fast access: Serve cached data with optimized performance and reliability Multi-source fallback: Retrieve data from trusted gateways, network peers, or directly from Arweave Content delivery: Stream complete transactions, individual chunks, or bundled data items Data Discovery & Indexing Structured queries: Enable efficient searches across transactions, bundles, and wallet data Real-time indexing: Process incoming data streams and maintain searchable databases ArNS routing: Provide human-readable name resolution for Arweave content Key Features Modular Architecture Gateways are built with interchangeable components that operators can customize: Configurable services: Enable or disable features based on specific needs Scalable storage: From SQLite for small deployments to cloud databases for enterprise scale Flexible infrastructure: Adaptable to different operational environments and requirements Network Connectivity Decentralized network: Connect to other AR.IO gateways for data sharing and redundancy Trust-minimized access: Cryptographically verify data integrity without relying on central authorities Performance optimization: Intelligent caching and content delivery strategies Developer Experience HTTP APIs: Standard web interfaces for all gateway functionality Monitoring & telemetry: Built-in observability for operational insights Content moderation: Configurable policies for community and compliance needs What Gateways Are Not It's important to understand the boundaries of what AR.IO gateways do and don't provide: Not Storage Providers Don't enforce Arweave protocol: Gateways don't validate consensus or mining rules Don't guarantee permanence: Storage permanence comes from Arweave itself, not gateways Don't replicate all data: Gateways cache popular content but aren't full blockchain replicas Not Compute Platforms Don't depend on AO: Gateways operate independently of any compute layer Don't execute smart contracts: Computation happens on AO or other platforms, not gateways Don't process application logic: Gateways focus purely on data access and delivery Not Centralized Services Don't control data: Content ownership and control remain with original creators Don't gate access: Anyone can run a gateway and access Arweave data Don't create vendor lock-in: Gateway APIs and protocols are open and interoperable Explore Gateways Gateway ArchitectureLearn about system design, components, and technology stackData RetrievalUnderstand multi-source fallback strategies and performance optimizationData VerificationLearn about cryptographic validation and integrity assuranceRun a GatewayDeploy and operate your own AR.IO gateway infrastructureHow is this guide?GoodBadWhat are Bundles?Learn about ANS-104 bundling standards and how they optimize data submission to ArweaveArchitectureLearn about the technical architecture of AR.IO gateways, their core dependencies, and key design decisions","estimatedWords":465,"lastModified":"2025-10-20T12:11:37.831Z","breadcrumbs":["learn","gateways"],"siteKey":"ario","siteName":"AR-IO Network","depth":2,"crawledAt":"2025-10-20T12:11:37.831Z"},{"url":"https://docs.ar.io/learn/ans-104-bundles","title":"What are Bundles","content":"IntroductionWhat are Bundles?Copy MarkdownOpenANS-104 bundles are data packaging standards that efficiently bundle multiple data items and submit them to Arweave as single transactions, reducing transaction overhead and improving network efficiency. The Problem ANS-104 Solves Individual Arweave transactions have inherent limitations: Transaction overhead - Each transaction requires separate processing and storage Network inefficiency - Multiple small transactions consume more network resources Indexing complexity - Individual transactions are harder to organize and query Storage fragmentation - Related data items are stored separately ANS-104 provides: Reduced transaction overhead by batching multiple data items Improved network efficiency through consolidated transactions Better indexing capabilities with structured data item format Standardized data format for interoperability across applications How ANS-104 Bundling Works The ANS-104 Standard ANS-104 is the official specification for bundling data on Arweave: Data Items - Individual pieces of data with standardized binary format Bundle - Single Arweave transaction containing multiple data items Binary Serialization - Consistent format for data item structure Standardized Format - Ensures interoperability across applications How ANS-104 Works Data Item Creation - Create individual data items with ANS-104 format Bundle Assembly - Combine multiple data items into a single bundle Transaction Creation - Submit bundle as one Arweave transaction Network Processing - Miners process the single bundle transaction Data Retrieval - Individual data items can be extracted and indexed Key Benefits of ANS-104 Reduced Overhead - Bundle multiple data items into a single transaction to reduce processing overhead Network Efficiency - Consolidate multiple uploads into fewer network transactions Standardized Format - Consistent binary serialization ensures interoperability across applications Better Indexing - Structured data item format enables more efficient data retrieval and querying Why ANS-104 Matters for the Permaweb ANS-104 bundles are essential for building scalable applications on the permaweb because they: Enable efficient data storage by reducing transaction overhead for multiple data items Improve network performance through consolidated transactions Support better data organization with standardized data item formats Enable scalable applications that need to store many related data items efficiently Explore Bundling Upload DataLearn how to upload data to Arweave using bundlingRun a BundlerDeploy your own bundling infrastructureGateway ExtensionsIntegrate bundling with your AR.IO gatewayTurbo SDKUse Turbo SDK for easy data uploads and bundlingHow is this guide?GoodBadWhat is the Permaweb?Understand how the permaweb works through its four-layer architecture: storage, compute, gateways, and applicationsAR.IO GatewaysAR.IO gateways bridge the Arweave network and applications, providing fast, reliable access to permanent data through specialized infrastructure.","estimatedWords":399,"lastModified":"2025-10-20T12:11:39.180Z","breadcrumbs":["learn","ans 104 bundles"],"siteKey":"ario","siteName":"AR-IO Network","depth":2,"crawledAt":"2025-10-20T12:11:39.180Z"},{"url":"https://docs.ar.io/learn/permaweb","title":"What is the Permaweb","content":"IntroductionWhat is the Permaweb?Copy MarkdownOpenThe permaweb is a decentralized, permanent layer of the internet where data, applications, and websites are stored forever and remain accessible through a global network of gateways. How the Permaweb Works Unlike the traditional web where data can disappear when servers go offline, the permaweb creates a permanent archive of human knowledge through a multi-layer architecture: Foundation: Arweave blockchain provides immutable storage Computation: AO and other platforms enable smart contracts and processing Access: AR.IO gateway network makes everything accessible globally Users: Developers and users interact through familiar web interfaces This architecture ensures that once something is published to the permaweb, it remains accessible forever, creating a truly permanent internet. Permaweb Network Architecture The permaweb operates through a layered system architecture where each component provides specialized services to create a permanent, decentralized internet. Applications & UsersWeb Apps, dApps, Developers, End UsersAODecentralized ComputeAR.IODecentralized AccessArweavePermanent Storage Foundation Permaweb Architecture: How It All Connects The permaweb operates through a four-layer architecture where each layer serves a specific purpose in creating permanent, accessible internet infrastructure: Layer 1: Permanent Storage (Arweave) Core Responsibility: Forever Data Preservation Arweave's primary job is to store data permanently with mathematical guarantees: Immutable storage - Once written, data cannot be changed or deleted Economic sustainability - Endowment model ensures miners are paid to store data forever Cryptographic verification - Proof that data exists and hasn't been tampered with Decentralized replication - Data survives even if most miners go offline Layer 2: Decentralized Compute (AO) Core Responsibility: Smart Contract Execution AO's primary job is to run programs that work with permanent data: Process execution - Runs smart contracts and applications on permanent data Message routing - Enables communication between different processes State management - Maintains application state using permanent storage Parallel computation - Scales processing across multiple nodes Layer 3: Decentralized Access (AR.IO) Core Responsibility: Data Access & Discovery AR.IO's primary job is to make permanent data fast and accessible: Data retrieval - Fetches and serves content from Arweave storage Content indexing - Organizes and catalogs data for search and discovery ArNS resolution - Converts human-readable names to Arweave transaction IDs Performance optimization - Caches popular content for faster access Quality assurance - Validates data integrity and provides reliable access Layer 4: Applications & Users Core Responsibility: User Interface & Experience Applications and users are responsible for interacting with the permanent web: User interfaces - Create familiar web experiences backed by permanent data Data submission - Upload new content and applications to the permaweb Application logic - Build decentralized apps using permanent storage and compute Content consumption - Browse, search, and interact with permanent web content The Vision of the Permaweb The permaweb represents a fundamental shift from the ephemeral nature of today's internet to a permanent, censorship-resistant foundation for human knowledge and applications. By combining Arweave's immutable storage with AO's decentralized compute and AR.IO's accessible gateway network, the permaweb creates an internet where data never disappears, applications run without central points of failure, and users maintain true ownership of their digital assets. This architecture enables a new generation of applications that can operate indefinitely without relying on traditional hosting services, where digital artifacts become truly permanent, and where the collective knowledge of humanity is preserved for future generations. The permaweb isn't just about storing data forever—it's about building a more resilient, equitable, and permanent foundation for the digital world. Explore the Permaweb What is AR.IO?Learn how AR.IO provides the gateway layer for accessing permanent dataWhat is Arweave?Understand the permanent storage foundation of the permawebArNS NamesDiscover how human-readable names work on the permawebStart BuildingGet started building applications on the permanent webHow is this guide?GoodBadWhat is AR.IO?AR.IO is the world's first permanent cloud network providing infrastructure for timeless, tamper-proof, and universally accessible dataWhat are Bundles?Learn about ANS-104 bundling standards and how they optimize data submission to Arweave","estimatedWords":638,"lastModified":"2025-10-20T12:11:39.952Z","breadcrumbs":["learn","permaweb"],"siteKey":"ario","siteName":"AR-IO Network","depth":2,"crawledAt":"2025-10-20T12:11:39.952Z"},{"url":"https://docs.ar.io/learn/what-is-ario","title":"What is ARIO","content":"IntroductionWhat is AR.IO?Copy MarkdownOpenThe AR.IO Network is the first permanent cloud network. A decentralized infrastructure layer built on Arweave and AO, making permanent data accessible, discoverable, and usable. Think of it as the gateway to Arweave's permaweb, turning its tamper-proof storage into a fully functional, user-friendly ecosystem for apps, websites, and data. Features of The AR.IO Network Gateways AR.IO operates a network of gateways — nodes that serve as entry points to Arweave’s data. These gateways fetch and deliver data quickly, supporting everything from static files to dynamic web apps. Arweave Name System (ArNS) The Arweave Name System (ArNS) is a decentralized naming system for Arweave. It allows users to register and resolve human-readable names to Arweave transaction IDs. Data Access AR.IO offers a range of tools for accessing and querying data on Arweave, including: HTTP Requests via gateways GraphQL Queries for finding data by tags and metadata ArNS for human-readable URLs Wayfinder for decentralized content discovery The Problem Arweave stores data forever, but accessing and organizing that data isn't always straightforward. Without efficient tools, retrieving files, serving websites, or finding specific content on Arweave's blockweave can be slow or complex, limiting its potential for developers and users. The Solution The AR.IO Network builds on Arweave's permanent storage to create a decentralized, scalable access layer. It provides gateways, domain names, and indexing services, making it easy to interact with permaweb content as seamlessly as the traditional web. How It Works Decentralized Gateways: AR.IO operates a network of gateways—nodes that serve as entry points to Arweave’s data. These gateways fetch and deliver data quickly, supporting everything from static files to dynamic web apps. ArNS (Arweave Name Service): AR.IO introduces decentralized domain names (e.g., yourname.arweave), mapping human-readable names to Arweave’s data IDs. This makes content easy to find and share, like URLs on the traditional web. Indexing and Querying: AR.IO enables efficient data indexing, allowing developers to search and retrieve specific content from Arweave’s vast storage without scanning the entire blockweave. Routing and verification: AR.IO 's ar:","estimatedWords":335,"lastModified":"2025-10-20T12:11:40.922Z","breadcrumbs":["learn","what is ario"],"siteKey":"ario","siteName":"AR-IO Network","depth":2,"crawledAt":"2025-10-20T12:11:40.922Z"},{"url":"https://docs.ar.io/learn/what-is-arweave","title":"What is Arweave","content":"IntroductionWhat is Arweave?Copy MarkdownOpenArweave is a decentralized storage network that ensures data is permanent, affordable, and scalable. Think of it as a global, tamper-proof hard drive where your files—photos, documents, or apps—stay accessible forever. It's the foundation for the AR.IO Network, powering a \"permaweb\" where data never disappears. Below, we break down Arweave's core features in a simple, beginner-friendly way. A Datachain for Permanent Storage Arweave is like Bitcoin, but for data. It solves one problem really well: storing data permanently. Once uploaded, your data—whether a tweet, NFT, or website—is immutable and preserved indefinitely. How Does It Work? Blockweave Architecture: Unlike a blockchain's single chain, Arweave's blockweave links each new data block to the previous one and a random older block. Data is split into 256 KiB chunks in a secure Merkle tree, ensuring miners keep all data to add new blocks. Succinct Proofs of Random Access (SPoRA): Miners prove they store multiple data copies by accessing random chunks, verified efficiently with Verifiable Delay Functions (VDFs). This combines proof-of-work and proof-of-storage, making data loss nearly impossible. In Simple Terms: Picture a library where new books reference older ones, and librarians must keep every book to add more. SPoRA ensures they prove they’ve got the books, keeping your data safe forever. Pay Once, Store Forever: No Recurring Fees Pay a one-time fee to upload data, and it's stored \"forever\"—no subscriptions or renewals. How Does It Work? Endowment Fund: Your fee, based on 200 years of storage for 20 replicas, goes mostly into a fund that slowly pays miners in AR tokens to maintain data. It assumes storage costs drop over time, making the fund sustainable. In Simple Terms: It's a “forever stamp” for data. Your payment funds a pot that keeps paying storage keepers, lasting longer as tech gets cheaper. Practically Unlimited Storage Arweave can practically store unlimited data, from small files to entire digital archives, without hitting a ceiling. The theoretical limit is 2^256 bytes which for scale is more atoms than there are in the universe. How Does It Work? Layer 1 Transactions: Data is stored as 256 KiB chunks on the blockweave, replicated across many nodes. As more nodes join with standard hardware, storage capacity grows limitlessly. Bundling with AR.IO Network and Turbo: Bundling packs multiple files into one transaction, reducing costs and congestion. AR.IO Network and Turbo (a Layer 2 tool) optimize this, enabling fast, cheap uploads of large datasets like websites. What Arweave doesn't solve well? Access Arweave solve's one problem and solve's it well. Storing your data for a very long-time. It doesn't, however, incentivise the indexing and access for data. Ready to Dive Deeper? Arweave powers a permaweb where apps, websites and data live forever. For AR.IO Network developers, it's the bedrock for unstoppable decentralized applications. Learn more in the next section: What is AR.IO Network. Explore Arweave What is AR.IO?Learn how AR.IO provides the gateway layer for accessing permanent dataThe PermawebUnderstand the permanent web architecture powered by ArweaveUpload DataStart storing your data permanently on ArweaveAccess DataLearn how to retrieve and query permanent dataHow is this guide?GoodBadIntroductionBuild on the permanent web with AR.IO's decentralized gateway protocol for ArweaveWhat is AR.IO?AR.IO is the world's first permanent cloud network providing infrastructure for timeless, tamper-proof, and universally accessible data","estimatedWords":542,"lastModified":"2025-10-20T12:11:41.726Z","breadcrumbs":["learn","what is arweave"],"siteKey":"ario","siteName":"AR-IO Network","depth":2,"crawledAt":"2025-10-20T12:11:41.726Z"},{"url":"https://docs.ar.io/apis","title":"APIs Reference","content":"APIs ReferenceCopy MarkdownOpenExplore the REST APIs available in the AR.IO ecosystem. Our services are built with a commitment to open source principles, and all repositories are publicly available under AGPL-3 licenses. Available Services AR.IO GatewayThe core gateway software providing access to data on Arweave. Includes data retrieval, ArNS resolution, and network management.TurboUpload and payment services providing fast, reliable data uploads to Arweave with instant confirmation and transparent pricing. AR.IO Gateway APIs The AR.IO Gateway serves as the primary interface for accessing Arweave data through the AR.IO network. Key endpoints include: Data Access - Retrieve transaction data and files from Arweave ArNS Resolution - Resolve human-readable names to Arweave transaction IDs Network Information - Query gateway health, pricing, and network status Transaction Management - Submit and track transactions Admin Functions - Gateway administration and configuration Turbo APIs Turbo provides high-performance upload services for the Arweave network with additional features: Data Upload - Fast, reliable uploads with instant confirmation Payment Processing - Transparent pricing and payment management Upload Tracking - Monitor upload status and metadata Credit Management - Handle payment credits and billing Open Source Commitment We believe strongly in open source development. All AR.IO services are: Publicly Available - Source code is open and accessible AGPL-3 Licensed - Ensuring software freedom and transparency Community Driven - Built with input from the developer community Auditable - Code can be reviewed and verified by anyone Getting Started Choose your service - Select the APIs that fit your needs Review the documentation - Each service has comprehensive APIs documentation Test endpoints - Use the interactive examples to explore functionality Integrate - Implement the APIs in your applications For SDK alternatives to these REST APIs, visit our SDK documentation. Explore More SDK DocumentationUse our TypeScript SDKs for easier integration and developmentQuick Start - UploadStart uploading data to Arweave with our upload guidesQuick Start - AccessLearn how to retrieve and query data from ArweaveRun a GatewayDeploy your own AR.IO gateway and access these APIs directlyHow is this guide?GoodBadAR.IO Gateway APIsCore gateway software for accessing, caching, and querying data on Arweave","estimatedWords":344,"lastModified":"2025-10-20T12:11:42.374Z","breadcrumbs":["apis"],"siteKey":"ario","siteName":"AR-IO Network","depth":1,"crawledAt":"2025-10-20T12:11:42.374Z"},{"url":"https://docs.ar.io/sdks","title":"Introduction","content":"IntroductionCopy MarkdownOpenBuild powerful applications with our comprehensive suite of SDKs designed for the AR.IO ecosystem. Upload data with the Turbo SDKHigh-performance data upload service for Arweave with instant confirmation and transparent pricingInteract with the AR.IO Network using the AR.IO SDKAccess AR.IO Network protocols, manage ArNS names, interact with ANTs, and integrate gateway servicesDecentralized access with Wayfinder SDKRobust, censorship-resistant access to Arweave data through the distributed AR.IO gateway network Choose Your SDK Each SDK serves a specific purpose in the AR.IO ecosystem: Turbo SDK - For applications that need fast, reliable data uploads to Arweave AR.IO SDK - For interacting with AR.IO Network smart contracts and services Wayfinder SDK - For decentralized data access with built-in verification and gateway routing All SDKs are available for both Node.js and browser environments, with TypeScript support included. Next Steps Quick Start - Upload DataGet started quickly by uploading your first file using the Turbo SDKQuick Start - Access DataLearn how to retrieve and query data using our access toolsAPI ReferenceExplore detailed API documentation for all available endpointsJoin the CommunityConnect with other developers building on AR.IOHow is this guide?GoodBad","estimatedWords":184,"lastModified":"2025-10-20T12:11:42.425Z","breadcrumbs":["sdks"],"siteKey":"ario","siteName":"AR-IO Network","depth":1,"crawledAt":"2025-10-20T12:11:42.425Z"},{"url":"https://docs.ar.io/build","title":"Get Started","content":"Get StartedCopy MarkdownOpenWelcome to AR.IO Network's developer documentation. This section will guide you through everything you need to know for building on top of AR.IO Network from uploading and accessing your data, to operating and extending your own infrastructure to support your unique use-case. If you're unfamiliar with Arweave's permanent storage and AR.IO Network we recommend reading this introduction section first. Get Started Building with AR.IO Uploading DataLearn how to permanently store files, websites, and application data on ArweaveAccessing DataQuery, retrieve, and interact with data stored on the permanent webRunning Your Own GatewayDeploy and operate AR.IO gateway infrastructure to support the network Developer Resources Turbo SDK ReferenceFast upload service SDK with payment processing and instant confirmationWayfinder SDK ReferenceDecentralized data access SDK with built-in verification and routingAR.IO SDK ReferenceComplete SDK documentation for interacting with AR.IO Network protocolsGateway API ReferenceDirect API access for custom implementations and integrations Use Cases & Guides Hosting Decentralized WebsitesBuild and deploy censorship-resistant web applications on the permanent webArNS Primary NamesSet up primary names for user-friendly wallet addressesArNS Undernames & VersioningManage subdomains and versioning for your ArNS namesArNS MarketplaceExplore the ArNS marketplace for name trading and management Get Help Browse ExamplesExplore code examples and sample applicationsJoin the CommunityConnect with other developers on Discord Ready to build on the permanent web? Choose your path above and start creating applications that last forever.How is this guide?GoodBadUpload DataStart uploading to Arweave's permanent storage network with our recommended tools and best practices","estimatedWords":240,"lastModified":"2025-10-20T12:11:42.472Z","breadcrumbs":["build"],"siteKey":"ario","siteName":"AR-IO Network","depth":1,"crawledAt":"2025-10-20T12:11:42.472Z"},{"url":"https://docs.ar.io/learn","title":"Introduction","content":"IntroductionCopy MarkdownOpenAR.IO is the decentralized gateway protocol for Arweave. Deploy gateways, register permanent names, and build applications with enterprise-grade infrastructure that lasts forever. For AI and LLM users: Access the complete documentation in plain text format at llms-full.txt for easy consumption by AI agents and language models. Explore the Documentation What is AR.IO?Learn about the decentralized gateway protocol and how it powers the permanent webBuildGet started building applications, running gateways, and uploading dataSDKsIntegrate AR.IO services into your applications with our developer SDKsAPI ReferenceComplete API documentation for AR.IO Node and Turbo services Quick Start Guides Upload Data to ArweaveLearn how to permanently store files and data using Turbo SDKRun a GatewayDeploy your own AR.IO gateway and participate in the networkRegister ArNS NamesGet human-readable names for your permanent applications AR.IO Ecosystem ArNS RegistryRegister and manage permanent names for your applicationsNetwork PortalMonitor gateway performance and network statisticsArDriveUser-friendly permanent storage for files and folders Join the Community Connect with developers, gateway operators, and the AR.IO team. Get help, share ideas, and stay updated on the latest developments. Join Discord View Guides How is this guide?GoodBadWhat is Arweave?Arweave is permanent information storage - a decentralized web inside an open ledger, like Bitcoin but for data","estimatedWords":200,"lastModified":"2025-10-20T12:11:42.523Z","breadcrumbs":["learn"],"siteKey":"ario","siteName":"AR-IO Network","depth":1,"crawledAt":"2025-10-20T12:11:42.523Z"},{"url":"https://docs.ar.io/sdks/ardrive-core-js","title":"ArDrive Core JS","content":"ArDrive Core JSCopy MarkdownOpenFor AI and LLM users: Access the complete ArDrive Core JS documentation in plain text format at llm.txt for easy consumption by AI agents and language models. ArDrive Core JS Please refer to the source code for SDK details.How is this guide?GoodBadEntity TypesJavaScript/TypeScript SDK for interacting with ArDrive","estimatedWords":51,"lastModified":"2025-10-20T12:11:44.067Z","breadcrumbs":["sdks","ardrive core js"],"siteKey":"ario","siteName":"AR-IO Network","depth":3,"crawledAt":"2025-10-20T12:11:44.067Z"},{"url":"https://docs.ar.io/sdks/turbo-sdk/logging","title":"Logging","content":"Turbo SDKLoggingCopy MarkdownOpenThe SDK uses winston for logging. You can set the log level using the setLogLevel method. TurboFactory.setLogLevel('debug');How is this guide?GoodBadEventsSDK for interacting with Turbo, a fast and efficient data upload service for ArweaveTurbo Credit SharingSDK for interacting with Turbo, a fast and efficient data upload service for Arweave","estimatedWords":50,"lastModified":"2025-10-20T12:11:45.030Z","breadcrumbs":["sdks","turbo sdk","logging"],"siteKey":"ario","siteName":"AR-IO Network","depth":3,"crawledAt":"2025-10-20T12:11:45.030Z"},{"url":"https://docs.ar.io/sdks/turbo-sdk/turbo-credit-sharing","title":"Turbo Credit Sharing","content":"Turbo SDKTurbo Credit SharingCopy MarkdownOpenUsers can share their purchased Credits with other users' wallets by creating Credit Share Approvals. These approvals are created by uploading a signed data item with tags indicating the recipient's wallet address, the amount of Credits to share, and an optional amount of seconds that the approval will expire in. The recipient can then use the shared Credits to pay for their own uploads to Turbo. Shared Credits cannot be re-shared by the recipient to other recipients. Only the original owner of the Credits can share or revoke Credit Share Approvals. Credits that are shared to other wallets may not be used by the original owner of the Credits for sharing or uploading unless the Credit Share Approval is revoked or expired. Approvals can be revoked at any time by similarly uploading a signed data item with tags indicating the recipient's wallet address. This will remove all approvals and prevent the recipient from using the shared Credits. All unused Credits from expired or revoked approvals are returned to the original owner of the Credits. To use the shared Credits, recipient users must provide the wallet address of the user who shared the Credits with them in the x-paid-by HTTP header when uploading data. This tells Turbo services to look for and use Credit Share Approvals to pay for the upload before using the signer's balance. For user convenience, during upload the Turbo CLI will use any available Credit Share Approvals found for the connected wallet before using the signing wallet's balance. To instead ignore all Credit shares and only use the signer's balance, use the --ignore-approvals flag. To use the signer's balance first before using Credit shares, use the --use-signer-balance-first flag. In contrast, the Turbo SDK layer does not provide this functionality and will only use approvals when paidBy is provided. The Turbo SDK provides the following methods to manage Credit Share Approvals: shareCredits: Creates a Credit Share Approval for the specified wallet address and amount of Credits. revokeCredits: Revokes all Credit Share Approvals for the specified wallet address. listShares: Lists all Credit Share Approvals for the specified wallet address or connected wallet. dataItemOpts: { ...opts, paidBy: string[] }: Upload methods now accept 'paidBy', an array of wallet addresses that have provided credit share approvals to the user from which to pay, in the order provided and as necessary, for the upload. The Turbo CLI provides the following commands to manage Credit Share Approvals: share-credits: Creates a Credit Share Approval for the specified wallet address and amount of Credits. revoke-credits: Revokes all Credit Share Approvals for the specified wallet address. list-shares: Lists all Credit Share Approvals for the specified wallet address or connected wallet. paidBy: --paid-by : Upload commands now accept '--paid-by', an array of wallet addresses that have provided credit share approvals to the user from which to pay, in the order provided and as necessary, for the upload. --ignore-approvals: Ignore all Credit Share Approvals and only use the signer's balance. --use-signer-balance-first: Use the signer's balance first before using Credit Share Approvals. How is this guide?GoodBadLoggingSDK for interacting with Turbo, a fast and efficient data upload service for ArweaveWayfinder SDK'sDecentralized access to Arweave data with built-in verification and gateway routing","estimatedWords":535,"lastModified":"2025-10-20T12:11:45.913Z","breadcrumbs":["sdks","turbo sdk","turbo credit sharing"],"siteKey":"ario","siteName":"AR-IO Network","depth":3,"crawledAt":"2025-10-20T12:11:45.913Z"},{"url":"https://docs.ar.io/sdks/ardrive-cli","title":"ArDrive CLI","content":"ArDrive CLICopy MarkdownOpenFor AI and LLM users: Access the complete ArDrive CLI documentation in plain text format at llm.txt for easy consumption by AI agents and language models. ArDrive CLI Please refer to the source code for SDK details.How is this guide?GoodBadArchitectureDecentralized access to Arweave data with built-in verification and gateway routingArFSCommand line interface for ArDrive","estimatedWords":56,"lastModified":"2025-10-20T12:11:46.793Z","breadcrumbs":["sdks","ardrive cli"],"siteKey":"ario","siteName":"AR-IO Network","depth":3,"crawledAt":"2025-10-20T12:11:46.793Z"},{"url":"https://docs.ar.io/sdks/turbo-sdk/llm.txt","title":"Llmtxt","content":"# TurboAuthenticatedClient (/(apis)/turboauthenticatedclient) #### getBalance() Issues a signed request to get the credit balance of a wallet measured in AR (measured in Winston Credits, or winc). ```typescript const { winc: balance } = await turbo.getBalance(); ``` #### signer.getNativeAddress() Returns the [native address][docs/native-address] of the connected signer. ```typescript const address = await turbo.signer.getNativeAddress(); ``` #### getWincForFiat() Returns the current amount of Winston Credits including all adjustments for the provided fiat currency, amount, and optional promo codes. ```typescript const { winc, paymentAmount, quotedPaymentAmount, adjustments } = await turbo.getWincForFiat({ amount: USD(100), promoCodes: ['MY_PROMO_CODE'], // promo codes require an authenticated client }); ``` #### createCheckoutSession() Creates a Stripe checkout session for a Turbo Top Up with the provided amount, currency, owner, and optional promo codes. The returned URL can be opened in the browser, all payments are processed by Stripe. Promo codes require an authenticated client. ```typescript const { url, winc, paymentAmount, quotedPaymentAmount, adjustments } = await turbo.createCheckoutSession({ amount: USD(10.0), // $10.00 USD owner: publicArweaveAddress, promoCodes: ['MY_PROMO_CODE'], // promo codes require an authenticated client }); // open checkout session in a browser window.open(url, '_blank'); ``` #### upload() The easiest way to upload data to Turbo. The `signal` is an optional [AbortSignal] that can be used to cancel the upload or timeout the request. `dataItemOpts` is an optional object that can be used to configure tags, target, and anchor for the data item upload. ```typescript const uploadResult = await turbo.upload({ data: 'The contents of my file!', signal: AbortSignal.timeout(10_000), // cancel the upload after 10 seconds dataItemOpts: { // optional }, events: { // optional }, }); ``` #### uploadFile() Signs and uploads a raw file. There are two ways to provide the file to the SDK: 1. Using a `file` parameter 2. Using a `fileStreamFactory` and `fileSizeFactory` ##### Using file` In Web with a file input: ```typescript const selectedFile = e.target.files[0]; const uploadResult = await turbo.uploadFile({ file: selectedFile, dataItemOpts: { tags: [{ name: 'Content-Type', value: 'text/plain' }], }, events: { onUploadProgress: ({ totalBytes, processedBytes }) => { console.log('Upload progress:', { totalBytes, processedBytes }); }, onUploadError: (error) => { console.log('Upload error:', { error }); }, onUploadSuccess: () => { console.log('Upload success!'); }, }, }); ``` In NodeJS with a file path: ```typescript const filePath = path.join(__dirname, './my-unsigned-file.txt'); const fileSize = fs.stateSync(filePath).size; const uploadResult = await turbo.uploadFile({ file: filePath, dataItemOpts: { tags: [{ name: 'Content-Type', value: 'text/plain' }], }, }); ``` ##### Using fileStreamFactory` and `fileSizeFactory` Note: The provided `fileStreamFactory` should produce a NEW file data stream each time it is invoked. The `fileSizeFactory` is a function that returns the size of the file. The `signal` is an optional [AbortSignal] that can be used to cancel the upload or timeout the request. `dataItemOpts` is an optional object that can be used to configure tags, target, and anchor for the data item upload. ```typescript const filePath = path.join(__dirname, './my-unsigned-file.txt'); const fileSize = fs.stateSync(filePath).size; const uploadResult = await turbo.uploadFile({ fileStreamFactory: () => fs.createReadStream(filePath), fileSizeFactory: () => fileSize, }); ``` ##### Customize Multi-Part Upload Behavior By default, the Turbo upload methods will split files that are larger than 10 MiB into chunks and send them to the upload service multi-part endpoints. This behavior can be customized with the following inputs: - `chunkByteCount`: The maximum size in bytes for each chunk. Must be between 5 MiB and 500 MiB. Defaults to 5 MiB. - `maxChunkConcurrency`: The maximum number of chunks to upload concurrently. Defaults to 5. Reducing concurrency will slow down uploads, but reduce memory utilization and serialize network calls. Increasing it will upload faster, but can strain available resources. - `chunkingMode`: The chunking mode to use. Can be 'auto', 'force', or 'disabled'. Defaults to 'auto'. Auto behavior means chunking is enabled if the file would be split into at least three chunks. - `maxFinalizeMs`: The maximum time in milliseconds to wait for the finalization of all chunks after the last chunk is uploaded. Defaults to 1 minute per GiB of the total file size. ```typescript // Customize chunking behavior await turbo.upload({ ...params, chunkByteCount: 1024 * 1024 * 500, // Max chunk size maxChunkConcurrency: 1, // Minimize concurrency }); ``` ```typescript // Disable chunking behavior await turbo.upload({ ...params, chunkingMode: 'disabled', }); ``` ```typescript // Force chunking behavior await turbo.upload({ ...params, chunkingMode: 'force', }); ``` #### On Demand Uploads With the upload methods, you can choose to Top Up with selected crypto token on demand if the connected wallet does not have enough credits to complete the upload. This is done by providing the `OnDemandFunding` class to the `fundingMode` parameter on upload methods. The `maxTokenAmount` (optional) is the maximum amount of tokens in the token type's smallest unit value (e.g: Winston for arweave token type) to fund the wallet with. The `topUpBufferMultiplier` (optional) is the multiplier to apply to the estimated top-up amount to avoid underpayment during on-demand top-ups due to price fluctuations on longer uploads. Defaults to 1.1, meaning a 10% buffer. Note: On demand API currently only available for $ARIO (`ario`), $SOL (`solana`), and $ETH on Base Network (`base-eth`) token types. ```typescript const turbo = TurboFactory.authenticated({ signer: arweaveSignerWithARIO, token: 'ario', }); await turbo.upload({ ...params, fundingMode: new OnDemandFunding({ maxTokenAmount: ARIOToTokenAmount(500), // Max 500 $ARIO topUpBufferMultiplier: 1.1, // 10% buffer to avoid underpayment }), }); ``` #### uploadFolder() Signs and uploads a folder of files. For NodeJS, the `folderPath` of the folder to upload is required. For the browser, an array of `files` is required. The `dataItemOpts` is an optional object that can be used to configure tags, target, and anchor for the data item upload. The `signal` is an optional [AbortSignal] that can be used to cancel the upload or timeout the request. The `maxConcurrentUploads` is an optional number that can be used to limit the number of concurrent uploads. The `throwOnFailure` is an optional boolean that can be used to throw an error if any upload fails. The `manifestOptions` is an optional object that can be used to configure the manifest file, including a custom index file, fallback file, or whether to disable manifests altogether. Manifests are enabled by default. ##### NodeJS Upload Folder ```typescript const folderPath = path.join(__dirname, './my-folder'); const { manifest, fileResponses, manifestResponse } = await turbo.uploadFolder({ folderPath, dataItemOpts: { // optional tags: [ { // User defined content type will overwrite file content type name: 'Content-Type', value: 'text/plain', }, { name: 'My-Custom-Tag', value: 'my-custom-value', }, ], // no timeout or AbortSignal provided }, manifestOptions: { // optional indexFile: 'custom-index.html', fallbackFile: 'custom-fallback.html', disableManifests: false, }, }); ``` ##### Browser Upload Folder ```html const folderInput = document.getElementById('folder'); folderInput.addEventListener('change', async (event) => { const selectedFiles = folderInput.files; console.log('Folder selected:', selectedFiles); const { manifest, fileResponses, manifestResponse } = await turbo.uploadFolder({ files: Array.from(selectedFiles).map((file) => file), }); console.log(manifest, fileResponses, manifestResponse); }); ``` #### topUpWithTokens() Tops up the connected wallet with Credits by submitting a payment transaction for the token amount to the Turbo wallet and then submitting that transaction id to Turbo Payment Service for top up processing. - The `tokenAmount` is the amount of tokens in the token type's smallest unit value (e.g: Winston for arweave token type) to fund the wallet with. - The `feeMultiplier` (optional) is the multiplier to apply to the reward for the transaction to modify its chances of being mined. Credits will be added to the wallet balance after the transaction is confirmed on the given blockchain. Defaults to 1.0, meaning no multiplier. ##### Arweave (AR) Crypto Top Up ```typescript const turbo = TurboFactory.authenticated({ signer, token: 'arweave' }); const { winc, status, id, ...fundResult } = await turbo.topUpWithTokens({ tokenAmount: WinstonToTokenAmount(100_000_000), // 0.0001 AR feeMultiplier: 1.1, // 10% increase in reward for improved mining chances }); ``` ##### AR.IO Network (ARIO) Crypto Top Up ```typescript const turbo = TurboFactory.authenticated({ signer, token: 'ario' }); const { winc, status, id, ...fundResult } = await turbo.topUpWithTokens({ tokenAmount: ARIOToTokenAmount(100), // 100 $ARIO }); ``` ##### Ethereum (ETH) Crypto Top Up ```typescript const turbo = TurboFactory.authenticated({ signer, token: 'ethereum' }); const { winc, status, id, ...fundResult } = await turbo.topUpWithTokens({ tokenAmount: ETHToTokenAmount(0.00001), // 0.00001 ETH }); ``` ##### Polygon (POL / MATIC) Crypto Top Up ```typescript const turbo = TurboFactory.authenticated({ signer, token: 'pol' }); const { winc, status, id, ...fundResult } = await turbo.topUpWithTokens({ tokenAmount: POLToTokenAmount(0.00001), // 0.00001 POL }); ``` ##### Eth on Base Network Crypto Top Up ```typescript const turbo = TurboFactory.authenticated({ signer, token: 'base-eth' }); const { winc, status, id, ...fundResult } = await turbo.topUpWithTokens({ tokenAmount: ETHToTokenAmount(0.00001), // 0.00001 ETH bridged on Base Network }); ``` ##### Solana (SOL) Crypto Top Up ```typescript const turbo = TurboFactory.authenticated({ signer, token: 'solana' }); const { winc, status, id, ...fundResult } = await turbo.topUpWithTokens({ tokenAmount: SOLToTokenAmount(0.00001), // 0.00001 SOL }); ``` ##### KYVE Crypto Top Up ```typescript const turbo = TurboFactory.authenticated({ signer, token: 'kyve' }); const { winc, status, id, ...fundResult } = await turbo.topUpWithTokens({ tokenAmount: KYVEToTokenAmount(0.00001), // 0.00001 KYVE }); ``` #### shareCredits() Shares credits from the connected wallet to the provided native address and approved winc amount. This action will create a signed data item for the approval ```typescript const { approvalDataItemId, approvedWincAmount } = await turbo.shareCredits({ approvedAddress: '2cor...VUa', approvedWincAmount: 800_000_000_000, // 0.8 Credits expiresBySeconds: 3600, // Credits will expire back to original wallet in 1 hour }); ``` #### revokeCredits() Revokes all credits shared from the connected wallet to the provided native address. ```typescript const revokedApprovals = await turbo.revokeCredits({ revokedAddress: '2cor...VUa', }); ``` #### getCreditShareApprovals() Returns all given or received credit share approvals for the connected wallet or the provided native address. ```typescript const { givenApprovals, receivedApprovals } = await turbo.getCreditShareApprovals({ userAddress: '2cor...VUa', }); ``` # TurboFactory (/(apis)/turbofactory) #### unauthenticated() Creates an instance of a client that accesses Turbo's unauthenticated services. ```typescript const turbo = TurboFactory.unauthenticated(); ``` #### authenticated() Creates an instance of a client that accesses Turbo's authenticated and unauthenticated services. Requires either a signer, or private key to be provided. ##### Arweave JWK ```typescript const jwk = await arweave.crypto.generateJWK(); const turbo = TurboFactory.authenticated({ privateKey: jwk }); ``` ##### ArweaveSigner ```typescript const signer = new ArweaveSigner(jwk); const turbo = TurboFactory.authenticated({ signer }); ``` ##### ArconnectSigner ```typescript const signer = new ArconnectSigner(window.arweaveWallet); const turbo = TurboFactory.authenticated({ signer }); ``` ##### EthereumSigner ```typescript const signer = new EthereumSigner(privateKey); const turbo = TurboFactory.authenticated({ signer }); ``` ##### Ethereum Private Key ```typescript const turbo = TurboFactory.authenticated({ privateKey: ethHexadecimalPrivateKey, token: 'ethereum', }); ``` ##### POL (MATIC) Private Key ```typescript const turbo = TurboFactory.authenticated({ privateKey: ethHexadecimalPrivateKey, token: 'pol', }); ``` ##### HexSolanaSigner ```typescript const signer = new HexSolanaSigner(bs58.encode(secretKey)); const turbo = TurboFactory.authenticated({ signer }); ``` ##### Solana Web Wallet Adapter ```typescript const turbo = TurboFactory.authenticated({ walletAdapter: window.solana, token: 'solana', }); ``` ##### Solana Secret Key ```typescript const turbo = TurboFactory.authenticated({ privateKey: bs58.encode(secretKey), token: 'solana', }); ``` ##### KYVE Private Key ```typescript const turbo = TurboFactory.authenticated({ privateKey: kyveHexadecimalPrivateKey, token: 'kyve', }); ``` ##### KYVE Mnemonic ```typescript const turbo = TurboFactory.authenticated({ privateKey: privateKeyFromKyveMnemonic(mnemonic), token: 'kyve', }); ``` # TurboUnauthenticatedClient (/(apis)/turbounauthenticatedclient) #### getSupportedCurrencies() Returns the list of currencies supported by the Turbo Payment Service for topping up a user balance of AR Credits (measured in Winston Credits, or winc). ```typescript const currencies = await turbo.getSupportedCurrencies(); ``` #### getSupportedCountries() Returns the list of countries supported by the Turbo Payment Service's top up workflow. ```typescript const countries = await turbo.getSupportedCountries(); ``` #### getFiatToAR() Returns the current raw fiat to AR conversion rate for a specific currency as reported by third-party pricing oracles. ```typescript const fiatToAR = await turbo.getFiatToAR({ currency: 'USD' }); ``` #### getFiatRates() Returns the current fiat rates for 1 GiB of data for supported currencies, including all top-up adjustments and fees. ```typescript const rates = await turbo.getFiatRates(); ``` #### getWincForFiat() Returns the current amount of Winston Credits including all adjustments for the provided fiat currency. ```typescript const { winc, actualPaymentAmount, quotedPaymentAmount, adjustments } = await turbo.getWincForFiat({ amount: USD(100), }); ``` #### getWincForToken() Returns the current amount of Winston Credits including all adjustments for the provided token amount. ```typescript const { winc, actualTokenAmount, equivalentWincTokenAmount } = await turbo.getWincForToken({ tokenAmount: WinstonToTokenAmount(100_000_000), }); ``` #### getFiatEstimateForBytes() Get the current price from the Turbo Payment Service, denominated in the specified fiat currency, for uploading a specified number of bytes to Turbo. ```typescript const turbo = TurboFactory.unauthenticated(); const { amount } = await turbo.getFiatEstimateForBytes({ byteCount: 1024 * 1024 * 1024, currency: 'usd', // specify the currency for the price }); console.log(amount); // Estimated usd price for 1 GiB ``` **Output:** ```json { \"byteCount\": 1073741824, \"amount\": 20.58, \"currency\": \"usd\", \"winc\": \"2402378997310\" } ``` #### getTokenPriceForBytes() Get the current price from the Turbo Payment Service, denominated in the specified token, for uploading a specified number of bytes to Turbo. ```typescript const turbo = TurboFactory.unauthenticated({ token: 'solana' }); const { tokenPrice } = await turbo.getTokenPriceForBytes({ byteCount: 1024 * 1024 * 100, }); console.log(tokenPrice); // Estimated SOL Price for 100 MiB ``` #### getUploadCosts() Returns the estimated cost in Winston Credits for the provided file sizes, including all upload adjustments and fees. ```typescript const [uploadCostForFile] = await turbo.getUploadCosts({ bytes: [1024] }); const { winc, adjustments } = uploadCostForFile; ``` #### uploadSignedDataItem() Uploads a signed data item. The provided `dataItemStreamFactory` should produce a NEW signed data item stream each time is it invoked. The `dataItemSizeFactory` is a function that returns the size of the file. The `signal` is an optional [AbortSignal] that can be used to cancel the upload or timeout the request. The `events` parameter is an optional object that can be used to listen to upload progress, errors, and success (refer to the [Events] section for more details). ```typescript const filePath = path.join(__dirname, './my-signed-data-item'); const dataItemSize = fs.statSync(filePath).size; const uploadResponse = await turbo.uploadSignedDataItem({ dataItemStreamFactory: () => fs.createReadStream(filePath), dataItemSizeFactory: () => dataItemSize, signal: AbortSignal.timeout(10_000), // cancel the upload after 10 seconds events: { // track upload events only onUploadProgress: ({ totalBytes, processedBytes }) => { console.log('Upload progress:', { totalBytes, processedBytes }); }, onUploadError: (error) => { console.log('Upload error:', { error }); }, onUploadSuccess: () => { console.log('Upload success!'); }, }, }); ``` #### createCheckoutSession() Creates a Stripe checkout session for a Turbo Top Up with the provided amount, currency, owner. The returned URL can be opened in the browser, all payments are processed by Stripe. To leverage promo codes, see [TurboAuthenticatedClient]. ##### Arweave (AR) Fiat Top Up ```typescript const { url, winc, paymentAmount, quotedPaymentAmount, adjustments } = await turbo.createCheckoutSession({ amount: USD(10.0), // $10.00 USD owner: publicArweaveAddress, // promo codes require an authenticated client }); // Open checkout session in a browser window.open(url, '_blank'); ``` ##### Ethereum (ETH) Fiat Top Up ```typescript const turbo = TurboFactory.unauthenticated({ token: 'ethereum' }); const { url, winc, paymentAmount } = await turbo.createCheckoutSession({ amount: USD(10.0), // $10.00 USD owner: publicEthereumAddress, }); ``` ##### Solana (SOL) Fiat Top Up ```typescript const turbo = TurboFactory.unauthenticated({ token: 'solana' }); const { url, winc, paymentAmount } = await turbo.createCheckoutSession({ amount: USD(10.0), // $10.00 USD owner: publicSolanaAddress, }); ``` ##### Polygon (POL / MATIC) Fiat Top Up ```typescript const turbo = TurboFactory.unauthenticated({ token: 'pol' }); const { url, winc, paymentAmount } = await turbo.createCheckoutSession({ amount: USD(10.0), // $10.00 USD owner: publicPolygonAddress, }); ``` ##### KYVE Fiat Top Up ```typescript const turbo = TurboFactory.unauthenticated({ token: 'kyve' }); const { url, winc, paymentAmount } = await turbo.createCheckoutSession({ amount: USD(10.0), // $10.00 USD owner: publicKyveAddress, }); ``` #### submitFundTransaction() Submits the transaction ID of a funding transaction to Turbo Payment Service for top up processing. The `txId` is the transaction ID of the transaction to be submitted. Use this API if you've already executed your token transfer to the Turbo wallet. Otherwise, consider using `topUpWithTokens` to execute a new token transfer to the Turbo wallet and submit its resulting transaction ID for top up processing all in one go ```typescript const turbo = TurboFactory.unauthenticated(); // defaults to arweave token type const { status, id, ...fundResult } = await turbo.submitFundTransaction({ txId: 'my-valid-arweave-fund-transaction-id', }); ``` # Events (/events) The SDK provides events for tracking the state signing and uploading data to Turbo. You can listen to these events by providing a callback function to the `events` parameter of the `upload`, `uploadFile`, and `uploadSignedDataItem` methods. - `onProgress` - emitted when the overall progress changes (includes both upload and signing). Each event consists of the total bytes, processed bytes, and the step (upload or signing) - `onError` - emitted when the overall upload or signing fails (includes both upload and signing) - `onSuccess` - emitted when the overall upload or signing succeeds (includes both upload and signing) - this is the last event emitted for the upload or signing process - `onSigningProgress` - emitted when the signing progress changes. - `onSigningError` - emitted when the signing fails. - `onSigningSuccess` - emitted when the signing succeeds - `onUploadProgress` - emitted when the upload progress changes - `onUploadError` - emitted when the upload fails - `onUploadSuccess` - emitted when the upload succeeds ```typescript const uploadResult = await turbo.upload({ data: 'The contents of my file!', signal: AbortSignal.timeout(10_000), // cancel the upload after 10 seconds dataItemOpts: { // optional }, events: { // overall events (includes signing and upload events) onProgress: ({ totalBytes, processedBytes, step }) => { const percentComplete = (processedBytes / totalBytes) * 100; console.log('Overall progress:', { totalBytes, processedBytes, step, percentComplete: percentComplete.toFixed(2) + '%', // eg 50.68% }); }, onError: (error) => { console.log('Overall error:', { error }); }, onSuccess: () => { console.log('Signed and upload data item!'); }, // upload events onUploadProgress: ({ totalBytes, processedBytes }) => { console.log('Upload progress:', { totalBytes, processedBytes }); }, onUploadError: (error) => { console.log('Upload error:', { error }); }, onUploadSuccess: () => { console.log('Upload success!'); }, // signing events onSigningProgress: ({ totalBytes, processedBytes }) => { console.log('Signing progress:', { totalBytes, processedBytes }); }, onSigningError: (error) => { console.log('Signing error:', { error }); }, onSigningSuccess: () => { console.log('Signing success!'); }, }, }); ``` # Turbo SDK (/index) **For AI and LLM users**: Access the complete Turbo SDK documentation in plain text format at llm.txt for easy consumption by AI agents and language models. # Turbo SDK Please refer to the [source code](https://github.com/ardriveapp/turbo-sdk) for SDK details. # Logging (/logging) The SDK uses winston for logging. You can set the log level using the `setLogLevel` method. ```typescript TurboFactory.setLogLevel('debug'); ``` # Turbo Credit Sharing (/turbo-credit-sharing) Users can share their purchased Credits with other users' wallets by creating Credit Share Approvals. These approvals are created by uploading a signed data item with tags indicating the recipient's wallet address, the amount of Credits to share, and an optional amount of seconds that the approval will expire in. The recipient can then use the shared Credits to pay for their own uploads to Turbo. Shared Credits cannot be re-shared by the recipient to other recipients. Only the original owner of the Credits can share or revoke Credit Share Approvals. Credits that are shared to other wallets may not be used by the original owner of the Credits for sharing or uploading unless the Credit Share Approval is revoked or expired. Approvals can be revoked at any time by similarly uploading a signed data item with tags indicating the recipient's wallet address. This will remove all approvals and prevent the recipient from using the shared Credits. All unused Credits from expired or revoked approvals are returned to the original owner of the Credits. To use the shared Credits, recipient users must provide the wallet address of the user who shared the Credits with them in the `x-paid-by` HTTP header when uploading data. This tells Turbo services to look for and use Credit Share Approvals to pay for the upload before using the signer's balance. For user convenience, during upload the Turbo CLI will use any available Credit Share Approvals found for the connected wallet before using the signing wallet's balance. To instead ignore all Credit shares and only use the signer's balance, use the `--ignore-approvals` flag. To use the signer's balance first before using Credit shares, use the `--use-signer-balance-first` flag. In contrast, the Turbo SDK layer does not provide this functionality and will only use approvals when `paidBy` is provided. The Turbo SDK provides the following methods to manage Credit Share Approvals: - `shareCredits`: Creates a Credit Share Approval for the specified wallet address and amount of Credits. - `revokeCredits`: Revokes all Credit Share Approvals for the specified wallet address. - `listShares`: Lists all Credit Share Approvals for the specified wallet address or connected wallet. - `dataItemOpts: { ...opts, paidBy: string[] }`: Upload methods now accept 'paidBy', an array of wallet addresses that have provided credit share approvals to the user from which to pay, in the order provided and as necessary, for the upload. The Turbo CLI provides the following commands to manage Credit Share Approvals: - `share-credits`: Creates a Credit Share Approval for the specified wallet address and amount of Credits. - `revoke-credits`: Revokes all Credit Share Approvals for the specified wallet address. - `list-shares`: Lists all Credit Share Approvals for the specified wallet address or connected wallet. - `paidBy: --paid-by `: Upload commands now accept '--paid-by', an array of wallet addresses that have provided credit share approvals to the user from which to pay, in the order provided and as necessary, for the upload. - `--ignore-approvals`: Ignore all Credit Share Approvals and only use the signer's balance. - `--use-signer-balance-first`: Use the signer's balance first before using Credit Share Approvals.","estimatedWords":3505,"lastModified":"2025-10-20T12:11:47.322Z","breadcrumbs":["sdks","turbo sdk","llm.txt"],"siteKey":"ario","siteName":"AR-IO Network","depth":3,"crawledAt":"2025-10-20T12:11:47.322Z"},{"url":"https://docs.ar.io/sdks/ar-io-sdk/pagination","title":"Pagination","content":"AR.IO SDKPaginationCopy MarkdownOpenOverview Certain APIs that could return a large amount of data are paginated using cursors. The SDK uses the cursor pattern (as opposed to pages) to better protect against changing data while paginating through a list of items. For more information on pagination strategies refer to this article. Paginated results include the following properties: items: the list of items on the current request, defaulted to 100 items. nextCursor: the cursor to use for the next batch of items. This is undefined if there are no more items to fetch. hasMore: a boolean indicating if there are more items to fetch. This is false if there are no more items to fetch. totalItems: the total number of items available. This may change as new items are added to the list, only use this for informational purposes. sortBy: the field used to sort the items, by default this is startTimestamp. sortOrder: the order used to sort the items, by default this is desc. To request all the items in a list, you can iterate through the list using the nextCursor until hasMore is false. let hasMore = true; let cursor: string | undefined; while (hasMore) { gateaways.push(...items); cursor = page.nextCursor; hasMore = page.hasMore; } Filtering Paginated APIs also support filtering by providing a filters parameter. Filters can be applied to any field in the response. When multiple keys are provided, they are treated as AND conditions (all conditions must match). When multiple values are provided for a single key (as an array), they are treated as OR conditions (any value can match). Example: In the example above, the query will return ArNS records where: The type is \"lease\" AND The processId is EITHER \"ZkgLfyHALs5koxzojpcsEFAKA8fbpzP7l-tbM7wmQNM\" OR \"r61rbOjyXx3u644nGl9bkwLWlWmArMEzQgxBo2R-Vu0\" How is this guide?GoodBadLoggingTypeScript/JavaScript SDK for interacting with the AR.IO ecosystemTurbo SDKSDK for interacting with Turbo, a fast and efficient data upload service for Arweave","estimatedWords":311,"lastModified":"2025-10-20T12:11:48.304Z","breadcrumbs":["sdks","ar io sdk","pagination"],"siteKey":"ario","siteName":"AR-IO Network","depth":3,"crawledAt":"2025-10-20T12:11:48.304Z"},{"url":"https://docs.ar.io/sdks/turbo-sdk/turbofactory","title":"TurboFactory","content":"Turbo SDKAPIsTurboFactoryCopy MarkdownOpenunauthenticated() Creates an instance of a client that accesses Turbo's unauthenticated services. authenticated() Creates an instance of a client that accesses Turbo's authenticated and unauthenticated services. Requires either a signer, or private key to be provided. Arweave JWK ArweaveSigner ArconnectSigner EthereumSigner Ethereum Private Key POL (MATIC) Private Key HexSolanaSigner Solana Web Wallet Adapter Solana Secret Key KYVE Private Key KYVE Mnemonic import { privateKeyFromKyveMnemonic } from '@ardrive/turbo-sdk'; How is this guide?GoodBadTurbo SDKSDK for interacting with Turbo, a fast and efficient data upload service for ArweaveTurboUnauthenticatedClientSDK for interacting with Turbo, a fast and efficient data upload service for Arweave","estimatedWords":100,"lastModified":"2025-10-20T12:11:49.669Z","breadcrumbs":["sdks","turbo sdk","turbofactory"],"siteKey":"ario","siteName":"AR-IO Network","depth":3,"crawledAt":"2025-10-20T12:11:49.669Z"},{"url":"https://docs.ar.io/apis/turbo/upload-service/pricing","title":"Pricing","content":"TurboUpload ServicePricingCopy MarkdownOpen Pricing calculation endpoints Gets the price in winc to upload a data itemloading...GET/price/:token/:byteCountSendPathPath ParameterstokenstringThe token to use for validating the transaction.Value in\"arweave\" | \"ethereum\" | \"solana\"byteCountintegerThe byte count of the data item.Response Body200400cURLJavaScriptGoPythonJavaC#curl -X GET \"https:","estimatedWords":39,"lastModified":"2025-10-20T12:11:54.472Z","breadcrumbs":["apis","turbo","upload service","pricing"],"siteKey":"ario","siteName":"AR-IO Network","depth":3,"crawledAt":"2025-10-20T12:11:54.472Z"},{"url":"https://docs.ar.io/apis/turbo/payment-service/balance","title":"Balance","content":"TurboPayment ServiceBalanceCopy MarkdownOpen Account balance and credit management Get Current Balance of wincUse a signed request or a previously obtained JWT to get the signing wallet's current service balance in wincloading...GET/balanceSendHeaderQueryQuery Parametersaddress?stringDestination wallet address, required if no signature headers are providedHeader Parametersx-signature?stringThe signature value derived from signing the request's data concatenated with the provided nonce using the private key from the provided public keyx-nonce?stringThe nonce value concatenated with the request's data when deriving the provided the signaturex-public-key?stringThe \"modulus\" of the JWK used to create the signature headerResponse Body200403404503cURLJavaScriptGoPythonJavaC#curl -X GET \"https:","estimatedWords":91,"lastModified":"2025-10-20T12:11:56.757Z","breadcrumbs":["apis","turbo","payment service","balance"],"siteKey":"ario","siteName":"AR-IO Network","depth":3,"crawledAt":"2025-10-20T12:11:56.757Z"},{"url":"https://docs.ar.io/apis/turbo/payment-service/payments","title":"Payments","content":"TurboPayment ServicePaymentsCopy MarkdownOpen Payment processing and top-up operations Get Top Up Quote for CreditsGet a top up quote and payment session for a given method (payment-intent or checkout-session), destination address, currency type, and payment amountloading...GET/top-up/{method}/{address}/{currency}/{amount}SendPathHeaderQueryPath ParametersmethodstringaddressstringDestination wallet addresscurrencystringCurrency type for a given payment amountamountintegerPayment amount in a given currency's smallest unit value. For example, $10 USD is 1000. 1 AR is 1000000000000Query ParameterspromoCode?stringComma-separated list of promo codesuiMode?stringWhich UI Mode to create the checkout session inDefault\"hosted\"returnUrl?stringThe URL to return to after a successful paymentDefault\"https:","estimatedWords":83,"lastModified":"2025-10-20T12:11:57.971Z","breadcrumbs":["apis","turbo","payment service","payments"],"siteKey":"ario","siteName":"AR-IO Network","depth":3,"crawledAt":"2025-10-20T12:11:57.971Z"},{"url":"https://docs.ar.io/apis/turbo/payment-service/pricing","title":"Pricing","content":"TurboPayment ServicePricingCopy MarkdownOpen Pricing and cost calculation endpoints Get Amount of Credits in winc for Byte CountReturns the current amount of winc it will cost to upload a given byte count worth of data itemsloading...GET/price/bytes/{byteCount}SendPathPath ParametersbyteCountintegerA positive integer representing a byte count of data itemsMatch^[0-9]+$Response Body200400503cURLJavaScriptGoPythonJavaC#curl -X GET \"https:","estimatedWords":49,"lastModified":"2025-10-20T12:11:59.082Z","breadcrumbs":["apis","turbo","payment service","pricing"],"siteKey":"ario","siteName":"AR-IO Network","depth":3,"crawledAt":"2025-10-20T12:11:59.082Z"},{"url":"https://docs.ar.io/apis/turbo/payment-service/approvals","title":"Approvals","content":"TurboPayment ServiceApprovalsCopy MarkdownOpen Credit sharing and approval management Get credit share approvals for a given payingAddress and approvedAddressGet credit share approvals for a given payingAddress and approvedAddressloading...GET/account/approvalsSendQueryQuery ParameterspayingAddressstringPaying wallet addressapprovedAddressstringApproved wallet addressResponse Body200400503cURLJavaScriptGoPythonJavaC#curl -X GET \"https:","estimatedWords":36,"lastModified":"2025-10-20T12:12:01.317Z","breadcrumbs":["apis","turbo","payment service","approvals"],"siteKey":"ario","siteName":"AR-IO Network","depth":3,"crawledAt":"2025-10-20T12:12:01.317Z"},{"url":"https://docs.ar.io/apis/turbo/payment-service/redemption","title":"Redemption","content":"TurboPayment ServiceRedemptionCopy MarkdownOpen Credit redemption and gift processing Redeem credits gifted via emailRedeem credits gifted via email by providing the destination wallet address for the credits, the redemption ID, and recipient email addressloading...GET/redeemSendQueryQuery ParametersdestinationAddressstringDestination wallet addressidstringID for the redemptionemailstringRecipient email address for the redemptionResponse Body200400503cURLJavaScriptGoPythonJavaC#curl -X GET \"https:","estimatedWords":48,"lastModified":"2025-10-20T12:12:01.982Z","breadcrumbs":["apis","turbo","payment service","redemption"],"siteKey":"ario","siteName":"AR-IO Network","depth":3,"crawledAt":"2025-10-20T12:12:01.982Z"},{"url":"https://docs.ar.io/apis/ar-io-node/farcaster-frames","title":"Farcaster Frames","content":"AR.IO GatewayFarcaster FramesCopy MarkdownOpen Retrieve and interact with Farcaster Frames using Arweave transactions. Handle a Farcaster initial Frame request.Responds to a Farcaster initial Frame GET request by returning the content of a specified Arweave transaction or data item.loading...GET/local/farcaster/frame/{txId}SendPathPath ParameterstxIdstringMatch^[0-9a-zA-Z_-]{43}$Response Body200cURLJavaScriptGoPythonJavaC#curl -X GET \"https:","estimatedWords":43,"lastModified":"2025-10-20T12:12:04.058Z","breadcrumbs":["apis","ar io node","farcaster frames"],"siteKey":"ario","siteName":"AR-IO Network","depth":3,"crawledAt":"2025-10-20T12:12:04.058Z"},{"url":"https://docs.ar.io/apis/ar-io-node/data","title":"Data","content":"AR.IO GatewayDataCopy MarkdownOpen Core data retrieval operations for accessing transaction and data item content. Supports manifest resolution, range requests, caching, and verification status. These endpoints serve as the primary interface for retrieving data from the Permaweb. Get transaction or data item contentGet the content of a specified transaction or data item. Supports manifest path resolution, range requests, and returns various informational headers about data verification and caching status.loading...GET/{txId}SendPathHeaderPath ParameterstxIdstringMatch^[0-9a-zA-Z_-]{43}$Header ParametersRange?stringByte range(s) to retrieveMatch^bytes=\\d*-\\d*(?:,\\d*-\\d*)*$Response Body200206301404416cURLJavaScriptGoPythonJavaC#curl -X GET \"https:","estimatedWords":77,"lastModified":"2025-10-20T12:12:05.421Z","breadcrumbs":["apis","ar io node","data"],"siteKey":"ario","siteName":"AR-IO Network","depth":3,"crawledAt":"2025-10-20T12:12:05.421Z"},{"url":"https://docs.ar.io/apis/ar-io-node/blocks","title":"Blocks","content":"AR.IO GatewayBlocksCopy MarkdownOpen Get current or historical Arweave block information Get information about the current block.Get detailed information about the current block on the Arweave network.loading...GET/current_blockSendResponse Body200cURLJavaScriptGoPythonJavaC#curl -X GET \"https:","estimatedWords":30,"lastModified":"2025-10-20T12:12:09.457Z","breadcrumbs":["apis","ar io node","blocks"],"siteKey":"ario","siteName":"AR-IO Network","depth":3,"crawledAt":"2025-10-20T12:12:09.457Z"},{"url":"https://docs.ar.io/apis/ar-io-node/chunks","title":"Chunks","content":"AR.IO GatewayChunksCopy MarkdownOpen Upload Arweave data chunks or get existing chunk offset information Get chunk offset information.Fetches information about the size and offset of a specified chunk.loading...GET/chunk/{offset}SendPathHeaderPath ParametersoffsetintegerHeader ParametersIf-None-Match?stringETag value for conditional request. Returns 304 if chunk hash matches.Response Body200304400404cURLJavaScriptGoPythonJavaC#curl -X GET \"https:","estimatedWords":43,"lastModified":"2025-10-20T12:12:10.433Z","breadcrumbs":["apis","ar io node","chunks"],"siteKey":"ario","siteName":"AR-IO Network","depth":3,"crawledAt":"2025-10-20T12:12:10.433Z"},{"url":"https://docs.ar.io/apis/ar-io-node/wallets","title":"Wallets","content":"AR.IO GatewayWalletsCopy MarkdownOpen Get Arweave wallet balance and last transaction information Get the current balance of a wallet.Get the current balance of AR, in Winston, of a specified wallet address.loading...GET/wallet/{address}/balanceSendPathPath ParametersaddressstringMatch^[0-9a-zA-Z_-]{43}$Response Body200cURLJavaScriptGoPythonJavaC#curl -X GET \"https:","estimatedWords":35,"lastModified":"2025-10-20T12:12:11.432Z","breadcrumbs":["apis","ar io node","wallets"],"siteKey":"ario","siteName":"AR-IO Network","depth":3,"crawledAt":"2025-10-20T12:12:11.432Z"},{"url":"https://docs.ar.io/apis/ar-io-node/gateway","title":"Gateway","content":"AR.IO GatewayGatewayCopy MarkdownOpen Operations related to the AR.IO Gateway server itself, including health checks, metrics, and gateway-specific information Gateway Info or Apex ContentReturns either gateway information or serves content based on configuration: If neither APEX_TX_ID nor APEX_ARNS_NAME is set, returns gateway information If APEX_TX_ID is set, serves that transaction's content If APEX_ARNS_NAME is set, resolves and serves that ArNS name's content The Content-Type of the response will match the content type of the transaction or ArNS-resolved data (e.g., text/html for HTML documents, application/json for JSON documents, application/octet-stream for binary data, etc.).loading...GET/SendResponse Body200cURLJavaScriptGoPythonJavaC#curl -X GET \"https:","estimatedWords":95,"lastModified":"2025-10-20T12:12:13.639Z","breadcrumbs":["apis","ar io node","gateway"],"siteKey":"ario","siteName":"AR-IO Network","depth":3,"crawledAt":"2025-10-20T12:12:13.639Z"},{"url":"https://docs.ar.io/apis/ar-io-node/index-querying","title":"Index Querying","content":"AR.IO GatewayIndex QueryingCopy MarkdownOpen Get data from the AR.IO Gateway index using GQL Query indexed data using GraphQLGraphQL endpoint for querying indexed transaction and block data. Supports: Transaction queries by ID, owner, recipient, tags, and bundle Block queries by ID and height Pagination and sorting Rich metadata including sizes, content types, and signatures See the GraphQL Playground at /graphql for full schema documentation and interactive querying.loading...POST/graphqlSendBodyRequest BodyquerystringGraphQL query stringvariables?objectQuery variablesEmpty ObjectResponse Body200400500cURLJavaScriptGoPythonJavaC#curl -X POST \"https:","estimatedWords":75,"lastModified":"2025-10-20T12:12:14.783Z","breadcrumbs":["apis","ar io node","index querying"],"siteKey":"ario","siteName":"AR-IO Network","depth":3,"crawledAt":"2025-10-20T12:12:14.783Z"},{"url":"https://docs.ar.io/apis/ar-io-node/pricing","title":"Pricing","content":"AR.IO GatewayPricingCopy MarkdownOpen Get the price (in winston) for an amount of bytes Get price to store an amount of data.Get the price, in Winston, required to store a specified amount of data, in bytes, on the Arweave network.loading...GET/price/{size}SendPathPath ParameterssizestringMatch^[0-9]+$Response Body200cURLJavaScriptGoPythonJavaC#curl -X GET \"https:","estimatedWords":44,"lastModified":"2025-10-20T12:12:15.813Z","breadcrumbs":["apis","ar io node","pricing"],"siteKey":"ario","siteName":"AR-IO Network","depth":3,"crawledAt":"2025-10-20T12:12:15.813Z"},{"url":"https://docs.ar.io/apis/ar-io-node/admin","title":"Admin","content":"AR.IO GatewayAdminCopy MarkdownOpen Access several password protected features and functions specific to your AR.IO Gateway. Admin debugging information.Get detailed information about the current operational state of your AR.IO Gateway, including information about any current warnings or errors.loading...GET/ar-io/admin/debugSendAuthorizationAuthorizationAuthorizationBearer ADMIN_API_KEY set in your .env file.In: headerResponse Body200401cURLJavaScriptGoPythonJavaC#curl -X GET \"https:","estimatedWords":48,"lastModified":"2025-10-20T12:12:17.187Z","breadcrumbs":["apis","ar io node","admin"],"siteKey":"ario","siteName":"AR-IO Network","depth":3,"crawledAt":"2025-10-20T12:12:17.187Z"},{"url":"https://docs.ar.io/build/guides/depin","title":"Storing DePIN Data on Arweave Using Turbo","content":"GuidesStoring DePIN Data on Arweave Using TurboCopy MarkdownOpenDePIN networks require scalable and cost-effective storage solutions they can trust. With vast amounts of data generated by decentralized physical infrastructure networks, traditional on-chain storage is prohibitively expensive, yet networks need reliable, long-term access to their device data. Arweave via AR.IO Network provides chain-agnostic, permanent and immutable storage for a one-time fee, ensuring networks can access any device data previously stored and verify it has not been tampered with. Getting Started with DePIN Data Storage Prepare Your Data StructureOrganize your DePIN device data in a consistent format. Here's an example for environmental sensor data:{ \"device_id\": \"airmon-007\", \"timestamp\": \"2025-09-22T14:31:05Z\", \"location\": { \"lat\": 51.5098, \"lon\": -0.118 }, \"pm25\": 16, \"co2_ppm\": 412, \"noise_dB\": 41.2 }Best Practices: Use consistent field names across all devices Include timestamps in ISO format Add device identifiers for tracking Consider data compression for large datasets Tag Your Data for DiscoveryProper tagging is essential for finding your data later. Consider these tags for DePIN data:{ \"name\": \"App-Name\", \"value\": \"AirQuality-DePIN-v1.0\" }, { \"name\": \"Device-ID\", \"value\": \"airmon-007\" }, { \"name\": \"Device-Type\", \"value\": \"Environmental-Sensor\" }, { \"name\": \"Network-Name\", \"value\": \"AirQuality-Network\" }, { \"name\": \"Data-Category\", \"value\": \"Air-Quality\" }, { \"name\": \"Location\", \"value\": \"London-UK\" }, { \"name\": \"Device-Timestamp\", \"value\": \"2025-09-22T14:31:05Z\" }Tagging Strategy: Use consistent naming conventions Include geographic identifiers Add device type classifications Include data categories for filtering For more detailed information on tagging, see our Tagging documentationUpload to ArweaveSelect the best method for your DePIN network's needs:Turbo SDKTurbo CLIimport { TurboFactory } from '@ardrive/turbo-sdk'","estimatedWords":247,"lastModified":"2025-10-20T12:12:19.578Z","breadcrumbs":["build","guides","depin"],"siteKey":"ario","siteName":"AR-IO Network","depth":4,"crawledAt":"2025-10-20T12:12:19.578Z"},{"url":"https://docs.ar.io/build/guides/deploy-dapp-with-ardrive-web","title":"Deploy a dApp with ArDrive Web","content":"GuidesDeploy a dApp with ArDrive WebCopy MarkdownOpenCreate permanent dApps using the ArDrive web interface. This guide shows you how to deploy your dApp or website to the permaweb using ArDrive's user-friendly interface. What You'll Learn How to deploy dApps using ArDrive web Creating manifests for proper file routing Assigning friendly ArNS names Updating your dApp with new versions Prerequisites For simple apps and websites: Your dApp files ready for deployment ArDrive account (free to create) For advanced applications: dApp prepared with hash routing and relative file paths Static files built (for frameworks like React) Learn more about preparing your dApp for deployment Step-by-Step Deployment Log into ArDriveGo to the ArDrive web app and log in using your preferred method. If you don't have an account, follow the instructions to create one.Select or Create a DriveNavigate to the drive where you want your project hosted. If you need a new drive: Click the big red \"New\" button at the top left Create a new drive Important: Set the drive to public for others to access your dApp Upload Your ProjectWith your drive selected: Click the big red \"New\" button again Select \"Upload Folder\" Navigate to your project's root directory (or built directory if required) Select the entire directory to maintain your project's file structure Confirm UploadReview the upload and associated cost. If everything looks correct, click \"Confirm\".Cost Note: Uploading to Arweave isn't free, but costs are usually quite small compared to the benefits of permanent hosting.Create the ManifestWhile ArDrive displays files as a traditional file structure, they don't actually exist that way on Arweave. The manifest acts as a map to all your dApp files: Navigate into your newly created folder by double-clicking it Click the big red \"New\" button again Select \"New Manifest\" in the \"Advanced\" section Name the manifest and save it inside the folder you just created Get the Data TX IDOnce the manifest is created: Click on it to expand its details Go to the \"Details\" tab Find the \"Data TX ID\" on the bottom right Copy this unique identifier for your dApp View and Share Your dAppYour dApp is now live on the permaweb forever! Append the Data TX ID to an Arweave gateway URL: https:","estimatedWords":370,"lastModified":"2025-10-20T12:12:20.466Z","breadcrumbs":["build","guides","deploy dapp with ardrive web"],"siteKey":"ario","siteName":"AR-IO Network","depth":4,"crawledAt":"2025-10-20T12:12:20.466Z"},{"url":"https://docs.ar.io/build/guides/crossmint-nft-minting-app","title":"Crossmint NFT Minting App","content":"GuidesCrossmint NFT Minting AppCopy MarkdownOpenBuild a completely decentralized NFT minting app that leverages the power of Arweave for permanent storage and Crossmint for simplified NFT creation. Learn how to store NFT content permanently, create and mint NFTs, build a frontend with authentication and payment options, and deploy your application to Arweave. What You'll Learn How to store NFT content permanently on Arweave How to create and mint NFTs using Crossmint's API How to build a frontend with authentication and payment options How to deploy your application to Arweave How to configure a human-readable ArNS domain Example Project Live Demo: https:","estimatedWords":100,"lastModified":"2025-10-20T12:12:21.595Z","breadcrumbs":["build","guides","crossmint nft minting app"],"siteKey":"ario","siteName":"AR-IO Network","depth":4,"crawledAt":"2025-10-20T12:12:21.595Z"},{"url":"https://docs.ar.io/build/guides/using-turbo-in-a-browser","title":"Using Turbo in a Browser","content":"GuidesUsing Turbo in a BrowserCopy MarkdownOpenUsing Turbo in a Browser Integrate the Turbo SDK directly into your web applications for fast, reliable data uploads to Arweave. Choose the approach that best fits your development workflow and framework preferences. What You Can Build With Turbo SDK in browsers, you can: Upload files directly from web applications to Arweave Pay with different tokens (AR, Ethereum, and more) Integrate with popular wallets (MetaMask, Wander, ArConnect) Build permanent web apps that store data on Arweave Create data marketplaces and decentralized applications Getting Started Vanilla HTMLStart with the simplest approach - no build tools requiredKey topics: CDN imports for instant setup Wallet integration examples Production deployment considerations Error handling and troubleshooting Next.jsFull-stack React applications with server-side renderingKey topics: Webpack polyfill configuration Client-side component setup TypeScript integration Production optimization ViteFast development with modern build toolsKey topics: Vite plugin configuration React and TypeScript setup Hot module replacement Bundle optimization Why Use Turbo SDK? Fast uploads - Upload data to Arweave in seconds, not minutes Multiple payment options - Pay with AR, Ethereum, or other supported tokens Wallet integration - Seamlessly connect with popular browser wallets Reliable infrastructure - Built on Arweave's permanent storage network Developer-friendly - Simple APIs with comprehensive documentationHow is this guide?GoodBadCrossmint NFT Minting AppBuild a decentralized NFT minting app with Arweave and CrossmintUsing Turbo SDK with Vanilla HTMLIntegrate Turbo SDK directly into vanilla HTML pages using CDN imports","estimatedWords":234,"lastModified":"2025-10-20T12:12:23.415Z","breadcrumbs":["build","guides","using turbo in a browser"],"siteKey":"ario","siteName":"AR-IO Network","depth":4,"crawledAt":"2025-10-20T12:12:23.415Z"},{"url":"https://docs.ar.io/build/upload/manifests","title":"Manifests","content":"Upload DataManifestsCopy MarkdownOpenManifests enable friendly-path-name routing for data on Arweave, greatly improving the programmability of data relationships. Instead of accessing data with complex transaction IDs, manifests allow you to organize files with readable paths and relative links. What are Manifests? Manifests, also known as \"Path Manifests\" or \"Arweave Manifests,\" are JSON objects that connect various Arweave data items and define relational paths for easy navigation. A common use case is permanently hosting websites on Arweave by linking all necessary files together. The Problem Manifests Solve Without manifests, accessing data on Arweave looks like this: http:","estimatedWords":95,"lastModified":"2025-10-20T12:12:24.799Z","breadcrumbs":["build","upload","manifests"],"siteKey":"ario","siteName":"AR-IO Network","depth":4,"crawledAt":"2025-10-20T12:12:24.799Z"},{"url":"https://docs.ar.io/sdks/ar-io-sdk/token-conversion","title":"Token Conversion","content":"AR.IO SDKToken ConversionCopy MarkdownOpenThe ARIO process stores all values as mARIO (milli-ARIO) to avoid floating-point arithmetic issues. The SDK provides an ARIOToken and mARIOToken classes to handle the conversion between ARIO and mARIO, along with rounding logic for precision. All process interactions expect values in mARIO. If numbers are provided as inputs, they are assumed to be in raw mARIO values. Converting ARIO to mARIO import { ARIOToken, mARIOToken } from '@ar.io/sdk'; How is this guide?GoodBadStatic MethodsTypeScript/JavaScript SDK for interacting with the AR.IO ecosystemLoggingTypeScript/JavaScript SDK for interacting with the AR.IO ecosystem","estimatedWords":91,"lastModified":"2025-10-20T12:12:25.721Z","breadcrumbs":["sdks","ar io sdk","token conversion"],"siteKey":"ario","siteName":"AR-IO Network","depth":3,"crawledAt":"2025-10-20T12:12:25.721Z"},{"url":"https://docs.ar.io/sdks/ar-io-sdk/logging","title":"Logging","content":"AR.IO SDKLoggingCopy MarkdownOpenThe library uses a lightweight console logger by default for both Node.js and web environments. The logger outputs structured JSON logs with timestamps. You can configure the log level via setLogLevel() API or provide a custom logger that satisfies the ILogger interface. Default Logger import { Logger } from '@ar.io/sdk';","estimatedWords":52,"lastModified":"2025-10-20T12:12:26.681Z","breadcrumbs":["sdks","ar io sdk","logging"],"siteKey":"ario","siteName":"AR-IO Network","depth":3,"crawledAt":"2025-10-20T12:12:26.681Z"},{"url":"https://docs.ar.io/sdks/ar-io-sdk/llm.txt","title":"Llmtxt","content":"# ARIO Integrations (/(ant-contracts)/ario-integrations) #### releaseName() Releases a name from the current owner and makes it available for purchase on the ARIO contract. The name must be permanently owned by the releasing wallet. If purchased within the recently returned name period (14 epochs), 50% of the purchase amount will be distributed to the ANT owner at the time of release. If no purchases in the recently returned name period, the name can be reregistered by anyone for the normal fee. _Note: Requires `signer` to be provided on `ANT.init` to sign the transaction._ ```typescript const { id: txId } = await ant.releaseName({ name: 'permalink', arioProcessId: ARIO_MAINNET_PROCESS_ID, // releases the name owned by the ANT and sends it to recently returned names on the ARIO contract }); ``` #### reassignName() Reassigns a name to a new ANT. This can only be done by the current owner of the ANT. _Note: Requires `signer` to be provided on `ANT.init` to sign the transaction._ ```typescript const { id: txId } = await ant.reassignName({ name: 'ardrive', arioProcessId: ARIO_MAINNET_PROCESS_ID, antProcessId: NEW_ANT_PROCESS_ID, // the new ANT process id that will take over ownership of the name }); ``` #### approvePrimaryNameRequest() Approves a primary name request for a given name or address. _Note: Requires `signer` to be provided on `ANT.init` to sign the transaction._ ```typescript const { id: txId } = await ant.approvePrimaryNameRequest({ name: 'arns', address: 't4Xr0_J4Iurt7caNST02cMotaz2FIbWQ4Kbj616RHl3', // must match the request initiator address arioProcessId: ARIO_MAINNET_PROCESS_ID, // the ARIO process id to use for the request }); ``` #### removePrimaryNames() Removes primary names from the ANT process. _Note: Requires `signer` to be provided on `ANT.init` to sign the transaction._ ```typescript const { id: txId } = await ant.removePrimaryNames({ names: ['arns', 'test_arns'], // any primary names associated with a base name controlled by this ANT will be removed arioProcessId: ARIO_MAINNET_PROCESS_ID, notifyOwners: true, // if true, the owners of the removed names will be send AO messages to notify them of the removal }); ``` # Balances (/(ant-contracts)/balances) #### getBalances() Returns all token balances for the ANT. ```typescript const balances = await ant.getBalances(); ``` **Output:** ```json { \"ccp3blG__gKUvG3hsGC2u06aDmqv4CuhuDJGOIg0jw4\": 1, \"aGzM_yjralacHIUo8_nQXMbh9l1cy0aksiL_x9M359f\": 0 } ``` #### getBalance() Returns the balance of a specific address. ```typescript const balance = await ant.getBalance({ address: 'ccp3blG__gKUvG3hsGC2u06aDmqv4CuhuDJGOIg0jw4', }); ``` **Output:** ```json 1 ``` # Controllers (/(ant-contracts)/controllers) #### addController() Adds a new controller to the list of approved controllers on the ANT. Controllers can set records and change the ticker and name of the ANT process. _Note: Requires `signer` to be provided on `ANT.init` to sign the transaction._ ```typescript const { id: txId } = await ant.addController( { controller: 'aGzM_yjralacHIUo8_nQXMbh9l1cy0aksiL_x9M359f' }, // optional additional tags { tags: [{ name: 'App-Name', value: 'My-Awesome-App' }] }, ); ``` #### removeController() Removes a controller from the list of approved controllers on the ANT. _Note: Requires `signer` to be provided on `ANT.init` to sign the transaction._ ```typescript const { id: txId } = await ant.removeController( { controller: 'aGzM_yjralacHIUo8_nQXMbh9l1cy0aksiL_x9M359f' }, // optional additional tags { tags: [{ name: 'App-Name', value: 'My-Awesome-App' }] }, ); ``` # Initialize (/(ant-contracts)/initialize) #### init() Factory function to that creates a read-only or writeable client. By providing a `signer` additional write APIs that require signing, like `setRecord` and `transfer` are available. By default, a read-only client is returned and no write APIs are available. ```typescript // in a browser environment with ArConnect const ant = ANT.init({ signer: new ArConnectSigner(window.arweaveWallet, Arweave.init({})), processId: 'bh9l1cy0aksiL_x9M359faGzM_yjralacHIUo8_nQXM' }); // in a node environment const ant = ANT.init({ signer: new ArweaveSigner(JWK), processId: 'bh9l1cy0aksiL_x9M359faGzM_yjralacHIUo8_nQXM' }); ``` # Metadata (/(ant-contracts)/metadata) #### setName() Sets the name of the ANT process. _Note: Requires `signer` to be provided on `ANT.init` to sign the transaction._ ```typescript const { id: txId } = await ant.setName( { name: 'My ANT' }, // optional additional tags { tags: [{ name: 'App-Name', value: 'My-Awesome-App' }] }, ); ``` #### setTicker() Sets the ticker of the ANT process. _Note: Requires `signer` to be provided on `ANT.init` to sign the transaction._ ```typescript const { id: txId } = await ant.setTicker( { ticker: 'ANT-NEW-TICKER' }, // optional tags { tags: [{ name: 'App-Name', value: 'My-Awesome-App' }] }, ); ``` #### setDescription() Sets the description of the ANT process. _Note: Requires `signer` to be provided on `ANT.init` to sign the transaction._ ```typescript const { id: txId } = await ant.setDescription( { description: 'A friendly description of this ANT' }, // optional tags { tags: [{ name: 'App-Name', value: 'My-Awesome-App' }] }, ); ``` #### setKeywords() Sets the keywords of the ANT process. _Note: Requires `signer` to be provided on `ANT.init` to sign the transaction._ ```typescript const { id: txId } = await ant.setKeywords( { keywords: ['Game', 'FPS', 'AO'] }, // optional tags { tags: [{ name: 'App-Name', value: 'My-Awesome-App' }] }, ); ``` #### getLogo() Returns the TX ID of the logo set for the ANT. ```typescript const logoTxId = await ant.getLogo(); ``` #### setLogo() Sets the Logo of the ANT - logo should be an Arweave transaction ID. _Note: Requires `signer` to be provided on `ANT.init` to sign the transaction._ ```typescript const { id: txId } = await ant.setLogo( { txId: 'U7RXcpaVShG4u9nIcPVmm2FJSM5Gru9gQCIiRaIPV7f' }, // optional tags { tags: [{ name: 'App-Name', value: 'My-Awesome-App' }] }, ); ``` # Records (/(ant-contracts)/records) #### setBaseNameRecord() Adds or updates the base name record for the ANT. This is the top level name of the ANT (e.g. ardrive.ar.io). Supports undername ownership delegation and metadata. _Note: Requires `signer` to be provided on `ANT.init` to sign the transaction._ ```typescript // get the ant for the base name const arnsRecord = await ario.getArNSRecord({ name: 'ardrive' }); const ant = await ANT.init({ processId: arnsName.processId }); // Basic usage const { id: txId } = await ant.setBaseNameRecord({ transactionId: '432l1cy0aksiL_x9M359faGzM_yjralacHIUo8_nQXM', ttlSeconds: 3600, }); // With ownership delegation and metadata const { id: txId } = await ant.setBaseNameRecord({ transactionId: '432l1cy0aksiL_x9M359faGzM_yjralacHIUo8_nQXM', ttlSeconds: 3600, owner: 'user-wallet-address-123...', // delegate ownership to another address displayName: 'ArDrive', // display name logo: 'logo-tx-id-123...', // logo transaction ID description: 'Decentralized storage application', keywords: ['storage', 'decentralized', 'web3'], }); // ardrive.ar.io will now resolve to the provided transaction id and include metadata ``` #### setUndernameRecord() Adds or updates an undername record for the ANT. An undername is appended to the base name of the ANT (e.g. dapp_ardrive.ar.io). Supports undername ownership delegation and metadata. _Note: Requires `signer` to be provided on `ANT.init` to sign the transaction._ > Records, or `undernames` are configured with the `transactionId` - the arweave transaction id the record resolves - and `ttlSeconds`, the Time To Live in the cache of client applications. ```typescript const arnsRecord = await ario.getArNSRecord({ name: 'ardrive' }); const ant = await ANT.init({ processId: arnsName.processId }); // Basic usage const { id: txId } = await ant.setUndernameRecord( { undername: 'dapp', transactionId: '432l1cy0aksiL_x9M359faGzM_yjralacHIUo8_nQXM', ttlSeconds: 900, }, // optional additional tags { tags: [{ name: 'App-Name', value: 'My-Awesome-App' }] }, ); // With ownership delegation and metadata const { id: txId } = await ant.setUndernameRecord( { undername: 'alice', transactionId: '432l1cy0aksiL_x9M359faGzM_yjralacHIUo8_nQXM', ttlSeconds: 900, owner: 'alice-wallet-address-123...', // delegate ownership to Alice displayName: \"Alice's Site\", // display name logo: 'avatar-tx-id-123...', // avatar/logo transaction ID description: 'Personal portfolio and blog', keywords: ['portfolio', 'personal', 'blog'], }, { tags: [{ name: 'App-Name', value: 'My-Awesome-App' }] }, ); // dapp_ardrive.ar.io will now resolve to the provided transaction id // alice_ardrive.ar.io will be owned by Alice and include metadata ``` #### removeUndernameRecord() Removes an undername record from the ANT process. _Note: Requires `signer` to be provided on `ANT.init` to sign the transaction._ ```typescript const { id: txId } = await ant.removeUndernameRecord( { undername: 'dapp' }, // optional additional tags { tags: [{ name: 'App-Name', value: 'My-Awesome-App' }] }, ); // dapp_ardrive.ar.io will no longer resolve to the provided transaction id ``` #### setRecord() Deprecated: Use `setBaseNameRecord` or `setUndernameRecord` instead. Adds or updates a record for the ANT process. The `undername` parameter is used to specify the record name. Use `@` for the base name record. _Note: Requires `signer` to be provided on `ANT.init` to sign the transaction._ > Records, or `undernames` are configured with the `transactionId` - the arweave transaction id the record resolves - and `ttlSeconds`, the Time To Live in the cache of client applications. ```typescript const { id: txId } = await ant.setRecord( { undername: '@', transactionId: '432l1cy0aksiL_x9M359faGzM_yjralacHIUo8_nQXM' ttlSeconds: 3600 }, // optional additional tags { tags: [{ name: 'App-Name', value: 'My-Awesome-App' }] }, ); ``` #### removeRecord() Deprecated: Use `removeUndernameRecord` instead. Removes a record from the ANT process. _Note: Requires `signer` to be provided on `ANT.init` to sign the transaction._ ```typescript const arnsRecord = await ario.getArNSRecord({ name: 'ardrive' }); const ant = await ANT.init({ processId: arnsName.processId }); const { id: txId } = await ant.removeRecord( { undername: 'dapp' }, // optional additional tags { tags: [{ name: 'App-Name', value: 'My-Awesome-App' }] }, ); // dapp_ardrive.ar.io will no longer resolve to the provided transaction id ``` # Spawn (/(ant-contracts)/spawn) #### spawn() Spawns a new ANT (Arweave Name Token) process. This static function creates a new ANT process on the AO network and returns the process ID. _Note: Requires `signer` to be provided to sign the spawn transaction._ ```typescript const processId = await ANT.spawn({ signer: new ArweaveSigner(jwk), state: { name: 'My ANT', ticker: 'MYANT', description: 'My custom ANT token', }, }); // Using a custom module ID const processId = await ANT.spawn({ signer: new ArweaveSigner(jwk), module: 'FKtQtOOtlcWCW2pXrwWFiCSlnuewMZOHCzhulVkyqBE', // Custom module ID state: { name: 'My Custom Module ANT', ticker: 'CUSTOM', description: 'ANT using a specific module version', }, }); ``` **CLI Usage:** ```bash ar.io spawn-ant --wallet-file wallet.json --name \"My ANT\" --ticker \"MYANT\" ar.io spawn-ant --wallet-file wallet.json --module FKtQtOOtlcWCW2pXrwWFiCSlnuewMZOHCzhulVkyqBE --name \"My Custom ANT\" --ticker \"CUSTOM\" ``` **Parameters:** - `signer: AoSigner` - The signer used to authenticate the spawn transaction - `module?: string` - Optional module ID to use; if not provided, gets latest from ANT registry - `ao?: AoClient` - Optional AO client instance (defaults to legacy mode connection) - `scheduler?: string` - Optional scheduler ID - `state?: SpawnANTState` - Optional initial state for the ANT including name, ticker, description, etc. - `antRegistryId?: string` - Optional ANT registry ID - `logger?: Logger` - Optional logger instance - `authority?: string` - Optional authority **Returns:** `Promise\\` - The process ID of the newly spawned ANT # State (/(ant-contracts)/state) #### getInfo() Retrieves the information of the ANT process. ```typescript const info = await ant.getInfo(); ``` **Output:** ```json { \"name\": \"ArDrive\", \"ticker\": \"ANT-ARDRIVE\", \"description\": \"This is the ANT for the ArDrive decentralized web app.\", \"keywords\": [\"File-sharing\", \"Publishing\", \"dApp\"], \"owner\": \"QGWqtJdLLgm2ehFWiiPzMaoFLD50CnGuzZIPEdoDRGQ\" } ``` #### getHandlers() Retrieves the handlers supported on the ANT ```typescript const handlers = await ant.getHandlers(); ``` **Output:** ```json [ \"_eval\", \"_default\", \"transfer\", \"balance\", \"balances\", \"totalSupply\", \"info\", \"addController\", \"removeController\", \"controllers\", \"setRecord\", \"removeRecord\", \"record\", \"records\", \"setName\", \"setTicker\", \"initializeState\", \"state\" ] ``` #### getState() Retrieves the state of the ANT process. ```typescript const state = await ant.getState(); ``` **Output:** ```json { \"TotalSupply\": 1, \"Balances\": { \"98O1_xqDLrBKRfQPWjF5p7xZ4Jx6GM8P5PeJn26xwUY\": 1 }, \"Controllers\": [], \"Records\": { \"v1-0-0_whitepaper\": { \"transactionId\": \"lNjWn3LpyhKC95Kqe-x8X2qgju0j98MhucdDKK85vc4\", \"ttlSeconds\": 900 }, \"@\": { \"transactionId\": \"2rMLb2uHAyEt7jSu6bXtKx8e-jOfIf7E-DOgQnm8EtU\", \"ttlSeconds\": 3600 }, \"alice\": { \"transactionId\": \"kMk95k_3R8x_7d3wB9tEOiL5v6n8QhR_VnFCh3aeE3f\", \"ttlSeconds\": 900, \"owner\": \"alice-wallet-address-123...\", \"displayName\": \"Alice's Portfolio\", \"logo\": \"avatar-tx-id-456...\", \"description\": \"Personal portfolio and blog\", \"keywords\": [\"portfolio\", \"personal\", \"blog\"] }, \"whitepaper\": { \"transactionId\": \"lNjWn3LpyhKC95Kqe-x8X2qgju0j98MhucdDKK85vc4\", \"ttlSeconds\": 900 } }, \"Initialized\": true, \"Ticker\": \"ANT-AR-IO\", \"Description\": \"A friendly description for this ANT.\", \"Keywords\": [\"keyword1\", \"keyword2\", \"keyword3\"], \"Logo\": \"Sie_26dvgyok0PZD_-iQAFOhOd5YxDTkczOLoqTTL_A\", \"Denomination\": 0, \"Name\": \"AR.IO Foundation\", \"Owner\": \"98O1_xqDLrBKRfQPWjF5p7xZ4Jx6GM8P5PeJn26xwUY\" } ``` #### getOwner() Returns the owner of the configured ANT process. ```typescript const owner = await ant.getOwner(); ``` **Output:** ```json \"ccp3blG__gKUvG3hsGC2u06aDmqv4CuhuDJGOIg0jw4\" ``` #### getName() Returns the name of the ANT (not the same as ArNS name). ```typescript const name = await ant.getName(); ``` **Output:** ```json \"ArDrive\" ``` #### getTicker() Returns the ticker symbol of the ANT. ```typescript const ticker = await ant.getTicker(); ``` **Output:** ```json \"ANT-ARDRIVE\" ``` #### getControllers() Returns the controllers of the configured ANT process. ```typescript const controllers = await ant.getControllers(); ``` **Output:** ```json [\"ccp3blG__gKUvG3hsGC2u06aDmqv4CuhuDJGOIg0jw4\"] ``` #### getRecords() Returns all records on the configured ANT process, including the required `@` record that resolve connected ArNS names. ```typescript const records = await ant.getRecords(); ``` **Output:** ```json { \"@\": { \"transactionId\": \"UyC5P5qKPZaltMmmZAWdakhlDXsBF6qmyrbWYFchRTk\", \"ttlSeconds\": 3600 }, \"alice\": { \"transactionId\": \"kMk95k_3R8x_7d3wB9tEOiL5v6n8QhR_VnFCh3aeE3f\", \"ttlSeconds\": 900, \"owner\": \"alice-wallet-address-123...\", \"displayName\": \"Alice's Portfolio\", \"logo\": \"avatar-tx-id-456...\", \"description\": \"Personal portfolio and blog\", \"keywords\": [\"portfolio\", \"personal\", \"blog\"] }, \"zed\": { \"transactionId\": \"-k7t8xMoB8hW482609Z9F4bTFMC3MnuW8bTvTyT8pFI\", \"ttlSeconds\": 900 }, \"ardrive\": { \"transactionId\": \"-cucucachoodwedwedoiwepodiwpodiwpoidpwoiedp\", \"ttlSeconds\": 900 } } ``` #### getRecord() Returns a specific record by its undername. ```typescript const record = await ant.getRecord({ undername: 'dapp' }); ``` **Output:** ```json { \"transactionId\": \"432l1cy0aksiL_x9M359faGzM_yjralacHIUo8_nQXM\", \"ttlSeconds\": 900, \"owner\": \"alice-wallet-address-123...\", \"displayName\": \"Alice's Site\", \"logo\": \"avatar-tx-id-456...\", \"description\": \"Personal portfolio and blog\", \"keywords\": [\"portfolio\", \"personal\", \"blog\"] } ``` # Static Methods (/(ant-contracts)/static-methods) #### ANT.fork() Forks an existing ANT process to create a new one with the same state but potentially a different module. This is used for upgrading ANTs to new versions. ```typescript const newProcessId = await ANT.fork({ signer: new ArweaveSigner(jwk), antProcessId: 'existing-ant-process-id', // Optional: specify a specific module ID, defaults to latest from registry module: 'new-module-id', onSigningProgress: (event, payload) => { console.log(`Fork progress: ${event}`); }, }); console.log(`Forked ANT to new process: ${newProcessId}`); ``` #### ANT.upgrade() Static method to upgrade an ANT by forking it to the latest version and reassigning names. ```typescript // Upgrade and reassign all affiliated names const result = await ANT.upgrade({ signer: new ArweaveSigner(jwk), antProcessId: 'existing-ant-process-id', reassignAffiliatedNames: true, arioProcessId: ARIO_MAINNET_PROCESS_ID }); // Upgrade and reassign specific names const result = await ANT.upgrade({ signer: new ArweaveSigner(jwk), antProcessId: 'existing-ant-process-id', names: ['ardrive', 'example'], reassignAffiliatedNames: false, arioProcessId: ARIO_MAINNET_PROCESS_ID }); console.log(`Upgraded to process: ${result.forkedProcessId}`); console.log(`Successfully reassigned names: ${Object.keys(result.reassignedNames)}`); console.log(`Failed reassignments: ${Object.keys(result.failedReassignedNames)}`); ``` # Transfer (/(ant-contracts)/transfer) #### transfer() Transfers ownership of the ANT to a new target address. Target MUST be an Arweave address. _Note: Requires `signer` to be provided on `ANT.init` to sign the transaction._ ```typescript const { id: txId } = await ant.transfer( { target: 'aGzM_yjralacHIUo8_nQXMbh9l1cy0aksiL_x9M359f' }, // optional additional tags { tags: [{ name: 'App-Name', value: 'My-Awesome-App' }] }, ); ``` # Undername Ownership (/(ant-contracts)/undername-ownership) NTs support ownership of undernames: 1. **ANT Owner** - Has full control over the ANT and all records 2. **Controllers** - Can manage records but cannot transfer ANT ownership 3. **Record Owners** - Can only update their specific delegated records When a record owner updates their own record, they **MUST** include their own address in the `owner` field. If the `owner` field is omitted or set to a different address, the record ownership will be transferred or renounced. #### transferRecord() Transfers ownership of a specific record (undername) to another address. This enables delegation of control for individual records within an ANT while maintaining the ANT owner's ultimate authority. The current record owner or ANT owner/controllers can transfer ownership. _Note: Requires `signer` to be provided on `ANT.init` to sign the transaction._ ```typescript const { id: txId } = await ant.transferRecord({ undername: 'alice', // the subdomain/record to transfer recipient: 'new-owner-address-123...', // address of the new owner }); // alice_ardrive.ar.io is now owned by the new owner address // The new owner can update the record but not other records in the ANT ``` **CLI Usage:** ```bash ar.io transfer-record \\ --process-id \"ANT_PROCESS_ID\" \\ --undername \"alice\" \\ --recipient \"new-owner-address-123...\" \\ --wallet-file \"path/to/wallet.json\" ``` #### Record Owner Workflow Examples **Checking Record Ownership:** ```typescript const record = await ant.getRecord({ undername: 'alice' }); console.log(`Record owner: ${record.owner}`); console.log(`Transaction ID: ${record.transactionId}`); ``` **Record Owner Updating Their Own Record:** ```typescript // Alice (record owner) updating her own record const aliceAnt = ANT.init({ processId: 'ANT_PROCESS_ID', signer: new ArweaveSigner(aliceJwk), // Alice's wallet }); // ✅ CORRECT: Alice includes her own address as owner const { id: txId } = await aliceAnt.setUndernameRecord({ undername: 'alice', transactionId: 'new-content-tx-id-456...', ttlSeconds: 1800, owner: 'alice-wallet-address-123...', // MUST be Alice's own address displayName: 'Alice Updated Portfolio', description: 'Updated personal portfolio and blog', }); // ❌ WRONG: Omitting owner field will renounce ownership const badUpdate = await aliceAnt.setUndernameRecord({ undername: 'alice', transactionId: 'new-content-tx-id-456...', ttlSeconds: 1800, // Missing owner field - this will renounce ownership! }); // ❌ WRONG: Setting different owner will transfer ownership const badTransfer = await aliceAnt.setUndernameRecord({ undername: 'alice', transactionId: 'new-content-tx-id-456...', ttlSeconds: 1800, owner: 'someone-else-address-789...', // This transfers ownership to someone else! }); ``` **What Happens When Record Ownership is Renounced:** If a record owner updates their record without including the `owner` field, the record becomes owned by the ANT owner/controllers again: ```typescript // Before: alice record is owned by alice-wallet-address-123... const recordBefore = await ant.getRecord({ undername: 'alice' }); console.log(recordBefore.owner); // \"alice-wallet-address-123...\" // Alice updates without owner field await aliceAnt.setUndernameRecord({ undername: 'alice', transactionId: 'new-tx-id...', ttlSeconds: 900, // No owner field = renounces ownership }); // After: record ownership reverts to ANT owner const recordAfter = await ant.getRecord({ undername: 'alice' }); console.log(recordAfter.owner); // undefined (controlled by ANT owner again) ``` # Upgrade (/(ant-contracts)/upgrade) #### upgrade() Upgrades an ANT by forking it to the latest version from the ANT registry and optionally reassigning ArNS names to the new process. This function first checks the version of the existing ANT, creates a new ANT using `.fork()` to the latest version, and then reassigns the ArNS names affiliated with this process to the new process. _Note: Requires `signer` to be provided on `ANT.init` to sign the transaction._ ```typescript // Upgrade ANT and reassign all affiliated ArNS names to the new process const result = await ant.upgrade(); // Upgrade ANT and reassign specific ArNS names to the new process const result = await ant.upgrade({ names: ['ardrive', 'example'], }); // with callbacks const result = await ant.upgrade({ names: ['ardrive', 'example'], onSigningProgress: (event, payload) => { console.log(`${event}:`, payload); if (event === 'checking-version') { console.log(`Checking version: ${payload.antProcessId}`); } if (event === 'fetching-affiliated-names') { console.log(`Fetching affiliated names: ${payload.arioProcessId}`); } if (event === 'reassigning-name') { console.log(`Reassigning name: ${payload.name}`); } if (event === 'validating-names') { console.log(`Validating names: ${payload.names}`); } // other callback events... }, }); console.log(`Upgraded to process: ${result.forkedProcessId}`); console.log(`Successfully reassigned names: ${result.reassignedNames}`); console.log(`Failed to reassign names: ${result.failedReassignedNames}`); ``` **Parameters:** - `reassignAffiliatedNames?: boolean` - If true, reassigns all names associated with this process to the new forked process (defaults to true when names is empty) - `names?: string[]` - Optional array of specific names to reassign (cannot be used with `reassignAffiliatedNames: true`). These names must be affiliated with this ANT on the provided ARIO process. - `arioProcessId?: string` - Optional ARIO process ID (defaults to mainnet) - `antRegistryId?: string` - Optional ANT registry process ID used to resolve the latest version (defaults to mainnet registry) - `skipVersionCheck?: boolean` - Skip checking if ANT is already latest version (defaults to false) - `onSigningProgress?: Function` - Optional progress callback for tracking upgrade steps **Returns:** `Promise, failedReassignedNames: Record }>` # Versions (/(ant-contracts)/versions) #### getModuleId() Gets the module ID of the current ANT process by querying its spawn transaction tags. Results are cached after the first successful fetch. ```typescript const moduleId = await ant.getModuleId(); console.log(`ANT was spawned with module: ${moduleId}`); // With custom GraphQL URL and retries const moduleId = await ant.getModuleId({ graphqlUrl: 'https://arweave.net/graphql', retries: 5 }); ``` **Output:** ```json \"FKtQtOOtlcWCW2pXrwWFiCSlnuewMZOHCzhulVkyqBE\" ``` #### getVersion() Gets the version string of the current ANT by matching its module ID with versions from the ANT registry. ```typescript const version = await ant.getVersion(); console.log(`ANT is running version: ${version}`); // With custom ANT registry const version = await ant.getVersion({ antRegistryId: 'custom-ant-registry-id' }); ``` **Output:** ```json \"23\" ``` #### isLatestVersion() Checks if the current ANT version is the latest according to the ANT registry. ```typescript const isLatest = await ant.isLatestVersion(); if (!isLatest) { console.log('ANT can be upgraded to the latest version'); } ``` **Output:** ```json true ``` #### getANTVersions Static method that returns the full array of available ANT versions and the latest version from the ANT registry. ```typescript // Get all available ANT versions const antVersions = ANT.versions; const versions = await antVersions.getANTVersions(); ``` Result: ```json { [ { \"moduleId\": \"FKtQtOOtlcWCW2pXrwWFiCSlnuewMZOHCzhulVkyqBE\", \"version\": \"23\", \"releaseNotes\": \"Initial release of the ANT module.\", \"releaseDate\": 1700000000000 } // ...other versions ], } ``` #### getLatestANTVersion() Static method that returns the latest ANT version from the ANT registry. ```typescript // Get the latest ANT version // Get all available ANT versions const antVersions = ANT.versions; const versions = await antVersions.getANTVersions(); const latestVersion = await antVersions.getLatestANTVersion(); ``` Result: ```json { \"moduleId\": \"FKtQtOOtlcWCW2pXrwWFiCSlnuewMZOHCzhulVkyqBE\", \"version\": \"23\", \"releaseNotes\": \"Initial release of the ANT module.\", \"releaseDate\": 1700000000000 } ``` # Arweave Name System (ArNS) (/(ario-contract)/arweave-name-system-arns) #### resolveArNSName() Resolves an ArNS name to the underlying data id stored on the names corresponding ANT id. ##### Resolving a base name ```typescript const ario = ARIO.mainnet(); const record = await ario.resolveArNSName({ name: 'ardrive' }); ``` **Output:** ```json { \"processId\": \"bh9l1cy0aksiL_x9M359faGzM_yjralacHIUo8_nQXM\", \"txId\": \"kvhEUsIY5bXe0Wu2-YUFz20O078uYFzmQIO-7brv8qw\", \"type\": \"lease\", \"recordIndex\": 0, \"undernameLimit\": 100, \"owner\": \"t4Xr0_J4Iurt7caNST02cMotaz2FIbWQ4Kbj616RHl3\", \"name\": \"ardrive\" } ``` ##### Resolving an undername ```typescript const ario = ARIO.mainnet(); const record = await ario.resolveArNSName({ name: 'logo_ardrive' }); ``` **Output:** ```json { \"processId\": \"bh9l1cy0aksiL_x9M359faGzM_yjralacHIUo8_nQXM\", \"txId\": \"kvhEUsIY5bXe0Wu2-YUFz20O078uYFzmQIO-7brv8qw\", \"type\": \"lease\", \"recordIndex\": 1, \"undernameLimit\": 100, \"owner\": \"t4Xr0_J4Iurt7caNST02cMotaz2FIbWQ4Kbj616RHl3\", \"name\": \"ardrive\" } ``` #### buyRecord() Purchases a new ArNS record with the specified name, type, processId, and duration. _Note: Requires `signer` to be provided on `ARIO.init` to sign the transaction._ **Arguments:** - `name` - _required_: the name of the ArNS record to purchase - `type` - _required_: the type of ArNS record to purchase - `processId` - _optional_: the process id of an existing ANT process. If not provided, a new ANT process using the provided `signer` will be spawned, and the ArNS record will be assigned to that process. - `years` - _optional_: the duration of the ArNS record in years. If not provided and `type` is `lease`, the record will be leased for 1 year. If not provided and `type` is `permabuy`, the record will be permanently registered. - `referrer` - _optional_: track purchase referrals for analytics (e.g. `my-app.com`) ```typescript const ario = ARIO.mainnet({ signer }); const record = await ario.buyRecord( { name: 'ardrive', type: 'lease', years: 1, processId: 'bh9l1cy0aksiL_x9M359faGzM_yjralacHIUo8_nQXM', // optional: assign to existing ANT process referrer: 'my-app.com', // optional: track purchase referrals for analytics }, { // optional tags tags: [{ name: 'App-Name', value: 'ArNS-App' }], onSigningProgress: (step, event) => { console.log(`Signing progress: ${step}`); if (step === 'spawning-ant') { console.log('Spawning ant:', event); } if (step === 'registering-ant') { console.log('Registering ant:', event); } if (step === 'verifying-state') { console.log('Verifying state:', event); } if (step === 'buying-name') { console.log('Buying name:', event); } }, }, ); ``` #### upgradeRecord() Upgrades an existing leased ArNS record to a permanent ownership. The record must be currently owned by the caller and be of type \"lease\". _Note: Requires `signer` to be provided on `ARIO.init` to sign the transaction._ ```typescript const ario = ARIO.mainnet({ signer }); const record = await ario.upgradeRecord( { name: 'ardrive', referrer: 'my-app.com', // optional: track purchase referrals for analytics }, { // optional tags tags: [{ name: 'App-Name', value: 'ArNS-App' }], }, ); ``` #### getArNSRecord() Retrieves the record info of the specified ArNS name. ```typescript const ario = ARIO.mainnet(); const record = await ario.getArNSRecord({ name: 'ardrive' }); ``` **Output:** ```json { \"processId\": \"bh9l1cy0aksiL_x9M359faGzM_yjralacHIUo8_nQXM\", \"endTimestamp\": 1752256702026, \"startTimestamp\": 1720720819969, \"type\": \"lease\", \"undernameLimit\": 100 } ``` #### getArNSRecords() Retrieves all registered ArNS records of the ARIO process, paginated and sorted by the specified criteria. The `cursor` used for pagination is the last ArNS name from the previous request. ```typescript const ario = ARIO.mainnet(); // get the newest 100 names const records = await ario.getArNSRecords({ limit: 100, sortBy: 'startTimestamp', sortOrder: 'desc', }); ``` Available `sortBy` options are any of the keys on the record object, e.g. `name`, `processId`, `endTimestamp`, `startTimestamp`, `type`, `undernames`. **Output:** ```json { \"items\": [ { \"name\": \"ao\", \"processId\": \"eNey-H9RB9uCdoJUvPULb35qhZVXZcEXv8xds4aHhkQ\", \"purchasePrice\": 75541282285, \"startTimestamp\": 1720720621424, \"endTimestamp\": 1752256702026, \"type\": \"permabuy\", \"undernameLimit\": 10 }, { \"name\": \"ardrive\", \"processId\": \"bh9l1cy0aksiL_x9M359faGzM_yjralacHIUo8_nQXM\", \"endTimestamp\": 1720720819969, \"startTimestamp\": 1720720620813, \"purchasePrice\": 75541282285, \"type\": \"lease\", \"undernameLimit\": 100 }, { \"name\": \"arweave\", \"processId\": \"bh9l1cy0aksiL_x9M359faGzM_yjralacHIUo8_nQXM\", \"endTimestamp\": 1720720819969, \"startTimestamp\": 1720720620800, \"purchasePrice\": 75541282285, \"type\": \"lease\", \"undernameLimit\": 100 }, { \"name\": \"ar-io\", \"processId\": \"bh9l1cy0aksiL_x9M359faGzM_yjralacHIUo8_nQXM\", \"endTimestamp\": 1720720819969, \"startTimestamp\": 1720720619000, \"purchasePrice\": 75541282285, \"type\": \"lease\", \"undernameLimit\": 100 }, { \"name\": \"fwd\", \"processId\": \"bh9l1cy0aksiL_x9M359faGzM_yjralacHIUo8_nQXM\", \"endTimestamp\": 1720720819969, \"startTimestamp\": 1720720220811, \"purchasePrice\": 75541282285, \"type\": \"lease\", \"undernameLimit\": 100 } // ...95 other records ], \"hasMore\": true, \"nextCursor\": \"fwdresearch\", \"totalItems\": 21740, \"sortBy\": \"startTimestamp\", \"sortOrder\": \"desc\" } ``` #### getArNSRecordsForAddress() Retrieves all registered ArNS records of the specified address according to the `ANTRegistry` access control list, paginated and sorted by the specified criteria. The `cursor` used for pagination is the last ArNS name from the previous request. ```typescript const ario = ARIO.mainnet(); const records = await ario.getArNSRecordsForAddress({ address: 't4Xr0_J4Iurt7caNST02cMotaz2FIbWQ4Kbj616RHl3', limit: 100, sortBy: 'startTimestamp', sortOrder: 'desc', }); ``` Available `sortBy` options are any of the keys on the record object, e.g. `name`, `processId`, `endTimestamp`, `startTimestamp`, `type`, `undernames`. **Output:** ```json { \"limit\": 1, \"totalItems\": 31, \"hasMore\": true, \"nextCursor\": \"ardrive\", \"items\": [ { \"startTimestamp\": 1740009600000, \"name\": \"ardrive\", \"endTimestamp\": 1777328018367, \"type\": \"permabuy\", \"purchasePrice\": 0, \"undernameLimit\": 100, \"processId\": \"hpF0HdijWlBLFePjWX6u_-Lg3Z2E_PrP_AoaXDVs0bA\" } ], \"sortOrder\": \"desc\", \"sortBy\": \"startTimestamp\" } ``` #### increaseUndernameLimit() Increases the undername support of a domain up to a maximum of 10k. Domains, by default, support up to 10 undernames. _Note: Requires `signer` to be provided on `ARIO.init` to sign the transaction._ ```typescript const ario = ARIO.mainnet({ signer: new ArweaveSigner(jwk) }); const { id: txId } = await ario.increaseUndernameLimit( { name: 'ar-io', qty: 420, referrer: 'my-app.com', // optional: track purchase referrals for analytics }, // optional additional tags { tags: [{ name: 'App-Name', value: 'My-Awesome-App' }] }, ); ``` #### extendLease() Extends the lease of a registered ArNS domain, with an extension of 1-5 years depending on grace period status. Permanently registered domains cannot be extended. ```typescript const ario = ARIO.mainnet({ signer: new ArweaveSigner(jwk) }); const { id: txId } = await ario.extendLease( { name: 'ar-io', years: 1, referrer: 'my-app.com', // optional: track purchase referrals for analytics }, // optional additional tags { tags: [{ name: 'App-Name', value: 'My-Awesome-App' }] }, ); ``` #### getTokenCost() Calculates the price in mARIO to perform the interaction in question, eg a 'Buy-Name' interaction, where args are the specific params for that interaction. ```typescript const price = await ario .getTokenCost({ intent: 'Buy-Name', name: 'ar-io', type: 'permabuy', }) .then((p) => new mARIOToken(p).toARIO()); // convert to ARIO for readability ``` **Output:** ```json 1642.34 ``` #### getCostDetails() Calculates the expanded cost details for the interaction in question, e.g a 'Buy-Name' interaction, where args are the specific params for that interaction. The fromAddress is the address that would be charged for the interaction, and fundFrom is where the funds would be taken from, either `balance`, `stakes`, or `any`. ```typescript const costDetails = await ario.getCostDetails({ intent: 'Buy-Name', fromAddress: 't4Xr0_J4Iurt7caNST02cMotaz2FIbWQ4Kbj616RHl3', fundFrom: 'stakes', name: 'ar-io', type: 'permabuy', }); ``` **Output:** ```json { \"tokenCost\": 2384252273, \"fundingPlan\": { \"address\": \"t4Xr0_J4Iurt7caNST02cMotaz2FIbWQ4Kbj616RHl3\", \"balance\": 0, \"stakes\": { \"Rc80LG6h27Y3p9TN6J5hwDeG5M51cu671YwZpU9uAVE\": { \"vaults\": [], \"delegatedStake\": 2384252273 } }, \"shortfall\": 0 }, \"discounts\": [] } ``` #### getDemandFactor() Retrieves the current demand factor of the network. The demand factor is a multiplier applied to the cost of ArNS interactions based on the current network demand. ```typescript const ario = ARIO.mainnet(); const demandFactor = await ario.getDemandFactor(); ``` **Output:** ```json 1.05256 ``` #### getArNSReturnedNames() Retrieves all active returned names of the ARIO process, paginated and sorted by the specified criteria. The `cursor` used for pagination is the last returned name from the previous request. ```typescript const ario = ARIO.mainnet(); const returnedNames = await ario.getArNSReturnedNames({ limit: 100, sortBy: 'endTimestamp', sortOrder: 'asc', // return the returned names ending soonest first }); ``` **Output:** ```json { \"items\": [ { \"name\": \"permalink\", \"endTimestamp\": 1730985241349, \"startTimestamp\": 1729775641349, \"baseFee\": 250000000, \"demandFactor\": 1.05256, \"initiator\": \"GaQrvEMKBpkjofgnBi_B3IgIDmY_XYelVLB6GcRGrHc\", \"settings\": { \"durationMs\": 1209600000, \"decayRate\": 0.000000000016847809193121693, \"scalingExponent\": 190, \"startPriceMultiplier\": 50 } } ], \"hasMore\": false, \"totalItems\": 1, \"sortBy\": \"endTimestamp\", \"sortOrder\": \"asc\" } ``` #### getArNSReturnedName() Retrieves the returned name data for the specified returned name. ```typescript const ario = ARIO.mainnet(); const returnedName = await ario.getArNSReturnedName({ name: 'permalink' }); ``` **Output:** ```json { \"name\": \"permalink\", \"endTimestamp\": 1730985241349, \"startTimestamp\": 1729775641349, \"baseFee\": 250000000, \"demandFactor\": 1.05256, \"initiator\": \"GaQrvEMKBpkjofgnBi_B3IgIDmY_XYelVLB6GcRGrHc\", \"settings\": { \"durationMs\": 1209600000, \"decayRate\": 0.000000000016847809193121693, \"scalingExponent\": 190, \"startPriceMultiplier\": 50 } } ``` # Configuration (/(ario-contract)/configuration) The ARIO client class exposes APIs relevant to the ar.io process. It can be configured to use any AO Process ID that adheres to the [ARIO Network Spec]. By default, it will use the current [ARIO Testnet Process]. Refer to [AO Connect] for more information on how to configure an ARIO process to use specific AO infrastructure. ```typescript // provide a custom ao infrastructure and process id const ario = ARIO.mainnet({ process: new AOProcess({ processId: 'ARIO_PROCESS_ID' ao: connect({ MODE: 'legacy', MU_URL: 'https://mu-testnet.xyz', CU_URL: 'https://cu-testnet.xyz', GRAPHQL_URL: 'https://arweave.net/graphql', GATEWAY_URL: 'https://arweave.net', }) }) }); ``` # Epochs (/(ario-contract)/epochs) #### getCurrentEpoch() Returns the current epoch data. ```typescript const ario = ARIO.mainnet(); const epoch = await ario.getCurrentEpoch(); ``` **Output:** ```json { \"epochIndex\": 0, \"startTimestamp\": 1720720621424, \"endTimestamp\": 1752256702026, \"startHeight\": 1350700, \"distributionTimestamp\": 1711122739, \"observations\": { \"failureSummaries\": { \"-Tk2DDk8k4zkwtppp_XFKKI5oUgh6IEHygAoN7mD-w8\": [ \"Ie2wEEUDKoU26c7IuckHNn3vMFdNQnMvfPBrFzAb3NA\" ] }, \"reports\": { \"IPdwa3Mb_9pDD8c2IaJx6aad51Ss-_TfStVwBuhtXMs\": \"B6UUjKWjjEWDBvDSMXWNmymfwvgR9EN27z5FTkEVlX4\" } }, \"prescribedNames\": [\"ardrive\", \"ar-io\", \"arweave\", \"fwd\", \"ao\"], \"prescribedObservers\": [ { \"gatewayAddress\": \"2Fk8lCmDegPg6jjprl57-UCpKmNgYiKwyhkU4vMNDnE\", \"observerAddress\": \"2Fk8lCmDegPg6jjprl57-UCpKmNgYiKwyhkU4vMNDnE\", \"stake\": 10000000000, \"start\": 1292450, \"stakeWeight\": 1, \"tenureWeight\": 0.4494598765432099, \"gatewayPerformanceRatio\": 1, \"observerRewardRatioWeight\": 1, \"compositeWeight\": 0.4494598765432099, \"normalizedCompositeWeight\": 0.002057032496835938 } ], \"distributions\": { \"distributedTimestamp\": 1711122739, \"totalEligibleRewards\": 100000000, \"rewards\": { \"IPdwa3Mb_9pDD8c2IaJx6aad51Ss-_TfStVwBuhtXMs\": 100000000 } } } ``` #### getEpoch() Returns the epoch data for the specified block height. If no epoch index is provided, the current epoch is used. ```typescript const ario = ARIO.mainnet(); const epoch = await ario.getEpoch({ epochIndex: 0 }); ``` **Output:** ```json { \"epochIndex\": 0, \"startTimestamp\": 1720720620813, \"endTimestamp\": 1752256702026, \"startHeight\": 1350700, \"distributionTimestamp\": 1752256702026, \"observations\": { \"failureSummaries\": { \"-Tk2DDk8k4zkwtppp_XFKKI5oUgh6IEHygAoN7mD-w8\": [ \"Ie2wEEUDKoU26c7IuckHNn3vMFdNQnMvfPBrFzAb3NA\" ] }, \"reports\": { \"IPdwa3Mb_9pDD8c2IaJx6aad51Ss-_TfStVwBuhtXMs\": \"B6UUjKWjjEWDBvDSMXWNmymfwvgR9EN27z5FTkEVlX4\" } }, \"prescribedNames\": [\"ardrive\", \"ar-io\", \"arweave\", \"fwd\", \"ao\"], \"prescribedObservers\": [ { \"gatewayAddress\": \"2Fk8lCmDegPg6jjprl57-UCpKmNgYiKwyhkU4vMNDnE\", \"observerAddress\": \"2Fk8lCmDegPg6jjprl57-UCpKmNgYiKwyhkU4vMNDnE\", \"stake\": 10000000000, // value in mARIO \"startTimestamp\": 1720720620813, \"stakeWeight\": 1, \"tenureWeight\": 0.4494598765432099, \"gatewayPerformanceRatio\": 1, \"observerRewardRatioWeight\": 1, \"compositeWeight\": 0.4494598765432099, \"normalizedCompositeWeight\": 0.002057032496835938 } ], \"distributions\": { \"totalEligibleGateways\": 1, \"totalEligibleRewards\": 100000000, \"totalEligibleObserverReward\": 100000000, \"totalEligibleGatewayReward\": 100000000, \"totalDistributedRewards\": 100000000, \"distributedTimestamp\": 1720720621424, \"rewards\": { \"distributed\": { \"IPdwa3Mb_9pDD8c2IaJx6aad51Ss-_TfStVwBuhtXMs\": 100000000 } } } } ``` #### getEligibleEpochRewards() Returns the eligible epoch rewards for the specified block height. If no epoch index is provided, the current epoch is used. ```typescript const ario = ARIO.mainnet(); const rewards = await ario.getEligibleEpochRewards({ epochIndex: 0 }); ``` **Output:** ```json { \"sortOrder\": \"desc\", \"hasMore\": true, \"totalItems\": 37, \"limit\": 1, \"sortBy\": \"cursorId\", \"items\": [ { \"cursorId\": \"xN_aVln30LmoCffwmk5_kRkcyQZyZWy1o_TNtM_CTm0_xN_aVln30LmoCffwmk5_kRkcyQZyZWy1o_TNtM_CTm0\", \"recipient\": \"xN_aVln30LmoCffwmk5_kRkcyQZyZWy1o_TNtM_CTm0\", \"gatewayAddress\": \"xN_aVln30LmoCffwmk5_kRkcyQZyZWy1o_TNtM_CTm0\", \"eligibleReward\": 2627618704, \"type\": \"operatorReward\" } ], \"nextCursor\": \"xN_aVln30LmoCffwmk5_kRkcyQZyZWy1o_TNtM_CTm0_xN_aVln30LmoCffwmk5_kRkcyQZyZWy1o_TNtM_CTm0\" } ``` #### getObservations() Returns the epoch-indexed observation list. If no epoch index is provided, the current epoch is used. ```typescript const ario = ARIO.mainnet(); const observations = await ario.getObservations(); ``` **Output:** ```json { \"0\": { \"failureSummaries\": { \"-Tk2DDk8k4zkwtppp_XFKKI5oUgh6IEHygAoN7mD-w8\": [ \"Ie2wEEUDKoU26c7IuckHNn3vMFdNQnMvfPBrFzAb3NA\", \"Ie2wEEUDKoU26c7IuckHNn3vMFdNQnMvfPBrFzAb3NA\" ] }, \"reports\": { \"IPdwa3Mb_9pDD8c2IaJx6aad51Ss-_TfStVwBuhtXMs\": \"B6UUjKWjjEWDBvDSMXWNmymfwvgR9EN27z5FTkEVlX4\", \"Ie2wEEUDKoU26c7IuckHNn3vMFdNQnMvfPBrFzAb3NA\": \"7tKsiQ2fxv0D8ZVN_QEv29fZ8hwFIgHoEDrpeEG0DIs\", \"osZP4D9cqeDvbVFBaEfjIxwc1QLIvRxUBRAxDIX9je8\": \"aatgznEvC_UPcxp1v0uw_RqydhIfKm4wtt1KCpONBB0\", \"qZ90I67XG68BYIAFVNfm9PUdM7v1XtFTn7u-EOZFAtk\": \"Bd8SmFK9-ktJRmwIungS8ur6JM-JtpxrvMtjt5JkB1M\" } } } ``` #### getDistributions() Returns the current rewards distribution information. If no epoch index is provided, the current epoch is used. ```typescript const ario = ARIO.mainnet(); const distributions = await ario.getDistributions({ epochIndex: 0 }); ``` **Output:** ```json { \"totalEligibleGateways\": 1, \"totalEligibleRewards\": 100000000, \"totalEligibleObserverReward\": 100000000, \"totalEligibleGatewayReward\": 100000000, \"totalDistributedRewards\": 100000000, \"distributedTimestamp\": 1720720621424, \"rewards\": { \"eligible\": { \"IPdwa3Mb_9pDD8c2IaJx6aad51Ss-_TfStVwBuhtXMs\": { \"operatorReward\": 100000000, \"delegateRewards\": {} } }, \"distributed\": { \"IPdwa3Mb_9pDD8c2IaJx6aad51Ss-_TfStVwBuhtXMs\": 100000000 } } } ``` #### saveObservations() Saves the observations of the current epoch. Requires `signer` to be provided on `ARIO.init` to sign the transaction. _Note: Requires `signer` to be provided on `ARIO.init` to sign the transaction._ ```typescript const ario = ARIO.mainnet({ signer: new ArweaveSigner(jwk) }); const { id: txId } = await ario.saveObservations( { reportTxId: 'fDrr0_J4Iurt7caNST02cMotaz2FIbWQ4Kcj616RHl3', failedGateways: ['t4Xr0_J4Iurt7caNST02cMotaz2FIbWQ4Kbj616RHl3'], }, { tags: [{ name: 'App-Name', value: 'My-Awesome-App' }], }, ); ``` #### getPrescribedObservers() Retrieves the prescribed observers of the ARIO process. To fetch prescribed observers for a previous epoch set the `epochIndex` to the desired epoch index. ```typescript const ario = ARIO.mainnet(); const observers = await ario.getPrescribedObservers({ epochIndex: 0 }); ``` **Output:** ```json [ { \"gatewayAddress\": \"BpQlyhREz4lNGS-y3rSS1WxADfxPpAuing9Lgfdrj2U\", \"observerAddress\": \"2Fk8lCmDegPg6jjprl57-UCpKmNgYiKwyhkU4vMNDnE\", \"stake\": 10000000000, // value in mARIO \"start\": 1296976, \"stakeWeight\": 1, \"tenureWeight\": 0.41453703703703704, \"gatewayPerformanceRatio\": 1, \"observerRewardRatioWeight\": 1, \"compositeWeight\": 0.41453703703703704, \"normalizedCompositeWeight\": 0.0018972019546783507 } ] ``` # Gateways (/(ario-contract)/gateways) #### getGateway() Retrieves a gateway's info by its staking wallet address. ```typescript const ario = ARIO.mainnet(); const gateway = await ario.getGateway({ address: '-7vXsQZQDk8TMDlpiSLy3CnLi5PDPlAaN2DaynORpck', }); ``` **Output:** ```json { \"observerAddress\": \"IPdwa3Mb_9pDD8c2IaJx6aad51Ss-_TfStVwBuhtXMs\", \"operatorStake\": 250000000000, \"settings\": { \"fqdn\": \"ar-io.dev\", \"label\": \"AR.IO Test\", \"note\": \"Test Gateway operated by PDS for the AR.IO ecosystem.\", \"port\": 443, \"properties\": \"raJgvbFU-YAnku-WsupIdbTsqqGLQiYpGzoqk9SCVgY\", \"protocol\": \"https\" }, \"startTimestamp\": 1720720620813, \"stats\": { \"failedConsecutiveEpochs\": 0, \"passedEpochCount\": 30, \"submittedEpochCount\": 30, \"totalEpochCount\": 31, \"totalEpochsPrescribedCount\": 31 }, \"status\": \"joined\", \"vaults\": {}, \"weights\": { \"compositeWeight\": 0.97688888893556, \"gatewayPerformanceRatio\": 1, \"tenureWeight\": 0.19444444444444, \"observerRewardRatioWeight\": 1, \"normalizedCompositeWeight\": 0.19247316211083, \"stakeWeight\": 5.02400000024 } } ``` #### getGateways() Retrieves registered gateways of the ARIO process, using pagination and sorting by the specified criteria. The `cursor` used for pagination is the last gateway address from the previous request. ```typescript const ario = ARIO.mainnet(); const gateways = await ario.getGateways({ limit: 100, sortOrder: 'desc', sortBy: 'operatorStake', }); ``` Available `sortBy` options are any of the keys on the gateway object, e.g. `operatorStake`, `start`, `status`, `settings.fqdn`, `settings.label`, `settings.note`, `settings.port`, `settings.protocol`, `stats.failedConsecutiveEpochs`, `stats.passedConsecutiveEpochs`, etc. **Output:** ```json { \"items\": [ { \"gatewayAddress\": \"QGWqtJdLLgm2ehFWiiPzMaoFLD50CnGuzZIPEdoDRGQ\", \"observerAddress\": \"IPdwa3Mb_9pDD8c2IaJx6aad51Ss-_TfStVwBuhtXMs\", \"operatorStake\": 250000000000, \"settings\": { \"fqdn\": \"ar-io.dev\", \"label\": \"AR.IO Test\", \"note\": \"Test Gateway operated by PDS for the AR.IO ecosystem.\", \"port\": 443, \"properties\": \"raJgvbFU-YAnku-WsupIdbTsqqGLQiYpGzoqk9SCVgY\", \"protocol\": \"https\" }, \"startTimestamp\": 1720720620813, \"stats\": { \"failedConsecutiveEpochs\": 0, \"passedEpochCount\": 30, \"submittedEpochCount\": 30, \"totalEpochCount\": 31, \"totalEpochsPrescribedCount\": 31 }, \"status\": \"joined\", \"vaults\": {}, \"weights\": { \"compositeWeight\": 0.97688888893556, \"gatewayPerformanceRatio\": 1, \"tenureWeight\": 0.19444444444444, \"observerRewardRatioWeight\": 1, \"normalizedCompositeWeight\": 0.19247316211083, \"stakeWeight\": 5.02400000024 } } ], \"hasMore\": true, \"nextCursor\": \"-4xgjroXENKYhTWqrBo57HQwvDL51mMdfsdsxJy6Y2Z_sA\", \"totalItems\": 316, \"sortBy\": \"operatorStake\", \"sortOrder\": \"desc\" } ``` #### getGatewayDelegates() Retrieves all delegates for a specific gateway, paginated and sorted by the specified criteria. The `cursor` used for pagination is the last delegate address from the previous request. ```typescript const ario = ARIO.mainnet(); const delegates = await ario.getGatewayDelegates({ address: 'QGWqtJdLLgm2ehFWiiPzMaoFLD50CnGuzZIPEdoDRGQ', limit: 3, sortBy: 'startTimestamp', sortOrder: 'desc', }); ``` **Output:** ```json { \"nextCursor\": \"ScEtph9-vfY7lgqlUWwUwOmm99ySeZGQhOX0MFAyFEs\", \"limit\": 3, \"sortBy\": \"startTimestamp\", \"totalItems\": 32, \"sortOrder\": \"desc\", \"hasMore\": true, \"items\": [ { \"delegatedStake\": 600000000, \"address\": \"qD5VLaMYyIHlT6vH59TgYIs6g3EFlVjlPqljo6kqVxk\", \"startTimestamp\": 1732716956301 }, { \"delegatedStake\": 508999038, \"address\": \"KG8TlcWk-8pvroCjiLD2J5zkG9rqC6yYaBuZNqHEyY4\", \"startTimestamp\": 1731828123742 }, { \"delegatedStake\": 510926479, \"address\": \"ScEtph9-vfY7lgqlUWwUwOmm99ySeZGQhOX0MFAyFEs\", \"startTimestamp\": 1731689356040 } ] } ``` #### joinNetwork() Joins a gateway to the ar.io network via its associated wallet. _Note: Requires `signer` to be provided on `ARIO.init` to sign the transaction._ ```typescript const ario = ARIO.mainnet({ signer: new ArweaveSigner(jwk) }); const { id: txId } = await ario.joinNetwork( { qty: new ARIOToken(10_000).toMARIO(), // minimum operator stake allowed autoStake: true, // auto-stake operator rewards to the gateway allowDelegatedStaking: true, // allows delegated staking minDelegatedStake: new ARIOToken(100).toMARIO(), // minimum delegated stake allowed delegateRewardShareRatio: 10, // percentage of rewards to share with delegates (e.g. 10%) label: 'john smith', // min 1, max 64 characters note: 'The example gateway', // max 256 characters properties: 'FH1aVetOoulPGqgYukj0VE0wIhDy90WiQoV3U2PeY44', // Arweave transaction ID containing additional properties of the Gateway observerWallet: '0VE0wIhDy90WiQoV3U2PeY44FH1aVetOoulPGqgYukj', // wallet address of the observer, must match OBSERVER_WALLET on the observer fqdn: 'example.com', // fully qualified domain name - note: you must own the domain and set the OBSERVER_WALLET on your gateway to match `observerWallet` port: 443, // port number protocol: 'https', // only 'https' is supported }, // optional additional tags { tags: [{ name: 'App-Name', value: 'My-Awesome-App' }] }, ); ``` #### leaveNetwork() Sets the gateway as `leaving` on the ar.io network. Requires `signer` to be provided on `ARIO.init` to sign the transaction. The gateways operator and delegate stakes are vaulted and will be returned after leave periods. The gateway will be removed from the network after the leave period. _Note: Requires `signer` to be provided on `ARIO.init` to sign the transaction._ ```typescript const ario = ARIO.mainnet({ signer: new ArweaveSigner(jwk) }); const { id: txId } = await ario.leaveNetwork( // optional additional tags { tags: [{ name: 'App-Name', value: 'My-Awesome-App' }] }, ); ``` #### updateGatewaySettings() Writes new gateway settings to the callers gateway configuration. _Note: Requires `signer` to be provided on `ARIO.init` to sign the transaction._ ```typescript const ario = ARIO.mainnet({ signer: new ArweaveSigner(jwk) }); const { id: txId } = await ario.updateGatewaySettings( { // any other settings you want to update minDelegatedStake: new ARIOToken(100).toMARIO(), }, // optional additional tags { tags: [{ name: 'App-Name', value: 'My-Awesome-App' }] }, ); ``` #### increaseDelegateStake() Increases the callers stake on the target gateway. _Note: Requires `signer` to be provided on `ARIO.init` to sign the transaction._ ```typescript const ario = ARIO.mainnet({ signer: new ArweaveSigner(jwk) }); const { id: txId } = await ario.increaseDelegateStake( { target: 't4Xr0_J4Iurt7caNST02cMotaz2FIbWQ4Kbj616RHl3', qty: new ARIOToken(100).toMARIO(), }, // optional additional tags { tags: [{ name: 'App-Name', value: 'My-Awesome-App' }] }, ); ``` #### decreaseDelegateStake() Decreases the callers stake on the target gateway. Can instantly decrease stake by setting instant to `true`. _Note: Requires `signer` to be provided on `ARIO.init` to sign the transaction._ ```typescript const ario = ARIO.mainnet({ signer: new ArweaveSigner(jwk) }); const { id: txId } = await ario.decreaseDelegateStake( { target: 't4Xr0_J4Iurt7caNST02cMotaz2FIbWQ4Kbj616RHl3', qty: new ARIOToken(100).toMARIO(), }, { tags: [{ name: 'App-Name', value: 'My-Awesome-App' }], }, ); ``` Pay the early withdrawal fee and withdraw instantly. ```typescript const ario = ARIO.mainnet({ signer: new ArweaveSigner(jwk) }); const { id: txId } = await ario.decreaseDelegateStake({ target: 't4Xr0_J4Iurt7caNST02cMotaz2FIbWQ4Kbj616RHl3', qty: new ARIOToken(100).toMARIO(), instant: true, // Immediately withdraw this stake and pay the instant withdrawal fee }); ``` #### getDelegations() Retrieves all active and vaulted stakes across all gateways for a specific address, paginated and sorted by the specified criteria. The `cursor` used for pagination is the last delegationId (concatenated gateway and startTimestamp of the delgation) from the previous request. ```typescript const ario = ARIO.mainnet(); const vaults = await ario.getDelegations({ address: 't4Xr0_J4Iurt7caNST02cMotaz2FIbWQ4Kbj616RHl3', cursor: 'QGWqtJdLLgm2ehFWiiPzMaoFLD50CnGuzZIPEdoDRGQ_123456789', limit: 2, sortBy: 'startTimestamp', sortOrder: 'asc', }); ``` **Output:** ```json { \"sortOrder\": \"asc\", \"hasMore\": true, \"totalItems\": 95, \"limit\": 2, \"sortBy\": \"startTimestamp\", \"items\": [ { \"type\": \"stake\", \"startTimestamp\": 1727815440632, \"gatewayAddress\": \"QGWqtJdLLgm2ehFWiiPzMaoFLD50CnGuzZIPEdoDRGQ\", \"delegationId\": \"QGWqtJdLLgm2ehFWiiPzMaoFLD50CnGuzZIPEdoDRGQ_1727815440632\", \"balance\": 1383212512 }, { \"type\": \"vault\", \"startTimestamp\": 1730996691117, \"gatewayAddress\": \"QGWqtJdLLgm2ehFWiiPzMaoFLD50CnGuzZIPEdoDRGQ\", \"delegationId\": \"QGWqtJdLLgm2ehFWiiPzMaoFLD50CnGuzZIPEdoDRGQ_1730996691117\", \"vaultId\": \"_sGDS7X1hyLCVpfe40GWioH9BSOb7f0XWbhHBa1q4-g\", \"balance\": 50000000, \"endTimestamp\": 1733588691117 } ], \"nextCursor\": \"QGWqtJdLLgm2ehFWiiPzMaoFLD50CnGuzZIPEdoDRGQ_1730996691117\" } ``` #### instantWithdrawal() Instantly withdraws an existing vault on a gateway. If no `gatewayAddress` is provided, the signer's address will be used. _Note: Requires `signer` to be provided on `ARIO.init` to sign the transaction._ ```typescript const ario = ARIO.mainnet({ signer: new ArweaveSigner(jwk) }); // removes a delegated vault from a gateway const { id: txId } = await ario.instantWithdrawal( { // gateway address where delegate vault exists gatewayAddress: 't4Xr0_J4Iurt7caNST02cMotaz2FIbWQ4Kbj616RHl3', // delegated vault id to cancel vaultId: 'fDrr0_J4Iurt7caNST02cMotaz2FIbWQ4Kcj616RHl3', }, // optional additional tags { tags: [{ name: 'App-Name', value: 'My-Awesome-App' }], }, ); // removes an operator vault from a gateway const { id: txId } = await ario.instantWithdrawal( { vaultId: 'fDrr0_J4Iurt7caNST02cMotaz2FIbWQ4Kcj616RHl3', }, ); ``` #### cancelWithdrawal() Cancels an existing vault on a gateway. The vaulted stake will be returned to the callers stake. If no `gatewayAddress` is provided, the signer's address will be used. _Note: Requires `signer` to be provided on `ARIO.init` to sign the transaction._ ```typescript const ario = ARIO.mainnet({ signer: new ArweaveSigner(jwk) }); // cancels a delegated vault from a gateway const { id: txId } = await ario.cancelWithdrawal( { // gateway address where vault exists gatewayAddress: 't4Xr0_J4Iurt7caNST02cMotaz2FIbWQ4Kbj616RHl3', // vault id to cancel vaultId: 'fDrr0_J4Iurt7caNST02cMotaz2FIbWQ4Kcj616RHl3', }, // optional additional tags { tags: [{ name: 'App-Name', value: 'My-Awesome-App' }] }, ); // cancels an operator vault from a gateway const { id: txId } = await ario.cancelWithdrawal( { // operator vault id to cancel vaultId: 'fDrr0_J4Iurt7caNST02cMotaz2FIbWQ4Kcj616RHl3', }, ); ``` #### getAllowedDelegates() Retrieves all allowed delegates for a specific address. The `cursor` used for pagination is the last address from the previous request. ```typescript const ario = ARIO.mainnet(); const allowedDelegates = await ario.getAllowedDelegates({ address: 'QGWqtJdLLgm2ehFWiiPzMaoFLD50CnGuzZIPEdoDRGQ', }); ``` **Output:** ```json { \"sortOrder\": \"desc\", \"hasMore\": false, \"totalItems\": 4, \"limit\": 100, \"items\": [ \"PZ5vIhHf8VY969TxBPQN-rYY9CNFP9ggNsMBqlWUzWM\", \"N4h8M9A9hasa3tF47qQyNvcKjm4APBKuFs7vqUVm-SI\", \"JcC4ZLUY76vmWha5y6RwKsFqYTrMZhbockl8iM9p5lQ\", \"31LPFYoow2G7j-eSSsrIh8OlNaARZ84-80J-8ba68d8\" ] } ``` #### getGatewayVaults() Retrieves all vaults across all gateways for a specific address, paginated and sorted by the specified criteria. The `cursor` used for pagination is the last vaultId from the previous request. ```typescript const ario = ARIO.mainnet(); const vaults = await ario.getGatewayVaults({ address: '\"PZ5vIhHf8VY969TxBPQN-rYY9CNFP9ggNsMBqlWUzWM', }); ``` **Output:** ```json { \"sortOrder\": \"desc\", \"hasMore\": false, \"totalItems\": 1, \"limit\": 100, \"sortBy\": \"endTimestamp\", \"items\": [ { \"cursorId\": \"PZ5vIhHf8VY969TxBPQN-rYY9CNFP9ggNsMBqlWUzWM_1728067635857\", \"startTimestamp\": 1728067635857, \"balance\": 50000000000, \"vaultId\": \"PZ5vIhHf8VY969TxBPQN-rYY9CNFP9ggNsMBqlWUzWM\", \"endTimestamp\": 1735843635857 } ] } ``` #### getAllGatewayVaults() Retrieves all vaults across all gateways, paginated and sorted by the specified criteria. The `cursor` used for pagination is the last vaultId from the previous request. ```typescript const ario = ARIO.mainnet(); const vaults = await ario.getAllGatewayVaults({ limit: 1, sortBy: 'endTimestamp', sortOrder: 'desc', }); ``` **Output:** ```json { \"sortOrder\": \"desc\", \"hasMore\": true, \"totalItems\": 95, \"limit\": 1, \"sortBy\": \"endTimestamp\", \"items\": [ { \"cursorId\": \"PZ5vIhHf8VY969TxBPQN-rYY9CNFP9ggNsMBqlWUzWM_E-QVU3dta36Wia2uQw6tQLjQk7Qw5uN0Z6fUzsoqzUc\", \"gatewayAddress\": \"PZ5vIhHf8VY969TxBPQN-rYY9CNFP9ggNsMBqlWUzWM\", \"startTimestamp\": 1728067635857, \"balance\": 50000000000, \"vaultId\": \"E-QVU3dta36Wia2uQw6tQLjQk7Qw5uN0Z6fUzsoqzUc\", \"endTimestamp\": 1735843635857 } ], \"nextCursor\": \"PZ5vIhHf8VY969TxBPQN-rYY9CNFP9ggNsMBqlWUzWM_E-QVU3dta36Wia2uQw6tQLjQk7Qw5uN0Z6fUzsoqzUc\" } ``` #### increaseOperatorStake() Increases the callers operator stake. Must be executed with a wallet registered as a gateway operator. _Note: Requires `signer` to be provided on `ARIO.init` to sign the transaction._ ```typescript const ario = ARIO.mainnet({ signer: new ArweaveSigner(jwk) }); const { id: txId } = await ario.increaseOperatorStake( { qty: new ARIOToken(100).toMARIO(), }, { tags: [{ name: 'App-Name', value: 'My-Awesome-App' }], }, ); ``` #### decreaseOperatorStake() Decreases the callers operator stake. Must be executed with a wallet registered as a gateway operator. Requires `signer` to be provided on `ARIO.init` to sign the transaction. _Note: Requires `signer` to be provided on `ARIO.init` to sign the transaction._ ```typescript const ario = ARIO.mainnet({ signer: new ArweaveSigner(jwk) }); const { id: txId } = await ario.decreaseOperatorStake( { qty: new ARIOToken(100).toMARIO(), }, { tags: [{ name: 'App-Name', value: 'My-Awesome-App' }], }, ); ``` #### redelegateStake() Redelegates the stake of a specific address to a new gateway. Vault ID may be optionally included in order to redelegate from an existing withdrawal vault. The redelegation fee is calculated based on the fee rate and the stake amount. Users are allowed one free redelegation every seven epochs. Each additional redelegation beyond the free redelegation will increase the fee by 10%, capping at a 60% redelegation fee. e.g: If 1000 mARIO is redelegated and the fee rate is 10%, the fee will be 100 mARIO. Resulting in 900 mARIO being redelegated to the new gateway and 100 mARIO being deducted back to the protocol balance. ```typescript const ario = ARIO.mainnet({ signer: new ArweaveSigner(jwk) }); const { id: txId } = await ario.redelegateStake({ target: 't4Xr0_J4Iurt7caNST02cMotaz2FIbWQ4Kbj616RHl3', source: 'HwFceQaMQnOBgKDpnFqCqgwKwEU5LBme1oXRuQOWSRA', stakeQty: new ARIOToken(1000).toMARIO(), vaultId: 'fDrr0_J4Iurt7caNST02cMotaz2FIbWQ4Kcj616RHl3', }); ``` #### getRedelegationFee() Retrieves the fee rate as percentage required to redelegate the stake of a specific address. Fee rate ranges from 0% to 60% based on the number of redelegations since the last fee reset. ```typescript const ario = ARIO.mainnet(); const fee = await ario.getRedelegationFee({ address: 't4Xr0_J4Iurt7caNST02cMotaz2FIbWQ4Kbj616RHl3', }); ``` **Output:** ```json { \"redelegationFeeRate\": 10, \"feeResetTimestamp\": 1730996691117 } ``` #### getAllDelegates() Retrieves all delegates across all gateways, paginated and sorted by the specified criteria. The `cursor` used for pagination is a `cursorId` derived from delegate address and the gatewayAddress from the previous request. e.g `address_gatewayAddress`. ```typescript const ario = ARIO.mainnet(); const delegates = await ario.getAllDelegates({ limit: 2, sortBy: 'startTimestamp', sortOrder: 'desc', }); ``` **Output:** ```json { \"sortOrder\": \"desc\", \"hasMore\": true, \"totalItems\": 95, \"limit\": 2, \"sortBy\": \"startTimestamp\", \"items\": [ { \"startTimestamp\": 1734709397622, \"cursorId\": \"9jfM0uzGNc9Mkhjo1ixGoqM7ygSem9wx_EokiVgi0Bs_E-QVU3dta36Wia2uQw6tQLjQk7Qw5uN0Z6fUzsoqzUc\", \"gatewayAddress\": \"E-QVU3dta36Wia2uQw6tQLjQk7Qw5uN0Z6fUzsoqzUc\", \"address\": \"9jfM0uzGNc9Mkhjo1ixGoqM7ygSem9wx_EokiVgi0Bs\", \"delegatedStake\": 2521349108, \"vaultedStake\": 0 }, { \"startTimestamp\": 1734593229454, \"cursorId\": \"LtV0aSqgK3YI7c5FmfvZd-wG95TJ9sezj_a4syaLMS8_M0WP8KSzCvKpzC-HPF1WcddLgGaL9J4DGi76iMnhrN4\", \"gatewayAddress\": \"M0WP8KSzCvKpzC-HPF1WcddLgGaL9J4DGi76iMnhrN4\", \"address\": \"LtV0aSqgK3YI7c5FmfvZd-wG95TJ9sezj_a4syaLMS8\", \"delegatedStake\": 1685148110, \"vaultedStake\": 10000000 } ], \"nextCursor\": \"PZ5vIhHf8VY969TxBPQN-rYY9CNFP9ggNsMBqlWUzWM_QGWqtJdLLgm2ehFWiiPzMaoFLD50CnGuzZIPEdoDRGQ\" } ``` # General (/(ario-contract)/general) #### init() Factory function to that creates a read-only or writeable client. By providing a `signer` additional write APIs that require signing, like `joinNetwork` and `delegateStake` are available. By default, a read-only client is returned and no write APIs are available. ```typescript // read-only client const ario = ARIO.init(); // read-write client for browser environments const ario = ARIO.init({ signer: new ArConnectSigner(window.arweaveWallet, Arweave.init({}))}); // read-write client for node environments const ario = ARIO.init({ signer: new ArweaveSigner(JWK) }); ``` #### getInfo() Retrieves the information of the ARIO process. ```typescript const ario = ARIO.mainnet(); const info = await ario.getInfo(); ``` **Output:** ```json { \"Name\": \"ARIO\", \"Ticker\": \"ARIO\", \"Owner\": \"QGWqtJdLLgm2ehFWiiPzMaoFLD50CnGuzZIPEdoDRGQ\", \"Denomination\": 6, \"Handlers\": [\"_eval\", \"_default_\"], // full list of handlers, useful for debugging \"LastCreatedEpochIndex\": 31, // epoch index of the last tick \"LastDistributedEpochIndex\": 31 // epoch index of the last distribution } ``` #### getTokenSupply() Retrieves the total supply of tokens, returned in mARIO. The total supply includes the following: - `total` - the total supply of all tokens - `circulating` - the total supply minus locked, withdrawn, delegated, and staked - `locked` - tokens that are locked in the protocol (a.k.a. vaulted) - `withdrawn` - tokens that have been withdrawn from the protocol by operators and delegators - `delegated` - tokens that have been delegated to gateways - `staked` - tokens that are staked in the protocol by gateway operators - `protocolBalance` - tokens that are held in the protocol's treasury. This is included in the circulating supply. ```typescript const ario = ARIO.mainnet(); const supply = await ario.getTokenSupply(); ``` **Output:** ```json { \"total\": 1000000000000000000, \"circulating\": 998094653842520, \"locked\": 0, \"withdrawn\": 560563387278, \"delegated\": 1750000000, \"staked\": 1343032770199, \"protocolBalance\": 46317263683761 } ``` #### getBalance() Retrieves the balance of the specified wallet address. ```typescript const ario = ARIO.mainnet(); // the balance will be returned in mARIO as a value const balance = await ario .getBalance({ address: 'QGWqtJdLLgm2ehFWiiPzMaoFLD50CnGuzZIPEdoDRGQ', }) .then((balance: number) => new mARIOToken(balance).toARIO()); // convert it to ARIO for readability ``` **Output:** ```json 100000 ``` #### getBalances() Retrieves the balances of the ARIO process in `mARIO`, paginated and sorted by the specified criteria. The `cursor` used for pagination is the last wallet address from the previous request. ```typescript const ario = ARIO.mainnet(); const balances = await ario.getBalances({ cursor: '-4xgjroXENKYhTWqrBo57HQwvDL51mMdfsdsxJy6Y2Z_sA', limit: 100, sortBy: 'balance', sortOrder: 'desc', }); ``` **Output:** ```json { \"items\": [ { \"address\": \"-4xgjroXENKYhTWqrBo57HQwvDL51mMvSxJy6Y2Z_sA\", \"balance\": 1000000 }, { \"address\": \"-7vXsQZQDk8TMDlpiSLy3CnLi5PDPlAaN2DaynORpck\", \"balance\": 1000000 } // ...98 other balances ], \"hasMore\": true, \"nextCursor\": \"-7vXsQZQDk8TMDlpiSLy3CnLi5PDPlAaN2DaynORpck\", \"totalItems\": 1789, \"sortBy\": \"balance\", \"sortOrder\": \"desc\" } ``` #### transfer() Transfers `mARIO` to the designated `target` recipient address. Requires `signer` to be provided on `ARIO.init` to sign the transaction. _Note: Requires `signer` to be provided on `ARIO.init` to sign the transaction._ ```typescript const ario = ARIO.mainnet({ signer: new ArweaveSigner(jwk), }); const { id: txId } = await ario.transfer( { target: '-5dV7nk7waR8v4STuwPnTck1zFVkQqJh5K9q9Zik4Y5', qty: new ARIOToken(1000).toMARIO(), }, // optional additional tags { tags: [{ name: 'App-Name', value: 'My-Awesome-App' }] }, ); ``` # Networks (/(ario-contract)/networks) The SDK provides the following process IDs for the mainnet and testnet environments: - `ARIO_MAINNET_PROCESS_ID` - Mainnet ARIO process ID (production) - `ARIO_TESTNET_PROCESS_ID` - Testnet ARIO process ID (testing and development) - `ARIO_DEVNET_PROCESS_ID` - Devnet ARIO process ID (development) As of `v3.8.1` the SDK defaults all API interactions to **mainnet**. To use the **testnet** or **devnet** provide the appropriate `ARIO_TESTNET_PROCESS_ID` or `ARIO_DEVNET_PROCESS_ID` when initializing the client. #### Mainnet As of `v3.8.1` the SDK defaults all API interactions to **mainnet**. To use the **testnet** or **devnet** provide the appropriate `ARIO_TESTNET_PROCESS_ID` or `ARIO_DEVNET_PROCESS_ID` when initializing the client. ```typescript const ario = ARIO.mainnet(); // or ARIO.init() ``` #### Testnet ```typescript const testnet = ARIO.testnet(); // or ARIO.init({ processId: ARIO_TESTNET_PROCESS_ID }) ``` ##### Faucet The SDK provides APIs for claiming tokens via a faucet on the AR.IO Testnet process (`tARIO`) via the [ar-io-testnet-faucet] service. All token requests require a captcha to be solved, and the faucet is rate limited to prevent abuse. To claim testnet tokens from the testnet token faucet, you can use one of the following methods: 1. Visit [faucet.ar.io](https://faucet.ar.io) - the easiest way to quickly get tokens for testing for a single address. 2. Programmatically via the SDK - useful if you need to claim tokens for multiple addresses or dynamically within your application. - `ARIO.testnet().faucet.captchaUrl()` - returns the captcha URL for the testnet faucet. Open this URL in a new browser window and listen for the `ario-jwt-success` event to be emitted. - `ARIO.testnet().faucet.claimWithAuthToken({ authToken, recipient, quantity })` - claims tokens for the specified recipient address using the provided auth token. - `ARIO.testnet().faucet.verifyAuthToken({ authToken })` - verifies if the provided auth token is still valid. Example client-side code for claiming tokens ```typescript const testnet = ARIO.testnet(); const captchaUrl = await ario.faucet.captchaUrl(); // open the captcha URL in the browser, and listen for the auth token event const captchaWindow = window.open( captchaUrl.captchaUrl, '_blank', 'width=600,height=600', ); /** * The captcha URL includes a window.parent.postMessage event that is used to send the auth token to the parent window. * You can store the auth token in localStorage and use it to claim tokens for the duration of the auth token's expiration (default 1 hour). */ window.parent.addEventListener('message', async (event) => { if (event.data.type === 'ario-jwt-success') { localStorage.setItem('ario-jwt', event.data.token); localStorage.setItem('ario-jwt-expires-at', event.data.expiresAt); // close our captcha window captchaWindow?.close(); // claim the tokens using the JWT token const res = await testnet.faucet .claimWithAuthToken({ authToken: event.data.token, recipient: await window.arweaveWallet.getActiveAddress(), quantity: new ARIOToken(100).toMARIO().valueOf(), // 100 ARIO }) .then((res) => { alert( 'Successfully claimed 100 ARIO tokens! Transaction ID: ' + res.id, ); }) .catch((err) => { alert(`Failed to claim tokens: ${err}`); }); } }); /** * Once you have a valid JWT, you can check if it is still valid and use it for subsequent requests without having to open the captcha again. */ if ( localStorage.getItem('ario-jwt-expires-at') && Date.now() # Primary Names (/(ario-contract)/primary-names) #### getPrimaryNames() Retrieves all primary names paginated and sorted by the specified criteria. The `cursor` used for pagination is the last name from the previous request. ```typescript const ario = ARIO.mainnet(); const names = await ario.getPrimaryNames({ cursor: 'ao', // this is the last name from the previous request limit: 1, sortBy: 'startTimestamp', sortOrder: 'desc', }); ``` **Output:** ```json { \"sortOrder\": \"desc\", \"hasMore\": true, \"totalItems\": 100, \"limit\": 1, \"sortBy\": \"startTimestamp\", \"cursor\": \"arns\", \"items\": [ { \"owner\": \"HwFceQaMQnOBgKDpnFqCqgwKwEU5LBme1oXRuQOWSRA\", \"startTimestamp\": 1719356032297, \"name\": \"arns\" } ] } ``` #### getPrimaryName() Retrieves the primary name for a given name or address. ```typescript const ario = ARIO.mainnet(); const name = await ario.getPrimaryName({ name: 'arns', }); // or const name = await ario.getPrimaryName({ address: 't4Xr0_J4Iurt7caNST02cMotaz2FIbWQ4Kbj616RHl3', }); ``` **Output:** ```json { \"owner\": \"HwFceQaMQnOBgKDpnFqCqgwKwEU5LBme1oXRuQOWSRA\", \"startTimestamp\": 1719356032297, \"name\": \"arns\" } ``` #### setPrimaryName() Sets an ArNS name already owned by the `signer` as their primary name. Note: `signer` must be the owner of the `processId` that is assigned to the name. If not, the transaction will fail. _Note: Requires `signer` to be provided on `ARIO.init` to sign the transaction._ ```typescript const signer = new ArweaveSigner(jwk); const ario = ARIO.mainnet({ signer }); await ario.setPrimaryName({ name: 'my-arns-name' }); // the caller must already have purchased the name my-arns-name and be assigned as the owner of the processId that is assigned to the name ``` #### requestPrimaryName() Requests a primary name for the `signer`'s address. The request must be approved by the new owner of the requested name via the `approvePrimaryNameRequest`[#approveprimarynamerequest-name-address-] API. _Note: Requires `signer` to be provided on `ARIO.init` to sign the transaction._ ```typescript const ario = ARIO.mainnet({ signer: new ArweaveSigner(jwk) }); const { id: txId } = await ario.requestPrimaryName({ name: 'arns', }); ``` #### getPrimaryNameRequest() Retrieves the primary name request for a a wallet address. ```typescript const ario = ARIO.mainnet(); const request = await ario.getPrimaryNameRequest({ initiator: 't4Xr0_J4Iurt7caNST02cMotaz2FIbWQ4Kbj616RHl3', }); ``` **Output:** ```json { \"initiator\": \"t4Xr0_J4Iurt7caNST02cMotaz2FIbWQ4Kbj616RHl3\", \"name\": \"arns\", \"startTimestamp\": 1728067635857, \"endTimestamp\": 1735843635857 } ``` # Vaults (/(ario-contract)/vaults) #### getVault() Retrieves the locked-balance user vault of the ARIO process by the specified wallet address and vault ID. ```typescript const ario = ARIO.mainnet(); const vault = await ario.getVault({ address: 'QGWqtJdLLgm2ehFWiiPzMaoFLD50CnGuzZIPEdoDRGQ', vaultId: 'vaultIdOne', }); ``` **Output:** ```json { \"balance\": 1000000, \"startTimestamp\": 123, \"endTimestamp\": 4567 } ``` #### getVaults() Retrieves all locked-balance user vaults of the ARIO process, paginated and sorted by the specified criteria. The `cursor` used for pagination is the last wallet address from the previous request. ```typescript const ario = ARIO.mainnet(); const vaults = await ario.getVaults({ cursor: '0', limit: 100, sortBy: 'balance', sortOrder: 'desc', }); ``` **Output:** ```json { \"items\": [ { \"address\": \"QGWqtJdLLgm2ehFWiiPzMaoFLD50CnGuzZIPEdoDRGQ\", \"vaultId\": \"vaultIdOne\", \"balance\": 1000000, \"startTimestamp\": 123, \"endTimestamp\": 4567 }, { \"address\": \"QGWqtJdLLgm2ehFWiiPzMaoFLD50CnGuzZIPEdoDRGQ\", \"vaultId\": \"vaultIdTwo\", \"balance\": 1000000, \"startTimestamp\": 123, \"endTimestamp\": 4567 } // ...98 other addresses with vaults ], \"hasMore\": true, \"nextCursor\": \"QGWqtJdLLgm2ehFWiiPzMaoFLD50CnGuzZIPEdoDRGQ\", \"totalItems\": 1789, \"sortBy\": \"balance\", \"sortOrder\": \"desc\" } ``` #### vaultedTransfer() Transfers `mARIO` to the designated `recipient` address and locks the balance for the specified `lockLengthMs` milliseconds. The `revokable` flag determines if the vaulted transfer can be revoked by the sender. _Note: Requires `signer` to be provided on `ARIO.init` to sign the transaction._ ```typescript const ario = ARIO.mainnet({ signer: new ArweaveSigner(jwk) }); const { id: txId } = await ario.vaultedTransfer( { recipient: '-5dV7nk7waR8v4STuwPnTck1zFVkQqJh5K9q9Zik4Y5', quantity: new ARIOToken(1000).toMARIO(), lockLengthMs: 1000 * 60 * 60 * 24 * 365, // 1 year revokable: true, }, // optional additional tags { tags: [{ name: 'App-Name', value: 'My-Awesome-App' }] }, ); ``` #### revokeVault() Revokes a vaulted transfer by the recipient address and vault ID. Only the sender of the vaulted transfer can revoke it. _Note: Requires `signer` to be provided on `ARIO.init` to sign the transaction._ ```typescript const ario = ARIO.mainnet({ signer: new ArweaveSigner(jwk) }); const { id: txId } = await ario.revokeVault({ recipient: '-5dV7nk7waR8v4STuwPnTck1zFVkQqJh5K9q9Zik4Y5', vaultId: 'IPdwa3Mb_9pDD8c2IaJx6aad51Ss-_TfStVwBuhtXMs', }); ``` #### createVault() Creates a vault for the specified `quantity` of mARIO from the signer's balance and locks it for the specified `lockLengthMs` milliseconds. ```typescript const ario = ARIO.mainnet({ signer: new ArweaveSigner(jwk) }); const { id: txId } = await ario.createVault({ lockLengthMs: 1000 * 60 * 60 * 24 * 365, // 1 year quantity: new ARIOToken(1000).toMARIO(), }); ``` #### extendVault() Extends the lock length of a signer's vault by the specified `extendLengthMs` milliseconds. ```typescript const ario = ARIO.mainnet({ signer: new ArweaveSigner(jwk) }); const { id: txId } = await ario.extendVault({ vaultId: 'vaultIdOne', extendLengthMs: 1000 * 60 * 60 * 24 * 365, // 1 year }); ``` #### increaseVault() Increases the balance of a signer's vault by the specified `quantity` of mARIO. ```typescript const ario = ARIO.mainnet({ signer: new ArweaveSigner(jwk) }); const { id: txId } = await ario.increaseVault({ vaultId: 'vaultIdOne', quantity: new ARIOToken(1000).toMARIO(), }); ``` # AR.IO SDK (/index) **For AI and LLM users**: Access the complete AR.IO SDK documentation in plain text format at llm.txt for easy consumption by AI agents and language models. # AR.IO SDK Please refer to the [source code](https://github.com/ar-io/ar-io-sdk) for SDK details. # Logging (/logging) The library uses a lightweight console logger by default for both Node.js and web environments. The logger outputs structured JSON logs with timestamps. You can configure the log level via `setLogLevel()` API or provide a custom logger that satisfies the `ILogger` interface. #### Default Logger ```typescript // set the log level Logger.default.setLogLevel('debug'); // Create a new logger instance with a specific level const logger = new Logger({ level: 'debug' }); ``` #### Custom Logger Implementation You can provide any custom logger that implements the `ILogger` interface: ```typescript // Custom logger example const customLogger: ILogger = { info: (message, ...args) => console.log(`[INFO] ${message}`, ...args), warn: (message, ...args) => console.warn(`[WARN] ${message}`, ...args), error: (message, ...args) => console.error(`[ERROR] ${message}`, ...args), debug: (message, ...args) => console.debug(`[DEBUG] ${message}`, ...args), setLogLevel: (level) => { /* implement level filtering */ }, }; // Use custom logger with any class const ario = ARIO.mainnet({ logger: customLogger }); // or set it as the default logger in the entire SDK Logger.default = customLogger; ``` #### Winston Logger (Optional) For advanced logging features, you can optionally install Winston and use the provided Winston logger adapter: ```bash yarn add winston ``` ```typescript // Create Winston logger with custom configuration const winstonLogger = new WinstonLogger({ level: 'debug', }); // Use with any class that accepts a logger const ario = ARIO.mainnet({ logger: winstonLogger }); // or set it as the default logger in the entire SDK Logger.default = winstonLogger; ``` #### Other Popular Loggers You can easily integrate other popular logging libraries: ```typescript // Bunyan example const bunyanLogger = bunyan.createLogger({ name: 'ar-io-sdk' }); const adapter: ILogger = { info: (message, ...args) => bunyanLogger.info({ args }, message), warn: (message, ...args) => bunyanLogger.warn({ args }, message), error: (message, ...args) => bunyanLogger.error({ args }, message), debug: (message, ...args) => bunyanLogger.debug({ args }, message), setLogLevel: (level) => bunyanLogger.level(level), }; const ario = ARIO.mainnet({ logger: adapter }); // or set it as the default logger in the entire SDK Logger.default = adapter; ``` # Pagination (/pagination) #### Overview Certain APIs that could return a large amount of data are paginated using cursors. The SDK uses the `cursor` pattern (as opposed to pages) to better protect against changing data while paginating through a list of items. For more information on pagination strategies refer to [this article](https://www.getknit.dev/blog/api-pagination-best-practices#api-pagination-techniques-). Paginated results include the following properties: - `items`: the list of items on the current request, defaulted to 100 items. - `nextCursor`: the cursor to use for the next batch of items. This is `undefined` if there are no more items to fetch. - `hasMore`: a boolean indicating if there are more items to fetch. This is `false` if there are no more items to fetch. - `totalItems`: the total number of items available. This may change as new items are added to the list, only use this for informational purposes. - `sortBy`: the field used to sort the items, by default this is `startTimestamp`. - `sortOrder`: the order used to sort the items, by default this is `desc`. To request all the items in a list, you can iterate through the list using the `nextCursor` until `hasMore` is `false`. ```typescript let hasMore = true; let cursor: string | undefined; const gateaways = []; while (hasMore) { const page = await ario.getGateways({ limit: 100, cursor }); gateaways.push(...items); cursor = page.nextCursor; hasMore = page.hasMore; } ``` #### Filtering Paginated APIs also support filtering by providing a `filters` parameter. Filters can be applied to any field in the response. When multiple keys are provided, they are treated as AND conditions (all conditions must match). When multiple values are provided for a single key (as an array), they are treated as OR conditions (any value can match). Example: ```typescript const records = await ario.getArNSRecords({ filters: { type: 'lease', processId: [ 'ZkgLfyHALs5koxzojpcsEFAKA8fbpzP7l-tbM7wmQNM', 'r61rbOjyXx3u644nGl9bkwLWlWmArMEzQgxBo2R-Vu0', ], }, }); ``` In the example above, the query will return ArNS records where: - The type is \"lease\" AND - The processId is EITHER \"ZkgLfyHALs5koxzojpcsEFAKA8fbpzP7l-tbM7wmQNM\" OR \"r61rbOjyXx3u644nGl9bkwLWlWmArMEzQgxBo2R-Vu0\" # Token Conversion (/token-conversion) The ARIO process stores all values as mARIO (milli-ARIO) to avoid floating-point arithmetic issues. The SDK provides an `ARIOToken` and `mARIOToken` classes to handle the conversion between ARIO and mARIO, along with rounding logic for precision. **All process interactions expect values in mARIO. If numbers are provided as inputs, they are assumed to be in raw mARIO values.** #### Converting ARIO to mARIO ```typescript const arioValue = 1; const mARIOValue = new ARIOToken(arioValue).toMARIO(); const mARIOValue = 1_000_000; const arioValue = new mARIOToken(mARIOValue).toARIO(); ```","estimatedWords":9594,"lastModified":"2025-10-20T12:12:27.680Z","breadcrumbs":["sdks","ar io sdk","llm.txt"],"siteKey":"ario","siteName":"AR-IO Network","depth":3,"crawledAt":"2025-10-20T12:12:27.680Z"},{"url":"https://docs.ar.io/sdks/ar-io-sdk/general","title":"General","content":"AR.IO SDKARIO ContractGeneralCopy MarkdownOpeninit() Factory function to that creates a read-only or writeable client. By providing a signer additional write APIs that require signing, like joinNetwork and delegateStake are available. By default, a read-only client is returned and no write APIs are available.","estimatedWords":43,"lastModified":"2025-10-20T12:12:29.574Z","breadcrumbs":["sdks","ar io sdk","general"],"siteKey":"ario","siteName":"AR-IO Network","depth":3,"crawledAt":"2025-10-20T12:12:29.574Z"},{"url":"https://docs.ar.io/sdks/wayfinder/packages","title":"Packages","content":"Wayfinder SDK'sPackagesCopy MarkdownOpenThis monorepo contains the following packages: @ar.io/wayfinder-core : Core JavaScript library for the Wayfinder routing and verification protocol @ar.io/wayfinder-react : React components for Wayfinder, including Hooks and Context provider @ar.io/wayfinder-extension : Chrome extension for Wayfinder @ar.io/wayfinder-cli (coming soon) : CLI for interacting with Wayfinder in the terminal How is this guide?GoodBadWayfinder SDK'sDecentralized access to Arweave data with built-in verification and gateway routingWhat is it?Decentralized access to Arweave data with built-in verification and gateway routing","estimatedWords":76,"lastModified":"2025-10-20T12:12:30.532Z","breadcrumbs":["sdks","wayfinder","packages"],"siteKey":"ario","siteName":"AR-IO Network","depth":3,"crawledAt":"2025-10-20T12:12:30.532Z"},{"url":"https://docs.ar.io/sdks/wayfinder/what-is-it","title":"What is it","content":"Wayfinder SDK'sWhat is it?Copy MarkdownOpenWayfinder is a simple, open-source client-side routing and verification protocol for the permaweb. It leverages the AR.IO Network to route users to the most optimal gateway for a given request.How is this guide?GoodBadPackagesDecentralized access to Arweave data with built-in verification and gateway routingWho is it for?Decentralized access to Arweave data with built-in verification and gateway routing","estimatedWords":60,"lastModified":"2025-10-20T12:12:31.372Z","breadcrumbs":["sdks","wayfinder","what is it"],"siteKey":"ario","siteName":"AR-IO Network","depth":3,"crawledAt":"2025-10-20T12:12:31.372Z"},{"url":"https://docs.ar.io/sdks/wayfinder/who-is-it-for","title":"Who is it for","content":"Wayfinder SDK'sWho is it for?Copy MarkdownOpen Builders who need reliable, decentralized access to Arweave data through the powerful AR.IO Network Browsers who demand complete control over their permaweb journey with customizable gateways and robust verification settings for enhanced security and reliability Operators who power the AR.IO Network and want to earn rewards* for serving wayfinder traffic to the growing permaweb ecosystem How is this guide?GoodBadWhat is it?Decentralized access to Arweave data with built-in verification and gateway routingContributingDecentralized access to Arweave data with built-in verification and gateway routing","estimatedWords":87,"lastModified":"2025-10-20T12:12:32.384Z","breadcrumbs":["sdks","wayfinder","who is it for"],"siteKey":"ario","siteName":"AR-IO Network","depth":3,"crawledAt":"2025-10-20T12:12:32.384Z"},{"url":"https://docs.ar.io/sdks/wayfinder/contributing","title":"Contributing","content":"Wayfinder SDK'sContributingCopy MarkdownOpen Branch from alpha Create a new branch for your changes (e.g. feat/my-feature) Make your changes on your branch, push them to your branch As you make commits/changes or once you're ready to release, create a changeset describing your changes via npx changeset. Follow the prompts to select the packages that are affected by your changes. Add and commit the changeset to your branch Request review from a maintainer, and once approved, merge your changes into the alpha branch A release PR will be automatically created with all pending changesets to the alpha branch The maintainer will review the PR and merge it into alpha, which will trigger the automated release process using all pending changesets How is this guide?GoodBadWho is it for?Decentralized access to Arweave data with built-in verification and gateway routingCreating a ChangesetDecentralized access to Arweave data with built-in verification and gateway routing","estimatedWords":147,"lastModified":"2025-10-20T12:12:33.349Z","breadcrumbs":["sdks","wayfinder","contributing"],"siteKey":"ario","siteName":"AR-IO Network","depth":3,"crawledAt":"2025-10-20T12:12:33.350Z"},{"url":"https://docs.ar.io/sdks/wayfinder/testing","title":"Testing","content":"Wayfinder SDK'sTestingCopy MarkdownOpen yarn test - runs all tests in all packages (monorepo) How is this guide?GoodBadManual Release ProcessDecentralized access to Arweave data with built-in verification and gateway routingLinting & FormattingDecentralized access to Arweave data with built-in verification and gateway routing","estimatedWords":41,"lastModified":"2025-10-20T12:12:34.225Z","breadcrumbs":["sdks","wayfinder","testing"],"siteKey":"ario","siteName":"AR-IO Network","depth":3,"crawledAt":"2025-10-20T12:12:34.225Z"},{"url":"https://docs.ar.io/sdks/wayfinder/linting-formatting","title":"Linting  Formatting","content":"Wayfinder SDK'sLinting & FormattingCopy MarkdownOpen yarn lint:check - checks for linting errors yarn lint:fix - fixes linting errors yarn format:check - checks for formatting errors yarn format:fix - fixes formatting errors How is this guide?GoodBadTestingDecentralized access to Arweave data with built-in verification and gateway routingArchitectureDecentralized access to Arweave data with built-in verification and gateway routing","estimatedWords":55,"lastModified":"2025-10-20T12:12:35.109Z","breadcrumbs":["sdks","wayfinder","linting formatting"],"siteKey":"ario","siteName":"AR-IO Network","depth":3,"crawledAt":"2025-10-20T12:12:35.109Z"},{"url":"https://docs.ar.io/sdks/wayfinder/architecture","title":"Architecture","content":"Wayfinder SDK'sArchitectureCopy MarkdownOpen Code to interfaces. Prefer type safety over runtime safety. Prefer composition over inheritance. Prefer integration tests over unit tests. How is this guide?GoodBadLinting & FormattingDecentralized access to Arweave data with built-in verification and gateway routingArDrive CLICommand line interface for ArDrive","estimatedWords":43,"lastModified":"2025-10-20T12:12:36.097Z","breadcrumbs":["sdks","wayfinder","architecture"],"siteKey":"ario","siteName":"AR-IO Network","depth":3,"crawledAt":"2025-10-20T12:12:36.097Z"},{"url":"https://docs.ar.io/sdks/wayfinder/llm.txt","title":"Llmtxt","content":"# Automated Releases (/(releases)/automated-releases) This repository is configured with GitHub Actions workflows that automate the release process: - **Main Branch**: When changes are merged to `main`, a standard release is created - **Alpha Branch**: When changes are merged to `alpha`, a prerelease (alpha tagged) is created The workflow automatically: 1. Determines whether to create a prerelease or standard release based on the branch 2. Versions packages using changesets 3. Publishes to npm 4. Creates GitHub releases 5. Pushes tags back to the repository To use the automated process: 1. Create changesets for your changes 2. Push your changes to a feature branch 3. Create a pull request to `alpha` (for prereleases) or `main` (for standard releases) 4. When the PR is merged, the release will be automatically created # Creating a Changeset (/(releases)/creating-a-changeset) To create a changeset when making changes: ```bash npx changeset ``` This will guide you through the process of documenting your changes and selecting which packages are affected. Changesets will be used during the release process to update package versions and generate changelogs. # Manual Release Process (/(releases)/manual-release-process) If you need to release manually, follow these steps: #### Alpha Releases To release a new alpha version: ```bash npx changeset version ``` 3. Review the version changes and changelogs 4. Commit the changes: ```bash git add . git commit -m \"chore(release): version packages\" ``` 5. Publish the packages to npm: ```bash npm run build npx changeset publish ``` 6. Push the changes and tags: ```bash git push origin main --follow-tags ``` #### Prerelease Mode For prerelease versions (e.g., beta, alpha): 1. Enter prerelease mode specifying the tag: ```bash npx changeset pre enter beta ``` 2. Create changesets as normal: ```bash npx changeset ``` 3. Version and publish as normal: ```bash npx changeset version git add . git commit -m \"chore(release): prerelease version packages\" npm run build npx changeset publish git push origin main --follow-tags ``` 4. Exit prerelease mode when ready for a stable release: ```bash npx changeset pre exit ``` 5. Follow the normal release process for the stable version. # Architecture (/architecture) - Code to interfaces. - Prefer type safety over runtime safety. - Prefer composition over inheritance. - Prefer integration tests over unit tests. # Contributing (/contributing) 1. Branch from `alpha` 2. Create a new branch for your changes (e.g. `feat/my-feature`) 3. Make your changes on your branch, push them to your branch 4. As you make commits/changes or once you're ready to release, create a changeset describing your changes via `npx changeset`. 5. Follow the prompts to select the packages that are affected by your changes. 6. Add and commit the changeset to your branch 7. Request review from a maintainer, and once approved, merge your changes into the `alpha` branch 8. A release PR will be automatically created with all pending changesets to the `alpha` branch 9. The maintainer will review the PR and merge it into `alpha`, which will trigger the automated release process using all pending changesets # Wayfinder SDK's (/index) **For AI and LLM users**: Access the complete Wayfinder SDK's documentation in plain text format at llm.txt for easy consumption by AI agents and language models. # Wayfinder SDK's Please refer to the [source code](https://github.com/ar-io/wayfinder) for SDK details. # Linting & Formatting (/linting-formatting) - `yarn lint:check` - checks for linting errors - `yarn lint:fix` - fixes linting errors - `yarn format:check` - checks for formatting errors - `yarn format:fix` - fixes formatting errors # Packages (/packages) This monorepo contains the following packages: - **[@ar.io/wayfinder-core](./packages/wayfinder-core)** ![npm](https://img.shields.io/npm/v/@ar.io/wayfinder-core.svg) : Core JavaScript library for the Wayfinder routing and verification protocol - **[@ar.io/wayfinder-react](./packages/wayfinder-react)** ![npm](https://img.shields.io/npm/v/@ar.io/wayfinder-react.svg) : React components for Wayfinder, including Hooks and Context provider - **[@ar.io/wayfinder-extension](./packages/wayfinder-extension)** ![chrome](https://img.shields.io/chrome-web-store/v/hnhmeknhajanolcoihhkkaaimapnmgil?label=chrome) : Chrome extension for Wayfinder - **[@ar.io/wayfinder-cli](./packages/cli)** (coming soon) : CLI for interacting with Wayfinder in the terminal # Testing (/testing) - `yarn test` - runs all tests in all packages (monorepo) # Custom Providers and Strategies (/wayfinder-core/(advanced-usage)/custom-providers-and-strategies) For advanced use cases, you can provide custom providers and strategies to `createWayfinderClient`: ```javascript const wayfinder = createWayfinderClient({ ario: ARIO.mainnet() // Gateway selection gatewaySelection: 'top-ranked', // Enable caching with custom TTL cache: { ttlSeconds: 3600 }, // 1 hour // Override 'routing' with custom routing strategy routingStrategy: new FastestPingRoutingStrategy({ timeoutMs: 1000, }), // Override 'verification' with custom verification strategy verificationStrategy: new HashVerificationStrategy({ trustedGateways: ['https://permagate.io'], }), }); ``` # Direct Constructor Usage (/wayfinder-core/(advanced-usage)/direct-constructor-usage) For complete control, you can use the Wayfinder constructor directly. This is useful when you need fine-grained control over the configuration: > _Wayfinder client that caches the top 10 gateways by operator stake from the ARIO Network for 1 hour and uses the fastest pinging routing strategy to select the fastest gateway for requests._ ```javascript const wayfinder = new Wayfinder({ // cache the top 10 gateways by operator stake from the ARIO Network for 1 hour gatewaysProvider: new SimpleCacheGatewaysProvider({ ttlSeconds: 60 * 60, // cache the gateways for 1 hour gatewaysProvider: new NetworkGatewaysProvider({ ario: ARIO.mainnet(), sortBy: 'operatorStake', sortOrder: 'desc', limit: 10, }), }), // routing settings routingSettings: { // use the fastest pinging strategy to select the fastest gateway for requests strategy: new FastestPingRoutingStrategy({ timeoutMs: 1000, }), // events events: { onRoutingStarted: (event) => { console.log('Routing started!', event); }, onRoutingSkipped: (event) => { console.log('Routing skipped!', event); }, onRoutingSucceeded: (event) => { console.log('Routing succeeded!', event); }, }, }, // verification settings verificationSettings: { // enable verification - if false, verification will be skipped for all requests enabled: true, // verify the data using the hash of the data against a list of trusted gateways strategy: new HashVerificationStrategy({ trustedGateways: ['https://permagate.io'], }), // strict verification - if true, verification failures will cause requests to fail strict: true, // events events: { onVerificationProgress: (event) => { console.log('Verification progress!', event); }, onVerificationSucceeded: (event) => { console.log('Verification succeeded!', event); }, onVerificationFailed: (event) => { console.log('Verification failed!', event); }, }, }, }); ``` # NetworkGatewaysProvider (/wayfinder-core/(gateway-providers)/networkgatewaysprovider) Returns a list of gateways from the ARIO Network based on on-chain metrics. You can specify on-chain metrics for gateways to prioritize the highest quality gateways. This requires installing the `@ar.io/sdk` package and importing the `ARIO` object. *It is recommended to use this provider for most use cases to leverage the AR.IO Network.* ```javascript // requests will be routed to one of the top 10 gateways by operator stake const gatewayProvider = new NetworkGatewaysProvider({ ario: ARIO.mainnet(), sortBy: 'operatorStake', // sort by 'operatorStake' | 'totalDelegatedStake' sortOrder: 'desc', // 'asc' limit: 10, // number of gateways to use filter: (gateway) => { // use only active gateways that did not fail in the last epoch return gateway.status === 'joined' && gateway.stats.failedConsecutiveEpochs === 0; }, }); ``` # StaticGatewaysProvider (/wayfinder-core/(gateway-providers)/staticgatewaysprovider) The static gateway provider returns a list of gateways that you provide. This is useful for testing or for users who want to use a specific gateway for all requests. ```javascript const gatewayProvider = new StaticGatewaysProvider({ gateways: ['https://arweave.net'], }); ``` # TrustedPeersGatewaysProvider (/wayfinder-core/(gateway-providers)/trustedpeersgatewaysprovider) Fetches a dynamic list of trusted peer gateways from an AR.IO gateway's `/ar-io/peers` endpoint. This provider is useful for discovering available gateways from a trusted source. ```javascript const gatewayProvider = new TrustedPeersGatewaysProvider({ trustedGateway: 'https://arweave.net', // Gateway to fetch peers from }); // The provider will fetch the peer list from https://arweave.net/ar-io/peers // and return an array of gateway URLs from the response ``` # Caching (/wayfinder-core/(installation-notes)/caching) Wayfinder supports intelligent caching: - **In browsers**: Uses localStorage for persistent caching across page reloads - **In Node.js**: Uses in-memory caching - **What's cached**: Gateway lists, routing decisions, and more - **Cache configuration**: - `cache: true` - Enable with default 5-minute TTL - `cache: { ttlSeconds: 3600 }` - Enable with custom TTL (in seconds) - `cache: false` - Disable caching (default) # Optional Dependencies (/wayfinder-core/(installation-notes)/optional-dependencies) The `@ar.io/sdk` package is an optional peer dependency. To use AR.IO network gateways, you must explicitly provide an `ario` instance: **With AR.IO SDK (Recommended):** ```bash npm install @ar.io/wayfinder-core @ar.io/sdk yarn add @ar.io/wayfinder-core @ar.io/sdk ``` - `createWayfinderClient({ ario: ARIO.mainnet() })` uses AR.IO network gateways - Supports intelligent gateway selection criteria - Dynamic gateway discovery and updates # Global request events (/wayfinder-core/(monitoring-and-events)/global-request-events) Wayfinder emits events during the routing and verification process for all requests, allowing you to monitor its operation. All events are emitted on the `wayfinder.emitter` event emitter, and are updated for each request. ```javascript // Provide events to the Wayfinder constructor for tracking all requests const wayfinder = new Wayfinder({ routingSettings: { events: { onRoutingStarted: (event) => { console.log('Routing started!', event); }, onRoutingSkipped: (event) => { console.log('Routing skipped!', event); }, onRoutingSucceeded: (event) => { console.log('Routing succeeded!', event); }, }, }, verificationSettings: { events: { onVerificationSucceeded: (event) => { console.log(`Verification passed for transaction: ${event.txId}`); }, onVerificationFailed: (event) => { console.error( `Verification failed for transaction: ${event.txId}`, event.error, ); }, onVerificationProgress: (event) => { const percentage = (event.processedBytes / event.totalBytes) * 100; console.log( `Verification progress for ${event.txId}: ${percentage.toFixed(2)}%`, ); }, }, }, }); // listen to the global wayfinder event emitter for all requests wayfinder.emitter.on('routing-succeeded', (event) => { console.log(`Request routed to: ${event.targetGateway}`); }); wayfinder.emitter.on('routing-failed', (event) => { console.error(`Routing failed: ${event.error.message}`); }); wayfinder.emitter.on('verification-progress', (event) => { console.log(`Verification progress: ${event.progress}%`); }); wayfinder.emitter.on('verification-succeeded', (event) => { console.log(`Verification succeeded: ${event.txId}`); }); wayfinder.emitter.on('verification-failed', (event) => { console.error(`Verification failed: ${event.error.message}`); }); ``` # Request-specific events (/wayfinder-core/(monitoring-and-events)/request-specific-events) You can also provide events to the `request` function to track a single request. These events are called for each request and are not updated for subsequent requests. Events are still emitted to the global event emitter for all requests. It is recommended to use the global event emitter for tracking all requests, and the request-specific events for tracking a single request. ```javascript // create a wayfinder instance with verification enabled const wayfinder = new Wayfinder({ verificationSettings: { enabled: true, strategy: new HashVerificationStrategy({ trustedGateways: ['https://permagate.io'], }), events: { onVerificationProgress: (event) => { console.log(`Global callback handler called for: ${event.txId}`); }, onVerificationSucceeded: (event) => { console.log(`Global callback handler called for: ${event.txId}`); }, }, }, }); const response = await wayfinder.request('ar://example-name', { verificationSettings: { // these callbacks will be triggered for this request only, the global callback handlers are still called events: { onVerificationProgress: (event) => { console.log(`Request-specific callback handler called for: ${event.txId}`); }, onVerificationSucceeded: (event) => { console.log(`Request-specific callback handler called for: ${event.txId}`); }, }, }, }); ``` # CompositeRoutingStrategy (/wayfinder-core/(routing-strategies)/compositeroutingstrategy) Chains multiple routing strategies together, trying each sequentially until one succeeds. This strategy provides maximum resilience by allowing complex fallback scenarios where you can combine different routing approaches. ```javascript import { CompositeRoutingStrategy, FastestPingRoutingStrategy, RandomRoutingStrategy, StaticRoutingStrategy, NetworkGatewaysProvider } from '@ar.io/wayfinder-core'; // Example 1: Try fastest ping first, fallback to random selection const strategy = new CompositeRoutingStrategy({ strategies: [ new FastestPingRoutingStrategy({ timeoutMs: 500, gatewaysProvider: new NetworkGatewaysProvider({ ario: ARIO.mainnet(), sortBy: 'operatorStake', limit: 10, }), }), new RandomRoutingStrategy(), // fallback if ping strategy fails ], }); // Example 2: Try preferred gateway, then fastest ping, then any random gateway const complexStrategy = new CompositeRoutingStrategy({ strategies: [ new StaticRoutingStrategy({ gateway: 'https://my-preferred-gateway.com' }), new FastestPingRoutingStrategy({ timeoutMs: 1000 }), new RandomRoutingStrategy(), // final fallback ], }); const gateway = await strategy.selectGateway({ gateways: [new URL('https://gateway1.com'), new URL('https://gateway2.com')], }); ``` **How it works:** 1. The composite strategy tries each routing strategy in order 2. If a strategy successfully returns a gateway, that gateway is used 3. If a strategy throws an error, the next strategy is tried 4. If all strategies fail, an error is thrown 5. The first successful strategy short-circuits the process (remaining strategies are not tried) **Common Use Cases:** - **Performance + Resilience**: Try fastest ping first, fallback to random if ping fails - **Preferred + Network**: Use your own gateway first, fallback to AR.IO network selection - **Multi-tier Fallback**: Try premium gateways, then standard gateways, then any available gateway - **Development + Production**: Use local gateway in development, fallback to production gateways # FastestPingRoutingStrategy (/wayfinder-core/(routing-strategies)/fastestpingroutingstrategy) Selects the fastest gateway based on simple HEAD request to the specified route. ```javascript // use with static gateways (override gatewaysProvider if provided) const routingStrategy = new FastestPingRoutingStrategy({ timeoutMs: 1000, }); const gateway = await routingStrategy.selectGateway({ gateways: [new URL('https://slow.net'), new URL('https://medium.net'), new URL('https://fast.net')], }); // use with gatewaysProvider (fetches dynamically) const routingStrategy2 = new FastestPingRoutingStrategy({ timeoutMs: 1000, gatewaysProvider: new NetworkGatewaysProvider({ ario: ARIO.mainnet(), sortBy: 'operatorStake', limit: 20, }), }); const gateway2 = await routingStrategy2.selectGateway({ path: '/ar-io/info' }); // uses gatewaysProvider // override the gatewaysProvider with a static list of gateways const gateway3 = await routingStrategy2.selectGateway({ gateways: [new URL('https://priority-gateway.net')], // overrides gatewaysProvider path: '/ar-io/info' }); ``` # PreferredWithFallbackRoutingStrategy (/wayfinder-core/(routing-strategies)/preferredwithfallbackroutingstrategy) Uses a preferred gateway, with a fallback strategy if the preferred gateway is not available. This is useful for builders who run their own gateways and want to use their own gateway as the preferred gateway, but also want to have a fallback strategy in case their gateway is not available. This strategy is built using `CompositeRoutingStrategy` internally. It first attempts to ping the preferred gateway (using `PingRoutingStrategy` with `StaticRoutingStrategy`), and if that fails, it falls back to the specified fallback strategy. ```javascript const routingStrategy = new PreferredWithFallbackRoutingStrategy({ preferredGateway: 'https://permagate.io', fallbackStrategy: new FastestPingRoutingStrategy({ timeoutMs: 500, }), }); ``` # RandomRoutingStrategy (/wayfinder-core/(routing-strategies)/randomroutingstrategy) Selects a random gateway from a list of gateways. ```javascript // Option 1: Use with static gateways (override gatewaysProvider if provided) const routingStrategy = new RandomRoutingStrategy(); const gateway = await routingStrategy.selectGateway({ gateways: [new URL('https://arweave.net'), new URL('https://permagate.io')], }); // Option 2: Use with gatewaysProvider (fetches dynamically) const routingStrategy2 = new RandomRoutingStrategy({ gatewaysProvider: new NetworkGatewaysProvider({ ario: ARIO.mainnet(), sortBy: 'operatorStake', limit: 10, }), }); const gateway2 = await routingStrategy2.selectGateway(); // uses gatewaysProvider // Option 3: Override gatewaysProvider with static gateways const gateway3 = await routingStrategy2.selectGateway({ gateways: [new URL('https://custom-gateway.net')], // overrides gatewaysProvider }); ``` # RoundRobinRoutingStrategy (/wayfinder-core/(routing-strategies)/roundrobinroutingstrategy) Selects gateways in round-robin order. The gateway list is stored in memory and is not persisted across instances. You must provide either `gateways` OR `gatewaysProvider` (not both). ```javascript // use with a static list of gateways const routingStrategy = new RoundRobinRoutingStrategy({ gateways: [new URL('https://arweave.net'), new URL('https://permagate.io')], }); // use with gatewaysProvider (loaded once and memoized) const routingStrategy2 = new RoundRobinRoutingStrategy({ gatewaysProvider: new NetworkGatewaysProvider({ ario: ARIO.mainnet(), sortBy: 'operatorStake', sortOrder: 'desc', limit: 10, }), }); const gateway = await routingStrategy.selectGateway(); // returns the next gateway in round-robin order ``` # StaticRoutingStrategy (/wayfinder-core/(routing-strategies)/staticroutingstrategy) ```javascript const routingStrategy = new StaticRoutingStrategy({ gateway: 'https://arweave.net', }); const gateway = await routingStrategy.selectGateway(); // always returns the same gateway ``` # Strategy Composition Examples (/wayfinder-core/(routing-strategies)/strategy-composition-examples) Here are a few “lego-style” examples showing how existing routing strategies can be composed to suit different use cases. Each strategy implements `RoutingStrategy`, so they can be wrapped and combined freely. #### Random + Ping health checks Pick a random gateway, then verify it responds with a `HEAD` request before returning it. ```ts import { RandomRoutingStrategy, PingRoutingStrategy, } from \"@ar.io/wayfinder-core\"; const strategy = new PingRoutingStrategy({ routingStrategy: new RandomRoutingStrategy(), retries: 2, timeoutMs: 500, }); ``` #### Fastest ping wrapped with a simple cache Find the lowest-latency gateway and cache the result for five minutes to avoid constant pings. ```ts import { FastestPingRoutingStrategy, SimpleCacheRoutingStrategy, } from \"@ar.io/wayfinder-core\"; const strategy = new SimpleCacheRoutingStrategy({ routingStrategy: new FastestPingRoutingStrategy({ timeoutMs: 500 }), ttlSeconds: 300, }); ``` #### Preferred gateway + network fallback strategy Attempt to use a favorite gateway, but fallback to a fastest pinging strategy using the ARIO Network if it fails. ```ts import { PreferredWithFallbackRoutingStrategy, RandomRoutingStrategy, PingRoutingStrategy, NetworkGatewaysProvider, } from \"@ar.io/wayfinder-core\"; // these will be our fallback gateways const gatewayProvider = new NetworkGatewaysProvider({ ario: ARIO.mainnet(), sortBy: 'operatorStake', limit: 5, }); // this is our fallback strategy if our preferred gateway fails const fastestPingStrategy = new FastestPingRoutingStrategy({ timeoutMs: 500, gatewaysProvider: gatewayProvider, }); // compose the strategies together, the preferred gateway will be used first, and if it fails, the fallback strategy will be used. const strategy = new PreferredWithFallbackRoutingStrategy({ preferredGateway: \"https://my-gateway.example\", fallbackStrategy: fastestPingStrategy, }); ``` #### Round-robin + ping verification Cycle through gateways sequentially, checking each one’s health before use. ```ts import { RoundRobinRoutingStrategy, PingRoutingStrategy, NetworkGatewaysProvider, } from \"@ar.io/wayfinder-core\"; // use static gateways const strategy = new PingRoutingStrategy({ routingStrategy: new RoundRobinRoutingStrategy({ gateways: [new URL(\"https://gw1\"), new URL(\"https://gw2\")], }), }); // use a dynamic list of gateways from the ARIO Network const strategy2 = new PingRoutingStrategy({ routingStrategy: new RoundRobinRoutingStrategy({ gatewaysProvider: new NetworkGatewaysProvider({ ario: ARIO.mainnet(), sortBy: 'operatorStake', limit: 5, }), }), }); ``` #### Cache around any composed strategy Because `SimpleCacheRoutingStrategy` accepts any `RoutingStrategy`, you can cache more complex compositions too. ```ts import { RandomRoutingStrategy, PingRoutingStrategy, SimpleCacheRoutingStrategy, NetworkGatewaysProvider, } from \"@ar.io/wayfinder-core\"; // use a dynamic list of gateways from the ARIO Network const randomStrategy = new RandomRoutingStrategy({ gatewaysProvider: new NetworkGatewaysProvider({ ario: ARIO.mainnet(), sortBy: 'operatorStake', limit: 20, }), }); // wrap the random strategy with a ping strategy const pingRandom = new PingRoutingStrategy({ routingStrategy: randomStrategy, }); // wrap the ping random strategy with a cache strategy, caching the selected gateway for 10 minutes const cachedStrategy = new SimpleCacheRoutingStrategy({ routingStrategy: pingRandom, ttlSeconds: 600, }); ``` #### Complex multi-strategy fallback with CompositeRoutingStrategy Chain multiple strategies together for maximum resilience - try fastest ping first, then fall back to random selection if ping fails. ```ts import { CompositeRoutingStrategy, FastestPingRoutingStrategy, RandomRoutingStrategy, NetworkGatewaysProvider, } from \"@ar.io/wayfinder-core\"; // Define gateway provider for both strategies const gatewayProvider = new NetworkGatewaysProvider({ ario: ARIO.mainnet(), sortBy: 'operatorStake', limit: 15, }); // Create a composite strategy that tries fastest ping first, then random const strategy = new CompositeRoutingStrategy({ strategies: [ // Try fastest ping first (high performance, but may fail if all gateways are slow) new FastestPingRoutingStrategy({ timeoutMs: 500, gatewaysProvider: gatewayProvider, }), // Fallback to random selection (guaranteed to work if gateways exist) new RandomRoutingStrategy({ gatewaysProvider: gatewayProvider, }), ], }); ``` In all cases, you can supply the composed strategy to `Wayfinder` (or whatever router factory you use) and pass in a gateways provider: ```ts const router = new Wayfinder({ gatewaysProvider: new StaticGatewaysProvider({ gateways: [new URL(\"https://gw1\"), new URL(\"https://gw2\")], }), routingStrategy: strategy, // any of the compositions above }); ``` # DataRootVerificationStrategy (/wayfinder-core/(verification-strategies)/datarootverificationstrategy) Verifies data integrity using Arweave by computing the data root for the transaction. This is useful for L1 transactions and is recommended for users who want to ensure the integrity of their data. ```javascript const wayfinder = new Wayfinder({ verificationSettings: { enabled: true, strategy: new DataRootVerificationStrategy({ trustedGateways: ['https://permagate.io'], }), }, }); ``` # HashVerificationStrategy (/wayfinder-core/(verification-strategies)/hashverificationstrategy) Verifies data integrity using SHA-256 hash comparison. This is the default verification strategy and is recommended for most users looking for a balance between security and performance. ```javascript const wayfinder = new Wayfinder({ verificationSettings: { enabled: true, strategy: new HashVerificationStrategy({ trustedGateways: ['https://permagate.io'], }), }, }); ``` # RemoteVerificationStrategy (/wayfinder-core/(verification-strategies)/remoteverificationstrategy) This strategy is used to verify data by checking the `x-ar-io-verified` header from the gateway that returned the data. If the header is set to `true`, the data is considered verified and trusted. This strategy is only recommended for users fetching data from their own gateways and want to avoid the overhead of the other verification strategies. ```javascript const wayfinder = new Wayfinder({ verificationSettings: { // no trusted gateways are required for this strategy enabled: true, strategy: new RemoteVerificationStrategy(), }, }); ``` # SignatureVerificationStrategy (/wayfinder-core/(verification-strategies)/signatureverificationstrategy) Verifies signatures of Arweave transactions and data items. Headers are retrieved from trusted gateways for use during verification. For a transaction, its data root is computed while streaming its data and then utilized alongside its headers for verification. For data items, the ANS-104 deep hash method of signature verification is used. ```javascript const wayfinder = new Wayfinder({ verificationSettings: { enabled: true, strategy: new SignatureVerificationStrategy({ trustedGateways: ['https://permagate.io'], }), }, }); ``` # Dynamic Routing (/wayfinder-core/dynamic-routing) Wayfinder supports a `resolveUrl` method which generates dynamic redirect URLs to a target gateway based on the provided routing strategy. This function can be used to directly replace any hard-coded gateway URLs, and instead use Wayfinder's routing logic to select a gateway for the request. #### ArNS names Given an ArNS name, the redirect URL will be the same as the original URL, but with the gateway selected by Wayfinder's routing strategy. ```javascript const redirectUrl = await wayfinder.resolveUrl({ arnsName: 'ardrive', }); // results in https://ardrive.\\ ``` #### Transaction Ids Given a txId, the redirect URL will be the same as the original URL, but with the gateway selected by Wayfinder's routing strategy. ```javascript const redirectUrl = await wayfinder.resolveUrl({ txId: 'example-tx-id', }); // results in https://\\/example-tx-id ``` #### Legacy arweave.net or arweave.dev URLs Given a legacy arweave.net or arweave.dev URL, the redirect URL will be the same as the original URL, but with the gateway selected by Wayfinder's routing strategy. ```javascript const redirectUrl = await wayfinder.resolveUrl({ originalUrl: 'https://arweave.net/example-tx-id', }); // results in https://\\/example-tx-id ``` #### ar:// URLs Given an ar:// URL, the redirect URL will be the same as the original URL, but with the gateway selected by Wayfinder's routing strategy. ```javascript const redirectUrl = await wayfinder.resolveUrl({ originalUrl: 'ar://example-name/subpath?query=value', }); // results in https://\\/example-name/subpath?query=value ``` # Wayfinder Core (/wayfinder-core) **For AI and LLM users**: Access the complete Wayfinder Core documentation in plain text format at llm.txt for easy consumption by AI agents and language models. # Wayfinder Core Please refer to the [source code](https://github.com/ar-io/wayfinder/tree/main/packages/wayfinder-core) for SDK details. # Request Flow (/wayfinder-core/request-flow) The following sequence diagram illustrates how Wayfinder processes requests: ```mermaid sequenceDiagram participant Client participant Wayfinder participant Gateways Provider participant Routing Strategy participant Selected Gateway participant Verification Strategy participant Trusted Gateways Client->>Wayfinder: request('ar://example') activate Wayfinder Wayfinder->>+Gateways Provider: getGateways() Gateways Provider-->>-Wayfinder: List of gateway URLs Wayfinder->>+Routing Strategy: selectGateway() from list of gateways Routing Strategy-->>-Wayfinder: Select gateway for request Wayfinder->>+Selected Gateway: Send HTTP request to target gateway Selected Gateway-->>-Wayfinder: Response with data & txId activate Verification Strategy Wayfinder->>+Verification Strategy: verifyData(responseData, txId) Verification Strategy->>Wayfinder: Emit 'verification-progress' events Verification Strategy->>Trusted Gateways: Request verification headers Trusted Gateways-->>Verification Strategy: Return verification headers Verification Strategy->>Verification Strategy: Compare computed vs trusted data Verification Strategy-->>-Wayfinder: Return request data with verification result alt Verification passed Wayfinder->>Wayfinder: Emit 'verification-passed' event Wayfinder-->>Client: Return verified response else Verification failed Wayfinder->>Wayfinder: Emit 'verification-failed' event Wayfinder-->>Client: Throw verification error end deactivate Wayfinder ``` # Telemetry (/wayfinder-core/telemetry) Wayfinder can optionally emit OpenTelemetry spans for every request. **By default, telemetry is disabled**. You can control this behavior with the `telemetrySettings` option. ```javascript const wayfinder = createWayfinderClient({ ario: ARIO.mainnet(), // other settings... telemetrySettings: { enabled: true, // disabled by default (must be explicitly enabled) sampleRate: 0.1, // 10% sample rate by default exporterUrl: 'https://your-custom-otel-exporter', // optional, defaults to https://api.honeycomb.io/v1/traces clientName: 'my-custom-client-name', // optional, defaults to wayfinder-core clientVersion: '1.0.0', // optional, defaults to empty }, }); ``` # useWayfinderRequest (/wayfinder-react/(hooks)/usewayfinderrequest) Fetch the data via wayfinder, and optionally verify the data. ```tsx function WayfinderData({ txId }: { txId: string }) { const request = useWayfinderRequest(); const [data, setData] = useState\\(null); const [dataLoading, setDataLoading] = useState(false); const [dataError, setDataError] = useState\\(null); useEffect(() => { (async () => { try { setDataLoading(true); setDataError(null); // fetch the data for the txId using wayfinder const response = await request(`ar://${txId}`, { verificationSettings: { enabled: true, // enable verification on the request strict: true, // don't use the data if it's not verified }, }); const data = await response.arrayBuffer(); // or response.json() if you want to parse the data as JSON setData(data); } catch (error) { setDataError(error as Error); } finally { setDataLoading(false); } })(); }, [request, txId]); if (dataError) { return Error loading data: {dataError.message}; } if (dataLoading) { return Loading data...; } if (!data) { return No data; } return ( {data} ); } ``` # useWayfinderUrl (/wayfinder-react/(hooks)/usewayfinderurl) Get a dynamic URL for an existing `ar://` URL or legacy `arweave.net`/`arweave.dev` URL. Example: ```tsx function WayfinderImage({ txId }: { txId: string }) { const { resolvedUrl, isLoading, error } = useWayfinderUrl({ txId }); if (error) { return Error resolving URL: {error.message}; } if (isLoading) { return Resolving URL...; } return ( ); } ``` # Wayfinder React (/wayfinder-react) **For AI and LLM users**: Access the complete Wayfinder React documentation in plain text format at llm.txt for easy consumption by AI agents and language models. # Wayfinder React Please refer to the [source code](https://github.com/ar-io/wayfinder/tree/main/packages/wayfinder-react) for SDK details. # What is it? (/what-is-it) Wayfinder is a simple, open-source client-side routing and verification protocol for the permaweb. It leverages the [AR.IO Network](https://ar.io) to route users to the most optimal gateway for a given request. # Who is it for? (/who-is-it-for) - **Builders** who need reliable, decentralized access to Arweave data through the powerful [AR.IO Network](https://ar.io) - **Browsers** who demand complete control over their permaweb journey with customizable gateways and robust verification settings for enhanced security and reliability - **Operators** who power the [AR.IO Network](https://ar.io) and want to earn rewards* for serving wayfinder traffic to the growing permaweb ecosystem","estimatedWords":4021,"lastModified":"2025-10-20T12:12:36.731Z","breadcrumbs":["sdks","wayfinder","llm.txt"],"siteKey":"ario","siteName":"AR-IO Network","depth":3,"crawledAt":"2025-10-20T12:12:36.731Z"},{"url":"https://docs.ar.io/sdks/turbo-sdk/turboauthenticatedclient","title":"TurboAuthenticatedClient","content":"Turbo SDKAPIsTurboAuthenticatedClientCopy MarkdownOpengetBalance() Issues a signed request to get the credit balance of a wallet measured in AR (measured in Winston Credits, or winc). const { winc: balance } = await turbo.getBalance(); signer.getNativeAddress() Returns the [native address][docs/native-address] of the connected signer. getWincForFiat() Returns the current amount of Winston Credits including all adjustments for the provided fiat currency, amount, and optional promo codes. const { winc, paymentAmount, quotedPaymentAmount, adjustments } = await turbo.getWincForFiat({ amount: USD(100), promoCodes: ['MY_PROMO_CODE'],","estimatedWords":76,"lastModified":"2025-10-20T12:12:38.402Z","breadcrumbs":["sdks","turbo sdk","turboauthenticatedclient"],"siteKey":"ario","siteName":"AR-IO Network","depth":4,"crawledAt":"2025-10-20T12:12:38.402Z"},{"url":"https://docs.ar.io/build/advanced/arfs","title":"ArFS Protocol","content":"AdvancedArFS ProtocolCopy MarkdownOpenArweave File System, or \"ArFS\" is a data modeling, storage, and retrieval protocol designed to emulate common file system operations and to provide aspects of mutability to your data hierarchy on Arweave's otherwise permanent, immutable data storage blockweave. Due to Arweave's permanent, immutable and public nature traditional file system operations such as permissions, file/folder renaming and moving, and file updates cannot be done by simply updating the on-chain data model. ArFS works around this by implementing a privacy and encryption pattern and defining an append-only transaction data model using tags within Arweave Transaction headers. Key Features File Structure ArFS organizes files and folders using a hierarchical structure. Files are stored as individual transactions on the Arweave blockchain, while folders are metadata that reference these file transactions. Metadata Each file and folder has associated metadata, such as the name, type, size, and modification timestamp. ArFS leverages Arweave's tagging system to store this metadata in a standardized format, which allows for easy querying and organization. File Permissions ArFS supports public and private file permissions. Public files can be accessed by anyone on the network, while private files are encrypted using the owner's private key, ensuring only they can decrypt and access the content. File Versioning ArFS supports versioning of files, allowing users to store multiple versions of a file and access previous versions at any time. This is achieved by linking new file transactions to previous versions through the use of metadata tags. Data Deduplication To minimize storage redundancy and costs, ArFS employs data deduplication techniques. If a user tries to store a file that already exists on the network, the protocol will simply create a new reference to the existing file instead of storing a duplicate copy. Search and Discovery ArFS enables users to search and discover files based on their metadata, such as file names, types, and tags. This is made possible by indexing the metadata stored within the Arweave blockchain. Interoperability ArFS is designed to be interoperable with other decentralized applications and services built on the Arweave network. This allows for seamless integration and collaboration between different applications and users. Getting Started To start using ArFS, you'll need to familiarize yourself with the Arweave ecosystem, acquire AR tokens to cover storage costs, and choose a compatible client or library to interact with the ArFS protocol. ArFS Version History VersionDateRelease Notes0.10August 2020The brief, beta version that was in use during initial testing of ArDrive across Web (Dart) and legacy CLI (Typescript).0.11September 2020Includes all of the major functionality supporting file systems on Arweave including new drives, folders, files, renames, moves and privacy.0.12December 2022Added Snapshot entities to support quick synchronization of drive state.0.13August 2023Added pins0.14January 2024Added isHidden property to file and folder metadata to enable clients to \"hide\" content from end users.0.15May 2025Added Drive-Signature entity type and Signature-Type metadata property on Drive entities. Next Steps Ready to dive deeper into ArFS? Here's what you should explore next: Entity Types - Understand the different ArFS entities and their structure Data Model - Learn how ArFS organizes data hierarchically Privacy & Encryption - Secure your data with private drives Creating Drives - Get started with your first ArFS drive Reading Data - Query and retrieve your ArFS data Resources For more information, documentation, and community support, refer to the following resources: Arweave Official Website Arweave Developer Documentation Arweave Community Forums How is this guide?GoodBadAdvancedAdvanced topics and specialized guides for building on Arweave and AR.IOEntity TypesUnderstanding ArFS entity types and their structure","estimatedWords":579,"lastModified":"2025-10-20T12:12:39.400Z","breadcrumbs":["build","advanced","arfs"],"siteKey":"ario","siteName":"AR-IO Network","depth":3,"crawledAt":"2025-10-20T12:12:39.401Z"},{"url":"https://docs.ar.io/build/advanced/normalized-addresses","title":"Normalized Addresses","content":"AdvancedNormalized AddressesCopy MarkdownOpenOverview Different blockchains use different formats for the public keys of wallets, and the native addresses for those wallets. In most cases, when a system in the Arweave ecosystem needs to display the wallet address of a wallet from a different blockchain, for instance in the Owner.address value of an AO process spawned by an ETH wallet, that address will be normalized into the format recognized by Arweave. Specifically, a 43 character base64url representation of the sha256 hash of the public key. This is done to prevent potential errors by systems in the Arweave ecosystem that expect these values to be a certain size and conform to a specific format. Essentially, normalized addresses are a way to represent public keys and wallet addresses from other blockchains in a way that is familiar to systems in the Arweave ecosystem. A tool for easily obtaining a normalized addresses from public keys can be found at ar:","estimatedWords":156,"lastModified":"2025-10-20T12:12:40.465Z","breadcrumbs":["build","advanced","normalized addresses"],"siteKey":"ario","siteName":"AR-IO Network","depth":3,"crawledAt":"2025-10-20T12:12:40.465Z"},{"url":"https://docs.ar.io/build/advanced/sandboxing","title":"Browser Sandboxing","content":"AdvancedBrowser SandboxingCopy MarkdownOpenOverview Browser sandboxing allows data requests to a gateway node to benefit from the security advantages of using a browser's same-origin policy by redirecting the requests to a pseudo-unique subdomain of the gateway's apex domain. For example, an attempt to access https:","estimatedWords":44,"lastModified":"2025-10-20T12:12:41.677Z","breadcrumbs":["build","advanced","sandboxing"],"siteKey":"ario","siteName":"AR-IO Network","depth":3,"crawledAt":"2025-10-20T12:12:41.677Z"},{"url":"https://docs.ar.io/build/advanced/ethareum","title":"EthAReum Protocol","content":"AdvancedEthAReum ProtocolCopy MarkdownOpenThe EthAReum protocol enables the generation of private keys for an Arweave wallet using a signature from an Ethereum or Solana wallet. This allows users to create an Arweave wallet directly through popular wallet providers like MetaMask, providing seamless cross-chain wallet management. Generated private keys provide a fully functional Arweave wallet, equipped to perform all standard operations, including holding AR tokens and Turbo Credits, and uploading data to the Arweave network. How It Works EthAReum uses a deterministic key derivation process that combines: Ethereum/Solana wallet signature - Provides the cryptographic foundation User-generated password - Adds additional entropy and security Standardized derivation algorithm - Ensures reproducible results The protocol generates a unique Arweave wallet that is cryptographically linked to your Ethereum or Solana wallet but remains completely independent. Browser Compatibility Recommended Browser: For optimal performance, use Chrome when working with EthAReum and MetaMask. While EthAReum functions correctly in most browsers, there are ongoing efforts to resolve some edge case compatibility issues in other environments. Password Security The EthAReum protocol incorporates a user-generated password in the wallet derivation process. This password provides an extra layer of security by contributing additional entropy to the wallet's derivation and serves as a critical verification step for wallet access. Permanent Password: The password used during the derivation of private keys is permanent and cannot be changed or recovered by any administrator. ArDrive is a decentralized platform with no account administration. It is crucial to keep this password secure. Password Requirements Must be set during initial wallet creation Used for all subsequent logins Required for encrypting private uploads Cannot be recovered if forgotten Wallet Addresses The public address of the generated Arweave wallet is derived from its public key and will be different from the public address of the Ethereum or Solana wallet used to generate it. Viewing Your Address The exact steps to obtain your generated wallet's public address depend on the dApp interface: ArDrive: Click the user profile icon in the top right when logged in Other dApps: Check the wallet settings or profile section Key Management Keyfiles vs Seed Phrases The Arweave ecosystem primarily uses keyfiles rather than seed phrases for wallet access: Keyfile: JSON file containing a Json Web Key (JWK) that acts as private keys Seed Phrase: Supported but not universally implemented across all dApps Keyfile Security: Always treat your keyfile with the same care as you would private keys for an Ethereum wallet. Learn more about keyfiles in the Arweave Cookbook. Accessing Your Keys Both keyfile and seed phrase are available for download in most dApps: ArDrive: Click the user profile icon in the top right when logged in Other dApps: Check wallet settings or export options Security Considerations One-Way Control EthAReum generates Arweave wallet private keys using a signature from your Ethereum/Solana wallet, ensuring that control only extends in one direction: ✅ EthAReum can generate Arweave wallets from Ethereum/Solana signatures ❌ EthAReum cannot access your Ethereum/Solana wallet or assets ✅ Your Ethereum/Solana assets remain completely secure and independent Signature Security Beware of Malicious dApps: Some malicious dApps or websites may disguise high-risk authorization transactions as simple signature requests. Always ensure that you only provide signatures to reputable and trusted dApps like ArDrive. Best Practices Verify dApp authenticity before providing signatures Use strong, unique passwords for wallet derivation Backup your keyfile in a secure location Never share your password or keyfile with anyone Test with small amounts before committing to large transactions Implementation Examples Basic Wallet Generation","estimatedWords":578,"lastModified":"2025-10-20T12:12:42.422Z","breadcrumbs":["build","advanced","ethareum"],"siteKey":"ario","siteName":"AR-IO Network","depth":3,"crawledAt":"2025-10-20T12:12:42.422Z"},{"url":"https://docs.ar.io/build/guides/using-turbo-in-a-browser/vite","title":"Using Turbo SDK with Vite","content":"GuidesUsing Turbo in a BrowserUsing Turbo SDK with ViteCopy MarkdownOpenUsing Turbo SDK with Vite Firefox Compatibility: Some compatibility issues have been reported with the Turbo SDK in Firefox browsers. At this time the below framework examples may not behave as expected in Firefox. Overview This guide demonstrates how to configure the @ardrive/turbo-sdk in a Vite application with proper polyfills for client-side usage. Vite provides excellent support for modern JavaScript features and can be easily configured to work with the Turbo SDK through plugins. Polyfills: Vite simplifies polyfill management compared to other bundlers. The vite-plugin-node-polyfills plugin handles most of the complexity automatically. Prerequisites Vite 5+ Node.js 18+ React 18+ (or your preferred framework) Basic familiarity with Vite configuration Install the main Turbo SDK package:npm install @ardrive/turbo-sdkAdd the Vite node polyfills plugin for browser compatibility:npm install --save-dev vite-plugin-node-polyfillsWallet Integration Dependencies: The Turbo SDK includes @dha-team/arbundles as a peer dependency, which provides the necessary signers for browser wallet integration (like InjectedEthereumSigner and ArconnectSigner). You can import these directly without additional installation.Add React and TypeScript dependencies (if using React):npm install react react-dom npm install --save-dev @vitejs/plugin-react @types/react @types/react-domCreate or update your vite.config.js file:import react from \"@vitejs/plugin-react\"; import { defineConfig } from \"vite\"; import { nodePolyfills } from \"vite-plugin-node-polyfills\"; export default defineConfig({ base: \"/\", plugins: [ react(), nodePolyfills({","estimatedWords":213,"lastModified":"2025-10-20T12:12:43.646Z","breadcrumbs":["build","guides","using turbo in a browser","vite"],"siteKey":"ario","siteName":"AR-IO Network","depth":3,"crawledAt":"2025-10-20T12:12:43.646Z"},{"url":"https://docs.ar.io/build/extensions/grafana","title":"Grafana","content":"Extensions proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header X-Forwarded-Proto $scheme; }Your complete nginx configuration should look like this:# Force redirects from HTTP to HTTPS server { listen 80; listen [::]:80; server_name .com *..com; location / { return 301 https:","estimatedWords":42,"lastModified":"2025-10-20T12:12:44.818Z","breadcrumbs":["build","extensions","grafana"],"siteKey":"ario","siteName":"AR-IO Network","depth":3,"crawledAt":"2025-10-20T12:12:44.819Z"},{"url":"https://docs.ar.io/build/extensions/clickhouse","title":"ClickHouse  Parquet","content":"Extensions & SidecarsClickHouse & ParquetCopy MarkdownOpenOverview AR.IO gateway Release 33 introduces a new configuration option for using Parquet files and ClickHouse to improve performance and scalability of your AR.IO gateway for large datasets. This guide will walk you through the process of setting up ClickHouse with your AR.IO gateway, and importing Parquet files to bootstrap your ClickHouse database. What is Parquet? Apache Parquet is a columnar storage file format designed for efficient data storage and retrieval. Unlike row-based storage formats like SQLite, Parquet organizes data by column rather than by row, which provides several advantages for analytical workloads: Efficient compression: Similar data is stored together, leading to better compression ratios Columnar access: You can read only the columns you need, reducing I/O operations Predicate pushdown: Filter operations can be pushed down to the storage layer, improving query performance For more information about Parquet, see the Parquet documentation. Current Integration with AR.IO Gateways In the current AR.IO gateway implementation, Parquet and ClickHouse run alongside SQLite rather than replacing it. This parallel architecture allows each database to handle what it does best: SQLite continues to handle transaction writes and updates ClickHouse with Parquet files is optimized for fast query performance, especially with large datasets The gateway continues to operate with SQLite just as it always has, maintaining all of its normal functionality. Periodically, the gateway will export batches of data from SQLite to Parquet files, which are then imported into ClickHouse. This batch-oriented approach is much more efficient than attempting to synchronize the databases in real-time, as it leverages Parquet's strength in handling large, immutable data sets. Note that despite Parquet's efficient compression, gateways may not see significant disk space reduction in all cases. While bundled transaction data is exported to Parquet, L1 data remains in SQLite. Without substantial unbundling and indexing filters, minimal data gets exported to Parquet, limiting potential storage savings. With ClickHouse integration enabled, GraphQL queries are primarily routed to ClickHouse, leveraging its superior performance for large datasets. This significantly improves response times while maintaining SQLite's reliability for transaction processing. For more information about gateway architecture and data processing, see our Gateway Architecture documentation. Parquet vs. SQLite in AR.IO Gateways While SQLite is excellent for transactional workloads and small to medium datasets, it faces challenges with very large datasets: FeatureSQLiteParquet + ClickHouseStorage modelRow-basedColumn-basedQuery optimizationBasicAdvanced analytical optimizationCompressionLimitedHigh compression ratiosScalingLimited by single fileDistributed processing capableWrite speedFast for small transactionsOptimized for batch operationsRead speed for analyticsSlower for large datasetsOptimized for analytical queriesIdeal use caseRecent transaction data, OLTPHistorical data, OLAP workloads Benefits for Gateway Operators Implementing Parquet and ClickHouse alongside SQLite in your AR.IO gateway offers several key advantages: Dramatically improved query performance for GraphQL endpoints, especially for large result sets Reduced storage requirements through efficient columnar compression Better scalability for growing datasets Faster bootstrapping of new gateways through Parquet file imports Reduced load on SQLite by offloading query operations to ClickHouse The primary focus of the Parquet/ClickHouse integration is the significant speed improvement for querying large datasets. Gateway operators managing significant volumes of data will notice substantial performance gains when using this configuration. Storage Considerations While Parquet files offer more efficient compression for the data they contain, it's important to understand the storage impact: Bundled transaction data is exported to Parquet and removed from SQLite, potentially saving space L1 data remains in SQLite regardless of Parquet configuration Space savings are highly dependent on your unbundling filters - without substantial unbundling configurations, minimal data gets exported to Parquet The more data you unbundle and export to Parquet, the greater the potential storage efficiency For gateway operators, this means proper filter configuration is crucial to realize storage benefits. The primary advantage remains significantly improved query performance for large datasets, with potential space savings as a secondary benefit depending on your specific configuration. The following sections will guide you through setting up ClickHouse with your AR.IO gateway, exporting data from SQLite to Parquet, and importing Parquet files to bootstrap your ClickHouse database. NoteThe below instructions are designed to be used in a linux environment. Windows and MacOS users must modify the instructions to use the appropriate package manager/ command syntax for their platform.Unless otherwise specified, all commands should be run from the root directory of the gateway. Installing ClickHouse ClickHouse is a powerful, open-source analytical database that excels at handling large datasets and complex queries. It is the tool used by the gateway to integrate with the Parquet format. For more information about ClickHouse, see the ClickHouse documentation. Add ClickHouse RepositoryIt is recommended to use official pre-compiled deb packages for Debian or Ubuntu. Run these commands to install packages:sudo apt-get install -y apt-transport-https ca-certificates curl gnupg curl -fsSL 'https:","estimatedWords":774,"lastModified":"2025-10-20T12:12:46.083Z","breadcrumbs":["build","extensions","clickhouse"],"siteKey":"ario","siteName":"AR-IO Network","depth":3,"crawledAt":"2025-10-20T12:12:46.084Z"},{"url":"https://docs.ar.io/build/extensions/bundler","title":"Bundler","content":"Extensions & SidecarsBundlerCopy MarkdownOpenOverview A Turbo ANS-104 data item bundler can be run alongside an AR.IO gateway. This allows gateways the ability to accept data items to be submitted to the Arweave blockweave. The bundler service can be easily run inside Docker in the same way that the gateway is. It utilizes a separate docker compose file for configuration and deployment, which also allows for the use of a separate file for environmental variables specific to the bundler service. Additionally, the separation allows operators to spin their bundler service up or down at any time without affecting their core gateway service. Despite the use of separate docker compose files, the bundler service shares a docker network with the AR.IO gateway, and so is able to directly interact with the gateway service and data. For more information on ANS-104 Bundles, see the ANS-104 Bundles page. Getting Started NOTE: The bundler service relies on GraphQL indexing of recently bundled and uploaded data to manage its pipeline operations. The AR.IO gateway should have its indexes synced up to Arweave's current block height before starting the bundler's service stack. Configure Environmental VariablesEnvironmental variables must be provided for the bundler to function and integrate properly with an existing AR.IO gateway. The gateway repository provides a .env.bundler.example file that can be renamed to .env.bundler and used as a starting point. It contains the following:BUNDLER_ARWEAVE_WALLET='Stringified JWK wallet. e.g: '{ \"n\": \"...\", ... }' BUNDLER_ARWEAVE_ADDRESS='Address for above wallet' APP_NAME='AR.IO bundler service' # Use localstack s3 bucket for shared data source between AR.IO gateway and bundler AWS_S3_BUCKET=ar.io AWS_S3_PREFIX='data' AWS_ACCESS_KEY_ID='test' AWS_SECRET_ACCESS_KEY='test' AWS_REGION='us-east-1' AWS_ENDPOINT='http:","estimatedWords":263,"lastModified":"2025-10-20T12:12:47.297Z","breadcrumbs":["build","extensions","bundler"],"siteKey":"ario","siteName":"AR-IO Network","depth":3,"crawledAt":"2025-10-20T12:12:47.297Z"},{"url":"https://docs.ar.io/build/extensions/compute-unit","title":"AO Compute Unit (CU)","content":"Extensions & SidecarsAO Compute Unit (CU)Copy MarkdownOpenOverview An AO Compute Unit (CU) is a critical component in the AO ecosystem responsible for executing AO processes and maintaining their state. CUs serve as the computational backbone of the AO network by: Processing Messages: CUs receive and process messages sent to AO processes Executing WASM Modules: CUs run the WebAssembly (WASM) code that defines process behavior Maintaining State: CUs track and update the state of AO processes Creating Checkpoints: CUs periodically save process state to the Arweave network as checkpoints Running a CU alongside your gateway allows you to: Process AO requests locally rather than relying on external services Improve response times for AO-related queries Contribute computational resources to the AO network Ensure your gateway has reliable access to AO functionality For more detailed information about Compute Units, please refer to the AO Cookbook: Units. System Requirements Before deploying a CU, ensure your system meets the following requirements: Recommended: At least 16GB RAM for optimal CU operation Minimum: 4GB RAM is possible with adjusted memory limits (see resource allocation settings) At least 100GB disk space dedicated to CU operation These requirements are separate from your gateway requirements Resource RequirementsRunning a CU is resource-intensive. Make sure your system has sufficient resources to handle both the gateway and the CU. While you can run a CU with less than the recommended RAM, you'll need to adjust the memory limits accordingly. Deploying an AO CU Navigate to Gateway DirectoryFirst, navigate to the root directory of your gateway:cd /path/to/your/gatewayConfigure Environment VariablesCopy the example environment file:cp .env.ao.example .env.aoDefault .env.ao.example ContentsThe default .env.ao.example file contains the following settings:CU_WALLET='[wallet json here]' PROCESS_CHECKPOINT_TRUSTED_OWNERS=fcoN_xJeisVsPXA-trzVAuIiqO3ydLQxM-L4XbrQKzY GATEWAY_URL=http:","estimatedWords":274,"lastModified":"2025-10-20T12:12:48.934Z","breadcrumbs":["build","extensions","compute unit"],"siteKey":"ario","siteName":"AR-IO Network","depth":3,"crawledAt":"2025-10-20T12:12:48.934Z"},{"url":"https://docs.ar.io/build/run-a-gateway/manage/filters","title":"Gateway Filters","content":"Run a GatewayManage your GatewayGateway FiltersCopy MarkdownOpenConfigure your AR.IO Gateway to efficiently process and index only the data you need. This comprehensive guide covers advanced filtering techniques, performance optimization, and real-world use cases. Overview The AR.IO Gateway uses a flexible JSON-based filtering system to control data processing and indexing. The system provides precise control over which bundles are processed and which data items are indexed for querying. Understanding the Filtering System The AR.IO Gateway uses two primary filters to control data processing: ANS104_UNBUNDLE_FILTER - Controls which bundles are processed and unbundled ANS104_INDEX_FILTER - Controls which data items from unbundled bundles are indexed for querying By default, gateways process no bundles and index no data items. You must explicitly configure filters to start processing data. Core Environment Variables Configure Data ManagementOptimize data storage and processing:# Number of new data items before flushing to stable storage DATA_ITEM_FLUSH_COUNT_THRESHOLD=1000 # Maximum time between flushes (in seconds) MAX_FLUSH_INTERVAL_SECONDS=600 # Maximum number of data items to queue for indexing MAX_DATA_ITEM_QUEUE_SIZE=100000 # Enable background verification ENABLE_BACKGROUND_DATA_VERIFICATION=trueSet Up GraphQL ConfigurationChoose between local-only or proxied queries:# For new gateways - proxy to arweave.net for complete index GRAPHQL_HOST=arweave.net GRAPHQL_PORT=443 # For local-only queries (uncomment to use) # GRAPHQL_HOST= Filter Construction .env formattingWhile the filters below are displayed on multiple lines for readability, they must be stored in the .env file as a single line for proper processing. Basic Filters The simplest filters you can use are \"always\" and \"never\" filters. The \"never\" filter is the default behavior and will match nothing, while the \"always\" filter matches everything. { \"never\": true","estimatedWords":260,"lastModified":"2025-10-20T12:12:50.164Z","breadcrumbs":["build","run a gateway","manage","filters"],"siteKey":"ario","siteName":"AR-IO Network","depth":3,"crawledAt":"2025-10-20T12:12:50.164Z"},{"url":"https://docs.ar.io/build/run-a-gateway/manage","title":"Manage your Gateway","content":"Run a GatewayManage your GatewayCopy MarkdownOpenMaster the advanced features and configurations of your AR.IO Gateway. These comprehensive guides cover everything from performance optimization to content moderation, helping you run a professional-grade gateway infrastructure. Gateway Management Importing SQLite Database SnapshotsLearn how to import pre-synchronized database snapshots to quickly bootstrap your gateway and reduce initial sync time from weeks to hours.Upgrading a GatewayStep-by-step guide to safely upgrade your AR.IO Gateway to the latest version without losing data or progress. Monitoring & Analytics Monitoring with GrafanaDeploy and configure Grafana for comprehensive gateway monitoring, performance analytics, and operational insights. Performance Optimization Gateway FiltersConfigure advanced filters to efficiently process and index only the data you need, optimizing performance and resource usage.Setting Apex Domain ContentCustomize your gateway's root domain to serve custom content, project information, or documentation instead of default network info. Content Management Content ModerationImplement content moderation policies using blocklisting and filtering to control what content your gateway serves. Configuration Reference Environment Variables ReferenceComprehensive reference for all AR.IO Gateway environment variables organized by service component. Support & Troubleshooting Gateway Troubleshooting & FAQComprehensive troubleshooting guide and FAQ for common gateway issues, failed epoch guidance, and frequently asked questions.How is this guide?GoodBadJoin the NetworkRegister your AR.IO Gateway with the network to start earning rewardsUpgrading your GatewayStep-by-step guide to upgrading your AR.IO Gateway to the latest version safely without losing data or progress","estimatedWords":225,"lastModified":"2025-10-20T12:12:51.393Z","breadcrumbs":["build","run a gateway","manage"],"siteKey":"ario","siteName":"AR-IO Network","depth":3,"crawledAt":"2025-10-20T12:12:51.393Z"},{"url":"https://docs.ar.io/build/run-a-gateway/manage/troubleshooting","title":"Troubleshooting","content":"Run a GatewayManage your GatewayTroubleshootingCopy MarkdownOpenWelcome to the comprehensive troubleshooting and FAQ resource for AR.IO Gateway operators. Use the quick lookup table below for fast answers, or browse the detailed sections for in-depth guidance. Quick Lookup Below is a quick summary of what you should check when troubleshooting your gateway. Find more detailed information in the sections below. IssueWhat to CheckMy release number is wrongPull the latest github updates and make sure you are on the main branchGateway appears offline on Viewblock or https:","estimatedWords":84,"lastModified":"2025-10-20T12:12:52.521Z","breadcrumbs":["build","run a gateway","manage","troubleshooting"],"siteKey":"ario","siteName":"AR-IO Network","depth":3,"crawledAt":"2025-10-20T12:12:52.521Z"},{"url":"https://docs.ar.io/build/access/fetch-data","title":"Fetch Data (via REST API)","content":"Access DataFetch Data (via REST API)Copy MarkdownOpenThe simplest way to access data on Arweave is through HTTP requests to gateways. This method works in any web browser and requires no additional setup. Fetching Data from Gateways Gateways are the most performant way to fetch data from Arweave, providing significant advantages over accessing Arweave nodes directly. Why Gateways Are Faster: Content Caching - Pre-cached data for instant retrieval Data Indexing - Fast search and query capabilities Network Optimization - Distributed infrastructure for better performance Content Delivery - Optimized serving with compression and CDN features REST APIs for Fetching Data Gateways support multiple API endpoints for accessing data: Standard Endpoint Access any transaction using this URL structure: https:","estimatedWords":116,"lastModified":"2025-10-20T12:12:53.560Z","breadcrumbs":["build","access","fetch data"],"siteKey":"ario","siteName":"AR-IO Network","depth":3,"crawledAt":"2025-10-20T12:12:53.560Z"},{"url":"https://docs.ar.io/build/access/find-data","title":"Find Data (via GraphQL)","content":"Access DataFind Data (via GraphQL)Copy MarkdownOpenUse GraphQL to find and identify Arweave data with powerful search and filtering capabilities. GraphQL is used for discovery - you query to get transaction IDs, then use those IDs to fetch the actual data. GraphQL is for Discovery, Not Direct Access GraphQL finds data, it doesn't access it directly. Use GraphQL to get transaction IDs, then use those IDs with the REST API to fetch the actual data. How GraphQL Works GraphQL on Arweave follows a two-step process: Find - Query GraphQL to discover transactions by tags, metadata, owner, or other criteria Fetch - Use the transaction IDs from your query results to retrieve the actual data via the REST API This separation allows for powerful data discovery while keeping data retrieval fast and efficient. GraphQL Providers arweave.net - https:","estimatedWords":136,"lastModified":"2025-10-20T12:12:54.805Z","breadcrumbs":["build","access","find data"],"siteKey":"ario","siteName":"AR-IO Network","depth":3,"crawledAt":"2025-10-20T12:12:54.805Z"},{"url":"https://docs.ar.io/build/access/arns","title":"Arweave Name System (ArNS)","content":"Access DataArweave Name System (ArNS)Copy MarkdownOpenArNS provides human-readable URLs for your Arweave data, making it easy to share and remember permanent addresses. What is ArNS? ArNS is a naming system that allows you to register human-readable names that point to your Arweave transactions. Instead of sharing long transaction IDs, you can use memorable URLs. Example: Before: https:","estimatedWords":57,"lastModified":"2025-10-20T12:12:56.070Z","breadcrumbs":["build","access","arns"],"siteKey":"ario","siteName":"AR-IO Network","depth":3,"crawledAt":"2025-10-20T12:12:56.070Z"},{"url":"https://docs.ar.io/build/access/wayfinder","title":"Wayfinder","content":"Access DataWayfinderCopy MarkdownOpenWayfinder is a client-side routing and verification protocol that provides decentralized, cryptographically verified access to data stored on Arweave via the AR.IO Network. What is Wayfinder? Wayfinder solves the challenge of reliable data access on the permaweb by: Intelligent Routing - Automatically selects the best gateway for each request Data Verification - Cryptographically verifies data integrity Decentralized Access - Eliminates single points of failure Seamless Integration - Works behind the scenes for fast, reliable access Learn More: For detailed information about Wayfinder architecture and how it works, see our Wayfinder Documentation. Get Started Installation: npmpnpmyarnbunnpm install @ar.io/wayfinder-core @ar.io/sdk Basic Usage: import { createWayfinderClient } from \"@ar.io/wayfinder-core\"; import { ARIO } from \"@ar.io/sdk\";","estimatedWords":114,"lastModified":"2025-10-20T12:12:57.266Z","breadcrumbs":["build","access","wayfinder"],"siteKey":"ario","siteName":"AR-IO Network","depth":3,"crawledAt":"2025-10-20T12:12:57.266Z"},{"url":"https://docs.ar.io/learn/gateways/data-verification","title":"Data Verification","content":"GatewaysData VerificationCopy MarkdownOpenAR.IO gateways continuously verify that data chunks are correctly stored and retrievable from Arweave. This ensures users receive authentic, uncorrupted data with cryptographic proof of integrity. The verification system is what makes AR.IO gateways trustworthy data providers for the permaweb. How Gateways Verify Data Data verification is an ongoing process that uses Merkle tree cryptography to provide mathematical proof of data integrity. The process involves multiple specialized components working together to ensure cached data matches what's stored on Arweave: The Verification Workflow: Gateways achieve verification through a systematic five-phase process orchestrated by the DataVerificationWorker. This process ensures that every piece of cached data cryptographically matches its original form on Arweave, providing mathematical proof of integrity before serving data to users. 1. Discovery Phase Periodically scan for unverified data items Priority-based queue management (higher priority items first) Track retry attempts for failed verifications 2. Data Retrieval Fetch data attributes from gateway storage Retrieve the complete data stream Gather metadata needed for verification 3. Cryptographic Computation Calculate Merkle data root from actual data stream Generate cryptographic proofs using the same algorithm as Arweave Create verifiable hash chains 4. Root Comparison Compare computed root against indexed root in database Verify data hasn't been corrupted or altered Validate chunk integrity against Merkle proofs 5. Action Based on Results Success: Mark data as verified with timestamp Failure: Trigger re-import from Arweave or unbundle from parent Error: Increment retry counter and requeue for later Verification Types AR.IO gateways handle different types of data verification based on the data's origin: Transaction Data Verification For individual Arweave transactions: Direct root validation against transaction data roots stored on-chain Complete data reconstruction from chunks to ensure availability Cryptographic proof that data matches what was originally stored Bundle Data Verification For ANS-104 data bundles (collections of data items): Bundle integrity checks to verify the container is valid Individual item verification within each bundle Recursive unbundling when verification fails to re-extract items Nested bundle support for bundles containing other bundles Chunk-Level Validation At the most granular level: Merkle proof validation for individual data chunks Sequential integrity ensuring chunks form complete data Parallel verification of multiple chunks for performance Why Verification Matters Cryptographic Trust Foundation Mathematical Proof: Merkle tree cryptography provides irrefutable proof of data integrity Independent Validation: Multiple gateways verify the same data independently Network Consensus: Distributed verification creates trust without central authority Data Integrity Guarantees Tamper Detection: Any alteration to data is immediately detectable Corruption Recovery: Automatic healing of corrupted data through re-import Permanent Storage Validation: Ensures Arweave's permanence promise is maintained Gateway Reliability Continuous Monitoring: Ongoing verification catches issues before users encounter them Self-Healing System: Automatic recovery mechanisms maintain data availability Transparent Operations: Verification status and timestamps provide audit trails Explore Gateway Systems Data RetrievalLearn how gateways fetch data from multiple sources with verificationGateway ArchitectureUnderstand the technical architecture behind verification systemsRun Your Own GatewaySet up a gateway with built-in verification capabilitiesGateway ConfigurationConfigure verification settings and optimization optionsHow is this guide?GoodBadData RetrievalHow AR.IO gateways retrieve and share data from multiple sources including trusted peers and Arweave nodesGateway RegistryThe AR.IO Network consists of AR.IO gateway nodes, which are identified by their registered Arweave wallet addresses and either their IP addresses or hostnames, as stored in the network","estimatedWords":537,"lastModified":"2025-10-20T12:12:58.342Z","breadcrumbs":["learn","gateways","data verification"],"siteKey":"ario","siteName":"AR-IO Network","depth":3,"crawledAt":"2025-10-20T12:12:58.342Z"},{"url":"https://docs.ar.io/build/upload/encryption","title":"Encryption","content":"Upload DataEncryptionCopy MarkdownOpenArweave has no built-in encryption. All encryption and decryption must be handled client-side before uploading data to the network. Arweave is completely data-agnostic - it stores whatever data you provide without any knowledge of whether it's encrypted or not. How Encryption Works on Arweave Critical Points: No native encryption: Arweave provides no encryption services whatsoever Client-side only: You must encrypt data before uploading Data-agnostic storage: Arweave stores any data type, including encrypted data Your responsibility: You handle all encryption, key management, and decryption Permanent security: Once encrypted and stored, data remains secure forever Encryption Options 1. Manual Client-Side Encryption Encrypt your data before uploading with Turbo: import CryptoJS from \"crypto-js\";","estimatedWords":112,"lastModified":"2025-10-20T12:12:59.332Z","breadcrumbs":["build","upload","encryption"],"siteKey":"ario","siteName":"AR-IO Network","depth":3,"crawledAt":"2025-10-20T12:12:59.332Z"},{"url":"https://docs.ar.io/build/run-a-gateway/join-the-network","title":"Join the Network","content":"Run a GatewayJoin the NetworkCopy MarkdownOpenTake control of the permanent web by running your own AR.IO Gateway. Join the decentralized network that powers the permaweb and earn rewards for providing infrastructure services. Prerequisites Running Gateway RequiredYou must have a fully functional AR.IO Gateway running with a custom domain and SSL certificates.Don't have a gateway yet? Follow our Production Setup Guide to get your gateway running with proper DNS configuration.Requirements: Gateway accessible via your custom domain (e.g., https:","estimatedWords":77,"lastModified":"2025-10-20T12:13:00.565Z","breadcrumbs":["build","run a gateway","join the network"],"siteKey":"ario","siteName":"AR-IO Network","depth":3,"crawledAt":"2025-10-20T12:13:00.565Z"},{"url":"https://docs.ar.io/build/upload/bundling-services","title":"Getting Started with Turbo","content":"Upload DataGetting Started with TurboCopy MarkdownOpenUpload data to Arweave using Turbo - the most reliable way to upload data to Arweave. Turbo provides enterprise-grade infrastructure with flexible payment options and optimized performance. What is Turbo? Turbo is a ultrahigh-throughput Permaweb service that streamlines the funding, indexing, and transmission of data to and from Arweave. It provides graphical and programmatic interfaces for payment options in fiat currency with credit or debit cards as well as cryptocurrencies such as ETH, SOL, USDC, and AR. It integrates two key components: a service that bundles uploads for efficiency and ease, and a payment system designed for straightforward transactions. Turbo Credits, which users can purchase within the ArDrive web app, the Turbo Top Up App, or by using the Turbo SDK/CLI, have the same storage purchasing power of AR tokens, along with the additional benefits provided by Turbo. These credits are meticulously calibrated, with the Winston Credit (winc) representing the smallest unit, ensuring users have precise control over their storage needs. As an open-source technology, Turbo encourages community engagement, allowing developers to contribute to its continuous enhancement. Get Started Install the SDKnpm install @ardrive/turbo-sdkSet Up Your WalletCreate a new wallet or use an existing one:# Create a new wallet (easy way) npx permaweb/wallet > key.jsonThen load it in your code:import { TurboFactory, ArweaveSigner } from \"@ardrive/turbo-sdk\"; import fs from \"fs\";","estimatedWords":225,"lastModified":"2025-10-20T12:13:01.543Z","breadcrumbs":["build","upload","bundling services"],"siteKey":"ario","siteName":"AR-IO Network","depth":3,"crawledAt":"2025-10-20T12:13:01.543Z"},{"url":"https://docs.ar.io/build/upload/advanced-uploading-with-turbo","title":"Advanced Uploading with Turbo","content":"Upload DataAdvanced Uploading with TurboCopy MarkdownOpenLearn how to upload data to Arweave using the Turbo SDK for a streamlined upload experience with multiple payment options and authentication methods. What You'll Learn How to install and authenticate with the Turbo SDK Different authentication methods (Arweave, Ethereum, Solana, etc.) How to purchase Turbo Credits How to upload files, strings, binary data, and entire folders to Arweave Browser and Node.js implementation examples Using the versatile upload method for all data types Prerequisites Node.js environment or modern web browser Wallet for authentication (Arweave, Ethereum, Solana, etc.) Basic understanding of JavaScript/TypeScript Quick Start Install the Turbo SDK# For Node.js npm install @ardrive/turbo-sdk # For Yarn users yarn add @ardrive/turbo-sdkAuthenticate with Your WalletChoose your preferred authentication method:Node.jsBrowserArweave JWKEthereumSolanaPolygonKYVEimport { TurboFactory } from '@ardrive/turbo-sdk' import fs from 'fs'","estimatedWords":131,"lastModified":"2025-10-20T12:13:02.655Z","breadcrumbs":["build","upload","advanced uploading with turbo"],"siteKey":"ario","siteName":"AR-IO Network","depth":3,"crawledAt":"2025-10-20T12:13:02.655Z"},{"url":"https://docs.ar.io/build/upload/turbo-credits","title":"Paying for Uploads","content":"Upload DataPaying for UploadsCopy MarkdownOpenTurbo Credits are the payment medium used by Turbo's upload service, providing a 1:1 conversion from Arweave's native AR token with additional benefits and flexible payment options. What are Turbo Credits? Turbo Credits represent upload power on the Arweave network, divisible down to 1 trillionth of a credit (Winston Credit). Unlike traditional crypto tokens, Turbo Credits cannot be traded, transferred, or exchanged - they exist solely for uploading data to Arweave. Important: Turbo Credits are non-refundable and cannot be withdrawn or exchanged for other cryptocurrencies. How to Purchase Credits Payment Methods Fiat Currency: Credit/debit cards via Stripe Crypto Tokens: AR, ETH, SOL, MATIC, ARIO, USDC, KYVE, ETH (BASE) Multiple Wallets: Ethereum, Solana, and Arweave wallets supported Supported Tokens & Purchase Methods Payment MethodTurbo SDKTurbo CLITurbo APITop Up AppArDrive AppFiat (credit/debit card)✅✅✅✅✅AR✅✅✅✅❌ETH✅✅✅✅❌SOL✅✅✅✅❌MATIC✅✅✅❌❌KYVE✅✅✅❌❌ETH (BASE)✅✅✅❌❌ARIO✅✅✅❌❌USDC❌✅✅❌❌ Wallet Compatibility: When purchasing with cryptocurrencies, credits are deposited into the corresponding wallet type. You cannot top up an ETH wallet by paying with AR. Purchase Options Option 1: Turbo Top Up App Visit turbo-topup.com Purchase with USD or AR tokens Credits can be purchased into Ethereum or Solana wallets Option 2: ArDrive App Use ArDrive for simple purchases Buy credits with USD directly in the app Perfect for occasional users Option 3: Turbo SDK/CLI","estimatedWords":211,"lastModified":"2025-10-20T12:13:03.795Z","breadcrumbs":["build","upload","turbo credits"],"siteKey":"ario","siteName":"AR-IO Network","depth":3,"crawledAt":"2025-10-20T12:13:03.795Z"},{"url":"https://docs.ar.io/build/upload/tagging","title":"Tagging","content":"Upload DataTaggingCopy MarkdownOpenTags are key-value pairs that provide metadata about your uploaded data on Arweave. They enable discoverability, proper content serving, and integration with various protocols. Essential Tags Every upload should include these tags: Content-Type: Required - tells gateways how to serve your data App-Name: Best practice - identifies your application for discoverability Common Tag Types Content Types image/jpeg, image/png - Images application/json - JSON data text/html - HTML pages video/mp4 - Videos application/pdf - Documents App-Specific Tags App-Name - Your application identifier (e.g., \"MyApp-v1.0\", \"PhotoGallery-2024\") Title - Human-readable title Description - Content description Author - Content creator Version - Application version Protocol Tags License - Universal Data License (UDL) transaction ID License-Fee - Fee for UDL licensing UDL Integration: Learn about the Universal Data License for monetizing your data. Advanced Tagging Folder Uploads Licensed Content App-Name Best Practices Naming Convention Use descriptive, versioned App-Name values for better organization: Include version: MyApp-v1.0, PhotoGallery-2024 Be specific: EcommerceStore-v2.1 instead of just Store Use consistent format: ProjectName-vMajor.Minor Include year for time-based apps: YearlyReport-2024 Tag Limitations 4KB total for bundled data items (Turbo) 2KB total for direct L1 uploads No maximum number of tags (limited by total size) Tag names are case-sensitive No duplicate tag names allowed Important: Total tag size is limited to 4KB (bundled) or 2KB (L1). For larger metadata, store it in the data payload instead. Querying Data by Tags Once you've tagged your data, you can use GraphQL to search and filter based on those tags. This enables powerful discovery and retrieval of your stored content. Learn GraphQL QueriesDiscover how to query for data based on tags Next Steps Understanding ManifestsOrganize files with manifests for better structure.Data EncryptionSecure your sensitive data with encryption.ArFS File SystemAdvanced file organization with ArFS.How is this guide?GoodBadPaying for UploadsUnderstanding Turbo's credit system for flexible payment options and enterprise featuresManifestsUnderstanding manifests for organizing folder structures and bundles","estimatedWords":311,"lastModified":"2025-10-20T12:13:04.887Z","breadcrumbs":["build","upload","tagging"],"siteKey":"ario","siteName":"AR-IO Network","depth":3,"crawledAt":"2025-10-20T12:13:04.887Z"},{"url":"https://docs.ar.io/learn/token/architecture","title":"Architecture","content":"TokenArchitectureCopy MarkdownOpenARIO Contract Architecture The $ARIO token operates through a smart contract built on AO Computer. The system is composed of several interconnected components that work together to provide a comprehensive network infrastructure. Core Components Balances The Balances component manages the fundamental token accounting for the ARIO ecosystem: Token Holdings: Tracks ARIO token balances for all network participants Transfer Logic: Handles secure token transfers between addresses Paginated Queries: Provides efficient balance lookups with cursor-based pagination Integration Layer: Connects with all other components for balance updates Gateway Registry The Gateway Registry manages the network's infrastructure providers and all delegation relationships: Gateway Management: Handles gateway registration, settings updates, and network participation Operator Stakes: Manages gateway operator stakes and minimum staking requirements Delegated Stakes: Coordinates delegated stake from token holders to gateway operators Performance Tracking: Monitors gateway performance metrics and eligibility for rewards ArNS Registry The ArNS (Arweave Name System) Registry provides decentralized domain name services: Name Registration: Manages the purchase and registration of friendly names Lease Management: Handles name renewals and lease extensions Primary Names: Allows users to set primary names for their addresses ANT Integration: Links registered names to their corresponding ANT processes Vaults The Vaults component implements token time-locking mechanisms for various ecosystem purposes: Multi-Purpose Locking: Locks tokens for RFPs, bug bounties, investors, and core team members Flexible Terms: Supports various lock periods and amounts based on purpose and requirements Extension Options: Allows participants to extend vault lock periods when needed Withdrawal Logic: Manages secure token release after lock expiration or completion of terms System Processes ANT Registry Process A utility process that facilitates ANT discovery and management: Discovery Service: Makes it easy to find ANTs owned by specific wallet addresses Ownership Tracking: Provides efficient lookup of ANT ownership relationships Integration Support: Connects with wallets and dApps for seamless ANT management Query Interface: Enables paginated queries for ANT discovery ArNS Name Tokens (ANTs) Transferable token processes that represent ownership and control of ArNS names: Name Ownership: Each ANT process controls a specific ArNS name Record Management: ANT holders manage DNS-like records for their names Undername Control: Support for creating and managing subdomains (undernames) Transferable Rights: ANTs can be bought, sold, and transferred as independent tokens Process-Based: Each ANT is its own AO process with autonomous functionality Security Model The architecture implements multiple layers of security: Economic Security Stake Requirements: Minimum stakes ensure operator commitment and skin in the game Performance-Based Removal: Gateways that fail observation for 30 consecutive epochs are removed from the network Complete Stake Slashing: 100% of stake is returned to the protocol balance when gateways are removed for poor performance Observation Consensus: Peer-to-peer monitoring ensures no single point of failure in performance evaluation Technical Security AO Computer: Leverages Arweave's permanent and decentralized compute layer Process Isolation: Separate processes for different system functions Cryptographic Verification: All transactions and state changes are cryptographically secured Governance Security Current Ownership: Currently owned by a multisig with intentions to make ownership immutable Path to Immutability: Plans to transition to fully immutable protocol without governance control Transparent Operations: All system state is publicly verifiable on Arweave Consensus-Based Evaluation: Gateway performance determined by peer consensus rather than centralized authority How is this guide?GoodBadTokenLearn about the ARIO token - the multifunction AO Computer based token that powers the AR.IO Network and its suite of permanent cloud applicationsStakingStaking desc","estimatedWords":553,"lastModified":"2025-10-20T12:13:05.865Z","breadcrumbs":["learn","token","architecture"],"siteKey":"ario","siteName":"AR-IO Network","depth":3,"crawledAt":"2025-10-20T12:13:05.866Z"},{"url":"https://docs.ar.io/learn/token/staking","title":"Staking","content":"TokenStakingCopy MarkdownOpenOverview Staking tokens within the AR.IO Network serves a dual primary purpose: it signifies a public commitment by gateway operators and qualifies them and their delegates for reward distributions. In the AR.IO ecosystem, \"staking\" refers to the process of locking a specified amount of ARIO tokens into a protocol-controlled vault. This act signifies an opportunity cost for the staker, acting both as a motivator and a public pledge to uphold the network's collective interests. Once staked, tokens remain locked until the staker initiates an 'unstake / withdraw' action or reaches the end of the vault’s lock period. It is important to note that the ARIO Token is non-inflationary, distinguishing the AR.IO Network's staking mechanism from yield-generation tools found in other protocols. Staking in this context is about eligibility for potential rewards rather than direct token yield. By staking tokens, gateway operators (and their delegates) demonstrate their commitment to the network, thereby gaining eligibility for protocol-driven rewards and access to the network’s shared resources. Gateway Staking A gateway operator must stake tokens to join their gateway to the network, which not only makes them eligible for protocol rewards but also promotes network reliability. This staking requirement reassures users and developers of the gateway's commitment to the network’s objectives, and gateways that adhere to or surpass network performance standards become eligible for these rewards. Gateway operators may increase their stake above the minimum, known as excess stake. A gateway’s total stake is impacted the following epoch once excess stake is added or removed. Delegated Staking To promote participation from a wider audience, the network allows anyone with available ARIO tokens to partake in delegated staking. Users can choose to take part in the risk and rewards of gateway operations by staking their tokens with an active gateway (or multiple gateways) through an act known as delegating. Delegators can select which gateways to stake with in gateways.ar.io - maximize their potential rewards based on operator performance, stakes, and weights How Delegated Staking Works Delegated staking allows you to participate in the AR.IO Network's reward system without running your own gateway. By staking your ARIO tokens on existing gateways, you can earn rewards while supporting network infrastructure. When you delegate stake to a gateway, you're essentially lending your tokens to increase that gateway's total stake. This increases the gateway's chances of being selected as an observer, which means more potential rewards for both the gateway operator and you as a delegator. Benefits Passive Income: Earn rewards without running infrastructure Network Participation: Support the AR.IO Network's growth Flexibility: Redelegate to different gateways as conditions change Low Barrier to Entry: No technical expertise required Transparent Rewards: Clear visibility into reward distribution Getting Started Get ARIO TokensYou'll need ARIO tokens to delegate. See our comprehensive guide on How to Get ARIO Tokens for detailed information about acquiring tokens through exchanges, swaps, and network participation.Choose a GatewayResearch gateways on the Gateway Portal to find one that matches your preferences for reward sharing and performance. Look for gateways with strong uptime, competitive reward sharing percentages, and reliable operation history.Delegate Your StakeUse the Gateway Portal to delegate your tokens. The process is straightforward and your tokens remain secure throughout.Monitor Your RewardsTrack your delegation performance and rewards through the portal's dashboard. Receive your share of the gateway's rewards based on the percentage set by the gateway operator. Important Considerations Gateway Performance: Your rewards depend on the gateway's performance and observer selection Reward Sharing: Gateway operators set the percentage of rewards shared with delegators Redelegation: You can move your stake between gateways as network conditions change Withdrawal Delays: There may be delays when withdrawing your delegated stake Stake Redelegation This feature enables existing stakers to reallocate their staked tokens between gateways, known as redelegation. Both delegated stakers and gateway operators with excess stake (stake above the minimum network-join requirement) can take advantage of this feature. Redelegation is intended to offer users flexibility and the ability to respond to changing network conditions. Redeeming Delegated Stake for ArNS Staked tokens generally have restricted liquidity to maintain a healthy degree of stability in the network. However, an exception to these restrictions allows delegated stakers to use their staked tokens for specific ArNS-related services. By leveraging their staking rewards, delegates can further engage with ArNS, strengthening the name system’s utilization and impact across the network. Expedited Withdrawal Fees Gateway operators and delegated stakers can shorten the standard withdrawal delay period after initiating a withdrawal (or being placed into an automatic withdrawal by protocol mechanisms); this action is subject to a dynamic fee. At any point during the delay, users can choose to expedite access to their pending withdrawal tokens by paying a fee to the protocol balance, calculated based on how much sooner they want to receive their funds. Once triggered, the tokens are returned immediately to the user’s wallet. Explore Staking Delegate StakeStart staking your ARIO tokens with gateways to earn rewardsGateway RegistryLearn how gateways join the network and stake requirementsObserver ProtocolUnderstand how gateway performance affects staking rewardsARIO TokenLearn about the token that powers the AR.IO NetworkHow is this guide?GoodBadArchitectureExplore the technical architecture of the $ARIO contract and the AR.IO Network system componentsGet the TokenLearn how to acquire $ARIO tokens through various methods including exchanges, swaps, and network participation","estimatedWords":875,"lastModified":"2025-10-20T12:13:06.521Z","breadcrumbs":["learn","token","staking"],"siteKey":"ario","siteName":"AR-IO Network","depth":3,"crawledAt":"2025-10-20T12:13:06.521Z"},{"url":"https://docs.ar.io/learn/token/get-the-token","title":"Get the Token","content":"TokenGet the TokenCopy MarkdownOpenAcquiring ARIO Tokens There are several ways to acquire ARIO tokens, depending on your needs and preferences. Here are the primary methods available: Exchanges and Trading Platforms Centralized Exchanges ARIO tokens are available on centralized exchanges: Gate.io: Trade ARIO with various cryptocurrency pairs Verify exchange security and reputation before trading Consider factors like trading fees, liquidity, and withdrawal limits Decentralized Exchanges (DEXs) Trade ARIO on decentralized platforms within the AO ecosystem: Dexi: Native AO-based decentralized exchange Botega: AO ecosystem trading platform Vento: Decentralized exchange for AO tokens Wallet Integration Wander App: Mobile wallet with built-in ARIO exchange and swap functionality Wander Extension: Browser extension wallet for seamless ARIO transactions Recommended: We recommend using Wander for the easiest ARIO acquisition experience. Wander provides the most user-friendly way to acquire, store, and manage your ARIO tokens with integrated exchange functionality. Network Participation Gateway Operation Earn ARIO tokens by operating network infrastructure: Set up a Gateway: Deploy and maintain an AR.IO gateway Stake Initial Tokens: Meet minimum staking requirements Provide Services: Offer reliable data storage and retrieval Earn Rewards: Receive ARIO tokens for network participation Token Delegation Earn rewards by supporting existing gateway operators: Choose an Operator: Research and select a trusted gateway Delegate Tokens: Stake your ARIO with the chosen operator Earn Passively: Receive a portion of the operator's rewards Maintain Flexibility: Undelegate tokens when needed Community Programs Grants and Bounties Participate in ecosystem development programs: Developer Grants: Build applications and tools for the AR.IO ecosystem Bug Bounties: Help secure the network by finding and reporting vulnerabilities Community Initiatives: Contribute to documentation, education, and outreach Ecosystem Participation Earn tokens through various community activities: Governance Participation: Engage in network decision-making processes Content Creation: Produce educational content and tutorials Community Building: Help grow and support the AR.IO community Remember that cryptocurrency investments carry risk, and you should only invest what you can afford to lose. Always do your own research and consider consulting with financial advisors when making investment decisions.How is this guide?GoodBadStakingStaking descAdd to WanderStep-by-step guide to adding the ARIO token to your Wander wallet for viewing balances and managing tokens","estimatedWords":351,"lastModified":"2025-10-20T12:13:07.608Z","breadcrumbs":["learn","token","get the token"],"siteKey":"ario","siteName":"AR-IO Network","depth":3,"crawledAt":"2025-10-20T12:13:07.608Z"},{"url":"https://docs.ar.io/learn/token/add-to-wander","title":"Add to Wander","content":"TokenAdd to WanderCopy MarkdownOpenAdding ARIO Token to Wander Wander (formerly ArConnect) is the primary wallet for the Arweave ecosystem and provides native support for AO tokens like ARIO. Follow this guide to add ARIO to your wallet and start viewing your token balance. Prerequisites Before adding ARIO to your Wander wallet, ensure you have: Wander Wallet Installed: Download from wander.app for desktop or mobile Wallet Setup Complete: Your wallet should be created and secured with a backup phrase Active Internet Connection: Required for token import and balance queries Step-by-Step Workflow Open Wander WalletLaunch your Wander wallet application: Desktop: Open the Wander desktop application Mobile: Tap the Wander app icon on your device Access Settings MenuNavigate to the settings section of your wallet:For Mobile Users: Tap the 3 vertical dots (⋮) in the top right corner of the screen Select \"Settings\" from the dropdown menu For Desktop Users: Click the hamburger menu icon (☰) in the bottom right corner Navigate to the settings section Navigate to Token Management In the Settings menu, select \"Tokens\" This opens the token management interface where you can view and add tokens Import New Token Click the \"Import Token\" button You'll see a form for adding new token details Configure Token Type (Desktop Only)For Desktop Users: Ensure the \"Asset/Collectible\" dropdown is set to \"Asset\" This tells Wander that you're adding a fungible token, not an NFT Enter ARIO Token Details In the Process ID field, enter the ARIO token process ID: qNvAoz0TgcH7DMg8BCVn8jF32QH5L6T29VjHxhHqqGE Once you enter the Process ID, Wander will automatically populate: Token Ticker: \"ARIO\" Token Name: \"AR.IO Network\" Verify that the auto-populated information is correct Complete the Import Click \"Add Token\" to complete the import process Wander will add ARIO to your token list and begin querying your balance Verify Token AdditionAfter successful import, you should see: ARIO listed in your wallet's token section Your current ARIO balance (if you hold any tokens) The ARIO token logo and ticker Viewing Your ARIO Balance Once ARIO is added to your Wander wallet: Main Wallet View Your total ARIO balance appears alongside other tokens Balances update automatically when you receive or send tokens Tap/click on ARIO to view detailed transaction history Token Details Balance: Current ARIO token holdings Value: Estimated value (if price data is available) Transactions: Recent ARIO transaction history Actions: Send, receive, and manage tokens Managing ARIO Tokens Sending ARIO Select ARIO from your token list Click \"Send\" Enter recipient address and amount Confirm transaction details and send Receiving ARIO Select ARIO from your token list Click \"Receive\" Share your wallet address or QR code Incoming tokens will appear automatically Transaction History View all ARIO transactions in the token detail view Check transaction status and confirmations Access transaction IDs for verification Troubleshooting Token Not Appearing If ARIO doesn't appear after import: Refresh: Try refreshing the wallet or restarting the app Process ID: Verify you entered the correct process ID Network: Check your internet connection Support: Contact Wander support if issues persist Balance Not Updating If your balance isn't showing correctly: Sync: Allow time for the wallet to sync with the network Manual Refresh: Use the refresh option in the token list Network Status: Check if there are known network issues Import Errors If you encounter errors during import: Format Check: Ensure the process ID is correctly formatted Network Connection: Verify stable internet connectivity Wallet Version: Update to the latest version of Wander Try Again: Sometimes retrying the import process works Next Steps After successfully adding ARIO to Wander: Buy an ArNS Name: Purchase an ArNS name directly in Wander and set it as your primary name for easy identification Join the Network: Visit https:","estimatedWords":609,"lastModified":"2025-10-20T12:13:08.613Z","breadcrumbs":["learn","token","add to wander"],"siteKey":"ario","siteName":"AR-IO Network","depth":3,"crawledAt":"2025-10-20T12:13:08.613Z"},{"url":"https://docs.ar.io/learn/wayfinder/use-cases","title":"Use Cases","content":"WayfinderUse CasesCopy MarkdownOpenDecentralized Web Hosting with Flexible Access With Wayfinder, not only can websites be hosted on the Arweave network, but their accessibility is also enhanced. By using the Wayfinder Protocol, web developers can ensure that if a specific AR.IO Gateway is down, the content can still be accessed through another gateway, offering a more reliable and resilient user experience. This is particularly valuable for: Personal websites that need to remain accessible Documentation sites that must be always available Portfolio sites for professionals and creators Digital Archives and Preservation with Enhanced Sharing Digitally archiving public domain works, especially in light of events like \"banned books week\", becomes more efficient with Wayfinder. Historical institutions or enthusiasts can easily share specific Wayfinder links to documents or media. Unlike hardcoded links which might break if a specific gateway goes offline, Wayfinder ensures that the content remains consistently accessible. This is ideal for: Historical documents and public domain works Academic research and scholarly articles Cultural preservation projects Legal documents that need permanent access Media Sharing Platforms with Consistent Content Delivery For platforms hosting user-generated content, the Wayfinder Protocol provides not just decentralized hosting but also a guarantee of content delivery. Even if a content piece becomes viral and one gateway gets congested, Wayfinder ensures that users can still access the content through another gateway, providing a seamless experience. Perfect for: Social media platforms with user-generated content Video sharing sites with viral content Image galleries and art platforms Podcast hosting and audio content Decentralized Applications (DApps) with Reliable Front-End Accessibility DApps, while benefiting from Arweave's permanent hosting, can further ensure their front-end remains consistently accessible to users by using Wayfinder. If a DApp's front-end is accessed frequently, causing strain on one gateway, Wayfinder can help ensure the load is distributed, and the DApp remains online and functional. This is essential for: DeFi applications that need high availability NFT marketplaces with high traffic Gaming platforms with real-time requirements Collaborative tools and productivity apps Branded Content Access Companies and individuals can brand their permaweb content, making it accessible through their domain, enhancing brand visibility and user trust. This is achieved through DNS TXT records that link domain names to Arweave content. Dynamic Content Updates Domain owners can easily update what Permaweb content their ar:","estimatedWords":377,"lastModified":"2025-10-20T12:13:09.521Z","breadcrumbs":["learn","wayfinder","use cases"],"siteKey":"ario","siteName":"AR-IO Network","depth":3,"crawledAt":"2025-10-20T12:13:09.521Z"},{"url":"https://docs.ar.io/learn/wayfinder/integration","title":"Integration","content":"WayfinderIntegrationCopy MarkdownOpenGetting Started Get the ExtensionThe easiest way to use Wayfinder is the Wayfinder Extension, available in the Chrome Web Store. Wayfinder Extension The wayfinder-extension is a simple Chrome extension that supports the ar:","estimatedWords":34,"lastModified":"2025-10-20T12:13:10.766Z","breadcrumbs":["learn","wayfinder","integration"],"siteKey":"ario","siteName":"AR-IO Network","depth":3,"crawledAt":"2025-10-20T12:13:10.766Z"},{"url":"https://docs.ar.io/learn/gateways/gateway-registry","title":"Gateway Registry","content":"GatewaysGateway RegistryCopy MarkdownOpenOverview The AR.IO Network consists of AR.IO gateway nodes, which are identified by their registered Arweave wallet addresses and either their IP addresses or hostnames, as stored in the network's smart contract Gateway Address Registry (GAR). Any gateway operator that wishes to join the AR.IO Network must register their node in the AR.IO smart contract's Gateway Address Registry. Registration involves staking a minimum amount of ARIO tokens and providing additional metadata describing the gateway service offered. These nodes adhere to the AR.IO Network's protocols, creating a collaborative environment of gateway nodes that vary in scale and specialization. The network promotes a fundamental level of service quality and trust minimization among its participants. The gateways.ar.io portal displays all gateways currently in the network, showing their stakes, performance scores, and operational metrics Benefits of Joining the Network Being part of the network grants AR.IO gateways an array of advantages: Simplified advertising of services and discovery by end users via the Gateway Address Registry More rapid bootstrapping of key gateway operational data due to prioritized data request fulfillment among gateways joined to the network Sharing of data processing results Auditability and transparency through the use of AGPL-3 licenses, which mandate public disclosure of any software changes, thereby reinforcing the network's integrity and reliability Improved network reliability and performance through an incentive protocol, which uses a system of evaluations and rewards to encourage high-quality service from gateways Eligibility to accept delegated staking improving a gateway's discoverability and reward opportunities Eligibility to receive distributions from the protocol balance - Gateways that have joined the network are eligible to receive token distributions based on their performance and contributions to the network How the GAR Works After joining the network, the operator's gateway can be easily discovered by permaweb apps, its health can be observed, and it can participate in data sharing protocols. A gateway becomes eligible to participate in the network's incentive protocol in the epoch following the one they joined in. The GAR advertises the specific attributes of each gateway including its stake, delegates, settings and services. This enables permaweb apps and users to discover which gateways are currently available and meet their needs. Apps that read the GAR can sort and filter it using the gateway metadata, for example, ranking gateways with the highest stake, reward performance, or feature set at the top of the list. This allows users to prefer the higher staked, more rewarded gateways with certain capabilities over lower staked, less rewarded gateways. Token Incentives and Network Monitoring The AR.IO network uses a sophisticated incentive system to ensure gateway quality and reliability: Token Incentives: Learn more about how gateways earn rewards and participate in the network economy in the Token section Observer Protocol: The network employs an Observer system that monitors gateway performance and ensures quality of service. Learn more about the Observer & Incentive Protocol and how it maintains network integrity Recap The Gateway Registry is the foundation of the AR.IO network's decentralized infrastructure. Key takeaways: Network Participation: Gateways must register and stake ARIO tokens to join the network Protocol Distributions: Registered gateways are eligible to receive token distributions from the protocol balance Observer Monitoring: The network employs an Observer and Incentives Protocol that monitors gateway performance and ensures quality of service Staking & Rewards: Gateways earn rewards based on performance through a sophisticated staking system that includes delegation opportunities Discoverability: The GAR enables apps and users to find suitable gateways based on their needs Performance-Based Selection: Gateway metadata allows for intelligent routing based on stake, performance, and capabilities Transparent Ecosystem: All gateway information is publicly accessible through the smart contract and at gateways.ar.io By joining the network, gateways become part of a collaborative ecosystem that rewards quality service and ensures reliable access to the permaweb. Explore the Gateway Ecosystem Join the NetworkLearn how to register your gateway and join the AR.IO NetworkDelegate StakeParticipate in the network by delegating stake to existing gatewaysObserver ProtocolUnderstand how the network monitors gateway performance and qualityGateway ConfigurationLearn about gateway settings, optimization, and best practicesHow is this guide?GoodBadData VerificationHow AR.IO gateways ensure data integrity by verifying chunks are correctly stored and retrievable from ArweaveObservation & Incentive ProtocolThe Observation and Incentive Protocol is designed to maintain and enhance the operational integrity of gateways on the AR.IO Network through a combination of incentivizing gateways for good performance and tasking those gateways to fulfill the role of observers","estimatedWords":732,"lastModified":"2025-10-20T12:13:12.104Z","breadcrumbs":["learn","gateways","gateway registry"],"siteKey":"ario","siteName":"AR-IO Network","depth":3,"crawledAt":"2025-10-20T12:13:12.104Z"},{"url":"https://docs.ar.io/learn/arns/pricing-model","title":"Pricing Model","content":"Arweave Name System (ArNS)Pricing ModelCopy MarkdownOpenAddressing Variable Market Conditions The future market landscape is unpredictable, and the AR.IO Network smart contract is designed to be immutable, operating without governance or manual intervention. Using a pricing oracle to fix name prices relative to a stable currency is not viable due to the infancy of available solutions and reliance on external dependencies. To address these challenges, ArNS is self-contained and adaptive, with name prices reflecting network activity and market conditions over time. To achieve this, ArNS incorporates: A dynamic pricing model that adjusts fees using a \"Demand Factor\" based on ArNS purchase activity A Returned Name Premium (RNP) system that applies a timed, descending multiplier to registration prices for names that have recently expired or been returned to the protocol This approach ensures that name valuations adapt to market conditions within the constraints of an immutable, maintenance-free smart contract framework. You can view current live pricing at ArNS.app to see these formulas in action. Key Definitions Protocol Revenue: Accumulated ARIO tokens from name registrations, lease extensions, and under_name sales Period (P): The time unit for DF adjustments, equivalent to one (1) day, denoted in milliseconds n: The current period indicator Price: The cost for permabuy or lease of a name Under_names: Subdomain equivalents, denoted by an underscore \"_\" prefixing the base domain Dynamic Pricing Model ArNS employs an adaptive pricing model to balance market demand with pricing fairness for name registration within the network. This model integrates static and dynamic elements, adjusting prices based on name length and purchase options like leasing, permanent acquisition, and undername amounts. Core Pricing Components Base Registration Fee (BRF) The fundamental price for names, varying by character length, adjusted periodically. Genesis Registration Fee (GRF) The starting price for name registrations varies by character length. This is superseded by Base Registration Fees as the protocol evolves. Table: Genesis Registration Fees Name LengthFee (ARIO)11,000,0002200,000320,000410,00052,50061,50078008500940010350113001225013-51200 Demand Factor (DF) A global price multiplier, reflecting namespace demand, adjusted each period based on revenue trends. DF Mechanics: Intent: The Demand Factor adjusts based on protocol revenue comparison to the Revenue Moving Average (RMA) Increase DF: When recent revenue is higher than or equal to (but non-zero) the RMA, the DF increases by 5.0% Decrease DF: When recent revenue is less than the RMA or both are zero, the DF decreases by 1.5% Maximum DF Value: Unbounded Minimum DF Value: 0.5 Starting Demand Factor: 1 (initial value at network launch) Revenue Moving Average (RMA) The average of protocol revenue from the past seven (7) periods. Pricing Formulas Adjusted Registration Fee (ARF) ARF=BRF×DFARF = BRF × DF ARF=BRF×DF Annual Fee Annual Fee=ARF×20Annual ~Fee = ARF × 20% Annual Fee=ARF×20 Lease Pricing Lease Registration Price: Lease Price=ARF+(Annual Fee×Years)Lease ~Price = ARF + (Annual ~Fee × Years) Lease Price=ARF+(Annual Fee×Years) Lease Extension/Renewal Price: Lease Renewal Price=Annual Fee×Years(max5years)Lease ~Renewal ~Price = Annual ~Fee × Years (max 5 years) Lease Renewal Price=Annual Fee×Years(max5years) Grace period: Two (2) weeks Permanent Purchases Permabuy Price: Permabuy Price=ARF+(AnnualFee×20years)Permabuy ~Price = ARF + (Annual Fee × 20 years) Permabuy Price=ARF+(AnnualFee×20years) Lease to Permabuy Price: Same as above Under Name Fees Initial Allocation: 10 under_names are included with each name registration For Leases: Lease Under Name Fee=BRF×DF×0.1Lease ~Under ~Name ~Fee = BRF × DF × 0.1% Lease Under Name Fee=BRF×DF×0.1 For Permabuys: Permabuy Under Name Fee=BRF×DF×0.5Permabuy ~Under ~Name ~Fee = BRF × DF × 0.5% Permabuy Under Name Fee=BRF×DF×0.5 Primary Name Fee Set or change primary name: The fee is equal to the associated fee for a single under_name purchase of a 51-character name of equivalent purchase type to the new primary name, regardless of the new primary name's length. Step Pricing Mechanics Synchronizes BRF (Base Rate Factor) with ARF (Adjusted Rate Factor) after seven (7) consecutive periods at the minimum DF value Resets DF to 1 following a step pricing adjustment Returned Name Premiums (RNP) ArNS applies a Returned Name Premium (RNP) to names that re-enter the market after expiration or permanent return. This premium starts at a maximum value and decreases linearly over a predefined window, ensuring fair and transparent pricing for re-registered names. RNP Mechanics Intent The premium starts at its maximum and decreases linearly until the name is purchased. If the name is not purchased before the premium window closes, it reverts to standard pricing and is no longer classified as \"recently returned.\" RNP Window Duration: Fourteen (14) periods Returned Name Premium Formula The premium multiplier follows a linearly declining function: RNP=50−(49/14)×tRNP = 50 - (49 / 14) × t RNP=50−(49/14)×t Where: RNP: The Returned Name Premium multiplier applied to the purchased name price t: Amount of time (or time-intervals) elapsed since the start of the return window RNP Registration Price Price=RNP×(Lease Or Permabuy) Registration PricePrice = RNP × (Lease~Or~Permabuy) ~Registration~ Price Price=RNP×(Lease Or Permabuy) Registration Price Permanent Name Return Proceeds Split 50% goes to the returning name owner 50% goes to the protocol balance The RNP multiplier is applied to the registration price of both permanently purchased and leased names. Gateway Operator ArNS Discount Gateway operators who demonstrate consistent, healthy participation in the network are eligible for a 20% discount on certain ArNS interactions. Qualification Requirements To qualify for the discount: The gateway must maintain a \"Gateway Performance Ratio Weight\" (GPRW) of 0.9 or higher The gateway must have a \"Tenure Weight\" (TW) of 1.0 or greater A gateway marked as \"Leaving\" shall not be eligible for this discount Eligible Discounted Interactions Purchasing a name Extending a lease Upgrading a lease to permabuy Increasing undernames capacity Next Steps Congratulations! You now understand the complete ArNS pricing system. Ready to get started? View Live PricingSee current ArNS pricing in real-time with the live pricing chart.Register a NameVisit ArNS.app to register your first name and explore the pricing in action.Explore GatewaysLearn about AR.IO gateways and how they integrate with ArNS.Build with ArNSStart building applications that leverage ArNS for decentralized naming.How is this guide?GoodBadArweave Name Tokens (ANTs)Learn about Arweave Name Tokens (ANTs) - the ownership and control system for ArNS namesWayfinder ProtocolLearn about the Wayfinder Protocol - a URI scheme for translating Arweave content requests into user-friendly ar:","estimatedWords":1015,"lastModified":"2025-10-20T12:13:13.414Z","breadcrumbs":["learn","arns","pricing model"],"siteKey":"ario","siteName":"AR-IO Network","depth":3,"crawledAt":"2025-10-20T12:13:13.414Z"},{"url":"https://docs.ar.io/learn/arns/name-registration","title":"Name Registration","content":"Arweave Name System (ArNS)Name RegistrationCopy MarkdownOpenThere are two different types of name registrations that can be utilized based upon the needs of the user: Registration Types Lease Registration A name may be leased on a yearly basis. A leased name can have its lease extended or renewed but only up to a maximum active lease of five (5) years at any time. Permanent Registration (Permabuy) A name may be purchased for an indefinite duration with no expiration date. Registering a name requires spending ARIO tokens corresponding to the name's character length and purchase type. Name Registry The ArNS Registry is a list of all registered names and their associated ANT Process IDs. Key rules embedded within the smart contract include: Genesis Prices: Set within the contract as starting conditions Dynamic Pricing: Varies based on name length, purchase type (lease vs buy), lease duration, and current Demand Factor Name Records: Include a pointer to the Arweave Name Token process identifier, lease end time (if applicable), and undername allocation Reassignment: Name registrations can be reassigned from one ANT to another Lease Extension: Anyone with available ARIO Tokens can extend any name's active lease Lease to Permanent Buy: Anyone with available ARIO Tokens can convert a name's lease to a permanent buy Undername Capacity: Additional undername capacity can be purchased for any actively registered name Name Removal: Name records can only be removed from the registry if a lease expires, or a permanent name is returned to the protocol Name Validation Rules All names registered must meet the following criteria: Valid characters: Only numbers 0-9, characters a-z and dashes Dash placement: Dashes cannot be leading or trailing characters Single character domains: Dashes cannot be used in single character domains Length limits: 1 character minimum, 51 characters maximum Reserved names: Cannot be an invalid name predesignated to prevent unintentional use/abuse such as www Lease Management Lease Expirations When a lease term ends, there is a grace period of two (2) weeks where the lease can be renewed before it fully expires. If this grace period elapses, the name is considered expired and returns to the protocol for public registration. Once expired, a name's associated undername registrations and capacity also expire. A recently expired name's registration shall be priced subject to the \"Returned Name Premium\" mechanics. Lease to Permabuy Conversions An actively leased name may be converted to a permanent registration. The price for this conversion shall be treated as if it were a new permanent name purchase. This functionality allows users to transition from leasing to permanent ownership based on changing needs and available resources. It generates additional protocol revenue through conversion fees, contributing to the ecosystem's financial health and reward system. Permanent Name Return Users have the option to \"return\" their permanently registered names back to the protocol. This process allows users to relinquish their ownership, returning the name to the protocol for public re-registration. Only the Owner of a name can initiate a name return. When a permanent name is returned, the name is subject to a \"Returned Name Premium\", similar to expired leases. A key difference is that if the name is repurchased during the premium window, the proceeds are split between the returning owner and the protocol balance. Primary Names The Arweave Name System (ArNS) supports the designation of a \"Primary Name\" for users, simplifying how Arweave addresses are displayed across applications. A Primary Name is a user-friendly alias that replaces complex wallet addresses, making interactions and profiles easier to manage and identify. Users can set one of their owned ArNS names as their Primary Name, subject to a small fee. This allows applications to use a single, human-readable identifier for a wallet, improving user experience across the network. Next Steps Now that you understand name registration, learn about Arweave Name Tokens (ANTs) to see how ownership and control work, or explore the Pricing Model to understand how costs are calculated.How is this guide?GoodBadArweave Name System (ArNS)ArNS is a censorship-resistant naming system stored on Arweave, powered by ARIO tokens, enabled through AR.IO gateway domains, and used to connect friendly domain names to permaweb apps, web pages, data, and identities.Arweave Name Tokens (ANTs)Learn about Arweave Name Tokens (ANTs) - the ownership and control system for ArNS names","estimatedWords":706,"lastModified":"2025-10-20T12:13:14.500Z","breadcrumbs":["learn","arns","name registration"],"siteKey":"ario","siteName":"AR-IO Network","depth":3,"crawledAt":"2025-10-20T12:13:14.500Z"},{"url":"https://docs.ar.io/learn/arns/ants","title":"Arweave Name Tokens (ANTs)","content":"Arweave Name System (ArNS)Arweave Name Tokens (ANTs)Copy MarkdownOpenTo establish ownership of a record in the ArNS Registry, each record contains both a friendly name and a reference to an Arweave Name Token, ANT. Name Tokens are unique AO Computer based tokens/processes that give their owners the ability to update the Arweave Transaction IDs that their associated friendly names point to. What is an ANT? The ANT smart contract process is a standardized contract that implements the specific Arweave Name Process specification required by AR.IO gateways who resolve ArNS names and their Arweave Transaction IDs. It also contains other basic functionality to establish ownership and the ability to transfer ownership and update the Arweave Transaction ID. Name Tokens have an owner, who can transfer the token and control its modifiable settings. These settings include modifying the address resolution time to live (ttl) for each name contained in the ANT, and other settings like the ANT Name, Ticker, and an ANT Controller. Ownership and Control The controller can only manage the ANT and set and update records, name, and the ticker, but cannot transfer the ANT. Note that ANTs are initially created in accordance with network standards by an end user who then has the ability to transfer its ownership or assign a controller as they see fit. Owners of names should ensure their ANT supports evolve ability if future modifications are desired. Loss of a private key for a permanently purchased name can result in the name being \"bricked\". Under_name Ownership Undernames can have an owner set on them. This owner is empowered to set that undername as their primary name, can remove that undername as their primary name, and has full control over that Undername's metadata, such as: Transaction Id - the data the record resolves to. TTL Seconds - the Time To Live in seconds the data is cached for by clients. Owner - the owner of the record. Description - the description of the record. Display Name - the display name for the owner of the record. Keywords - the keywords for the record. Logo - the logo of the record. They do NOT have control over the priority of the undername, which is restricted to the ANT Controllers and Owner. ANT Interactions The table below indicates some of the possible interactions with the ArNS registry, corresponding ANTs, and who can perform them: TypeANT OwnerANT ControllerUndername OwnerAny ARIO Token HolderTransfer ownership✔Add / remove controllers✔Approve/Remove Primary name✔✔Reassign name to new ANT process✔Return a permanent name✔Set records (pointers, record metadata)✔✔✔Update records, name, ticker✔✔Update descriptions and keywords✔✔Create and assign undernames✔✔Extend / renew lease✔✔✔✔Increase undernames✔✔✔✔Convert lease to permanent✔✔✔✔ Under_names ANT owners and controllers can configure multiple subdomains for their registered ArNS name known as \"under_names\" or more easily written \"undernames\". These undernames are assigned individually at the time of registration or can be added on to any registered name at any time. Undernames use an underscore \"\" in place of a more typically used dot \".\" to separate the subdomain from the main ArNS domain. Secondary Markets Secondary markets could be created by ecosystem partners that facilitate the trading of Name Tokens. Additionally, tertiary markets could be created that support the leasing of these friendly names to other users. Such markets, if any, would be created by third parties unrelated to and outside of the scope of this paper or control of the Foundation. Next Steps Ready to understand how pricing works? Learn about the Pricing Model to see how costs are calculated dynamically, or go back to Name Registration to review the registration process.How is this guide?GoodBadName RegistrationLearn about registering ArNS names, including lease vs permanent options, validation rules, and the registration processPricing ModelLearn about ArNS dynamic pricing, demand factors, and returned name premiums","estimatedWords":623,"lastModified":"2025-10-20T12:13:15.573Z","breadcrumbs":["learn","arns","ants"],"siteKey":"ario","siteName":"AR-IO Network","depth":3,"crawledAt":"2025-10-20T12:13:15.573Z"},{"url":"https://docs.ar.io/learn/oip/reward-distribution","title":"Distributions","content":"Observation & Incentive ProtocolDistributionsCopy MarkdownOpenProtocol Balance and Funding The AR.IO network maintains a protocol balance that funds all gateway and observer rewards. This balance is primarily funded through ArNS name purchases, ensuring sustainable network incentives aligned with usage. Epoch Allocation Each epoch, a portion of the protocol balance is earmarked for distribution as rewards. This value shall begin at 0.1% per epoch for the first year of operation, then linearly decline down to and stabilize at 0.05% over the following 6 months. Funding Sources ArNS Name Purchases: Primary funding mechanism - fees from ArNS name registrations and renewals Network Genesis Allocation: Initial ARIO tokens allocated at network launch Undistributed Rewards: Rewards not claimed due to poor performance roll forward to future epochs From this allocation, two distinct reward categories are derived: Base Rewards Base Gateway Reward (BGR) This is the portion of the reward allocated to each Functional Gateway within the network and is calculated as: BGR=Epoch Reward Allocation×0.9Total Gateways in the NetworkBGR = \\frac{\\text{Epoch Reward Allocation} \\times 0.9}{\\text{Total Gateways in the Network}} BGR=Total Gateways in the NetworkEpoch Reward Allocation×0.9​ Base Observer Reward (BOR) Observers, due to their additional responsibilities, have a separate reward calculated as: BOR=Epoch Reward Allocation×0.1Total Selected Observers for the EpochBOR = \\frac{\\text{Epoch Reward Allocation} \\times 0.1}{\\text{Total Selected Observers for the Epoch}} BOR=Total Selected Observers for the EpochEpoch Reward Allocation×0.1​ Distribution Based on Performance The reward distribution is contingent on the performance classifications derived from the Performance Evaluation: Functional Gateways: Gateways that meet the performance criteria receive the Base Gateway Reward. Deficient Gateways: Gateways falling short in performance do not receive any gateway rewards. Functional Observers: Observers that fulfilled their duty receive the Base Observer Reward. Deficient Observers: Observers failing to meet their responsibilities do not receive observer rewards. Furthermore, if they are also Functional Gateways, their gateway reward is reduced by 25% for that epoch as a consequence for not performing their observation duty. Epoch reward distributions showing the relationship between eligible rewards (total available) and distributed rewards (actually paid out) across epochs. The difference represents rewards not distributed due to gateway or observer deficiencies. Auto-Staking Gateways shall be given the option to have their reward tokens \"auto-staked\" to their existing stake or sent to their wallet as unlocked tokens. The default setting shall be \"auto-staked\". Distribution to Delegates The protocol will automatically distribute a Functional Gateway's shared rewards with its delegates. The distribution will consider the gateway's total reward for the period (including observation rewards), the gateway's \"Delegate Reward Share Ratio\", and each delegate's stake proportional to the total delegation. Each individual delegate reward is calculated as: DRi=Total Rewards×Reward Share Ratio×Delegate’s StakeTotal Delegated StakeDR_i = \\text{Total Rewards} \\times \\text{Reward Share Ratio} \\times \\frac{\\text{Delegate's Stake}}{\\text{Total Delegated Stake}} DRi​=Total Rewards×Reward Share Ratio×Total Delegated StakeDelegate’s Stake​ Unlike gateways, token reward distributions to delegated stakers will only be \"auto-staked\" in that they will be automatically added to the delegate's existing stake associated with the rewarded gateway. The delegated staker is then free to withdraw their staked rewards at any time (subject to withdrawal delays). Undistributed Rewards In cases where rewards are not distributed, either due to the inactivity or deficiency of gateways or observers, the allocated tokens shall remain in the protocol balance and carry forward to the next epoch. This mechanism is in place to discourage observers from frivolously marking their peers as offline in hopes of attaining a higher portion of the reward pool. Note that if a gateway (and its delegates) leaves the network or a delegate fully withdraws stake from a gateway, they become ineligible to receive rewards within the corresponding epoch and the earmarked rewards will not be distributed. Handling Deficient Gateways To maintain network efficiency and reduce contract state bloat, gateways that are marked as deficient, and thus fail to receive rewards, for thirty (30) consecutive epochs will automatically trigger a \"Network Leave\" action and be subject to the associated stake withdrawal durations for both gateway stake and any delegated stake. In addition, the gateway shall have its minimum network-join stake slashed by 100%. The slashed stake shall be immediately sent to the protocol balance. Next Steps Congratulations! You now understand the complete OIP system. Ready to learn more? Explore Gateways → Gateway Documentation for technical details Learn about ArNS → ArNS Documentation for naming system details Back to Introduction → OIP Introduction to review the basics How is this guide?GoodBadPerformance and WeightsLearn about how gateways are evaluated and how weights impact observer selectionArweave Name System (ArNS)ArNS is a censorship-resistant naming system stored on Arweave, powered by ARIO tokens, enabled through AR.IO gateway domains, and used to connect friendly domain names to permaweb apps, web pages, data, and identities.","estimatedWords":776,"lastModified":"2025-10-20T12:13:16.669Z","breadcrumbs":["learn","oip","reward distribution"],"siteKey":"ario","siteName":"AR-IO Network","depth":3,"crawledAt":"2025-10-20T12:13:16.669Z"},{"url":"https://docs.ar.io/learn/oip/observer-selection","title":"Observer Selection","content":"Observation & Incentive ProtocolObserver SelectionCopy MarkdownOpenEpochs and Selection Timeline The AR.IO network operates on 24-hour epochs, during which the observer selection and evaluation process takes place. At the start of each epoch: 50 observers are selected to monitor the network 2 prescribed ArNS names are chosen for all observers to test 8 additional names are selected by each observer individually Gateway subset is selected for chunk/offset validation based on sampling rate This creates a consistent evaluation framework where all observers test the same baseline names while having flexibility to choose additional targets for comprehensive network monitoring, plus advanced data integrity verification. Selection Process Up to fifty (50) gateways are selected as observers per epoch using a sophisticated weighted random selection system. The selection uses hashchain entropy from previous AR.IO contract state messages to ensure unpredictable and tamper-resistant selection. The hashchain-based entropy provides cryptographic randomness for selecting: Observer Gateways: The 50 gateways chosen to perform observations Prescribed ArNS Names: The 2 common names all observers must evaluate This approach prevents manipulation while maintaining weighted probabilities based on gateway performance and commitment. gateways.ar.io/#/observers shows the current epoch prescribed observers and arns names, as well as their submission status Weighted Selection Criteria Observer selection is based on normalized composite weights that combine multiple performance and commitment factors. These weights determine each gateway's probability of being selected as an observer for the epoch. The selection considers four key factors that are multiplied together to create a composite weight (CW): Stake Weight (SW): Financial commitment to the network Tenure Weight (TW): Length of network participation Gateway Performance Ratio Weight (GPRW): Historical gateway performance Observer Performance Ratio Weight (OPRW): Historical observer performance These weights are then normalized across all eligible gateways to create selection probabilities. For detailed weight calculations and formulas, see Performance Evaluation. Hashchain Random Selection The selection process uses hashchain entropy from previous AR.IO contract state messages to achieve cryptographically secure randomness: How Hashchain Selection Works Entropy Source: Random numbers are generated from the hashchain of previous contract state messages Weighted Mapping: Each random number maps to normalized weight ranges of eligible gateways Observer Selection: The gateway whose weight range contains each random number is selected Prescribed Names: The same entropy selects 2 ArNS names that all observers must test This creates tamper-resistant selection where higher-weighted gateways have proportionally better chances of selection, while maintaining true randomness that cannot be predicted or manipulated. Chunk/Offset Sampling In addition to observer selection, the protocol includes a separate sampling process for chunk/offset validation: Gateway Selection for Chunk Validation Deterministic Selection: Uses PRNG seeded with observation entropy to select gateway subset Sampling Rate: Configurable percentage of gateways tested per observation (default: 1%) Minimum Guarantee: At least 1 gateway is always selected for testing Offset Selection: Random offsets within the stable weave range are chosen for each selected gateway Initial Implementation: During the initial rollout phase, only a very small portion of gateways will be checked for chunk/offset validation, and the current validation criteria are extremely lenient to ensure smooth network operation. Validation Process Chunk Retrieval: Observers request chunk data using GET /chunk/{offset} Merkle Proof Verification: Cryptographic validation ensures data integrity Early Stopping: Tests stop immediately upon first successful validation Performance Optimization: Uses LRU caching for efficient transaction lookup Fairness and Meritocracy This system ensures: Meritocratic Selection: Higher-performing gateways have better selection odds Fair Opportunity: All gateways maintain non-zero selection probability Tamper Resistance: Hashchain entropy prevents manipulation Consistent Standards: Prescribed names create common evaluation baseline The selection is saved in the contract state at epoch start to ensure that activities during the epoch do not affect selection or reward distribution. Next Steps Ready to understand how performance is evaluated? Learn about Performance Evaluation to see how gateways are scored, or explore Reward Distribution to understand how rewards are calculated and distributed.How is this guide?GoodBadObservation & Incentive ProtocolThe Observation and Incentive Protocol is designed to maintain and enhance the operational integrity of gateways on the AR.IO Network through a combination of incentivizing gateways for good performance and tasking those gateways to fulfill the role of observersReportingLearn about observer responsibilities for submitting reports to Arweave and the AR.IO Smart Contract","estimatedWords":689,"lastModified":"2025-10-20T12:13:17.670Z","breadcrumbs":["learn","oip","observer selection"],"siteKey":"ario","siteName":"AR-IO Network","depth":3,"crawledAt":"2025-10-20T12:13:17.670Z"},{"url":"https://docs.ar.io/learn/oip/reporting","title":"Reporting","content":"Observation & Incentive ProtocolReportingCopy MarkdownOpenObserver Responsibilities Selected observers have specific duties each epoch: test gateways, document results, and submit findings through two channels. Proper completion of these responsibilities determines observer rewards and future selection chances. Dual Submission Process Observers must submit their findings through both channels to fulfill their duties: 1. Detailed Reports to Arweave Format: Comprehensive JSON reports with full evaluation data Purpose: Permanent audit trail and transparency Content: Complete test results, timing data, and failure details 2. Contract Interactions to AR.IO Smart Contract Format: List of failed gateways Purpose: Efficient vote tallying for consensus Content: Binary pass/fail determinations for each gateway tested Observer Evaluations Observers test assigned gateways against 10 ArNS names (2 prescribed + 8 chosen) and document their findings: Evaluation Results Passing Report: Gateway successfully resolves ArNS names with correct status codes (200), transaction IDs, and data hashes.Failing Report: Gateway fails ArNS resolution tests due to ownership issues, timeouts (5000ms), or missing content. Observers evaluate gateways based on: Gateway Wallet Ownership: Verifies correct wallet address ArNS Resolution: Tests successful name-to-transaction resolution Content Hash Verification: Ensures data integrity Response Times: Measures performance within limits Chunk/Offset Validation: Cryptographic verification of data chunks (for selected gateways) Chunk/Offset Assessment Reporting For gateways selected for chunk/offset validation, observers perform additional testing and reporting: Validation Process Offset Selection: Random offsets within the stable weave range are chosen for testing Chunk Retrieval: Observers request chunk data using GET /chunk/{offset} endpoint Merkle Proof Verification: Cryptographic validation ensures chunk authenticity Binary Search: Efficient transaction lookup using cached metadata for proof validation Reporting Details Individual Assessments: Each offset test is tracked with pass/fail/skipped status Enforcement Status: Reports include whether chunk/offset failures affect gateway status Performance Metrics: Response times and validation results are documented Early Stopping: Tests stop immediately upon first successful validation Report Structure { \"offsetAssessments\": { \"plannedOffsets\": [12345, 67890, ...], \"actualAssessments\": [...], \"validatedOffset\": 12345, \"pass\": true, \"enforcementEnabled\": true } } Initial Implementation: During the initial rollout phase, only a very small portion of gateways will be checked for chunk/offset validation, and the current validation criteria are extremely lenient to ensure smooth network operation. Observer Rewards and Penalties Observer performance directly impacts rewards and future participation: Successful Observer Performance Observer Reward: Observers who submit both reports and contract interactions receive the Observer Reward Future Selection: Successful reporting improves Observer Performance Ratio Weight (OPRW) Increased Chances: Higher OPRW increases likelihood of future observer selection and more reward opportunities Failed Observer Performance No Observer Reward: Observers who fail to submit required reports forfeit their Observer Reward Gateway Penalty: If the deficient observer is also a functional gateway, their gateway reward is reduced by 25% Reduced Selection: Failed submissions decrease OPRW, diminishing future observer selection chances Lost Opportunities: Lower selection probability means fewer chances to earn Observer Rewards Observer Accountability The system tracks observer performance to ensure network quality: Submission Tracking: Both Arweave reports and contract interactions must be submitted Performance History: Observer submission record affects future selection probability Reward Impact: Consistent reporting builds credibility and increases earning potential Next Steps Ready to understand how these reports are processed? Learn about Performance Evaluation to see how reports become votes and determine gateway rewards, or explore Reward Distribution to understand the complete incentive structure.How is this guide?GoodBadObserver SelectionLearn about how gateways are selected as observers each epoch and how ArNS names are chosen using weighted random selection and Hashchain entropyPerformance and WeightsLearn about how gateways are evaluated and how weights impact observer selection","estimatedWords":573,"lastModified":"2025-10-20T12:13:18.654Z","breadcrumbs":["learn","oip","reporting"],"siteKey":"ario","siteName":"AR-IO Network","depth":3,"crawledAt":"2025-10-20T12:13:18.654Z"},{"url":"https://docs.ar.io/learn/oip/performance-evaluation","title":"Performance and Weights","content":"Observation & Incentive ProtocolPerformance and WeightsCopy MarkdownOpenGateway Classifications Consider the following classifications: Functional or Passed Gateways: are gateways that meet or surpass the network's performance and quality standards, including ArNS resolution and chunk/offset validation (if selected). Deficient or Failed Gateways: are gateways that fall short of the network's performance expectations, including failures in ArNS resolution or chunk/offset validation. Functional or Submitted Observers: are selected observers who diligently perform their duties and submit observation reports and contract interactions. Deficient or Failed Observers: are selected observers who do not fulfill their duty of submitting observation reports and contract interactions. Evaluation Process At the end of an epoch, the AR.IO Smart Contract processes observer submissions to determine gateway performance through a consensus-based vote tallying system. This evaluation transforms individual observer reports into network-wide performance assessments. Vote Tallying and Gateway Classification After observers submit their detailed reports (see Reporting for submission details), the smart contract performs consensus calculation: Vote Processing: Data Collection: All observer contract interactions for each gateway are collected Vote Counting: Each observer submission contributes either a PASS or FAIL vote Majority Determination: If ≥50% of submitted observer interactions indicate PASS, the gateway is considered Functional Binary Classification: Gateways are classified as either Functional (eligible for rewards) or Deficient (ineligible for rewards) Consensus Mechanism: Multiple observers evaluate each gateway independently, ensuring reliable assessment The 50% threshold requires majority agreement for positive performance determination Binary scoring provides clear, unambiguous performance classification Vote tallying occurs after the 40-minute confirmation period to ensure all interactions are finalized Chunk/Offset Validation Criteria For gateways selected for chunk/offset validation, additional performance criteria are evaluated: Validation Requirements Chunk Retrieval: Gateway must successfully respond to GET /chunk/{offset} requests Data Integrity: Chunk data must be non-empty and within reasonable size limits (<1MB) Merkle Proof Validation: Cryptographic proof must decode correctly and validate against transaction data_root Performance Standards: Response times must meet network expectations Initial Implementation: During the initial rollout phase, only a very small portion of gateways will be checked for chunk/offset validation, and the current validation criteria are extremely lenient to ensure smooth network operation. Assessment Process Binary Scoring: Each offset test results in pass/fail determination Consensus Integration: Chunk/offset results are integrated into overall gateway assessment Performance Tracking: Individual offset assessments are tracked and reported Weight Impact on Gateway Performance Gateway performance directly affects multiple weighted factors that influence future observer selection and overall network participation: Gateway Performance Ratio Weight (GPRW) A gateway's evaluation results directly impact their Gateway Performance Ratio Weight, which affects their likelihood of being selected as an observer in future epochs: GPRW=1+Passed Epochs1+Participated EpochsGPRW = \\frac{1 + \\text{Passed Epochs}}{1 + \\text{Participated Epochs}} GPRW=1+Participated Epochs1+Passed Epochs​ Impact: Functional Gateways: Increase their passed epochs count, improving their GPRW Deficient Gateways: Decrease their GPRW as participated epochs increase without corresponding passes Observer Selection: Higher GPRW increases chances of being selected as an observer Observer Performance Ratio Weight (OPRW) For gateways selected as observers, their performance in submitting reports affects future selection: OPRW=1+Submitted Epochs1+Selected EpochsOPRW = \\frac{1 + \\text{Submitted Epochs}}{1 + \\text{Selected Epochs}} OPRW=1+Selected Epochs1+Submitted Epochs​ Impact: Functional Observers: Who submit reports increase their OPRW Deficient Observers: Who fail to submit reports see their OPRW decrease Future Selection: Higher OPRW improves chances of future observer selection Composite Weight Calculation All performance factors combine to determine overall network influence: CW=SW×TW×GPRW×OPRWCW = SW \\times TW \\times GPRW \\times OPRW CW=SW×TW×GPRW×OPRW Where: SW = Stake Weight (financial commitment) TW = Tenure Weight (network longevity) GPRW = Gateway Performance Ratio Weight OPRW = Observer Performance Ratio Weight Long-term Effects: Consistently functional gateways accumulate higher composite weights Poor performers see diminishing influence and selection chances Performance history creates compounding effects on network participation Evaluation Timeline Rewards are distributed at the end of each epoch by the AR.IO Smart Contract directly based on the tallied observer votes. The smart contract processes all observer submissions and automatically distributes rewards to functional gateways and observers based on their performance during the epoch. Key Features Majority Rule: Gateway performance is determined by majority vote from observers Binary Scoring: Simple pass/fail system for clear performance assessment Network Confirmation: Delay ensures all votes are confirmed before evaluation Transparent Process: All evaluations are based on onchain data Consequences of Performance Functional Gateways Eligible for gateway rewards Maintain good standing in the network Continue to be considered for observer selection Deficient Gateways Ineligible for gateway rewards Risk being marked as deficient for multiple epochs May face additional penalties for prolonged poor performance Observer Performance Functional observers receive observer rewards Deficient observers forfeit observer rewards Deficient observers who are also functional gateways have their gateway reward reduced by 25% Next Steps Ready to understand how rewards are distributed? Learn about Reward Distribution to see the formulas and mechanics, or go back to Observer Selection to review the selection process.How is this guide?GoodBadReportingLearn about observer responsibilities for submitting reports to Arweave and the AR.IO Smart ContractDistributionsLearn about how rewards are calculated and distributed in the OIP system","estimatedWords":822,"lastModified":"2025-10-20T12:13:19.414Z","breadcrumbs":["learn","oip","performance evaluation"],"siteKey":"ario","siteName":"AR-IO Network","depth":3,"crawledAt":"2025-10-20T12:13:19.414Z"},{"url":"https://docs.ar.io/learn/gateways/architecture","title":"Architecture","content":"GatewaysArchitectureCopy MarkdownOpenAR.IO gateways are sophisticated data access layers built on top of the Arweave network. They transform the raw Arweave blockchain into a performant, reliable, and developer-friendly platform for storing and retrieving data. Gateways act as bridges between applications and the permanent storage capabilities of Arweave. Core Technology Stack AR.IO gateways are built using modern, scalable technologies designed for high-performance data operations: Runtime and Language Node.js: The primary runtime environment for all gateway services TypeScript: Core services written with flexible interfaces for customization Event-driven architecture: Enables efficient handling of concurrent operations Data Storage SQLite: Four specialized databases handle different aspects of gateway operations: Chain data indexing Bundle transaction processing Data item management Configuration and metadata Redis: High-speed caching layer for frequently accessed data File system storage: Local caching for frequently accessed data Processing Model Worker-based concurrency: Specialized workers handle different background tasks Event-driven processing: Loosely coupled components communicate via events Streaming data handling: Minimizes memory overhead for large data operations Key Architectural Decisions Several important design decisions shape how AR.IO gateways operate: Data Retrieval Strategy AR.IO gateways use a sophisticated hierarchical fallback system for data retrieval: Trusted gateways: Prioritize data from verified, high-performance peers AR.IO network: Leverage the broader network of AR.IO gateways Chunks data items: Reconstruct data from individual chunks when needed Transaction data: Fall back to raw Arweave transaction data This approach ensures data availability while optimizing for speed and reliability. Verification and Trust Model Multi-level cryptographic verification: Data integrity is verified at multiple points Trust hierarchy: Cached verified data → trusted cached data → network streams Self-healing mechanisms: Automatic recovery and re-verification of corrupted data Verification headers: HTTP headers indicate the verification status of returned data Worker Specialization Different background workers handle specific responsibilities: Block synchronization workers: Keep the gateway synchronized with Arweave blocks Bundle processing workers: Handle Layer 2 bundled data items (ANS-104) Data verification workers: Continuously verify stored data integrity Maintenance workers: Perform cleanup and optimization tasks Scalability and Configuration AR.IO gateways are designed to scale from small personal deployments to large enterprise installations: Modular Architecture Gateway services can be independently configured or disabled based on operator needs: Data serving: Serve cached data to applications Data indexing: Index and process new Arweave data Bundle processing: Handle Layer 2 bundled transactions ArNS routing: Provide Arweave Name System resolution Core Philosophy: Builder Independence A fundamental principle of AR.IO gateway architecture is empowering builders to do the things they care about without relying on any centralized resource to leverage Arweave. This philosophy manifests in several key ways: Extensibility Through Modularity Gateways are designed as extensible platforms that operators can customize through Extensions, sidecar services, and plugin architectures for specialized functionality. Data Sovereignty Operators maintain complete control through Data Retrieval strategies and Data Verification systems that ensure independence from trusted intermediaries. Network Resilience The modular design creates a resilient ecosystem where distributed infrastructure and customizable trust models prevent single points of failure. This architecture ensures that builders can create powerful applications on Arweave while maintaining independence from any centralized infrastructure or service provider. Explore Gateway Capabilities Data RetrievalLearn how gateways fetch data from multiple sources with hierarchical fallback strategiesData VerificationUnderstand how gateways ensure data integrity through cryptographic verificationRun Your Own GatewaySet up and operate your own AR.IO gateway to join the networkBuild ExtensionsExtend gateway functionality with custom plugins and integrationsHow is this guide?GoodBadAR.IO GatewaysAR.IO gateways bridge the Arweave network and applications, providing fast, reliable access to permanent data through specialized infrastructure.Data RetrievalHow AR.IO gateways retrieve and share data from multiple sources including trusted peers and Arweave nodes","estimatedWords":588,"lastModified":"2025-10-20T12:13:20.382Z","breadcrumbs":["learn","gateways","architecture"],"siteKey":"ario","siteName":"AR-IO Network","depth":3,"crawledAt":"2025-10-20T12:13:20.382Z"},{"url":"https://docs.ar.io/learn/gateways/data-retrieval","title":"Data Retrieval","content":"GatewaysData RetrievalCopy MarkdownOpenAR.IO gateways use a sophisticated multi-tier architecture to retrieve and serve Arweave data. This system ensures high availability, fast response times, and data integrity by leveraging multiple data sources with automatic fallback mechanisms. How Gateways Retrieve Data When a gateway needs to serve data, it follows a hierarchical retrieval pattern, trying each source in order until the data is successfully retrieved: Data Sources AR.IO gateways can retrieve data from multiple sources, each with different characteristics: 1. Trusted Gateways Purpose: Peer-to-peer data sharing between verified AR.IO gateways Benefits: Distributed redundancy, load balancing, network resilience Trust Mechanism: Performance-based trust scores and reciprocity monitoring Selection: Prioritized based on established trust relationships 2. AR.IO Network (Untrusted Peers) Purpose: Broader network of AR.IO gateways without established trust Benefits: Geographic distribution, expanded data availability Selection: Weighted random selection based on performance metrics Validation: Enhanced verification required due to untrusted nature 3. Chunk Assembly Purpose: Direct reconstruction from Arweave chunks via known offsets Benefits: Data integrity guarantee, no intermediary trust required Process: Fetches individual chunks efficiently and assembles them into complete data Optimization: Uses offset awareness for faster chunk retrieval 4. TX Data Purpose: Direct access to transaction data from Arweave nodes Benefits: Authoritative data source, complete historical access Trade-off: Higher latency but guaranteed availability Use Case: Final fallback when other sources fail Retrieval Strategies Gateways employ different strategies based on the use case: On-Demand Retrieval Optimized for user requests with emphasis on speed: Priority order: Trusted Gateways → Untrusted Peers (AR.IO Network) → Chunks Assembly → Arweave Aggressive timeouts: Quick fallback to next source Parallel attempts: May query multiple sources simultaneously Response streaming: Begin serving data as soon as available Background Retrieval Used specifically for unbundling and verification processes: Unbundling operations: Extracting individual data items from ANS-104 bundles Data verification: Comprehensive validation of retrieved data integrity Integrity focus: Prefers authoritative sources for accurate processing Relaxed timeouts: Allows for slower but reliable retrieval during verification Verification priority: Extensive validation before caching verified data Trust and Validation Peer Trust Management Gateways maintain sophisticated trust relationships: Trust factors include: Response performance: Latency and throughput metrics Success rates: Percentage of successful requests Data validity: Cryptographic verification results Reciprocity: Mutual data sharing behavior Data Validation Process Every piece of retrieved data undergoes validation: Hash Verification: Computed hash must match expected value Merkle Proof Validation: Chunks proven against transaction root Signature Verification: Transaction signatures validated Size Confirmation: Data size matches header declaration Why Multi-Source Retrieval Matters For Gateway Operators Reduced infrastructure costs: Leverage peer resources Improved reliability: Multiple fallback options Better performance: Optimal source selection Network effects: Benefit from collective infrastructure For Users Faster access: Data served from optimal source High availability: Multiple paths to data Geographic optimization: Nearby sources preferred Consistent experience: Transparent source selection The data retrieval system is fundamental to AR.IO's mission of providing reliable, performant access to the permaweb. This sophisticated architecture ensures that Arweave's permanent data remains accessible through a resilient, distributed gateway network. Related Gateway Concepts Data VerificationLearn how gateways ensure data integrity through cryptographic verificationGateway ArchitectureUnderstand the technical architecture and design decisions of AR.IO gatewaysAccess DataPractical guide to retrieving data from Arweave using various methodsOptimize Your GatewayConfigure indexing and filtering to optimize gateway performanceHow is this guide?GoodBadArchitectureLearn about the technical architecture of AR.IO gateways, their core dependencies, and key design decisionsData VerificationHow AR.IO gateways ensure data integrity by verifying chunks are correctly stored and retrievable from Arweave","estimatedWords":564,"lastModified":"2025-10-20T12:13:21.361Z","breadcrumbs":["learn","gateways","data retrieval"],"siteKey":"ario","siteName":"AR-IO Network","depth":3,"crawledAt":"2025-10-20T12:13:21.361Z"},{"url":"https://docs.ar.io/sdks/ardrive-core-js/contributing","title":"Contributing","content":"ArDrive Core JSContributingCopy MarkdownOpen Fork the repository Create your feature branch (git checkout -b feature/amazing-feature) Commit your changes (git commit -m 'Add some amazing feature') Push to the branch (git push origin feature/amazing-feature) Open a Pull Request How is this guide?GoodBadArLocal TestingJavaScript/TypeScript SDK for interacting with ArDriveLicenseJavaScript/TypeScript SDK for interacting with ArDrive","estimatedWords":52,"lastModified":"2025-10-20T12:13:22.254Z","breadcrumbs":["sdks","ardrive core js","contributing"],"siteKey":"ario","siteName":"AR-IO Network","depth":4,"crawledAt":"2025-10-20T12:13:22.254Z"},{"url":"https://docs.ar.io/sdks/ardrive-core-js/llm.txt","title":"Llmtxt","content":"# Anonymous Operations (/(advanced-features)/anonymous-operations) Use ArDrive without a wallet for read-only operations: ```typescript const anonymousArDrive = arDriveAnonymousFactory({}); // Read public data const publicFile = await anonymousArDrive.getPublicFile({ fileId }); const folderContents = await anonymousArDrive.listPublicFolder({ folderId }); ``` # Bundle Support (/(advanced-features)/bundle-support) Large uploads are automatically bundled for efficiency: ```typescript // Bundling happens automatically for multiple files const bulkResult = await arDrive.uploadAllEntities({ entitiesToUpload: manyFiles, // Bundling is handled internally }); ``` # Caching (/(advanced-features)/caching) ArDrive Core maintains a metadata cache for improved performance: ```shell Windows: /ardrive-caches/metadata Non-Windows: /.ardrive/caches/metadata ``` Enable cache logging: ```bash ``` # Community Features (/(advanced-features)/community-features) Send tips to the ArDrive community: ```typescript // Send community tip await arDrive.sendCommunityTip({ tokenAmount: new Winston(1000000000000), // 1 AR walletAddress, communityWalletAddress }); ``` # Manifest Creation (/(advanced-features)/manifest-creation) Create Arweave manifests for web hosting: ```typescript // Create a manifest for a folder const manifest = await arDrive.uploadPublicManifest({ folderId, destManifestName: 'index.html', conflictResolution: 'upsert' }); // Access: https://arweave.net/{manifestId} ``` # Progress Tracking (/(advanced-features)/progress-tracking) Enable upload progress logging: ```bash ``` Progress will be logged to stderr: ``` Uploading file transaction 1 of total 2 transactions... Transaction _GKQasQX194a364Hph8Oe-oku1AdfHwxWOw9_JC1yjc Upload Progress: 0% Transaction _GKQasQX194a364Hph8Oe-oku1AdfHwxWOw9_JC1yjc Upload Progress: 35% Transaction _GKQasQX194a364Hph8Oe-oku1AdfHwxWOw9_JC1yjc Upload Progress: 66% Transaction _GKQasQX194a364Hph8Oe-oku1AdfHwxWOw9_JC1yjc Upload Progress: 100% ``` # Turbo Integration (/(advanced-features)/turbo-integration) Enable Turbo for optimized uploads: ```typescript // Enable Turbo const arDriveWithTurbo = arDriveFactory({ wallet: myWallet, turboSettings: {} }); // Uploads will automatically use Turbo const result = await arDriveWithTurbo.uploadAllEntities({ entitiesToUpload: [{ wrappedEntity, destFolderId }] }); ``` # Bulk Operations (/(api-reference)/bulk-operations) #### Upload Multiple Files and Folders ```typescript // Prepare entities for upload const folder1 = wrapFileOrFolder('/path/to/folder1'); const folder2 = wrapFileOrFolder('/path/to/folder2'); const file1 = wrapFileOrFolder('/path/to/file1.txt'); // Upload everything in one operation const bulkUpload = await arDrive.uploadAllEntities({ entitiesToUpload: [ // Public folder { wrappedEntity: folder1, destFolderId: rootFolderId }, // Private folder { wrappedEntity: folder2, destFolderId: rootFolderId, driveKey: privateDriveKey }, // Public file { wrappedEntity: file1, destFolderId: someFolderId } ], conflictResolution: 'upsert' }); // Results include all created entities console.log('Created folders:', bulkUpload.created.length); console.log('Total cost:', bulkUpload.totalCost.toString()); ``` #### Create Folder and Upload Contents ```typescript // Create folder and upload all children const folderWithContents = await arDrive.createPublicFolderAndUploadChildren({ parentFolderId, wrappedFolder: wrapFileOrFolder('/path/to/folder'), conflictResolution: 'skip' }); ``` # Conflict Resolution (/(api-reference)/conflict-resolution) Available strategies when uploading files/folders that already exist: ```typescript // Skip existing files await arDrive.uploadAllEntities({ entitiesToUpload: [...], conflictResolution: 'skip' }); // Replace all existing files await arDrive.uploadAllEntities({ entitiesToUpload: [...], conflictResolution: 'replace' }); // Update only if content differs (default) await arDrive.uploadAllEntities({ entitiesToUpload: [...], conflictResolution: 'upsert' }); // Rename conflicting files await arDrive.uploadAllEntities({ entitiesToUpload: [...], conflictResolution: 'rename' }); // Throw error on conflicts await arDrive.uploadAllEntities({ entitiesToUpload: [...], conflictResolution: 'error' }); // Interactive prompt (CLI only) await arDrive.uploadAllEntities({ entitiesToUpload: [...], conflictResolution: 'ask' }); ``` # Custom Metadata (/(api-reference)/custom-metadata) Attach custom metadata to files: ```typescript const fileWithMetadata = wrapFileOrFolder( '/path/to/file.txt', 'text/plain', { metaDataJson: { 'Custom-Field': 'Custom Value', 'Version': '1.0' }, metaDataGqlTags: { 'App-Name': ['MyApp'], 'App-Version': ['1.0.0'] }, dataGqlTags: { 'Content-Type': ['text/plain'] } } ); // Upload with custom metadata await arDrive.uploadPublicFile({ parentFolderId, wrappedFile: fileWithMetadata }); ``` # Download Operations (/(api-reference)/download-operations) #### Download Files ```typescript // Download public file const publicData = await arDrive.downloadPublicFile({ fileId }); // publicData is a Buffer/Uint8Array // Download private file (automatically decrypted) const privateData = await arDrive.downloadPrivateFile({ fileId, driveKey }); ``` #### Download Folders ```typescript // Download entire folder const folderData = await arDrive.downloadPublicFolder({ folderId, destFolderPath: '/local/download/path' }); // Download private folder const privateFolderData = await arDrive.downloadPrivateFolder({ folderId, driveKey, destFolderPath: '/local/download/path' }); ``` # Drive Operations (/(api-reference)/drive-operations) #### Creating Drives ```typescript // Public drive const publicDrive = await arDrive.createPublicDrive({ driveName: 'My Public Drive' }); // Private drive with password const privateDrive = await arDrive.createPrivateDrive({ driveName: 'My Private Drive', drivePassword: 'mySecretPassword' }); ``` #### Reading Drive Information ```typescript // Get public drive const publicDriveInfo = await arDrive.getPublicDrive({ driveId }); // Get private drive (requires drive key) const privateDriveInfo = await arDrive.getPrivateDrive({ driveId, driveKey }); // Get all drives for an address const allDrives = await arDrive.getAllDrivesForAddress({ address: walletAddress, privateKeyData: wallet.getPrivateKey() }); ``` #### Renaming Drives ```typescript // Rename public drive await arDrive.renamePublicDrive({ driveId, newName: 'Updated Drive Name' }); // Rename private drive await arDrive.renamePrivateDrive({ driveId, driveKey, newName: 'Updated Private Name' }); ``` # Encryption & Security (/(api-reference)/encryption-security) #### Key Derivation ```typescript // Derive drive key from password const driveKey = await deriveDriveKey( 'myPassword', driveId.toString(), JSON.stringify(wallet.getPrivateKey()) ); // File keys are automatically derived from drive keys const fileKey = await deriveFileKey(driveKey, fileId); ``` #### Manual Encryption/Decryption ```typescript // Encrypt data const { cipher, cipherIV } = await driveEncrypt(driveKey, data); // Decrypt data const decrypted = await driveDecrypt(cipherIV, driveKey, cipher); ``` # File Operations (/(api-reference)/file-operations) #### Uploading Files ```typescript // Wrap file for upload const wrappedFile = wrapFileOrFolder('/path/to/file.pdf'); // Upload public file const publicUpload = await arDrive.uploadPublicFile({ parentFolderId, wrappedFile, conflictResolution: 'upsert' // skip, replace, upsert, or error }); // Upload private file const privateUpload = await arDrive.uploadPrivateFile({ parentFolderId, driveKey, wrappedFile }); ``` #### Reading File Information ```typescript // Get public file metadata const publicFile = await arDrive.getPublicFile({ fileId }); // Get private file metadata const privateFile = await arDrive.getPrivateFile({ fileId, driveKey }); ``` #### Moving and Renaming Files ```typescript // Move file await arDrive.movePublicFile({ fileId, newParentFolderId }); // Rename file await arDrive.renamePublicFile({ fileId, newName: 'renamed-file.pdf' }); ``` # Folder Operations (/(api-reference)/folder-operations) #### Creating Folders ```typescript // Public folder const publicFolder = await arDrive.createPublicFolder({ folderName: 'Documents', driveId, parentFolderId }); // Private folder const privateFolder = await arDrive.createPrivateFolder({ folderName: 'Secret Documents', driveId, driveKey, parentFolderId }); ``` #### Listing Folder Contents ```typescript // List public folder const publicContents = await arDrive.listPublicFolder({ folderId, maxDepth: 2, // Optional: limit recursion depth includeRoot: true // Optional: include root folder in results }); // List private folder const privateContents = await arDrive.listPrivateFolder({ folderId, driveKey, maxDepth: 1 }); ``` #### Moving and Renaming Folders ```typescript // Move folder await arDrive.movePublicFolder({ folderId, newParentFolderId }); // Rename folder await arDrive.renamePublicFolder({ folderId, newName: 'New Folder Name' }); ``` # Pricing & Cost Estimation (/(api-reference)/pricing-cost-estimation) ```typescript // Get price estimator const priceEstimator = arDrive.getArDataPriceEstimator(); // Estimate cost for data size const cost = await priceEstimator.getARPriceForByteCount( new ByteCount(1024 * 1024) // 1MB ); // Get base Winston price (without tips) const basePrice = await priceEstimator.getBaseWinstonPriceForByteCount( new ByteCount(5 * 1024 * 1024) // 5MB ); ``` # Entity IDs (/(core-concepts)/entity-ids) Use the type-safe entity ID constructors: ```typescript // Generic entity ID const entityId = EID('10108b54a-eb5e-4134-8ae2-a3946a428ec7'); // Specific entity IDs const driveId = new DriveID('12345674a-eb5e-4134-8ae2-a3946a428ec7'); const folderId = new FolderID('47162534a-eb5e-4134-8ae2-a3946a428ec7'); const fileId = new FileID('98765432a-eb5e-4134-8ae2-a3946a428ec7'); ``` # Entity Types (/(core-concepts)/entity-types) ArDrive uses a hierarchical structure: - **Drives**: Top-level containers (public or private) - **Folders**: Organize files within drives - **Files**: Individual files stored on Arweave Each entity has a unique ID (`DriveID`, `FolderID`, `FileID`) and can be either public (unencrypted) or private (encrypted). # Wallet Management (/(core-concepts)/wallet-management) ```typescript // Create wallet from JWK const wallet = new JWKWallet(jwkKey); // Check wallet balance const balance = await wallet.getBalance(); ``` # Building (/(development)/building) ```shell yarn build yarn dev ``` # Environment Setup (/(development)/environment-setup) We use nvm and Yarn for development: 1. Install nvm [using their instructions][nvm-install] 2. Install correct Node version: `nvm install && nvm use` 3. Install Yarn 3.x: Follow [Yarn installation][yarn-install] 4. Enable git hooks: `yarn husky install` 5. Install dependencies: `yarn install --check-cache` # Linting and Formatting (/(development)/linting-and-formatting) ```shell yarn lint yarn lintfix yarn format yarn typecheck ``` # Recommended VS Code Extensions (/(development)/recommended-vs-code-extensions) - [ESLint][eslint-vscode] - [EditorConfig][editor-config-vscode] - [Prettier][prettier-vscode] - [ZipFS][zipfs-vscode] # ArLocal Testing (/(testing)/arlocal-testing) For integration testing with a local Arweave instance: ```shell yarn arlocal-docker-test ``` # Running Tests (/(testing)/running-tests) ```shell yarn test yarn test -g 'My specific test' yarn coverage yarn power-assert -g 'My test case' ``` # Test Organization (/(testing)/test-organization) - Unit tests: Located next to source files (`*.test.ts`) - Integration tests: Located in `/tests` directory # Contributing (/contributing) 1. Fork the repository 2. Create your feature branch (`git checkout -b feature/amazing-feature`) 3. Commit your changes (`git commit -m 'Add some amazing feature'`) 4. Push to the branch (`git push origin feature/amazing-feature`) 5. Open a Pull Request # ArDrive Core JS (/index) **For AI and LLM users**: Access the complete ArDrive Core JS documentation in plain text format at llm.txt for easy consumption by AI agents and language models. # ArDrive Core JS Please refer to the [source code](https://github.com/ardriveapp/ardrive-core-js) for SDK details. # License (/license) AGPL-3.0-or-later # Support (/support) - [Discord Community](https://discord.gg/7RuTBckX) - [GitHub Issues](https://github.com/ardriveapp/ardrive-core-js/issues) - [ArDrive Website](https://ardrive.io) [yarn-install]: https://yarnpkg.com/getting-started/install [nvm-install]: https://github.com/nvm-sh/nvm#installing-and-updating [editor-config-vscode]: https://marketplace.visualstudio.com/items?itemName=EditorConfig.EditorConfig [prettier-vscode]: https://marketplace.visualstudio.com/items?itemName=esbenp.prettier-vscode [zipfs-vscode]: https://marketplace.visualstudio.com/items?itemName=arcanis.vscode-zipfs [eslint-vscode]: https://marketplace.visualstudio.com/items?itemName=dbaeumer.vscode-eslint [mocha]: https://github.com/mochajs/mocha [chai]: https://github.com/chaijs/chai [sinon]: https://github.com/sinonjs/sinon","estimatedWords":1389,"lastModified":"2025-10-20T12:13:23.801Z","breadcrumbs":["sdks","ardrive core js","llm.txt"],"siteKey":"ario","siteName":"AR-IO Network","depth":4,"crawledAt":"2025-10-20T12:13:23.801Z"},{"url":"https://docs.ar.io/sdks/ardrive-core-js/entity-types","title":"Entity Types","content":"ArDrive Core JSCore ConceptsEntity TypesCopy MarkdownOpenArDrive uses a hierarchical structure: Drives: Top-level containers (public or private) Folders: Organize files within drives Files: Individual files stored on Arweave Each entity has a unique ID (DriveID, FolderID, FileID) and can be either public (unencrypted) or private (encrypted).How is this guide?GoodBadArDrive Core JSJavaScript/TypeScript SDK for interacting with ArDriveWallet ManagementJavaScript/TypeScript SDK for interacting with ArDrive","estimatedWords":61,"lastModified":"2025-10-20T12:13:25.003Z","breadcrumbs":["sdks","ardrive core js","entity types"],"siteKey":"ario","siteName":"AR-IO Network","depth":4,"crawledAt":"2025-10-20T12:13:25.003Z"},{"url":"https://docs.ar.io/sdks/ardrive-cli/arfs","title":"ArFS","content":"ArDrive CLIArFSCopy MarkdownOpen[ArFS] is a data modeling, storage, and retrieval protocol designed to emulate common file system operations and to provide aspects of mutability to your data hierarchy on [Arweave]'s otherwise permanent, immutable data storage blockweave.How is this guide?GoodBadArDrive CLICommand line interface for ArDriveData PortabilityCommand line interface for ArDrive","estimatedWords":49,"lastModified":"2025-10-20T12:13:25.843Z","breadcrumbs":["sdks","ardrive cli","arfs"],"siteKey":"ario","siteName":"AR-IO Network","depth":4,"crawledAt":"2025-10-20T12:13:25.844Z"},{"url":"https://docs.ar.io/sdks/ardrive-cli/data-portability","title":"Data Portability","content":"ArDrive CLIData PortabilityCopy MarkdownOpenData uploaded via the ArDrive CLI, once indexed by Arweave's Gateways and sufficiently seeded across enough nodes on the network, can be accessed via all other ArDrive applications including the [ArDrive Web application][ardrive-web-app] at https:","estimatedWords":38,"lastModified":"2025-10-20T12:13:26.769Z","breadcrumbs":["sdks","ardrive cli","data portability"],"siteKey":"ario","siteName":"AR-IO Network","depth":4,"crawledAt":"2025-10-20T12:13:26.769Z"},{"url":"https://docs.ar.io/sdks/ardrive-cli/intended-audience","title":"Intended Audience","content":"ArDrive CLIIntended AudienceCopy MarkdownOpenThis tool is intended for use by: ArDrive power users with advanced workflows and resource efficiency in mind: bulk uploaders, those with larger storage demand, game developers, nft creators, storage/db admins, etc. Automation tools Services Terminal aficionados Extant and aspiring cypherpunks For deeper integrations with the [ArDrive] platform, consider using the [ArDrive Core][ardrive-core] (Node) library's configurable and intuitive class interfaces directly within your application. To simply install the latest version of the CLI to your local system and get started, follow the Quick Start instructions. To build and/or develop the CLI from source, follow the Build and Run from Source instructions. In either case, be sure to satisfy the requirements in the Prerequisites section.How is this guide?GoodBadData PortabilityCommand line interface for ArDriveGitCommand line interface for ArDrive","estimatedWords":129,"lastModified":"2025-10-20T12:13:27.912Z","breadcrumbs":["sdks","ardrive cli","intended audience"],"siteKey":"ario","siteName":"AR-IO Network","depth":4,"crawledAt":"2025-10-20T12:13:27.912Z"},{"url":"https://docs.ar.io/sdks/ardrive-cli/limitations","title":"Limitations","content":"ArDrive CLILimitationsCopy MarkdownOpenNumber of files in a bulk upload: Theoretically unlimited Max individual file size: 2GB (Node.js limitation) Max file name length: 255 bytes Max ANS-104 bundled transaction size: 500 MiB per bundle. App will handle creating multiple bundles. Max ANS-104 data item counts per bundled transaction: 250 Files per bundle (500 Data Items). Using the CLIHow is this guide?GoodBadRecommended Visual Studio Code extensions (Developers Only)Command line interface for ArDriveCLI HelpCommand line interface for ArDrive","estimatedWords":75,"lastModified":"2025-10-20T12:13:29.540Z","breadcrumbs":["sdks","ardrive cli","limitations"],"siteKey":"ario","siteName":"AR-IO Network","depth":4,"crawledAt":"2025-10-20T12:13:29.540Z"},{"url":"https://docs.ar.io/sdks/ardrive-cli/cli-version","title":"CLI Version","content":"ArDrive CLICLI VersionCopy MarkdownOpenYou can print out the version by running any of: ardrive --version ardrive -VHow is this guide?GoodBadCLI HelpCommand line interface for ArDriveWallet OperationsCommand line interface for ArDrive","estimatedWords":30,"lastModified":"2025-10-20T12:13:31.374Z","breadcrumbs":["sdks","ardrive cli","cli version"],"siteKey":"ario","siteName":"AR-IO Network","depth":4,"crawledAt":"2025-10-20T12:13:31.374Z"},{"url":"https://docs.ar.io/sdks/ardrive-cli/wallet-operations","title":"Wallet Operations","content":"ArDrive CLIWallet OperationsCopy MarkdownOpenBrowsing of ArDrive public data is possible without the need for an [Arweave wallet][kb-wallets]. However, for all write operations, or read operations without encryption/decryption keys, you'll need a wallet. As you utilize the CLI, you can use either your wallet file or your seed phrase interchangeably. Consider the security implications of each approach for your particular use case carefully. If at any time you'd like to generate a new wallet altogether, start by generating a new seed phase. And if you'd like to use that seed phrase in the form of a wallet file, or if you'd like to recover an existing wallet via its seed phrase, use either or both of the following commands: ardrive generate-seedphrase \"this is an example twelve word seed phrase that you could use\" ardrive generate-wallet -s \"this is an example twelve word seed phrase that you could use\" > /path/to/wallet/file.json Public attributes of Arweave wallets can be retrieved via their 43-character Arweave wallet address. You can retrieve the wallet address associated with [your wallet file or 12-word seed phrase][kb-wallets] (e.g. wallets generated by [ArConnect][arconnect]) like so: ardrive get-address -w /path/to/wallet/file.json ardrive get-address -s \"this is an example twelve word seed phrase that you could use\" HTTn8F92tR32N8wuo-NIDkjmqPknrbl10JWo5MZ9x2k You'll need AR in your wallet for any write operations you perform in ArDrive. You can always check your wallet balance (in both AR and Winston units) by performing: ardrive get-balance -w /path/to/wallet/file.json ardrive get-balance -a \"HTTn8F92tR32N8wuo-NIDkjmqPknrbl10JWo5MZ9x2k\" 1500000000000 Winston 1.5 AR If, at any time, you need to send AR out of your wallet to another wallet address, you may perform: ardrive send-ar -w /path/to/wallet/file.json --dest-address \"HTTn8F92tR32N8wuo-NIDkjmqPknrbl10JWo5MZ9x2k\" --ar-amount 2.12345How is this guide?GoodBadCLI VersionCommand line interface for ArDriveDry RunCommand line interface for ArDrive","estimatedWords":287,"lastModified":"2025-10-20T12:13:32.332Z","breadcrumbs":["sdks","ardrive cli","wallet operations"],"siteKey":"ario","siteName":"AR-IO Network","depth":4,"crawledAt":"2025-10-20T12:13:32.333Z"},{"url":"https://docs.ar.io/sdks/(clis)/llm.txt","title":"Llmtxt","content":"# Husky (Developers Only) (/ardrive-cli/(build-and-run-from-source)/husky-developers-only) We use husky 6.x to manage the git commit hooks that help to improve the quality of our commits. Please run: ```shell yarn husky install ``` to enable git hooks for your local checkout. Without doing so, you risk committing non-compliant code to the repository. # Install Yarn 3 (/ardrive-cli/(build-and-run-from-source)/install-yarn-3) Both the ArDrive CLI and ArDrive Core JS use Yarn 3 to manage dependencies and initiate workflows, so follow the [yarn installation instructions][yarn-install] in order to get the latest version. In most cases: ```shell brew install yarn npm install -g yarn ``` # Installing and Starting the CLI From Source (/ardrive-cli/(build-and-run-from-source)/installing-and-starting-the-cli-from-source) Now that your runtime and/or development environment is set up, to install the package simply run: ```shell yarn && yarn build ``` And then start the CLI (always from the root of this repository): ```shell yarn ardrive ``` For convenience in the **non-developer case**, you can install the CLI globally on your system by performing the following step: ```shell yarn pack npm install i -g /path/to/package.tgz ardrive ``` # Recommended Visual Studio Code extensions (Developers Only) (/ardrive-cli/(build-and-run-from-source)/recommended-visual-studio-code-extensions-developers-only) To ensure your environment is compatible, we also recommend the following VSCode extensions: - [ES-Lint][eslint-vscode] - [Editor-Config][editor-config-vscode] - [Prettier][prettier-vscode] - [ZipFS][zipfs-vscode] # Using a custom ArDrive-Core-JS (Optional) (/ardrive-cli/(build-and-run-from-source)/using-a-custom-ardrive-core-js-optional) To test a with a custom version of the `ardrive-core-js` library on your local system, change the `\"ardrive-core-js\"` line in `package.json` to the root of your local `ardrive-core-js` repo: ```diff - \"ardrive-core-js\": \"1.0.0\" + \"ardrive-core-js\": \"../ardrive-core-js/\" ``` # Dealing With Network Congestion (/ardrive-cli/(other-utility-operations)/dealing-with-network-congestion) Currently, Arweave blocks hold up to 1000 transactions per block. The \"mempool\", where pending transactions reside until they've been included into a block, will only hold a transaction for 50 blocks (~100-150 minutes) before it's discarded by the network resulting in no fees or data being transacted. During periods of network congestion (i.e. those where the mempool contains 1000 or more pending transactions), it may make sense to either: a) wait for congestion to dissipate before attempting your transactions. b) apply the fee boost multiplier to your transactions rewards with the --boost parameter during write operations in order to front-run some of the congestion. #### Check for network congestion before uploading ```shell ardrive get-mempool ardrive get-mempool | jq 'length' ``` #### Front-run Congestion By Boosting Miner Rewards ```shell ardrive upload-file --wallet-file /path/to/my/wallet.json --parent-folder-id \"f0c58c11-430c-4383-8e54-4d864cc7e927\" --local-path ./helloworld.txt --boost 1.5 ``` #### Send AR Transactions From a Cold Wallet The best cold wallet storage never exposes your seed phrase and/or private keys to the Internet or a compromised system interface. You can use the ArDrive CLI to facilitate cold storage and transfer of AR. If you need a new cold AR wallet, generate one from an air-gapped machine capable of running the ArDrive CLI by following the instructions in the [Wallet Operations](#wallet-operations) section. Fund your cold wallet from whatever external sources you'd like. NOTE: Your cold wallet won't appear on chain until it has received AR. The workflow to send the AR out from your cold wallet requires you to generate a signed transaction with your cold wallet on your air-gapped machine via the ArDrive CLI, and then to transfer the signed transaction (e.g. by a file on a clean thumb drive) to an Internet-connected machine and send the transaction to the network via the ArDrive CLI. You'll need two inputs from the Internet-connected machine: - the last transaction sent OUT from the cold wallet (or an empty string if none has ever been sent out) - the base fee for an Arweave transaction (i.e. a zero bye transaction). Note that this value could change if a sufficient amount of time passes between the time you fetch this value, create the transaction, and send the transaction. To get the last transaction sent from your cold wallet, use the `last-tx` command and specify your wallet address e.g.: ``` ardrive last-tx -a \\ ``` To get the base transaction reward required for an AR transaction, use the `base-reward` function, optionally applying a reward boost multiple if you're looking to front-run network congestion: ``` ardrive base-reward --boost 1.5 ``` Write down or securely copy the values you derived from the Internet-connected machine and run the following commands on the airgapped machine, piping the outputted signed transaction data to a file in the process, e.g. `sendme.json` (if that's your signed transaction transfer medium preference): ``` ardrive create-tx -w /path/to/wallet/file.json -d \\ -a \\ --last-tx \\ --reward \"\\\" > sendme.json ``` Transport your signed transaction to the Internet-connected machine and run the following command to send your transaction to the Arweave network: ``` ardrive send-tx -x /path/to/sendme.json ``` # Monitoring Transactions (/ardrive-cli/(other-utility-operations)/monitoring-transactions) Block time on Arweave is typically between 2-3 minutes in duration, so transactions can be mined within that time frame when [network congestion](#dealing-with-network-congestion) is low. Transactions, in the general case, proceed through the following set of states: - Pending: the transaction is waiting the \"mempool\" to be mined - Confirming: the transaction was mined on an Arweave Node, but has not yet been confirmed by at least 15 total nodes on the network - Confirmed: the transaction was mined on an Arweave Node and confirmed by at least 15 total nodes on the network - Not Found: the transaction is not available for any of the following reasons: - Insufficient reward to join the mempool - Insufficient reward to be mined within 50 blocks during a period of network congestion - Transaction is transitioning between states - Transaction ID is invalid Monitor any Arweave transaction's status via its transaction ID by performing: ```shell ardrive tx-status -t \"ekSMckikdRJ8RGIkFa-X3xq3427tvM7J9adv8HP3Bzs\" ``` Example output: ```shell ekSMckikdRJ8RGIkFa-X3xq3427tvM7J9adv8HP3Bzs: Mined at block height 775810 with 22439 confirmations ``` ```shell watch -n 10 ardrive tx-status -t \"ekSMckikdRJ8RGIkFa-X3xq3427tvM7J9adv8HP3Bzs\" ``` # Persistent Caching of ArFS Entity Metadata (/ardrive-cli/(other-utility-operations)/persistent-caching-of-arfs-entity-metadata) To avoid redundant requests to the Arweave network for immutable ArFS entity metadata, a persistent file cache is created and maintained at: ``` Windows: /ardrive-caches/metadata Non-Windows: /.ardrive/caches/metadata ``` The `XDG_CACHE_HOME` environment variable is honored, where applicable, and will be used in place of `os.homedir()` in the scenarios described above. Metadata cache logging to stderr can be enabled by setting the `ARDRIVE_CACHE_LOG` environment variable to `1`. Cache performance is UNDEFINED for multi-process scenarios, but is presumed to be generally usable. The cache can be manually cleared safely at any time that any integrating app is not in operation. ```shell █████╗ ██████╗ ██████╗ ██████╗ ██╗██╗ ██╗███████╗ ██╔══██╗██╔══██╗██╔══██╗██╔══██╗██║██║ ██║██╔════╝ ███████║██████╔╝██║ ██║██████╔╝██║██║ ██║█████╗ ██╔══██║██╔══██╗██║ ██║██╔══██╗██║╚██╗ ██╔╝██╔══╝ ██║ ██║██║ ██║██████╔╝██║ ██║██║ ╚████╔╝ ███████╗ ╚═╝ ╚═╝╚═╝ ╚═╝╚═════╝ ╚═╝ ╚═╝╚═╝ ╚═══╝ ╚══════╝ ██████╗██╗ ██╗ ██╔════╝██║ ██║ ██║ ██║ ██║ ██║ ██║ ██║ ╚██████╗███████╗██║ ╚═════╝╚══════╝╚═╝ Write ArFS =========== create-drive create-folder upload-file create-manifest move-file move-folder retry-tx Read ArFS =========== file-info folder-info drive-info list-folder list-drive list-all-drives download-file download-folder download-drive Wallet Ops =========== generate-seedphrase generate-wallet get-address get-balance send-ar get-drive-key get-file-key last-tx Arweave Ops =========== base-reward get-mempool create-tx send-tx tx-status ardrive \\ --help ``` [ArDrive Community Discord][ardrive-discord] [ardrive]: https://ardrive.io [arweave]: https://ardrive.io/what-is-arweave/ [ardrive-github]: https://github.com/ardriveapp/ [arfs]: https://ardrive.atlassian.net/l/c/m6P1vJDo [ardrive-web-app]: https://app.ardrive.io [ardrive-core]: https://github.com/ardriveapp/ardrive-core-js [yarn-install]: https://yarnpkg.com/getting-started/install [nvm-install]: https://github.com/nvm-sh/nvm#installing-and-updating [wsl-install]: https://code.visualstudio.com/docs/remote/wsl [editor-config-vscode]: https://marketplace.visualstudio.com/items?itemName=EditorConfig.EditorConfig [prettier-vscode]: https://marketplace.visualstudio.com/items?itemName=esbenp.prettier-vscode [zipfs-vscode]: https://marketplace.visualstudio.com/items?itemName=arcanis.vscode-zipfs [eslint-vscode]: https://marketplace.visualstudio.com/items?itemName=dbaeumer.vscode-eslint [viewblock blockchain explorer]: https://viewblock.io/arweave/ [ardrive-discord]: https://discord.com/invite/ya4hf2H [arconnect]: https://arconnect.io/ [kb-wallets]: https://ardrive.atlassian.net/l/c/FpK8FuoQ [arweave-manifests]: https://github.com/ArweaveTeam/arweave/wiki/Path-Manifests [example-manifest-webpage]: https://arweave.net/qozq9YIUPEHfZhoTp9DkBpJuA_KNULBnfLiMroj5pZI [arlocal]: https://github.com/textury/arlocal [mozilla-mime-types]: https://developer.mozilla.org/en-US/docs/Web/HTTP/Basics_of_HTTP/MIME_types/Common_types [viewblock]: https://viewblock.io/arweave/ [tx_anchors]: https://docs.arweave.org/developers/server/http-api#field-definitions [gql-guide]: https://gql-guide.vercel.app/#owners [ardrive-turbo]: https://ardrive.io/turbo/ # Using a Custom Arweave Gateway (/ardrive-cli/(other-utility-operations)/using-a-custom-arweave-gateway) On each command that uses a gateway, it is possible to supply your own custom Arweave gateway using the flag `--gateway` or by setting an environment variable named `ARWEAVE_GATEWAY`. For example, you could test out that your ArFS transactions are working as expected on a local test network such as [ArLocal] with this flow: ```shell npx arlocal curl http://localhost:1984/mint/{ your public wallet address }/99999999999999 ardrive create-drive --gateway http://127.0.0.1:1984 -w /path/to/wallet -n 'my-test-drive' curl \"$ARWEAVE_GATEWAY/mine\" ardrive upload-file -F { root folder id from create drive } -l /path/to/file -w /path/to/wallet curl \"$ARWEAVE_GATEWAY/mine\" ardrive list-drive -d { drive id from create drive } ardrive download-file -f { file id from upload file } ``` # Git (/ardrive-cli/(prerequisites)/git) Some of ArDrive's dependencies are transitively installed via Git. Install it, if necessary, and ensure that it's available within your terminal environment: [Download Git](https://git-scm.com/downloads) # NVM (Optional - Recommended) (/ardrive-cli/(prerequisites)/nvm-optional-recommended) This project uses the Node Version Manager (NVM) and an `.nvmrc` file to lock the recommended Node version used by the latest version of `ardrive-core-js`. **Note for Windows: We recommend using WSL for setting up NVM on Windows using the [instructions described here][wsl-install]** Follow these steps to get NVM up and running on your system: 1. Install NVM using [these installation instructions][nvm-install]. 2. Navigate to this project's root directory 3. Ensure that the correct version of Node is installed by performing: `nvm install` 4. Use the correct version of Node, by performing: `nvm use` **IT IS STRONGLY RECOMMENDED THAT YOU AVOID GENERATING WALLETS VIA SEED PHRASE WITH THE CLI USING ANY NODE VERSION OTHER THAN THE ONE SPECIFIED IN `.nvmrc`.** # Creating Drives (/ardrive-cli/(working-with-drives)/creating-drives) ```shell ardrive create-drive --wallet-file /path/to/my/wallet.json --drive-name \"My Public Archive\" ardrive create-drive --wallet-file /path/to/my/wallet.json --drive-name \"Teenage Love Poetry\" -P ``` # List Drive Pipeline Examples (/ardrive-cli/(working-with-drives)/list-drive-pipeline-examples) You can utilize `jq` and the list commands to reshape the commands' output data into useful forms and stats for many use cases. Here are a few examples: ```shell ardrive list-drive -d a44482fd-592e-45fa-a08a-e526c31b87f1 | jq '.[] | select(.entityType == \"file\") | \"https://app.ardrive.io/#/file/\" + .entityId + \"/view\"' ``` Example output: ```shell \"https://app.ardrive.io/#/file/1337babe-f000-dead-beef-ffffffffffff/view\" \"https://app.ardrive.io/#/file/cdbc9ddd-1cab-41d9-acbd-fd4328929de3/view\" \"https://app.ardrive.io/#/file/f19bc712-b57a-4e0d-8e5c-b7f1786b34a1/view\" \"https://app.ardrive.io/#/file/4f8e081b-42f2-442d-be41-57f6f906e1c8/view\" \"https://app.ardrive.io/#/file/0e02d254-c853-4ff0-9b6e-c4d23d2a95f5/view\" \"https://app.ardrive.io/#/file/c098b869-29d1-4a86-960f-a9e10433f0b0/view\" \"https://app.ardrive.io/#/file/4afc8cdf-4d27-408a-bfb9-0a2ec21eebf8/view\" \"https://app.ardrive.io/#/file/85fe488d-fcf7-48ca-9df8-2b39958bbf15/view\" ... ``` ```shell ardrive list-drive -d 13c3c232-6687-4d11-8ac1-35284102c7db | jq ' map(select(.entityType == \"file\") | .size) | add' ``` ```shell ardrive list-drive -d 01ea6ba3-9e58-42e7-899d-622fd110211c | jq '[ .[] | select(.entityType == \"file\") ] | length' ``` # Listing Drives for an Address (/ardrive-cli/(working-with-drives)/listing-drives-for-an-address) You can list all the drives associated with any Arweave wallet address, though the details of private drives will be obfuscated from you unless you provide the necessary decryption data. ```shell ardrive list-all-drives -w /path/to/my/wallet.json -P ardrive list-all-drives --address \"HTTn8F92tR32N8wuo-NIDkjmqPknrbl10JWo5MZ9x2k\" ``` # Listing Every Entity in a Drive (/ardrive-cli/(working-with-drives)/listing-every-entity-in-a-drive) Useful notes on listing the contents of drives: - Listing a drive is effectively the same as listing its root folder. - You can control the tree depth of the data returned. - path, txPath, and entityIdPath properties on entities can provide useful handholds for other forms of data navigation ```shell ardrive list-drive -d \"c7f87712-b54e-4491-bc96-1c5fa7b1da50\" -w /path/to/my/wallet.json -P ardrive list-drive -d \"c7f87712-b54e-4491-bc96-1c5fa7b1da50\" -w /path/to/my/wallet.json -P --with-keys ardrive list-drive -d \"c7f87712-b54e-4491-bc96-1c5fa7b1da50\" --max-depth 2 ``` # Managing Drive Passwords (/ardrive-cli/(working-with-drives)/managing-drive-passwords) The ArDrive CLI's private drive and folder functions all require either a drive password OR a drive key. Private file functions require either the drive password or the file key. **Keys and passwords are sensitive data, so manage the entry, display, storage, and transmission of them very carefully.** Drive passwords are the most portable, and fundamental, encryption facet, so a few options are available during private drive operations for supplying them: - Environment Variable - STDIN - Secure Prompt #### Supplying Your Password: Environment Variable ```shell read -rs TMP_ARDRIVE_PW ardrive \\ -w /path/to/wallet.json -P ``` #### Supplying Your Password: STDIN ```shell cat /path/to/my/drive/password.txt | ardrive \\ -w /path/to/wallet.json -P ardrive \\ -w /path/to/wallet.json -P -w /path/to/wallet.json -P ? Enter drive password: › ******** ``` # Understanding Drive and File Keys (/ardrive-cli/(working-with-drives)/understanding-drive-and-file-keys) Private Drives achieve privacy via end-to-end encryption facilitated by hash-derived \"Keys\". Drive Keys encrypt/decrypt Drive and Folder data, and File Keys encrypt/decrypt File Data. The relationships among your data and their keys is as follows: - Drive Key = functionOf(Wallet Signature, Randomly Generated Drive ID, User-specified Drive Password) - File Key = functionOf(Randomly Generated File ID, Drive Key) When you create private entities, the returned JSON data from the ArDrive CLI will contain the keys needed to decrypt the encrypted representation of your entity that is now securely and permanently stored on the blockweave. To derive the drive key again for a drive, perform the following: ```shell ardrive get-drive-key -w /path/to/my/wallet.json -d \"6939b9e0-cc98-42cb-bae0-5888eca78885\" -P ``` To derive the file key again for a file, perform the following: ```shell ardrive get-file-key --file-id \"bd2ce978-6ede-4b0d-8f79-2d7bc235a0e0\" --drive-id \"6939b9e0-cc98-42cb-bae0-5888eca78885\" --drive-key \"yHdCjpCK3EcuhQcKNx2d/NN5ReEjoKfZVqKunlCnPEo\" ``` # Understanding Drive Hierarchies (/ardrive-cli/(working-with-drives)/understanding-drive-hierarchies) At the root of every data tree is a \"Drive\" entity. When a drive is created, a Root Folder is also created for it. The entity IDs for both are generated and returned when you create a new drive: ```shell ardrive create-drive --wallet-file /path/to/my/wallet.json --drive-name \"Teenage Love Poetry\" | tee created_drive.json | jq '[.created[] | del(.metadataTxId, .entityName, .bundledIn)]' [ { \"type\": \"drive\", \"entityId\": \"6939b9e0-cc98-42cb-bae0-5888eca78885\" } { \"type\": \"folder\", \"entityId\": \"d1535126-fded-4990-809f-83a06f2a1118\" } ] ``` The relationship between the drive and its root folder is clearly visible when retrieving the drive's info: ```shell ardrive drive-info -d \"6939b9e0-cc98-42cb-bae0-5888eca78885\" | jq '{driveId, rootFolderId}' { \"driveId\": \"6939b9e0-cc98-42cb-bae0-5888eca78885\", \"rootFolderId\": \"d1535126-fded-4990-809f-83a06f2a1118\" } ``` All file and folder entities in the drive will be anchored to it by a \"Drive-ID\" GQL Tag. And they'll each be anchored to a parent folder ID, tracked via the \"Parent-Folder-ID\" GQL tag, forming a tree structure whose base terminates at the Root Folder. # Dry Run (/ardrive-cli/(working-with-entities)/dry-run) An important feature of the ArDrive CLI is the `--dry-run` flag. On each command that would write an ArFS entity, there is the option to run it as a \"dry run\". This will run all of the steps and print the outputs of a regular ArFS write, but will skip sending the actual transaction: ```shell ardrive \\ \\ --dry-run ``` This can be very useful for gathering price estimations or to confirm that you've copy-pasted your entity IDs correctly before committing to an upload. # Uploading to Turbo (BETA) (/ardrive-cli/(working-with-entities)/uploading-to-turbo-beta) Users can optionally choose to send each ArFS entities created to [ArDrive Turbo][ardrive-turbo] using the `--turbo` flag. Instead of using AR from an Arweave wallet, you can use Turbo Credits or take advantage of free/discounted upload promotions. ```shell ardrive \\ \\ --turbo ``` This flag will skip any balance check on the CLI side. Turbo will check a user's balance and accept/reject a data item at the time of upload. The `--turbo` flag by default will send your files to `upload.ardrive.io` to be bundled. To change the Turbo destination, users can use the `--turbo-url` flag. # Download a Single file (BETA) (/ardrive-cli/(working-with-files)/download-a-single-file-beta) By using the `download-file` command you can download a file on chain to a folder in your local storage specified by --local-path (or to your current working directory if not specified): ```shell ardrive download-file -w /path/to/wallet.json --file-id \"ff450770-a9cb-46a5-9234-89cbd9796610\" --local-path /my_ardrive_downloads/ ``` Specify a filename in the --local-path if you'd like to use a different name than the one that's used in your drive: ```shell ardrive download-file -w /path/to/wallet.json --file-id \"ff450770-a9cb-46a5-9234-89cbd9796610\" --local-path /my_ardrive_downloads/my_pic.png ``` # Downloading a Drive (/ardrive-cli/(working-with-files)/downloading-a-drive) To download the whole drive you can use the `download-drive` command. ```shell ardrive download-drive -d \"c0c8ba1c-efc5-420d-a07c-a755dc67f6b2\" ``` This is equivalent to running the `download-folder` command against the root folder of the drive. # Downloading a Folder with Files (/ardrive-cli/(working-with-files)/downloading-a-folder-with-files) You can download a folder from ArDrive to your local machine with the `download-folder` command. In the following examples, assume that a folder with ID \"47f5bde9-61ba-49c7-b409-1aa4a9e250f6\" exists in your drive and is named \"MyArDriveFolder\". ```shell ardrive download-folder -f \"47f5bde9-61ba-49c7-b409-1aa4a9e250f6\" ``` By specifying the `--local-path` option, you can choose the local parent folder into which the on-chain folder will be downloaded. When the parameter is omitted, its value defaults to the current working directory (i.e. `./`). ```shell ardrive download-folder -f \"47f5bde9-61ba-49c7-b409-1aa4a9e250f6\" --local-path /my_ardrive_downloads/ ``` The `--max-depth` parameter lets you to choose a custom folder depth to download. When omitted, the entire subtree of the folder will be downloaded. In the following example, only the immediate children of the folder will be downloaded: ```shell ardrive download-folder -f \"47f5bde9-61ba-49c7-b409-1aa4a9e250f6\" --max-depth 0 ``` The behaviors of `--local-path` are similar to those of `cp` and `mv` in Unix systems, e.g.: ```shell ardrive download-folder -f \"47f5bde9-61ba-49c7-b409-1aa4a9e250f6\" --local-path \"/existing_folder\" ardrive download-folder -f \"47f5bde9-61ba-49c7-b409-1aa4a9e250f6\" --local-path \"/existing_folder/MyArDriveFolder\" ardrive download-folder -f \"47f5bde9-61ba-49c7-b409-1aa4a9e250f6\" --local-path \"/existing_folder/non_existent_folder\" ardrive download-folder -f \"47f5bde9-61ba-49c7-b409-1aa4a9e250f6\" --local-path \"/non_existent_folder_1/non_existent_folder_2\" ``` # Fetching the Metadata of a File Entity (/ardrive-cli/(working-with-files)/fetching-the-metadata-of-a-file-entity) Simply perform the file-info command to retrieve the metadata of a file: ```shell ardrive file-info --file-id \"e5ebc14c-5b2d-4462-8f59-7f4a62e7770f\" ``` Example output: ```shell { \"appName\": \"ArDrive-Web\", \"appVersion\": \"0.1.0\", \"arFS\": \"0.11\", \"contentType\": \"application/json\", \"driveId\": \"51062487-2e8b-4af7-bd81-4345dc28ea5d\", \"entityType\": \"file\", \"name\": \"2_depth.png\", \"txId\": \"CZKdjqwnmxbWchGA1hjSO5ZH--4OYodIGWzI-FmX28U\", \"unixTime\": 1633625081, \"size\": 41946, \"lastModifiedDate\": 1605157729000, \"parentFolderId\": \"a2c8a0cb-0ca7-4dbb-8bf8-93f75f308e63\", \"entityId\": \"e5ebc14c-5b2d-4462-8f59-7f4a62e7770f\", \"fileId\": \"e5ebc14c-5b2d-4462-8f59-7f4a62e7770f\", \"dataTxId\": \"Jz0WsWyAGVc0aE3UzACo-YJqG8OPrN3UucmDdt8Fbjc\", \"dataContentType\": \"image/png\" } ``` # IPFS CID Tagging (/ardrive-cli/(working-with-files)/ipfs-cid-tagging) Certain nodes on the Arweave network may be running the [IPFS+Arweave bridge](https://arweave.medium.com/arweave-ipfs-persistence-for-the-interplanetary-file-system-9f12981c36c3). Tagging your file upload transaction with its IPFS v1 CID value in the 'IPFS-Add' tag may allow you to take advantage of this system. It can also be helpful for finding data on Arweave via GQL based on its CID. To include the CID tag on your **PUBLIC** file uploads, you may use the '--add-ipfs-tag' flag: ```shell ardrive upload-file --add-ipfs-tag --local-path /path/to/file.txt --parent-folder-id \"9af694f6-4cfc-4eee-88a8-1b02704760c0\" -w /path/to/wallet.json ``` # Moving Files (/ardrive-cli/(working-with-files)/moving-files) Files can be moved from one folder to another within the same drive. Moving a file is simply the process of uploading a new file metadata revision with an updated File ID Parent Folder ID relationship. The following command will move a file from its current location in a public drive to a new parent folder in that drive: ```shell ardrive move-file --file-id \"e5ebc14c-5b2d-4462-8f59-7f4a62e7770f\" --parent-folder-id \"a2c8a0cb-0ca7-4dbb-8bf8-93f75f308e63\" ``` # Name Conflict Resolution on Upload (/ardrive-cli/(working-with-files)/name-conflict-resolution-on-upload) By default, the `upload-file` command will use the upsert behavior if existing entities are encountered in the destination folder tree that would cause naming conflicts. Expect the behaviors from the following table for each of these resolution settings: | Source Type | Conflict at Dest | `skip` | `replace` | `upsert` (default) | | ----------- | ---------------- | ------ | --------- | ------------------ | | File | None | Insert | Insert | Insert | | File | Matching File | Skip | Update | Skip | | File | Different File | Skip | Update | Update | | File | Folder | Skip | Fail | Fail | | Folder | None | Insert | Insert | Insert | | Folder | File | Skip | Fail | Fail | | Folder | Folder | Re-use | Re-use | Re-use | The default upsert behavior will check the destination folder for a file with a conflicting name. If no conflicts are found, it will insert (upload) the file. In the case that there is a FILE to FILE name conflict found, it will only update it if necessary. To determine if an update is necessary, upsert will compare the last modified dates of conflicting file and the file being uploaded. When they are matching, the upload will be skipped. Otherwise the file will be updated as a new revision. To override the upsert behavior, use the `--replace` option to always make new revisions of a file or the `--skip` option to always skip the upload on name conflicts: ```shell ardrive upload-file --replace --local-path /path/to/file.txt --parent-folder-id \"9af694f6-4cfc-4eee-88a8-1b02704760c0\" -w /path/to/wallet.json ``` ```shell ardrive upload-file --skip --local-path /path/to/file.txt --parent-folder-id \"9af694f6-4cfc-4eee-88a8-1b02704760c0\" -w /path/to/wallet.json ``` Alternatively, the upload-file commands now also supports the `--ask` conflict resolution option. This setting will always provide an interactive prompt on name conflicts that allows users to decide how to resolve each conflict found: ```shell ardrive upload-file --ask --local-file-path /path/to/file.txt --parent-folder-id \"9af694f6-4cfc-4eee-88a8-1b02704760c0\" -w /path/to/wallet.json Destination folder has a file to file name conflict! File name: 2.png File ID: efbc0370-b69f-44d9-812c-0d272b019027 This file has a DIFFERENT last modified date Please select how to proceed: › - Use arrow-keys. Return to submit. ❯ Replace as new file revision Upload with a different file name Skip this file upload ``` # Progress Logging of Transaction Uploads (/ardrive-cli/(working-with-files)/progress-logging-of-transaction-uploads) Progress logging of transaction uploads to stderr can be enabled by setting the `ARDRIVE_PROGRESS_LOG` environment variable to `1`: ```shell Uploading file transaction 1 of total 2 transactions... Transaction _GKQasQX194a364Hph8Oe-oku1AdfHwxWOw9_JC1yjc Upload Progress: 0% Transaction _GKQasQX194a364Hph8Oe-oku1AdfHwxWOw9_JC1yjc Upload Progress: 35% Transaction _GKQasQX194a364Hph8Oe-oku1AdfHwxWOw9_JC1yjc Upload Progress: 66% Transaction _GKQasQX194a364Hph8Oe-oku1AdfHwxWOw9_JC1yjc Upload Progress: 100% Uploading file transaction 2 of total 2 transactions... Transaction nA1stCdTkuf290k0qsqvmJ78isEC0bwgrAi3D8Cl1LU Upload Progress: 0% Transaction nA1stCdTkuf290k0qsqvmJ78isEC0bwgrAi3D8Cl1LU Upload Progress: 13% Transaction nA1stCdTkuf290k0qsqvmJ78isEC0bwgrAi3D8Cl1LU Upload Progress: 28% Transaction nA1stCdTkuf290k0qsqvmJ78isEC0bwgrAi3D8Cl1LU Upload Progress: 42% Transaction nA1stCdTkuf290k0qsqvmJ78isEC0bwgrAi3D8Cl1LU Upload Progress: 60% Transaction nA1stCdTkuf290k0qsqvmJ78isEC0bwgrAi3D8Cl1LU Upload Progress: 76% Transaction nA1stCdTkuf290k0qsqvmJ78isEC0bwgrAi3D8Cl1LU Upload Progress: 91% Transaction nA1stCdTkuf290k0qsqvmJ78isEC0bwgrAi3D8Cl1LU Upload Progress: 100% ``` # Rename a Single File (/ardrive-cli/(working-with-files)/rename-a-single-file) To rename an on-chain file you can make use of the `rename-file` command. The required parameters are the file ID and the new name, as well as the owner wallet or seed phrase. ```shell ardrive rename-file --file-id \"290a3f9a-37b2-4f0f-a899-6fac983833b3\" --file-name \"My custom file name.txt\" --wallet-file \"wallet.json\" ``` # Retrying a Failed File Data Transaction (Public Unbundled Files Only) (/ardrive-cli/(working-with-files)/retrying-a-failed-file-data-transaction-public-unbundled-files-only) Arweave data upload transactions are split into two phases: transaction posting and chunks uploading. Once the transaction post phase has been completed, you've effectively \"paid\" the network for storage of the data chunks that you'll send in the next stage. If your system encounters an error while posting the transaction, you can retry posting the transaction for as long as your tx_anchor is valid ([learn more about tx_anchors here][tx_anchors]). You may retry and/or resume posting chunks at any time after your transaction has posted. The ArDrive CLI allows you to take advantage of this Arweave protocol capability. Using the CLI, when the transaction post has succeeded but the chunk upload step fails, the data transaction's ID could be lost. There are a few options to recover this ID. If the failed transaction is the most recent one sent from a wallet, the transaction ID can be recovered with the `ardrive last-tx -w /path/to/wallet` command AFTER the transaction's headers have been mined (It can take 5-10 minutes for the tx-id to become available with the last-tx approach). Other options for finding the partially uploaded transaction's ID include: - Using an Arweave gateway GQL http endpoint to search for transactions that belong to the wallet. See this [Arweave GQL Guide][gql-guide] for more info. - Browse the recent transactions associated with the wallet via a block explorer tool like [ViewBlock][viewblock]. In order to re-seed the chunks for an unbundled ArFS data transaction, a user must have the data transaction ID, the original file data, and either a destination folder ID or a valid file ID for the file. Supply that information to the `retry-tx` command like so: ```shell ardrive retry-tx --tx-id { Data Transaction ID } --parent-folder-id { Destination Folder ID } --local-path /path/to/file --wallet-file /path/to/wallet ``` **Note: Retry feature is currently only available for PUBLIC unbundled file transactions. It is also perfectly safe to mistakenly re-seed the chunks of a healthy transaction, the transaction will remain stable and the wallet balance will not be affected.** # Understanding Bundled Transactions (/ardrive-cli/(working-with-files)/understanding-bundled-transactions) The ArDrive CLI currently uses two different methods for uploading transactions to the Arweave network: standard transactions and Direct to Network (D2N) bundled transactions. By default, the CLI will send a D2N bundled transaction for any action that would result in multiple transactions. This bundling functionality is currently used on the `upload-file` and `create-drive` commands. D2N bundled transactions come with several benefits and implications: - Bundling saves AR and enhances ArFS reliability by sending associated ArFS transactions up as one atomic bundle. - Bundled transactions are treated as a single data transaction by the Arweave network, but can be presented as separate transactions by the Arweave Gateway once they have been \"unbundled\". - Un-bundling can take anywhere from a few minutes up to an hour. During that time, the files in the bundle will neither appear in list- commands nor be downloadable. Similarly, they will not appear in the web app after syncs until un-bundling is complete. **This can negatively affect the accuracy of upsert operations**, so it's best to wait before retrying bulk uploads. - Bundling reliability on the gateway side degrades once bundles reach either 500 data items (or ~250 files) or 500 MiB, so the CLI will create and upload multiple bundles as necessary, or will send files that are simply too large for reliable bundling as unbundled txs. # Uploading a Custom Manifest (/ardrive-cli/(working-with-files)/uploading-a-custom-manifest) Using the custom content type feature, it is possible for users to upload their own custom manifests. The Arweave gateways use this special content type in order to identify an uploaded file as a manifest: ```shell application/x.arweave-manifest+json ``` In addition to this content type, the manifest must also adhere to the [correct JSON structure](#manifest-json) of an Arweave manifest. A user can create their own manifest from scratch, or start by piping a generated manifest to a JSON file and editing it to their specifications: ```shell ardrive create-manifest -w /path/to/wallet -f \"6c312b3e-4778-4a18-8243-f2b346f5e7cb\" --dry-run | jq '{manifest}.manifest' > my-custom-manifest.json ``` After editing the generated manifest, simply perform an `upload-file` command with the custom Arweave manifest content type to any PUBLIC folder: ```shell ardrive upload-file --content-type \"application/x.arweave-manifest+json\" --local-path my-custom-manifest.json --parent-folder-id \"9af694f6-4cfc-4eee-88a8-1b02704760c0\" -w /path/to/wallet.json ``` The returned `dataTxId` field on the created `file` entity will be the endpoint that the manifest can be found on Arweave, just as explained in the [manifest sections](#uploading-manifests) above: ```shell https://arweave.net/{dataTxId} https://arweave.net/{dataTxId}/custom-file-1 https://arweave.net/{dataTxId}/custom-file-2 ``` # Uploading a Folder with Files (Bulk Upload) (/ardrive-cli/(working-with-files)/uploading-a-folder-with-files-bulk-upload) Users can perform a bulk upload by using the upload-file command on a target folder. The command will reconstruct the folder hierarchy on local disk as ArFS folders on the permaweb and upload each file into their corresponding folders: ```shell ardrive upload-file --local-path /path/to/folder --parent-folder-id \"9af694f6-4cfc-4eee-88a8-1b02704760c0\" -w /path/to/wallet.json ``` # Uploading a Non-Bundled Transaction (NOT RECOMMENDED) (/ardrive-cli/(working-with-files)/uploading-a-non-bundled-transaction-not-recommended) While not recommended, the CLI does provide the option to forcibly send all transactions as standard transactions rather than attempting to bundle them together. To do this, simply add the `--no-bundle` flag to the `upload-file` or `create-drive` command: ```shell ardrive upload-file --no-bundle --local-path /path/to/file --parent-folder-id \"9af694f6-4cfc-4eee-88a8-1b02704760c0\" -w /path/to/wallet.json ``` # Uploading a Single File (/ardrive-cli/(working-with-files)/uploading-a-single-file) To upload a file, you'll need a parent folder id, the file to upload's file path, and the path to your wallet: ```shell ardrive upload-file --local-path /path/to/file.txt --parent-folder-id \"9af694f6-4cfc-4eee-88a8-1b02704760c0\" -w /path/to/wallet.json ``` Example output: ```shell { \"created\": [ { \"type\": \"file\", \"entityName\": \"file.txt\" \"entityId\": \"6613395a-cf19-4420-846a-f88b7b765c05\" \"dataTxId\": \"l4iNWyBapfAIj7OU-nB8z9XrBhawyqzs5O9qhk-3EnI\", \"metadataTxId\": \"YfdDXUyerPCpBbGTm_gv_x5hR3tu5fnz8bM-jPL__JE\", \"bundledIn\": \"1zwdfZAIV8E26YjBs2ZQ4xjjP_1ewalvRgD_GyYw7f8\", \"sourceUri\": \"file:///path/to/file.txt\" }, { \"type\": \"bundle\", \"bundleTxId\": \"1zwdfZAIV8E26YjBs2ZQ4xjjP_1ewalvRgD_GyYw7f8\" } ], \"tips\": [ { \"txId\": \"1zwdfZAIV8E26YjBs2ZQ4xjjP_1ewalvRgD_GyYw7f8\", \"recipient\": { \"address\": \"3mxGJ4xLcQQNv6_TiKx0F0d5XVE0mNvONQI5GZXJXkt\" }, \"winston\": \"10000000\" } ], \"fees\": { \"1zwdfZAIV8E26YjBs2ZQ4xjjP_1ewalvRgD_GyYw7f8\": 42819829 } } ``` NOTE: To upload to the root of a drive, specify its root folder ID as the parent folder ID for the upload destination. You can retrieve it like so: ```shell ardrive drive-info -d \"c7f87712-b54e-4491-bc96-1c5fa7b1da50\" | jq -r '.rootFolderId' ``` # Uploading Files with Custom MetaData (/ardrive-cli/(working-with-files)/uploading-files-with-custom-metadata) ArDrive CLI has the capability of attaching custom metadata to ArFS File and Folder MetaData Transactions during the `upload-file` command. This metadata can be applied to either the GQL tags on the MetaData Transaction and/or into the MetaData Transaction's Data JSON. All custom metadata applied must ultimately adhere to the following JSON shapes: ```ts // GQL Tags type CustomMetaDataGqlTags = Record; // Data JSON Fields type CustomMetaDataJsonFields = Record; | string | number | boolean | null | { [member: string]: JsonSerializable } | JsonSerializable[]; ``` e.g: ```shell { IPFS-Add: 'MY_HASH' } { 'Custom Name': ['Val 1', 'Val 2'] } ``` When the custom metadata is attached to the MetaData Transaction's GQL tags, they will become visible on any Arweave GQL gateway and also third party tools that read GQL data. When these tags are added to the MetaData Transaction's Data JSON they can be read by downloading the JSON data directly from `https://arweave.net/METADATA_TX_ID`. To add this custom metadata to your file metadata transactions, CLI users can pass custom metadata these parameters: - `--metadata-file path/to/json/schema` - `--metadata-json '{\"key\": \"val\", \"key-2\": true, \"key-3\": 420, \"key-4\": [\"more\", 1337]}'` - `--metadata-gql-tags \"Tag-Name\" \"Tag Val\"` The `--metadata-file` will accept a file path to JSON file containing custom metadata: ```shell ardrive upload-file --metadata-file path/to/metadata/json # ... ``` This JSON schema object must contain instructions on where to put this metadata with the `metaDataJson` and `metaDataGqlTags` keys. e.g: ```json { \"metaDataJson\": { \"Tag-Name\": [\"Value-1\", \"Value-2\"] }, \"metaDataGqlTags\": { \"GQL Tag Name\": \"Tag Value\" } } ``` The `--metadata-gql-tags` parameter accepts an array of string values to be applied to the MetaData Tx GQL Tags. This method of CLI input does not support multiple tag values for a given tag name and the input must be an EVEN number of string values. (Known bug: String values starting with the `\"-\"` character are currently not supported. Use --metadata-file parameter instead.) e.g: ```shell upload-file --metadata-gql-tags \"Custom Tag Name\" \"Custom Value\" # ... ``` And the `--metadata-json` parameter will accept a stringified JSON input. It will apply all declared JSON fields directly to the MetaData Tx's Data JSON. e.g: ```shell upload-file --metadata-json ' { \"json field\": \"value\", \"another fields\": false } ' # ... ``` Custom metadata applied to files and/or folders during the `upload-file` command will be read back through all existing read commands. e.g: ```shell ardrive file-info -f 067c4008-9cbe-422e-b697-05442f73da2b { \"appName\": \"ArDrive-CLI\", \"appVersion\": \"1.17.0\", \"arFS\": \"0.11\", \"contentType\": \"application/json\", \"driveId\": \"967215ca-a489-494b-97ec-0dd428d7be34\", \"entityType\": \"file\", \"name\": \"unique-name-9718\", \"txId\": \"sxg8bNu6_bbaHkJTxAINVVoz_F-LiFe6s7OnxzoJJk4\", \"unixTime\": 1657655070, \"size\": 262148, \"lastModifiedDate\": 1655409872705, \"dataTxId\": \"ublZcIff77ejl3m0uEA8lXEfnTWmSBOFoz-HibqKeyk\", \"dataContentType\": \"text/plain\", \"parentFolderId\": \"97bc4fb5-aca4-4ffe-938f-1285153d98ca\", \"entityId\": \"067c4008-9cbe-422e-b697-05442f73da2b\", \"fileId\": \"067c4008-9cbe-422e-b697-05442f73da2b\", \"IPFS-Add\": \"MY_HASH\", \"Tag-1\": \"Val\", \"Tag-2\": \"Val\", \"Tag-3\": \"Val\", \"Boost\": \"1.05\" } ``` #### Applying Unique Custom MetaData During Bulk Workflows With some custom scripting and the `--metadata-file` parameter, the ArDrive CLI can be used to apply custom metadata to each file individually in a bulk workflow. For example, if you choose a numbered file naming pattern you can make use of a `for` loop: ```shell for i in {1..5} do ardrive upload-file -F f0c58c11-430c-4383-8e54-4d864cc7e927 --local-path \"../uploads/test-file-$i.txt\" -w \"/path/to/wallet.json\" --metadata-file \"../custom/metadata-$i.json\" --dry-run > \"file-result-$i.json\" done ``` # Uploading From a Remote URL (/ardrive-cli/(working-with-files)/uploading-from-a-remote-url) You can upload a file from an existing url using the `--remote-path` flag. This must be used in conjunction with `--dest-file-name`. You can use a custom content type using the `--content-type` flag, but if this isn't used the app will use the content type from the response header of the request for the remote data. ```shell ardrive upload-file --remote-path \"https://url/to/file\" --parent-folder-id \"9af694f6-4cfc-4eee-88a8-1b02704760c0\" -d \"example.jpg\" -w /path/to/wallet.json ``` # Uploading Manifests (/ardrive-cli/(working-with-files)/uploading-manifests) [Arweave Path Manifests][arweave-manifests] are are special `.json` files that instruct Arweave Gateways to map file data associated with specific, unique transaction IDs to customized, hosted paths relative to that of the manifest file itself. So if, for example, your manifest file had an arweave.net URL like: ```shell https://arweave.net/{manifest tx id} ``` Then, all the mapped transactions and paths in the manifest file would be addressable at URLs like: ```shell https://arweave.net/{manifest tx id}/foo.txt https://arweave.net/{manifest tx id}/bar/baz.png ``` ArDrive supports the creation of these Arweave manifests using any of your PUBLIC folders. The generated manifest paths will be links to each of the file entities within the specified folder. The manifest file entity will be created at the root of the folder. To create a manifest of an entire public drive, specify the root folder of that drive: ```shell ardrive create-manifest -f \"bc9af866-6421-40f1-ac89-202bddb5c487\" -w \"/path/to/wallet\" ``` You can also create a manifest of a folder's file entities at a custom depth by using the `--max-depth` option: ```shell ardrive create-manifest --max-depth 0 -f \"867228d8-4413-4c0e-a499-e1decbf2ea38\" -w \"/path/to/wallet\" ``` Creating a `.json` file of your manifest links output can be accomplished here with some `jq` parsing and piping to a file: ```shell ardrive create-manifest -w /path/to/wallet -f \"6c312b3e-4778-4a18-8243-f2b346f5e7cb\" | jq '{links}' > links.json ``` If you'd like to preview the contents of your manifest before uploading, you can perform a dry run and do some lightweight post processing to isolate the data: ```shell ardrive create-manifest -w /path/to/wallet -f \"6c312b3e-4778-4a18-8243-f2b346f5e7cb\" --dry-run | jq '{manifest}.manifest' ``` ```json { \"manifest\": \"arweave/paths\", \"version\": \"0.1.0\", \"index\": { \"path\": \"index.html\" }, \"paths\": { \"hello_world.txt\": { \"id\": \"Y7GFF8r9y0MEU_oi1aZeD87vrmai97JdRQ2L0cbGJ68\" }, \"index.html\": { \"id\": \"pELonjVebHyBsdxVymvxbGTmHD96v9PuuUXj8GUHGoY\" } } } ``` The manifest data transaction is tagged with a unique content-type, `application/x.arweave-manifest+json`, which tells the gateway to treat this file as a manifest. The manifest file itself is a `.json` file that holds the paths (the data transaction ids) to each file within the specified folder. When your folder is later changed by adding files or updating them with new revisions, the original manifest will NOT be updated on its own. A manifest is a permanent record of your files in their current state. However, creating a subsequent manifest with the same manifest name will create a new revision of that manifest in its new current state. Manifests follow the same name conflict resolution as outlined for files above (upsert by default). #### Hosting a Webpage with Manifest When creating a manifest, it is possible to host a webpage or web app. You can do this by creating a manifest on a folder that has an `index.html` file in its root. Using generated build folders from popular frameworks works as well. One requirement here to note is that the `href=` paths from your generated `index.html` file must not have leading a `/`. This means that the manifest will not resolve a path of `/dist/index.js` but it will resolve `dist/index.js` or `./dist/index.js`. As an example, here is a flow of creating a React app and hosting it with an ArDrive Manifest. First, generate a React app: ```shell yarn create react-app my-app ``` Next, add this field to the generated `package.json` so that the paths will resolve correctly: ```json \"homepage\": \".\", ``` Then, create an optimized production build from within the app's directory: ```shell yarn build ``` Now, we can create and upload that produced build folder on ArDrive to any of your existing ArFS folder entities: ```shell ardrive upload-file -l \"/build\" -w \"/path/to/wallet\" --parent-folder-id \"bc9af866-6421-40f1-ac89-202bddb5c487\" ``` And finally, create the manifest using the generated Folder ID from the build folder creation: ```shell ardrive create-manifest -f \"41759f05-614d-45ad-846b-63f3767504a4\" -w \"/path/to/wallet\" ``` In the return output, the top link will be a link to the deployed web app: ```shell \"links\": [ \"https://arweave.net/0MK68J8TqGhaaOpPe713Zn0jdpczMt2NGS2CtRYiuAg\", \"https://arweave.net/0MK68J8TqGhaaOpPe713Zn0jdpczMt2NGS2CtRYiuAg/asset-manifest.json\", \"https://arweave.net/0MK68J8TqGhaaOpPe713Zn0jdpczMt2NGS2CtRYiuAg/favicon.ico\", \"https://arweave.net/0MK68J8TqGhaaOpPe713Zn0jdpczMt2NGS2CtRYiuAg/index.html\", # ... ``` This is effectively hosting a web app with ArDrive. Check out the ArDrive Price Calculator React App hosted as an [ArDrive Manifest][example-manifest-webpage]. # Uploading Multiple Files (/ardrive-cli/(working-with-files)/uploading-multiple-files) To upload an arbitrary number of files or folders, pass a space-separated list of paths to `--local-paths`: ```shell ardrive upload-file -w wallet.json -F \"6939b9e0-cc98-42cb-bae0-5888eca78885\" --local-paths ./image.png ~/backups/ ../another_file.txt ardrive upload-file -w wallet.json -F \"6939b9e0-cc98-42cb-bae0-5888eca78885\" --local-paths ./*.json ``` # Uploading With a Custom Content Type (/ardrive-cli/(working-with-files)/uploading-with-a-custom-content-type) Each file uploaded to the Arweave network receives a `\"Content-Type\"` GraphQL tag that contains the MIME type for the file. The gateway will use this content type to determine how to serve that file's data transaction at the `arweave.net/{data tx id}` endpoint. By default, the CLI will attempt to derive this content type from the file extension of the provided file. In most cases, the content type that is derived will be correct and the gateway will properly serve the file. The CLI also provides the option for users to upload files with a custom content type using the `--content-type` flag: ```shell ardrive upload-file --content-type \"application/json\" --local-path /path/to/file --parent-folder-id \"9af694f6-4cfc-4eee-88a8-1b02704760c0\" -w /path/to/wallet.json ``` It is currently possible to set this value to any given string, but the gateway will still only serve valid content types. Check out this list of commonly used MIME types to ensure you're providing a valid content type: [Common MIME types][mozilla-mime-types]. Note: In the case of multi-file uploads or recursive folder uploads, setting this `--content-type` flag will set the provided custom content type on EVERY file entity within a given upload. # Creating Folders (/ardrive-cli/(working-with-folders)/creating-folders) Creating folders manually is straightforward: ```shell ardrive create-folder --parent-folder-id \"63153bb3-2ca9-4d42-9106-0ce82e793321\" --folder-name \"My Awesome Folder\" -w /path/to/wallet.json ``` Example output: ```shell { \"created\": [ { \"type\": \"folder\", \"metadataTxId\": \"AYFMBVmwqhbg9y5Fbj3Iasy5oxUqhauOW7PcS1sl4Dk\", \"entityId\": \"d1b7c514-fb12-4603-aad8-002cf63015d3\", \"key\": \"yHdCjpCKD2cuhQcKNx2d/XF5ReEjoKfZVqKunlCnPEk\", \"entityName\": \"My Awesome Folder\" } ], \"tips\": [], \"fees\": { \"AYFMBVmwqhbg9y5Fbj3Iasy5oxUqhauOW7PcS1sl4Dk\": 1378052 } } ``` Note: Folders can also be created by supplying a folder as the --local-path of an upload-file command. In this case, the folder hierarchy on the local disk will be reconstructed on chain during the course of the recursive bulk upload. # Listing Contents of a Folder (/ardrive-cli/(working-with-folders)/listing-contents-of-a-folder) Similar to drives, the `list-folder` command can be used to fetch the metadata of each entity within a folder. But by default, the command will fetch only the immediate children of that folder (`--max-depth 0`): ```shell ardrive list-folder --parent-folder-id \"29850ab7-56d4-4e1f-a5be-cb86d5513940\" ``` Example output: ```shell [ { \"appName\": \"ArDrive-CLI\", \"appVersion\": \"2.0\", \"arFS\": \"0.11\", \"contentType\": \"application/json\", \"driveId\": \"01ea6ba3-9e58-42e7-899d-622fd110211a\", \"entityType\": \"folder\", \"name\": \"mytestfolder\", \"txId\": \"HYiKyfLwY7PT9NleTQoTiM_-qPVUwf4ClDhx1sjUAEU\", \"unixTime\": 1635102772, \"parentFolderId\": \"29850ab7-56d4-4e1f-a5be-cb86d5513940\", \"entityId\": \"03df2929-1440-4ab4-bbf0-9dc776e1ed96\", \"path\": \"/My Public Folder/mytestfolder\", \"txIdPath\": \"/09_x0X2eZ3flXXLS72WdTDq6uaa5g2LjsT-QH1m0zhU/HYiKyfLwY7PT9NleTQoTiM_-qPVUwf4ClDhx1sjUAEU\", \"entityIdPath\": \"/29850ab7-56d4-4e1f-a5be-cb86d5513940/03df2929-1440-4ab4-bbf0-9dc776e1ed96\" }, { \"appName\": \"ArDrive-CLI\", \"appVersion\": \"2.0\", \"arFS\": \"0.11\", \"contentType\": \"application/json\", \"driveId\": \"01ea6ba3-9e58-42e7-899d-622fd110211a\", \"entityType\": \"folder\", \"name\": \"Super sonic public folder\", \"txId\": \"VUk1B_vo1va2-EHLtqjsotzy0Rdn6lU4hQo3RD2xoTI\", \"unixTime\": 1631283259, \"parentFolderId\": \"29850ab7-56d4-4e1f-a5be-cb86d5513940\", \"entityId\": \"452c6aec-43dc-4015-9abd-20083068d432\", \"path\": \"/My Public Folder/Super sonic sub folder\", \"txIdPath\": \"/09_x0X2eZ3flXXLS72WdTDq6uaa5g2LjsT-QH1m0zhU/VUk1B_vo1va2-EHLtqjsotzy0Rdn6lU4hQo3RD2xoTI\", \"entityIdPath\": \"/29850ab7-56d4-4e1f-a5be-cb86d5513940/452c6aec-43dc-4015-9abd-20083068d432\" }, { \"appName\": \"ArDrive-CLI\", \"appVersion\": \"2.0\", \"arFS\": \"0.11\", \"contentType\": \"application/json\", \"driveId\": \"01ea6ba3-9e58-42e7-899d-622fd110211a\", \"entityType\": \"file\", \"name\": \"test-number-twelve.txt\", \"txId\": \"429zBqnd7ZBNzgukaix26RYz3g5SeXCCo_oIY6CPZLg\", \"unixTime\": 1631722234, \"size\": 47, \"lastModifiedDate\": 1631722217028, \"dataTxId\": \"vA-BxAS7I6n90cH4Fzsk4cWS3EOPb1KOhj8yeI88dj0\", \"dataContentType\": \"text/plain\", \"parentFolderId\": \"29850ab7-56d4-4e1f-a5be-cb86d5513940\", \"entityId\": \"e5948327-d6de-4acf-a6fe-e091ecf78d71\", \"path\": \"/My Public Folder/test-number-twelve.txt\", \"txIdPath\": \"/09_x0X2eZ3flXXLS72WdTDq6uaa5g2LjsT-QH1m0zhU/429zBqnd7ZBNzgukaix26RYz3g5SeXCCo_oIY6CPZLg\", \"entityIdPath\": \"/29850ab7-56d4-4e1f-a5be-cb86d5513940/e5948327-d6de-4acf-a6fe-e091ecf78d71\" }, { \"appName\": \"ArDrive-CLI\", \"appVersion\": \"2.0\", \"arFS\": \"0.11\", \"contentType\": \"application/json\", \"driveId\": \"01ea6ba3-9e58-42e7-899d-622fd110211a\", \"entityType\": \"file\", \"name\": \"wonderful-test-file.txt\", \"txId\": \"6CokwlzB81Fx7dq-lB654VM0XQykdU6eYohDmEJ2gk4\", \"unixTime\": 1631671275, \"size\": 23, \"lastModifiedDate\": 1631283389232, \"dataTxId\": \"UP8THwA_1gvyRqNRqYmTpWvU4-UzNWBN7SiX_AIihg4\", \"dataContentType\": \"text/plain\", \"parentFolderId\": \"29850ab7-56d4-4e1f-a5be-cb86d5513940\", \"entityId\": \"3274dae9-3487-41eb-94d5-8d5d3d8bc343\", \"path\": \"/My Public Folder/wonderful-test-file.txt\", \"txIdPath\": \"/09_x0X2eZ3flXXLS72WdTDq6uaa5g2LjsT-QH1m0zhU/6CokwlzB81Fx7dq-lB654VM0XQykdU6eYohDmEJ2gk4\", \"entityIdPath\": \"/29850ab7-56d4-4e1f-a5be-cb86d5513940/3274dae9-3487-41eb-94d5-8d5d3d8bc343\" } ] ``` To list further than the immediate children, you can make use of the flags: `--all` and `--max-depth`. ```shell ardrive list-folder --parent-folder-id \"9af694f6-4cfc-4eee-88a8-1b02704760c0\" --all ardrive list-folder --parent-folder-id \"9af694f6-4cfc-4eee-88a8-1b02704760c0\" --max-depth 2 ``` In the case of private entitites, the `--with-keys` flag will make the command to include the keys in the output. ```shell ardrive list-folder --parent-folder-id \"1b027047-4cfc-4eee-88a8-9af694f660c0\" -w /my/wallet.json --with-keys ``` # Moving Folders (/ardrive-cli/(working-with-folders)/moving-folders) Moving a folder is as simple as supplying a new parent folder ID. Note that naming collisions among entities within a folder are not allowed. ```shell ardrive move-folder --folder-id \"9af694f6-4cfc-4eee-88a8-1b02704760c0\" --parent-folder-id \"29850ab7-56d4-4e1f-a5be-cb86d5513921\" -w /path/to/wallet.json ``` # Renaming Folders (/ardrive-cli/(working-with-folders)/renaming-folders) In order to rename a folder you must provide a name different from its current one, and it must not create naming conflicts with its sibling entities. ```shell ardrive rename-folder --folder-id \"568d5eba-dbf3-4a49-8129-1c58f7fd35bc\" --folder-name \"Folder with cool stuff\" -w \"./wallet.json\" ``` # Viewing Folder Metadata (/ardrive-cli/(working-with-folders)/viewing-folder-metadata) To view the metadata of a folder, users can use the `folder-info` command: ```shell ardrive folder-info --folder-id \"9af694f6-4cfc-4eee-88a8-1b02704760c0\" ``` # ArFS (/ardrive-cli/arfs) [ArFS] is a data modeling, storage, and retrieval protocol designed to emulate common file system operations and to provide aspects of mutability to your data hierarchy on [Arweave]'s otherwise permanent, immutable data storage blockweave. # CLI Help (/ardrive-cli/cli-help) Learn to use any command: ```shell ardrive --help ``` # CLI Version (/ardrive-cli/cli-version) You can print out the version by running any of: ```shell ardrive --version ardrive -V ``` # Data Portability (/ardrive-cli/data-portability) Data uploaded via the ArDrive CLI, once indexed by Arweave's Gateways and sufficiently seeded across enough nodes on the network, can be accessed via all other ArDrive applications including the [ArDrive Web application][ardrive-web-app] at https://app.ardrive.io. All transactions successfully executed by ArDrive can always be inspected in the [Viewblock blockchain explorer]. # ArDrive CLI (/ardrive-cli) **For AI and LLM users**: Access the complete ArDrive CLI documentation in plain text format at llm.txt for easy consumption by AI agents and language models. # ArDrive CLI Please refer to the [source code](https://github.com/ardriveapp/ardrive-cli) for SDK details. # Intended Audience (/ardrive-cli/intended-audience) This tool is intended for use by: - ArDrive power users with advanced workflows and resource efficiency in mind: bulk uploaders, those with larger storage demand, game developers, nft creators, storage/db admins, etc. - Automation tools - Services - Terminal aficionados - Extant and aspiring cypherpunks For deeper integrations with the [ArDrive] platform, consider using the [ArDrive Core][ardrive-core] (Node) library's configurable and intuitive class interfaces directly within your application. To simply install the latest version of the CLI to your local system and get started, follow the [Quick Start](#quick-start) instructions. To build and/or develop the CLI from source, follow the [Build and Run from Source](#build-and-run-from-source) instructions. In either case, be sure to satisfy the requirements in the [Prerequisites](#prerequisites) section. # Limitations (/ardrive-cli/limitations) **Number of files in a bulk upload:** Theoretically unlimited **Max individual file size**: 2GB (Node.js limitation) **Max file name length**: 255 bytes **Max ANS-104 bundled transaction size:** 500 MiB per bundle. App will handle creating multiple bundles. **Max ANS-104 data item counts per bundled transaction:** 250 Files per bundle (500 Data Items). # Using the CLI # Wallet Operations (/ardrive-cli/wallet-operations) Browsing of ArDrive public data is possible without the need for an [Arweave wallet][kb-wallets]. However, for all write operations, or read operations without encryption/decryption keys, you'll need a wallet. As you utilize the CLI, you can use either your wallet file or your seed phrase interchangeably. Consider the security implications of each approach for your particular use case carefully. If at any time you'd like to generate a new wallet altogether, start by generating a new seed phase. And if you'd like to use that seed phrase in the form of a wallet file, or if you'd like to recover an existing wallet via its seed phrase, use either or both of the following commands: ```shell ardrive generate-seedphrase \"this is an example twelve word seed phrase that you could use\" ardrive generate-wallet -s \"this is an example twelve word seed phrase that you could use\" > /path/to/wallet/file.json ``` Public attributes of Arweave wallets can be retrieved via their 43-character Arweave wallet address. You can retrieve the wallet address associated with [your wallet file or 12-word seed phrase][kb-wallets] (e.g. wallets generated by [ArConnect][arconnect]) like so: ```shell ardrive get-address -w /path/to/wallet/file.json ardrive get-address -s \"this is an example twelve word seed phrase that you could use\" HTTn8F92tR32N8wuo-NIDkjmqPknrbl10JWo5MZ9x2k ``` You'll need AR in your wallet for any write operations you perform in ArDrive. You can always check your wallet balance (in both AR and Winston units) by performing: ```shell ardrive get-balance -w /path/to/wallet/file.json ardrive get-balance -a \"HTTn8F92tR32N8wuo-NIDkjmqPknrbl10JWo5MZ9x2k\" 1500000000000 Winston 1.5 AR ``` If, at any time, you need to send AR out of your wallet to another wallet address, you may perform: ```shell ardrive send-ar -w /path/to/wallet/file.json --dest-address \"HTTn8F92tR32N8wuo-NIDkjmqPknrbl10JWo5MZ9x2k\" --ar-amount 2.12345 ```","estimatedWords":6946,"lastModified":"2025-10-20T12:13:33.042Z","breadcrumbs":["sdks","(clis)","llm.txt"],"siteKey":"ario","siteName":"AR-IO Network","depth":4,"crawledAt":"2025-10-20T12:13:33.042Z"},{"url":"https://docs.ar.io/sdks/turbo-sdk/turbounauthenticatedclient","title":"TurboUnauthenticatedClient","content":"Turbo SDKAPIsTurboUnauthenticatedClientCopy MarkdownOpengetSupportedCurrencies() Returns the list of currencies supported by the Turbo Payment Service for topping up a user balance of AR Credits (measured in Winston Credits, or winc). getSupportedCountries() Returns the list of countries supported by the Turbo Payment Service's top up workflow. getFiatToAR() Returns the current raw fiat to AR conversion rate for a specific currency as reported by third-party pricing oracles. getFiatRates() Returns the current fiat rates for 1 GiB of data for supported currencies, including all top-up adjustments and fees. getWincForFiat() Returns the current amount of Winston Credits including all adjustments for the provided fiat currency. const { winc, actualPaymentAmount, quotedPaymentAmount, adjustments } = await turbo.getWincForFiat({ amount: USD(100), }); getWincForToken() Returns the current amount of Winston Credits including all adjustments for the provided token amount. const { winc, actualTokenAmount, equivalentWincTokenAmount } = await turbo.getWincForToken({ tokenAmount: WinstonToTokenAmount(100_000_000), }); getFiatEstimateForBytes() Get the current price from the Turbo Payment Service, denominated in the specified fiat currency, for uploading a specified number of bytes to Turbo. const { amount } = await turbo.getFiatEstimateForBytes({ byteCount: 1024 * 1024 * 1024, currency: 'usd',","estimatedWords":181,"lastModified":"2025-10-20T12:13:34.318Z","breadcrumbs":["sdks","turbo sdk","turbounauthenticatedclient"],"siteKey":"ario","siteName":"AR-IO Network","depth":4,"crawledAt":"2025-10-20T12:13:34.318Z"},{"url":"https://docs.ar.io/sdks/ar-io-sdk/static-methods","title":"Static Methods","content":"AR.IO SDKANT ContractsStatic MethodsCopy MarkdownOpenANT.fork() Forks an existing ANT process to create a new one with the same state but potentially a different module. This is used for upgrading ANTs to new versions. }, }); console.log(`Forked ANT to new process: ${newProcessId}`); ANT.upgrade() Static method to upgrade an ANT by forking it to the latest version and reassigning names.","estimatedWords":58,"lastModified":"2025-10-20T12:13:35.015Z","breadcrumbs":["sdks","ar io sdk","static methods"],"siteKey":"ario","siteName":"AR-IO Network","depth":4,"crawledAt":"2025-10-20T12:13:35.015Z"},{"url":"https://docs.ar.io/sdks/ar-io-sdk/networks","title":"Networks","content":"AR.IO SDKARIO ContractNetworksCopy MarkdownOpenThe SDK provides the following process IDs for the mainnet and testnet environments: ARIO_MAINNET_PROCESS_ID - Mainnet ARIO process ID (production) ARIO_TESTNET_PROCESS_ID - Testnet ARIO process ID (testing and development) ARIO_DEVNET_PROCESS_ID - Devnet ARIO process ID (development) As of v3.8.1 the SDK defaults all API interactions to mainnet. To use the testnet or devnet provide the appropriate ARIO_TESTNET_PROCESS_ID or ARIO_DEVNET_PROCESS_ID when initializing the client. Mainnet As of v3.8.1 the SDK defaults all API interactions to mainnet. To use the testnet or devnet provide the appropriate ARIO_TESTNET_PROCESS_ID or ARIO_DEVNET_PROCESS_ID when initializing the client. import { ARIO } from '@ar.io/sdk';","estimatedWords":100,"lastModified":"2025-10-20T12:13:36.051Z","breadcrumbs":["sdks","ar io sdk","networks"],"siteKey":"ario","siteName":"AR-IO Network","depth":4,"crawledAt":"2025-10-20T12:13:36.051Z"},{"url":"https://docs.ar.io/sdks/ar-io-sdk/vaults","title":"Vaults","content":"AR.IO SDKARIO ContractVaultsCopy MarkdownOpengetVault() Retrieves the locked-balance user vault of the ARIO process by the specified wallet address and vault ID. Output: { \"balance\": 1000000, \"startTimestamp\": 123, \"endTimestamp\": 4567 } getVaults() Retrieves all locked-balance user vaults of the ARIO process, paginated and sorted by the specified criteria. The cursor used for pagination is the last wallet address from the previous request. Output: { \"items\": [ { \"address\": \"QGWqtJdLLgm2ehFWiiPzMaoFLD50CnGuzZIPEdoDRGQ\", \"vaultId\": \"vaultIdOne\", \"balance\": 1000000, \"startTimestamp\": 123, \"endTimestamp\": 4567 }, { \"address\": \"QGWqtJdLLgm2ehFWiiPzMaoFLD50CnGuzZIPEdoDRGQ\", \"vaultId\": \"vaultIdTwo\", \"balance\": 1000000, \"startTimestamp\": 123, \"endTimestamp\": 4567 }","estimatedWords":89,"lastModified":"2025-10-20T12:13:37.193Z","breadcrumbs":["sdks","ar io sdk","vaults"],"siteKey":"ario","siteName":"AR-IO Network","depth":4,"crawledAt":"2025-10-20T12:13:37.193Z"},{"url":"https://docs.ar.io/sdks/ar-io-sdk/gateways","title":"Gateways","content":"AR.IO SDKARIO ContractGatewaysCopy MarkdownOpengetGateway() Retrieves a gateway's info by its staking wallet address. Output: { \"observerAddress\": \"IPdwa3Mb_9pDD8c2IaJx6aad51Ss-_TfStVwBuhtXMs\", \"operatorStake\": 250000000000, \"settings\": { \"fqdn\": \"ar-io.dev\", \"label\": \"AR.IO Test\", \"note\": \"Test Gateway operated by PDS for the AR.IO ecosystem.\", \"port\": 443, \"properties\": \"raJgvbFU-YAnku-WsupIdbTsqqGLQiYpGzoqk9SCVgY\", \"protocol\": \"https\" }, \"startTimestamp\": 1720720620813, \"stats\": { \"failedConsecutiveEpochs\": 0, \"passedEpochCount\": 30, \"submittedEpochCount\": 30, \"totalEpochCount\": 31, \"totalEpochsPrescribedCount\": 31 }, \"status\": \"joined\", \"vaults\": {}, \"weights\": { \"compositeWeight\": 0.97688888893556, \"gatewayPerformanceRatio\": 1, \"tenureWeight\": 0.19444444444444, \"observerRewardRatioWeight\": 1, \"normalizedCompositeWeight\": 0.19247316211083, \"stakeWeight\": 5.02400000024 } } getGateways() Retrieves registered gateways of the ARIO process, using pagination and sorting by the specified criteria. The cursor used for pagination is the last gateway address from the previous request. Available sortBy options are any of the keys on the gateway object, e.g. operatorStake, start, status, settings.fqdn, settings.label, settings.note, settings.port, settings.protocol, stats.failedConsecutiveEpochs, stats.passedConsecutiveEpochs, etc. Output: { \"items\": [ { \"gatewayAddress\": \"QGWqtJdLLgm2ehFWiiPzMaoFLD50CnGuzZIPEdoDRGQ\", \"observerAddress\": \"IPdwa3Mb_9pDD8c2IaJx6aad51Ss-_TfStVwBuhtXMs\", \"operatorStake\": 250000000000, \"settings\": { \"fqdn\": \"ar-io.dev\", \"label\": \"AR.IO Test\", \"note\": \"Test Gateway operated by PDS for the AR.IO ecosystem.\", \"port\": 443, \"properties\": \"raJgvbFU-YAnku-WsupIdbTsqqGLQiYpGzoqk9SCVgY\", \"protocol\": \"https\" }, \"startTimestamp\": 1720720620813, \"stats\": { \"failedConsecutiveEpochs\": 0, \"passedEpochCount\": 30, \"submittedEpochCount\": 30, \"totalEpochCount\": 31, \"totalEpochsPrescribedCount\": 31 }, \"status\": \"joined\", \"vaults\": {}, \"weights\": { \"compositeWeight\": 0.97688888893556, \"gatewayPerformanceRatio\": 1, \"tenureWeight\": 0.19444444444444, \"observerRewardRatioWeight\": 1, \"normalizedCompositeWeight\": 0.19247316211083, \"stakeWeight\": 5.02400000024 } } ], \"hasMore\": true, \"nextCursor\": \"-4xgjroXENKYhTWqrBo57HQwvDL51mMdfsdsxJy6Y2Z_sA\", \"totalItems\": 316, \"sortBy\": \"operatorStake\", \"sortOrder\": \"desc\" } getGatewayDelegates() Retrieves all delegates for a specific gateway, paginated and sorted by the specified criteria. The cursor used for pagination is the last delegate address from the previous request. Output: { \"nextCursor\": \"ScEtph9-vfY7lgqlUWwUwOmm99ySeZGQhOX0MFAyFEs\", \"limit\": 3, \"sortBy\": \"startTimestamp\", \"totalItems\": 32, \"sortOrder\": \"desc\", \"hasMore\": true, \"items\": [ { \"delegatedStake\": 600000000, \"address\": \"qD5VLaMYyIHlT6vH59TgYIs6g3EFlVjlPqljo6kqVxk\", \"startTimestamp\": 1732716956301 }, { \"delegatedStake\": 508999038, \"address\": \"KG8TlcWk-8pvroCjiLD2J5zkG9rqC6yYaBuZNqHEyY4\", \"startTimestamp\": 1731828123742 }, { \"delegatedStake\": 510926479, \"address\": \"ScEtph9-vfY7lgqlUWwUwOmm99ySeZGQhOX0MFAyFEs\", \"startTimestamp\": 1731689356040 } ] } joinNetwork() Joins a gateway to the ar.io network via its associated wallet. Note: Requires signer to be provided on ARIO.init to sign the transaction. const { id: txId } = await ario.joinNetwork( { qty: new ARIOToken(10_000).toMARIO(),","estimatedWords":321,"lastModified":"2025-10-20T12:13:39.083Z","breadcrumbs":["sdks","ar io sdk","gateways"],"siteKey":"ario","siteName":"AR-IO Network","depth":4,"crawledAt":"2025-10-20T12:13:39.083Z"},{"url":"https://docs.ar.io/sdks/ar-io-sdk/arweave-name-system-arns","title":"Arweave Name System (ArNS)","content":"AR.IO SDKARIO ContractArweave Name System (ArNS)Copy MarkdownOpenresolveArNSName() Resolves an ArNS name to the underlying data id stored on the names corresponding ANT id. Resolving a base name Output: { \"processId\": \"bh9l1cy0aksiL_x9M359faGzM_yjralacHIUo8_nQXM\", \"txId\": \"kvhEUsIY5bXe0Wu2-YUFz20O078uYFzmQIO-7brv8qw\", \"type\": \"lease\", \"recordIndex\": 0, \"undernameLimit\": 100, \"owner\": \"t4Xr0_J4Iurt7caNST02cMotaz2FIbWQ4Kbj616RHl3\", \"name\": \"ardrive\" } Resolving an undername Output: { \"processId\": \"bh9l1cy0aksiL_x9M359faGzM_yjralacHIUo8_nQXM\", \"txId\": \"kvhEUsIY5bXe0Wu2-YUFz20O078uYFzmQIO-7brv8qw\", \"type\": \"lease\", \"recordIndex\": 1, \"undernameLimit\": 100, \"owner\": \"t4Xr0_J4Iurt7caNST02cMotaz2FIbWQ4Kbj616RHl3\", \"name\": \"ardrive\" } buyRecord() Purchases a new ArNS record with the specified name, type, processId, and duration. Note: Requires signer to be provided on ARIO.init to sign the transaction. Arguments: name - required: the name of the ArNS record to purchase type - required: the type of ArNS record to purchase processId - optional: the process id of an existing ANT process. If not provided, a new ANT process using the provided signer will be spawned, and the ArNS record will be assigned to that process. years - optional: the duration of the ArNS record in years. If not provided and type is lease, the record will be leased for 1 year. If not provided and type is permabuy, the record will be permanently registered. referrer - optional: track purchase referrals for analytics (e.g. my-app.com) if (step === 'spawning-ant') { console.log('Spawning ant:', event); } if (step === 'registering-ant') { console.log('Registering ant:', event); } if (step === 'verifying-state') { console.log('Verifying state:', event); } if (step === 'buying-name') { console.log('Buying name:', event); } }, }, ); upgradeRecord() Upgrades an existing leased ArNS record to a permanent ownership. The record must be currently owned by the caller and be of type \"lease\". Note: Requires signer to be provided on ARIO.init to sign the transaction. getArNSRecord() Retrieves the record info of the specified ArNS name. Output: { \"processId\": \"bh9l1cy0aksiL_x9M359faGzM_yjralacHIUo8_nQXM\", \"endTimestamp\": 1752256702026, \"startTimestamp\": 1720720819969, \"type\": \"lease\", \"undernameLimit\": 100 } getArNSRecords() Retrieves all registered ArNS records of the ARIO process, paginated and sorted by the specified criteria. The cursor used for pagination is the last ArNS name from the previous request.","estimatedWords":326,"lastModified":"2025-10-20T12:13:40.250Z","breadcrumbs":["sdks","ar io sdk","arweave name system arns"],"siteKey":"ario","siteName":"AR-IO Network","depth":4,"crawledAt":"2025-10-20T12:13:40.250Z"},{"url":"https://docs.ar.io/sdks/ar-io-sdk/epochs","title":"Epochs","content":"AR.IO SDKARIO ContractEpochsCopy MarkdownOpengetCurrentEpoch() Returns the current epoch data. Output: { \"epochIndex\": 0, \"startTimestamp\": 1720720621424, \"endTimestamp\": 1752256702026, \"startHeight\": 1350700, \"distributionTimestamp\": 1711122739, \"observations\": { \"failureSummaries\": { \"-Tk2DDk8k4zkwtppp_XFKKI5oUgh6IEHygAoN7mD-w8\": [ \"Ie2wEEUDKoU26c7IuckHNn3vMFdNQnMvfPBrFzAb3NA\" ] }, \"reports\": { \"IPdwa3Mb_9pDD8c2IaJx6aad51Ss-_TfStVwBuhtXMs\": \"B6UUjKWjjEWDBvDSMXWNmymfwvgR9EN27z5FTkEVlX4\" } }, \"prescribedNames\": [\"ardrive\", \"ar-io\", \"arweave\", \"fwd\", \"ao\"], \"prescribedObservers\": [ { \"gatewayAddress\": \"2Fk8lCmDegPg6jjprl57-UCpKmNgYiKwyhkU4vMNDnE\", \"observerAddress\": \"2Fk8lCmDegPg6jjprl57-UCpKmNgYiKwyhkU4vMNDnE\", \"stake\": 10000000000, \"start\": 1292450, \"stakeWeight\": 1, \"tenureWeight\": 0.4494598765432099, \"gatewayPerformanceRatio\": 1, \"observerRewardRatioWeight\": 1, \"compositeWeight\": 0.4494598765432099, \"normalizedCompositeWeight\": 0.002057032496835938 } ], \"distributions\": { \"distributedTimestamp\": 1711122739, \"totalEligibleRewards\": 100000000, \"rewards\": { \"IPdwa3Mb_9pDD8c2IaJx6aad51Ss-_TfStVwBuhtXMs\": 100000000 } } } getEpoch() Returns the epoch data for the specified block height. If no epoch index is provided, the current epoch is used. Output: { \"epochIndex\": 0, \"startTimestamp\": 1720720620813, \"endTimestamp\": 1752256702026, \"startHeight\": 1350700, \"distributionTimestamp\": 1752256702026, \"observations\": { \"failureSummaries\": { \"-Tk2DDk8k4zkwtppp_XFKKI5oUgh6IEHygAoN7mD-w8\": [ \"Ie2wEEUDKoU26c7IuckHNn3vMFdNQnMvfPBrFzAb3NA\" ] }, \"reports\": { \"IPdwa3Mb_9pDD8c2IaJx6aad51Ss-_TfStVwBuhtXMs\": \"B6UUjKWjjEWDBvDSMXWNmymfwvgR9EN27z5FTkEVlX4\" } }, \"prescribedNames\": [\"ardrive\", \"ar-io\", \"arweave\", \"fwd\", \"ao\"], \"prescribedObservers\": [ { \"gatewayAddress\": \"2Fk8lCmDegPg6jjprl57-UCpKmNgYiKwyhkU4vMNDnE\", \"observerAddress\": \"2Fk8lCmDegPg6jjprl57-UCpKmNgYiKwyhkU4vMNDnE\", \"stake\": 10000000000,","estimatedWords":143,"lastModified":"2025-10-20T12:13:41.163Z","breadcrumbs":["sdks","ar io sdk","epochs"],"siteKey":"ario","siteName":"AR-IO Network","depth":4,"crawledAt":"2025-10-20T12:13:41.163Z"},{"url":"https://docs.ar.io/sdks/ar-io-sdk/primary-names","title":"Primary Names","content":"AR.IO SDKARIO ContractPrimary NamesCopy MarkdownOpengetPrimaryNames() Retrieves all primary names paginated and sorted by the specified criteria. The cursor used for pagination is the last name from the previous request. Output: { \"sortOrder\": \"desc\", \"hasMore\": true, \"totalItems\": 100, \"limit\": 1, \"sortBy\": \"startTimestamp\", \"cursor\": \"arns\", \"items\": [ { \"owner\": \"HwFceQaMQnOBgKDpnFqCqgwKwEU5LBme1oXRuQOWSRA\", \"startTimestamp\": 1719356032297, \"name\": \"arns\" } ] } getPrimaryName() Retrieves the primary name for a given name or address.","estimatedWords":66,"lastModified":"2025-10-20T12:13:42.558Z","breadcrumbs":["sdks","ar io sdk","primary names"],"siteKey":"ario","siteName":"AR-IO Network","depth":4,"crawledAt":"2025-10-20T12:13:42.558Z"},{"url":"https://docs.ar.io/sdks/ar-io-sdk/configuration","title":"Configuration","content":"AR.IO SDKARIO ContractConfigurationCopy MarkdownOpenThe ARIO client class exposes APIs relevant to the ar.io process. It can be configured to use any AO Process ID that adheres to the [ARIO Network Spec]. By default, it will use the current [ARIO Testnet Process]. Refer to [AO Connect] for more information on how to configure an ARIO process to use specific AO infrastructure. import { ARIO , AOProcess } from '@ar.io/sdk'; import { connect } from '@permaweb/aoconnect';","estimatedWords":74,"lastModified":"2025-10-20T12:13:43.620Z","breadcrumbs":["sdks","ar io sdk","configuration"],"siteKey":"ario","siteName":"AR-IO Network","depth":4,"crawledAt":"2025-10-20T12:13:43.620Z"},{"url":"https://docs.ar.io/sdks/wayfinder/creating-a-changeset","title":"Creating a Changeset","content":"Wayfinder SDK'sReleasesCreating a ChangesetCopy MarkdownOpenTo create a changeset when making changes: npx changeset This will guide you through the process of documenting your changes and selecting which packages are affected. Changesets will be used during the release process to update package versions and generate changelogs.How is this guide?GoodBadContributingDecentralized access to Arweave data with built-in verification and gateway routingAutomated ReleasesDecentralized access to Arweave data with built-in verification and gateway routing","estimatedWords":69,"lastModified":"2025-10-20T12:13:46.976Z","breadcrumbs":["sdks","wayfinder","creating a changeset"],"siteKey":"ario","siteName":"AR-IO Network","depth":4,"crawledAt":"2025-10-20T12:13:46.976Z"},{"url":"https://docs.ar.io/sdks/wayfinder/manual-release-process","title":"Manual Release Process","content":"Wayfinder SDK'sReleasesManual Release ProcessCopy MarkdownOpenIf you need to release manually, follow these steps: Alpha Releases To release a new alpha version: npx changeset version Review the version changes and changelogs Commit the changes: git add . git commit -m \"chore(release): version packages\" Publish the packages to npm: npm run build npx changeset publish Push the changes and tags: git push origin main --follow-tags Prerelease Mode For prerelease versions (e.g., beta, alpha): Enter prerelease mode specifying the tag: npx changeset pre enter beta Create changesets as normal: npx changeset Version and publish as normal: npx changeset version git add . git commit -m \"chore(release): prerelease version packages\" npm run build npx changeset publish git push origin main --follow-tags Exit prerelease mode when ready for a stable release: npx changeset pre exit Follow the normal release process for the stable version. How is this guide?GoodBadAutomated ReleasesDecentralized access to Arweave data with built-in verification and gateway routingTestingDecentralized access to Arweave data with built-in verification and gateway routing","estimatedWords":164,"lastModified":"2025-10-20T12:13:47.957Z","breadcrumbs":["sdks","wayfinder","manual release process"],"siteKey":"ario","siteName":"AR-IO Network","depth":4,"crawledAt":"2025-10-20T12:13:47.957Z"},{"url":"https://docs.ar.io/build/advanced/arfs/entity-types","title":"Entity Types","content":"AdvancedArFS ProtocolEntity TypesCopy MarkdownOpenOverview Arweave transactions provide for a separation between data and metadata about that data via the use of headers. Key-value tags in the headers provide for expressive description about the data as well as searchability via gateway GraphQL APIs. ArFS adds an additional layer of separation between data and metadata by using separate transactions for ArFS metadata and, where applicable, ArFS file data. But it also makes use of tags and data separation within an ArFS metadata transaction by including data critical to tracking drive composition in the tags space of ArFS metadata transactions and having most of the other metadata encoded as JSON in the data body of the metadata transaction. In the case of private entities, JSON data and file data payloads are always encrypted according to the protocol processes defined below. Drive entities require a single metadata transaction, with standard Drive tags and encoded JSON with secondary metadata. Folder entities require a single metadata transaction, with standard Folder tags and an encoded JSON with secondary metadata. File entities require a metadata transaction, with standard File tags and an encoded Data JSON with secondary metadata relating to the file. File entities also require a second data transaction, which includes a limited set of File tags and the actual file data itself. Snapshot entities require a single transaction, which contains a Data JSON with all of the Drive's rolled up ArFS metadata and standard Snapshot GQL tags that identify the Snapshot. ArFS v0.14 introduces the isHidden property. isHidden is a boolean (true/false) that tells clients if they should display the file or folder. Hidden files still exist and will be included in snapshots, but should not be rendered by clients. If isHidden is not present, its value should be assumed false. ArFS v0.15 introduces the Signature-Type metadata property on Drive entities, and a new entity type DriveSignature. Drive A drive is the highest level logical grouping of folders and files. All folders and files must be part of a drive, and reference the Drive ID of that drive. When creating a Drive, a corresponding \"root\" folder must be created as well. This separation of drive and folder entity enables features such as folder view queries, renaming, and linking. ArFS: \"0.15\", Cipher?: \"AES256-GCM\", Cipher-IV?: \"\", Content-Type: \"\", Drive-Id: \"\", Drive-Privacy: \"\", Drive-Auth-Mode?: \"password\", Entity-Type: \"drive\", Signature-Type?: \"1\", Unix-Time: \"\" Metadata JSON { \"name\": \"\", \"rootFolderId\": \"\", \"isHidden\": false } Drive-Signature ArFS versions prior to v0.15 applied encryption to drive contents with a signing scheme that, while secure, is now deprecated in modern Arweave software wallets. ArFS v0.15 introduces an updated signing scheme compatible with these wallets and as well as \"Drive Signatures\", a new entity type to help bridge the signature derivation schemes across ArFS versions. A drive signature uses the v0.15 encryption scheme to encrypt and store the pre-v0.15 wallet signature for a private drive that is necessary for deriving the \"drive key\" for that drive. This allows for continued access of historical drive contents into the future. ArFS: \"0.15\", Entity-Type: \"drive-signature\", Signature-Format: \"1\", Cipher?: \"AES256-GCM\", Cipher-IV: \"\" {data: } The encrypted \"type 1\" signature for the drive must be provided in the data field of the transaction creating the drive-signature entity. Folder A folder is a logical grouping of other folders and files. Folder entity metadata transactions without a parent folder id are considered the Drive Root Folder of their corresponding Drives. All other Folder entities must have a parent folder id. Since folders do not have underlying data, there is no Folder data transaction required. ArFS: \"0.15\", Cipher?: \"AES256-GCM\", Cipher-IV?: \"\", Content-Type: \"\", Drive-Id: \"\", Entity-Type: \"folder\", Folder-Id: \"\", Parent-Folder-Id?: \"\", Unix-Time: \"\" Metadata JSON { \"name\": \"\", \"isHidden\": false } File A File contains uploaded data, like a photo, document, or movie. In the Arweave File System, a single file is broken into 2 parts - its metadata and its data. A File entity metadata transaction does not include the actual File data. Instead, the File data must be uploaded as a separate transaction, called the File Data Transaction. The File JSON metadata transaction contains a reference to the File Data Transaction ID so that it can retrieve the actual data. This separation allows for file metadata to be updated without requiring the file itself to be reuploaded. It also ensures that private files can have their JSON Metadata Transaction encrypted as well, ensuring that no one without authorization can see either the file or its metadata. ArFS: \"0.15\", Cipher?: \"AES256-GCM\", Cipher-IV?: \"\", Content-Type: \"\", Drive-Id: \"\", Entity-Type: \"file\", File-Id: \"\", Parent-Folder-Id: \"\", Unix-Time: \"\" Metadata JSON { \"name\": \"\", \"size\": , \"lastModifiedDate\": , \"dataTxId\": \"\", \"dataContentType\": \"\", \"isHidden\": false, \"pinnedDataOwner\": \"\", # Optional } Pinning Files Since the version v0.13, ArFS supports Pins. Pins are files whose data may be any transaction uploaded to Arweave, that may or may not be owned by the wallet that created the pin. When a new File Pin is created, the only created transaction is the Metadata Transaction. The dataTxId field will point it to any transaction in Arweave, and the optional pinnedDataOwner field is gonna hold the address of the wallet that owns the original copy of the data transaction. File Data Transaction Example The File Data Transaction contains limited information about the file, such as the information required to decrypt it, or the Content-Type (mime-type) needed to view in the browser. Cipher?: \"AES256-GCM\", Cipher-IV?: \"\", Content-Type: \"\", { File Data - Encrypted if private } File Metadata Transaction Example The File Metadata Transaction contains the GQL Tags necessary to identify the file within a drive and folder. Its data contains the JSON metadata for the file. This includes the file name, size, last modified date, data transaction id, and data content type. ArFS: \"0.15\", Cipher?: \"AES256-GCM\", Cipher-IV?: \"\", Content-Type: \"\", Drive-Id: \"\", Entity-Type: \"file\", File-Id: \"\", Parent-Folder-Id: \"\", Unix-Time: \"\", { File JSON Metadata - Encrypted if private } Snapshot ArFS applications generate the latest state of a drive by querying for all ArFS transactions made relating to a user's particular Drive-Id. This includes both paged queries for indexed ArFS data via GQL, as well as the ArFS JSON metadata entries for each ArFS transaction. For small drives (less than 1000 files), a few thousand requests for very small volumes of data can be achieved relatively quickly and reliably. For larger drives, however, this results in long sync times to pull every piece of ArFS metadata when the local database cache is empty. This can also potentially trigger rate-limiting related ArWeave Gateway delays. Once a drive state has been completely, and accurately generated, in can be rolled up into a single snapshot and uploaded as an Arweave transaction. ArFS clients can use GQL to find and retrieve this snapshot in order to rapidly reconstitute the total state of the drive, or a large portion of it. They can then query individual transactions performed after the snapshot. This optional method offers convenience and resource efficiency when building the drive state, at the cost of paying for uploading the snapshot data. Using this method means a client will only have to iterate through a few snapshots instead of every transaction performed on the drive. Snapshot Entity Tags Snapshot entities require the following tags. These are queried by ArFS clients to find drive snapshots, organize them together with any other transactions not included within them, and build the latest state of the drive. ArFS: \"0.15\", Drive-Id: \"\", Entity-Type: \"snapshot\", Snapshot-Id: \"\", Content-Type: \"\", Block-Start: \"\", Block-End: \"\", Data-Start: \"\" Snapshot Entity Data A JSON data object must also be uploaded with every ArFS Snapshot entity. This data contains all ArFS Drive, Folder, and File metadata changes within the associated drive, as well as any previous Snapshots. The Snapshot Data contains an array txSnapshots. Each item includes both the GQL and ArFS metadata details of each transaction made for the associated drive, within the snapshot's start and end period. A tsSnapshot contains a gqlNode object which uses the same GQL tags interface returned by the Arweave Gateway. It includes all of the important block, owner, tags, and bundledIn information needed by ArFS clients. It also contains a dataJson object which stores the correlated Data JSON for that ArFS entity. For private drives, the dataJson object contains the JSON-string-escaped encrypted text of the associated file or folder. This encrypted text uses the file's existing Cipher and Cipher-IV. This ensures clients can decrypt this information quickly using the existing ArFS privacy protocols. { \"txSnapshots\": [ { \"gqlNode\": { \"id\": \"bWCvIc3cOzwVgquD349HUVsn5Dd1_GIri8Dglok41Vg\", \"owner\": { \"address\": \"hlWRbyJ6WUoErm3b0wqVgd1l3LTgaQeLBhB36v2HxgY\" }, \"bundledIn\": { \"id\": \"39n5evzP1Ip9MhGytuFm7F3TDaozwHuVUbS55My-MBk\" }, \"block\": { \"height\": 1062005, \"timestamp\": 1669053791 }, \"tags\": [ { \"name\": \"Content-Type\", \"value\": \"application/json\" }, { \"name\": \"ArFS\", \"value\": \"0.11\" }, { \"name\": \"Entity-Type\", \"value\": \"drive\" }, { \"name\": \"Drive-Id\", \"value\": \"f27abc4b-ed6f-4108-a9f5-e545fc4ff55b\" }, { \"name\": \"Drive-Privacy\", \"value\": \"public\" }, { \"name\": \"App-Name\", \"value\": \"ArDrive-App\" }, { \"name\": \"App-Platform\", \"value\": \"Web\" }, { \"name\": \"App-Version\", \"value\": \"1.39.0\" }, { \"name\": \"Unix-Time\", \"value\": \"1669053323\" } ] }, \"dataJson\": \"{\\\"name\\\":\\\"november\\\",\\\"rootFolderId\\\":\\\"71dfc1cb-5368-4323-972a-e9dd0b1c63a0\\\", \\\"isHidden\\\":false}\" } ] } Schema Diagrams The following diagrams show complete examples of Drive, Folder, and File entity Schemas. Public Drive Private Drive Next Steps Now that you understand the different ArFS entity types, explore how they work together: Data Model - Learn how entities relate to each other Privacy & Encryption - Understand how private entities work Creating Drives - Start building with ArFS How is this guide?GoodBadArFS ProtocolA decentralized file system on Arweave for structured data storage and retrievalData ModelUnderstanding how ArFS organizes data hierarchically using entity relationships","estimatedWords":1587,"lastModified":"2025-10-20T12:13:49.154Z","breadcrumbs":["build","advanced","arfs","entity types"],"siteKey":"ario","siteName":"AR-IO Network","depth":4,"crawledAt":"2025-10-20T12:13:49.154Z"},{"url":"https://docs.ar.io/build/advanced/arfs/data-model","title":"Data Model","content":"AdvancedArFS ProtocolData ModelCopy MarkdownOpenBecause of Arweave's permanent and immutable nature, traditional file structure operations such as renaming and moving files or folders cannot be accomplished by simply updating on-chain data. ArFS works around this by defining an append-only transaction data model based on the metadata tags found in the Arweave Transaction Headers. This model uses a bottom-up reference method, which avoids race conditions in file system updates. Each file contains metadata that refers to the parent folder, and each folder contains metadata that refers to its parent drive. A top-down data model would require the parent model (i.e. a folder) to store references to its children. These defined entities allow the state of the drive to be constructed by a client to look and feel like a file system: Drive Entities contain folders and files Folder Entities contain other folders or files File Entities contain both the file data and metadata Snapshot entities contain a state rollups of all entities' (such as drive, folder, file and snapshot) metadata within a drive Entity Relationships The following diagram shows the high level relationships between drive, folder, and file entities, and their associated data. More detailed information about each Entity Type can be found in the ArFS specification documentation. As you can see, each file and folder contains metadata which points to both the parent folder and the parent drive. The drive entity contains metadata about itself, but not the child contents. So clients must build drive states from the lowest level and work their way up. Metadata Format Metadata stored in any Arweave transaction tag will be defined in the following manner: { \"name\": \"Example-Tag\", \"value\": \"example-data\" } Metadata stored in the Transaction Data Payload will follow JSON formatting like below: { \"exampleField\": \"exampleData\" } Fields with a ? suffix are optional. { \"name\": \"My Project\", \"description\": \"This is a sample project.\", \"version?\": \"1.0.0\", \"author?\": \"John Doe\" } Enumerated field values (those which must adhere to certain values) are defined in the format \"value 1 | value 2\". All UUIDs used for Entity-Ids are based on the Universally Unique Identifier standard. There are no requirements to list ArFS tags in any specific order. Building Drive State To construct the current state of a drive, clients must: Query for all entities associated with a specific Drive-Id Sort by block height to establish chronological order Process entities bottom-up starting with files and folders Build the hierarchy by following parent-child relationships Handle conflicts by using the most recent entity version Example Drive State Construction Entity Lifecycle Each ArFS entity follows a specific lifecycle pattern: Creation Generate unique UUID for entity Create metadata transaction with required tags For files: create separate data transaction Upload to Arweave network Updates Create new entity with same ID Update metadata as needed Upload new transaction Client processes both versions and uses latest Deletion Mark entity as hidden (isHidden: true) Upload new transaction Entity remains in history but hidden from UI Data Integrity ArFS ensures data integrity through: Immutable transactions - Once uploaded, data cannot be modified Cryptographic signatures - All transactions are signed by the owner Version tracking - Multiple versions of entities can exist Conflict resolution - Clients use block height and timestamps to resolve conflicts Performance Considerations For large drives, consider these optimization strategies: Use snapshots for quick state reconstruction Implement caching for frequently accessed data Batch operations when possible Query by date ranges to limit data transfer Next Steps Now that you understand the ArFS data model, learn how to work with it: Privacy & Encryption - Secure your data with private drives Creating Drives - Start building with ArFS Reading Data - Query and retrieve your data How is this guide?GoodBadEntity TypesUnderstanding ArFS entity types and their structurePrivacy & EncryptionSecure your data with ArFS private drives and encryption","estimatedWords":632,"lastModified":"2025-10-20T12:13:50.424Z","breadcrumbs":["build","advanced","arfs","data model"],"siteKey":"ario","siteName":"AR-IO Network","depth":4,"crawledAt":"2025-10-20T12:13:50.424Z"},{"url":"https://docs.ar.io/build/advanced/arfs/privacy","title":"Privacy  Encryption","content":"AdvancedArFS ProtocolPrivacy & EncryptionCopy MarkdownOpenThe Arweave blockweave is inherently public. But with apps that use ArFS, like ArDrive, your private data never leaves your computer without using military grade (and quantum resistant) encryption. This privacy layer is applied at the Drive level, and users determine whether a Drive is public or private when they first create it. Private drives must follow the ArFS privacy model. With ArDrive specifically, every file within a Private Drive is symmetrically encrypted using AES-256-GCM (for small files and metadata transactions) or AES-256-CTR (for large files, over 100MiB). Every Private drive has a master \"Drive Key\" which uses a combination of the user's Arweave wallet signature, a user defined drive password, and a unique drive identifier (uuidv4). Each file has its own \"File Key\" derived from the \"Drive Key\". This allows for single files to be shared without exposing access to the other files within the Drive. Once a file is encrypted and stored on Arweave, it is locked forever and can only be decrypted using its file key. NOTE: Usable encryption standards are not limited to AES-256-GCM or AES-256-CTR. Any Encryption method may be used so long as it is clearly indicated in the cipher tag. Deriving Keys Private drives have a global drive key, D, and multiple file keys F, for encryption. This enables a drive to have as many uniquely encrypted files as needed. One key is used for all versions of a single file (since new file versions use the same File-Id) D is used for encrypting both Drive and Folder metadata, while F is used for encrypting File metadata and the actual stored data. Having these different keys, D and F, allows a user to share specific files without revealing the contents of their entire drive. D is derived using HKDF-SHA256 with an unsalted RSA-PSS signature of the drive's id and a user provided password. F is also derived using HKDF-SHA256 with the drive key and the file's id. Other wallets (like ArConnect) integrate with this Key Derivation protocol just exposing an API to collect a signature from a given Arweave Wallet in order to get the SHA-256 signature needed for the HKDF to derive the Drive Key. An example implementation, using Dart, is available here, with a Typescript implementation here. Private Drives Drives can store either public or private data. This is indicated by the Drive-Privacy tag in the Drive entity metadata. Drive-Privacy: \"\" If a Drive entity is private, an additional tag Drive-Auth-Mode must also be used to indicate how the Drive Key is derived. ArDrive clients currently leverage a secure password along with the Arweave Wallet private key signature to derive the global Drive Key. Drive-Auth-Mode?: 'password' On every encrypted Drive Entity, a Cipher tag must be specified, along with the public parameters for decrypting the data. This is done by specifying the parameter with a Cipher-* tag. eg. Cipher-IV. If the parameter is byte data, it must be encoded as Base64 in the tag. ArDrive clients currently leverage AES256-GCM for all symmetric encryption, which requires a Cipher Initialization Vector consisting of 12 random bytes. Cipher?: \"AES256-GCM\" Cipher-IV?: \"\" Additionally, all encrypted transactions must have the Content-Type tag application/octet-stream as opposed to application/json Private Drive Entities and their corresponding Root Folder Entities will both use these keys and ciphers generated to symmetrically encrypt the JSON files that are included in the transaction. This ensures that only the Drive Owner (and whomever the keys have been shared with) can open the drive, discover the root folder, and continue to load the rest of the children in the drive. Private Files When a file is uploaded to a private drive, it by default also becomes private and leverages the same drive keys used for its parent drive. Each unique file in a drive will get its own set of file keys based off of that file's unique FileId. If a single file gets a new version, its File-Id will be reused, effectively leveraging the same File Key for all versions in that file's history. These file keys can be shared by the drive's owner as needed. Private File entities have both its metadata and data transactions encrypted using the same File Key, ensuring all facets of the data is truly private. As such, both the file's metadata and data transactions must both have a unique Cipher-IV and Cipher tag: Cipher?: \"AES256-GCM\" Cipher-IV?: \"\" Just like drives, private files must have the Content-Type tag set as application/octet-stream in both its metadata and data transactions: Content-Type: \"application/octet-stream\" Encryption Process Here's how the encryption process works for private drives: Security Best Practices When working with private drives, follow these security guidelines: Password Management Use strong, unique passwords for each drive Consider using a password manager Never share passwords in plain text Key Storage Never store drive keys in plain text Use secure key derivation functions Implement proper key rotation if needed Access Control Share file keys only with authorized users Implement proper access logging Regularly audit drive access Data Handling Encrypt data before transmission Use secure communication channels Implement proper error handling Drive Signature (ArFS v0.15) ArFS v0.15 introduces a new Drive-Signature entity type to help bridge signature derivation schemes across ArFS versions. This is particularly important for maintaining access to private drives created with older wallet signing methods. The drive signature entity stores an encrypted version of the pre-v0.15 wallet signature that's necessary for deriving the drive key. This allows continued access to historical drive contents while using modern wallet signing APIs. Next Steps Ready to implement privacy in your ArFS applications? Creating Private Drives - Learn how to create secure drives Upgrading Private Drives - Update legacy drives to v0.15 Reading Data - Query and decrypt your private data How is this guide?GoodBadData ModelUnderstanding how ArFS organizes data hierarchically using entity relationshipsCreating DrivesLearn how to create ArFS drives, folders, and files","estimatedWords":980,"lastModified":"2025-10-20T12:13:51.417Z","breadcrumbs":["build","advanced","arfs","privacy"],"siteKey":"ario","siteName":"AR-IO Network","depth":4,"crawledAt":"2025-10-20T12:13:51.418Z"},{"url":"https://docs.ar.io/build/advanced/arfs/creating-drives","title":"Creating Drives","content":"AdvancedArFS ProtocolCreating DrivesCopy MarkdownOpenTo properly create a new drive, two new entities need to be created: a new Drive entity and a new Folder entity to serve as the root folder of that drive. New Drive Entity The user must specify a name of the drive which is stored within the Drive Entity's metadata JSON. ArDrive generates a new unique uuidv4 for the drive entity's Drive-Id. ArDrive also generates a new unique uuidv4 for the drive entity's rootFolderId, which will refer to the Folder-Id of the new folder entity that will be created. This rootFolderId is stored within the Drive Entity's metadata JSON. Drive Entity Metadata transactions must have Entity-Type: \"drive\". ArDrive will that the current local system time as seconds since Unix epoch for the Drive Entity's Unix-Time. The Drive Entity's Drive-Privacy must also be set to public or private in order for its subfolders and files to have the correct security settings. If the drive is private: Its Cipher tag must be filled out with the correct encryption algorithm (currently AES256-GCM). Its Cipher-IV tag must be filled out with the generated Initialization Vector for the private drive. The ArFS client must derive the Drive Key and encrypt the Drive Entity's metadata JSON using the assigned Cipher and Cipher-IV. New Root Folder Entity The name of the drive and folder entities must be the same. This name is stored within the Folder Entity's metadata JSON. The Folder Entity's Folder-Id must match the rootFolderId previously created for the Drive Entity. The Folder Entity's Drive-Id must match the Drive-Id previously created for the Drive Entity. The Folder Entity must not include a Parent-Folder-Id tag. This is how it is determined to be the root folder for a drive. Folder Entity metadata transactions must have Entity-Type: 'folder'. The client gets the user's local time for the Unix-Time tag, represented as seconds since Unix Epoch. Public folders must have the content type Content-Type: \"application/json\". If the folder is private Its Cipher tag must be filled out with the correct encryption algorithm (currently AES256-GCM). Its Cipher-IV tag must be filled out with the generated Initialization Vector for the private folder. Its content type must be Content-Type: \"application/octet-stream\". The ArFS client must encrypt the Drive Entity's metadata JSON using the assigned Cipher and Cipher-IV. Creating Files Files in ArFS require two separate transactions: File Metadata Transaction - Contains file information and references File Data Transaction - Contains the actual file data File Metadata Transaction ArFS: \"0.15\", Cipher?: \"AES256-GCM\", Cipher-IV?: \"\", Content-Type: \"\", Drive-Id: \"\", Entity-Type: \"file\", File-Id: \"\", Parent-Folder-Id: \"\", Unix-Time: \"\" Metadata JSON { \"name\": \"\", \"size\": , \"lastModifiedDate\": , \"dataTxId\": \"\", \"dataContentType\": \"\", \"isHidden\": false, \"pinnedDataOwner\": \"\" } File Data Transaction Cipher?: \"AES256-GCM\", Cipher-IV?: \"\", Content-Type: \"\", { File Data - Encrypted if private } Creating Folders Folders are simpler than files as they only require a metadata transaction: ArFS: \"0.15\", Cipher?: \"AES256-GCM\", Cipher-IV?: \"\", Content-Type: \"\", Drive-Id: \"\", Entity-Type: \"folder\", Folder-Id: \"\", Parent-Folder-Id?: \"\", Unix-Time: \"\" Metadata JSON { \"name\": \"\", \"isHidden\": false } Creating Snapshots Snapshots provide a way to quickly synchronize drive state by rolling up all metadata into a single transaction: ArFS: \"0.15\", Drive-Id: \"\", Entity-Type: \"snapshot\", Snapshot-Id: \"\", Content-Type: \"\", Block-Start: \"\", Block-End: \"\", Data-Start: \"\", Data-End: \"\", Unix-Time: \"\" Implementation Example Here's a practical example of creating a complete drive structure: Best Practices Naming Conventions Use descriptive names for drives, folders, and files Avoid special characters that might cause issues Keep names under 255 characters Use consistent casing Organization Create logical folder structures Use meaningful folder names Implement proper versioning Document your structure Performance Batch operations when possible Use efficient queries Implement caching Consider file sizes Security Use strong passwords for private drives Implement proper key management Follow encryption best practices Regular security audits Error Handling When creating ArFS entities, handle these common scenarios: Transaction Failures Implement retry logic for failed uploads Validate data before uploading Check transaction confirmation status Validation Errors Verify required tags are present Check data format compliance Validate UUID formats Network Issues Implement timeout handling Provide user feedback Graceful degradation Next Steps Now that you know how to create ArFS entities, learn how to work with them: Reading Data - Query and retrieve your ArFS data Privacy & Encryption - Secure your data with private drives Upgrading Private Drives - Update legacy drives to v0.15 How is this guide?GoodBadPrivacy & EncryptionSecure your data with ArFS private drives and encryptionReading DataLearn how to query and retrieve ArFS data from the Arweave network","estimatedWords":748,"lastModified":"2025-10-20T12:13:52.690Z","breadcrumbs":["build","advanced","arfs","creating drives"],"siteKey":"ario","siteName":"AR-IO Network","depth":4,"crawledAt":"2025-10-20T12:13:52.690Z"},{"url":"https://docs.ar.io/build/advanced/arfs/reading-data","title":"Reading Data","content":"AdvancedArFS ProtocolReading DataCopy MarkdownOpenClients can perform read operations to create a timeline of entity write transactions which can then be replayed to construct the Drive state. This is done by querying an Arweave GraphQL index for the user's respective transactions. Arweave GraphQL Guide can provide more information on how to use Arweave GraphQL. If no GraphQL index is available, drive state can only be generated by downloading and inspecting all transactions made by the user's wallet. This timeline of transactions should be grouped by the block number of each transaction. At every step of the timeline, the client can check if the entity was written by an authorized user. This also conveniently enables the client to surface a trusted entity version history to the user. To determine the owner of a Drive, clients must check for who created the first Drive Entity transaction using that Drive-Id. Until a trusted permissions or ACL system is put in place, any transaction in a drive created by any wallet other than the one who created the first Drive Entity transaction could be considered spam. The Unix-Time defined on each transaction should be reserved for tie-breaking same entity updates in the same block and should not be trusted as the source of truth for entity write ordering. This is unimportant for single owner drives but is crucial for multi-owner drives with updateable permissions (currently undefined in this spec) as a malicious user could fake the Unix-Time to modify the drive timeline for other users. Drives that have been updated many times can have a long entity timeline which can be a performance bottleneck. To avoid this, clients can cache the drive state locally and sync updates to the file system by only querying for entities in blocks higher than the last time they checked. Not checking for Drive Ownership could result in seeing incorrect drive state and GraphQL queries. Folder/File Paths ArweaveFS does not store folder or file paths along with entities as these paths will need to be updated whenever the parent folder name changes which can require many updates for deeply nested file systems. Instead, folder/file paths are left for the client to generate from the folder/file names. Folder View Queries Clients that want to provide users with a quick view of a single folder can simply query for an entity timeline for a particular folder by its id. Clients with multi-owner permissions will additionally have to query for the folder's parent drive entity for permission based filtering of the timeline. Basic Query Patterns Query All Drive Entities query { transactions( tags: [ { name: \"ArFS\", values: [\"0.15\"] } { name: \"Entity-Type\", values: [\"drive\"] } { name: \"Drive-Id\", values: [\"your-drive-id\"] } ] ) { edges { node { id block { height timestamp } tags { name value } } } } } Query Folder Contents query ($parentFolderId: String!) { transactions( tags: [ { name: \"ArFS\", values: [\"0.15\"] } { name: \"Parent-Folder-Id\", values: [$parentFolderId] } ] ) { edges { node { id block { height timestamp } tags { name value } } } } } Query File Entities query ($fileId: String!) { transactions( tags: [ { name: \"ArFS\", values: [\"0.15\"] } { name: \"Entity-Type\", values: [\"file\"] } { name: \"File-Id\", values: [$fileId] } ] ) { edges { node { id block { height timestamp } tags { name value } } } } } Building Drive State The process of building drive state involves several steps: Step-by-Step Process Query for all entities associated with a specific Drive-Id Sort by block height to establish chronological order Process entities bottom-up starting with files and folders Build the hierarchy by following parent-child relationships Handle conflicts by using the most recent entity version Example Implementation async } return driveState; } Using Snapshots For large drives, snapshots can significantly improve performance: Snapshot Query query ($driveId: String!) { transactions( tags: [ { name: \"ArFS\", values: [\"0.15\"] } { name: \"Entity-Type\", values: [\"snapshot\"] } { name: \"Drive-Id\", values: [$driveId] } ] sort: HEIGHT_DESC first: 1 ) { edges { node { id block { height timestamp } tags { name value } } } } } Performance Optimization Caching Strategies Local caching - Store frequently accessed data locally Incremental updates - Only fetch new transactions since last sync Snapshot usage - Use snapshots for large drives Batch queries - Combine multiple queries when possible Query Optimization Use specific tags - Narrow down queries with relevant tags Limit results - Use pagination for large result sets Filter by date - Query specific time ranges Index utilization - Leverage GraphQL indexes effectively Error Handling Common Issues Network timeouts - Implement retry logic Invalid data - Validate entity structure Missing entities - Handle incomplete data gracefully Decryption errors - Proper error handling for private data Best Practices Validate ownership - Check drive ownership before processing Handle conflicts - Resolve entity version conflicts Graceful degradation - Provide fallbacks for missing data User feedback - Inform users of sync status Security Considerations Data Validation Verify signatures - Check transaction signatures Validate ownership - Ensure drive ownership Check timestamps - Validate entity timestamps Sanitize data - Clean user-provided data Privacy Protection Decrypt carefully - Handle private data securely Key management - Protect encryption keys Access control - Implement proper permissions Audit logging - Track data access Next Steps Now that you understand how to read ArFS data, explore these related topics: Privacy & Encryption - Secure your data with private drives Upgrading Private Drives - Update legacy drives to v0.15 Creating Drives - Start building with ArFS How is this guide?GoodBadCreating DrivesLearn how to create ArFS drives, folders, and filesUpgrading Private DrivesLearn how to upgrade legacy private drives to ArFS v0.15","estimatedWords":949,"lastModified":"2025-10-20T12:13:53.805Z","breadcrumbs":["build","advanced","arfs","reading data"],"siteKey":"ario","siteName":"AR-IO Network","depth":4,"crawledAt":"2025-10-20T12:13:53.805Z"},{"url":"https://docs.ar.io/build/advanced/arfs/upgrading-drives","title":"Upgrading Private Drives","content":"AdvancedArFS ProtocolUpgrading Private DrivesCopy MarkdownOpenOverview Private drives rely on a combination of user-set password and a wallet signature for encryption and decryption. Wander, formerly ArConnect, is a popular Arweave wallet that is deprecating its signature() method in favor of signDataItem() or signMessage(). In order to preserve access to private drive contents that were secured via drive keys created via 'signature()', ArFS v0.15 introduces a new drive key derivation scheme that both utilizes the modern signing APIs and bridges historical drive keys for usage with it. Because private drive entities exist on chain and their encryption cannot be altered, an upgrade is required to allow continued access to \"V1\" private drives. This upgrade essentially takes a signature from the drive owner wallet, encrypts it using the required signature structure for V2 private drives, and places it on Arweave as a new \"Drive-Signature\" entity. This allows the signature to be fetched and decrypted using the latest methods before using it to decrypt the private drive in the V1 format. Deprecation PeriodThe below instructions for upgrading a private drive will work during the deprecation period for the signature() method from Wanter. Once this period is over, and signature() loses all support, additional steps will be required to obtain the correct signature format to decrypt V1 private drives in order to upgrade them.There is, at this time, no set date for when the deprecation period will end. The Upgrade Process The upgrade process involves creating a new Drive-Signature entity that contains an encrypted version of the legacy signature needed to decrypt the private drive. Drive-Signature Entity The Drive-Signature entity stores the encrypted legacy signature: ArFS: \"0.15\", Entity-Type: \"drive-signature\", Signature-Format: \"1\", Cipher?: \"AES256-GCM\", Cipher-IV: \"\" {data: } Updated Drive Entity The drive entity is updated with a new Signature-Type tag: ArFS: \"0.15\", Cipher?: \"AES256-GCM\", Cipher-IV?: \"\", Content-Type: \"\", Drive-Id: \"\", Drive-Privacy: \"\", Drive-Auth-Mode?: \"password\", Entity-Type: \"drive\", Signature-Type?: \"1\", Unix-Time: \"\" Metadata JSON { \"name\": \"\", \"rootFolderId\": \"\", \"isHidden\": false } Using ArDrive The upgrade process has been made simple by using the ArDrive app. Step 1: Log into ArDrive If the connected wallet has V1 private drives that need to be updated, a banner will appear at the top of the screen. Step 2: Click \"Update Now!\" This will open a modal listing the drives that need to be updated, and linking to more information about the upgrade process. Step 3: Click \"Update\" The process of upgrading the private drives will begin, and involve signing messages depending on how many drives are being upgraded. When the process is complete, a new modal will appear listing the drives that have been successfully updated. Manual Upgrade Process If you need to upgrade drives programmatically, here's the process: 1. Identify V1 Drives Query for drives that don't have the Signature-Type tag: query { transactions( tags: [ { name: \"ArFS\", values: [\"0.15\"] } { name: \"Entity-Type\", values: [\"drive\"] } { name: \"Drive-Privacy\", values: [\"private\"] } ] ) { edges { node { id tags { name value } } } } } 2. Create Drive-Signature Entity async 3. Update Drive Entity async Verification After upgrading, verify the process was successful: Check Drive-Signature Entity query ($driveId: String!) { transactions( tags: [ { name: \"ArFS\", values: [\"0.15\"] } { name: \"Entity-Type\", values: [\"drive-signature\"] } { name: \"Drive-Id\", values: [$driveId] } ] ) { edges { node { id block { height timestamp } tags { name value } } } } } Check Updated Drive Entity query ($driveId: String!) { transactions( tags: [ { name: \"ArFS\", values: [\"0.15\"] } { name: \"Entity-Type\", values: [\"drive\"] } { name: \"Drive-Id\", values: [$driveId] } { name: \"Signature-Type\", values: [\"1\"] } ] ) { edges { node { id block { height timestamp } tags { name value } } } } } Troubleshooting Common Issues Signature not found - Ensure the wallet supports the required signing methods Encryption errors - Verify the encryption parameters are correct Upload failures - Check network connectivity and retry Permission denied - Ensure you own the drive being upgraded Error Handling async catch (error) { console.error(\"Upgrade failed:\", error);","estimatedWords":679,"lastModified":"2025-10-20T12:13:55.520Z","breadcrumbs":["build","advanced","arfs","upgrading drives"],"siteKey":"ario","siteName":"AR-IO Network","depth":4,"crawledAt":"2025-10-20T12:13:55.520Z"},{"url":"https://docs.ar.io/build/guides/using-turbo-in-a-browser/html","title":"Using Turbo SDK with Vanilla HTML","content":"GuidesUsing Turbo in a BrowserUsing Turbo SDK with Vanilla HTMLCopy MarkdownOpenUsing Turbo SDK with Vanilla HTML Firefox Compatibility: Some compatibility issues have been reported with the Turbo SDK in Firefox browsers. At this time the below framework examples may not behave as expected in Firefox. Overview This guide demonstrates how to integrate the @ardrive/turbo-sdk directly into vanilla HTML pages using CDN imports. No build tools, bundlers, or polyfills are required - just modern ES modules support in browsers. Note: Vanilla HTML implementation is the simplest way to get started with the Turbo SDK. It's perfect for prototyping, simple applications, or when you want to avoid build complexity. Prerequisites Modern browser with ES modules support (Chrome 61+, Firefox 60+, Safari 10.1+, Edge 16+) Basic understanding of HTML, CSS, and JavaScript HTTPS hosting for production (required for browser wallet integrations) Create a basic HTML file with Turbo SDK integration: Turbo SDK Example body { font-family: Arial, sans-serif; max-width: 800px; margin: 0 auto; padding: 20px; } .section { margin: 20px 0; padding: 20px; border: 1px solid #ddd; border-radius: 8px; } .loading { color: #666; font-style: italic; } .error { color: red; } .success { color: green; } button { background: #007cba; color: white; border: none; padding: 10px 20px; border-radius: 4px; cursor: pointer; margin: 5px; } button:hover { background: #005a87; } button:disabled { background: #ccc; cursor: not-allowed; } Turbo SDK - Vanilla HTML Demo Current Rates Loading rates... Upload File Upload File import { TurboFactory } from \"https:","estimatedWords":244,"lastModified":"2025-10-20T12:13:56.795Z","breadcrumbs":["build","guides","using turbo in a browser","html"],"siteKey":"ario","siteName":"AR-IO Network","depth":4,"crawledAt":"2025-10-20T12:13:56.795Z"},{"url":"https://docs.ar.io/build/guides/using-turbo-in-a-browser/nextjs","title":"Using Turbo SDK with Nextjs","content":"GuidesUsing Turbo in a BrowserUsing Turbo SDK with Next.jsCopy MarkdownOpenUsing Turbo SDK with Next.js Firefox Compatibility: Some compatibility issues have been reported with the Turbo SDK in Firefox browsers. At this time the below framework examples may not behave as expected in Firefox. Overview This guide demonstrates how to configure the @ardrive/turbo-sdk in a Next.js application with proper polyfills for client-side usage. Next.js uses webpack under the hood, which requires specific configuration to handle Node.js modules that the Turbo SDK depends on. Polyfills: Polyfills are required when using the Turbo SDK in Next.js applications. The SDK relies on Node.js modules like crypto, buffer, process, and stream that are not available in the browser by default. Prerequisites Next.js 13+ (with App Router or Pages Router) Node.js 18+ Basic familiarity with Next.js configuration Install the main Turbo SDK package:npm install @ardrive/turbo-sdkAdd required polyfill packages for browser compatibility:npm install --save-dev crypto-browserify stream-browserify process bufferWallet Integration Dependencies: The Turbo SDK includes @dha-team/arbundles as a peer dependency, which provides the necessary signers for browser wallet integration (like InjectedEthereumSigner and ArconnectSigner). You can import these directly without additional installation.Create or update your next.config.js file to include the necessary polyfills:","estimatedWords":193,"lastModified":"2025-10-20T12:13:58.175Z","breadcrumbs":["build","guides","using turbo in a browser","nextjs"],"siteKey":"ario","siteName":"AR-IO Network","depth":4,"crawledAt":"2025-10-20T12:13:58.175Z"},{"url":"https://docs.ar.io/build/run-a-gateway/manage/upgrading-a-gateway","title":"Upgrading your Gateway","content":"Run a GatewayManage your GatewayUpgrading your GatewayCopy MarkdownOpenTo ensure the optimal performance and security of your AR.IO Gateway, it's essential to regularly upgrade to the latest version. Notably, indexed data resides separate from Docker. As a result, neither upgrading the Gateway nor pruning Docker will erase your data or progress. Here's how you can perform the upgrade: Prerequisites Your Gateway should have been cloned using git. If you haven't, follow the installation instructions. Checking your Release Number Effective with release 3, you can view the currently implemented release on any gateway by visiting https:","estimatedWords":94,"lastModified":"2025-10-20T12:13:59.165Z","breadcrumbs":["build","run a gateway","manage","upgrading a gateway"],"siteKey":"ario","siteName":"AR-IO Network","depth":4,"crawledAt":"2025-10-20T12:13:59.165Z"},{"url":"https://docs.ar.io/build/run-a-gateway/manage/ssl-certs","title":"Automating SSL Certificate Renewal","content":"Run a GatewayManage your GatewayAutomating SSL Certificate RenewalCopy MarkdownOpenSecure your AR.IO Gateway with automated SSL certificate renewal using Certbot and DNS challenge validation. This guide covers setup for different DNS providers to automatically renew certificates without manual intervention. Overview Using DNS challenge validation with Certbot allows you to: Automatically renew SSL certificates Support wildcard certificates Avoid manual certificate management Ensure continuous gateway security Prerequisites A running AR.IO Gateway Domain name configured with your DNS provider Administrative access to your server API access to your DNS provider DNS Provider Setup CloudflareNamecheapCloudflare ConfigurationCreate Cloudflare API TokenNavigate to Cloudflare → My Profile → API Tokens → Create TokenConfigure the token with these permissions: Zone → Zone → Read Zone → DNS → Edit Install Certbot and Cloudflare Pluginapt update apt install certbot python3-certbot-dns-cloudflare -yConfigure API CredentialsCreate the credentials file:nano /etc/letsencrypt/cloudflare.iniAdd your API token:dns_cloudflare_api_token = your_api_token_hereSecure the file:chmod 600 /etc/letsencrypt/cloudflare.iniGenerate SSL CertificateRequest the certificate with wildcard support:certbot certonly --dns-cloudflare \\ --dns-cloudflare-credentials /etc/letsencrypt/cloudflare.ini \\ -d example.com -d *.example.comExpected output:Successfully received certificate. Certificate is saved at: /etc/letsencrypt/live/example.com/fullchain.pem Key is saved at: /etc/letsencrypt/live/example.com/privkey.pemTest Automatic RenewalPerform a dry run to validate the renewal process:certbot renew --dry-runExpected output:Saving debug log to /var/log/letsencrypt/letsencrypt.log - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - Processing /etc/letsencrypt/renewal/example.com.conf - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - Account registered. Simulating renewal of an existing certificate for example.com and *.example.com Waiting 10 seconds for DNS changes to propagate - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - Congratulations, all simulated renewals succeeded: /etc/letsencrypt/live/example.com/fullchain.pem (success)Verify Automatic Renewal TimerCheck that the certbot timer is active:systemctl list-timers | grep certbotExpected output:Tue 2024-11-05 02:22:10 UTC 3h 21min Mon 2024-11-04 17:16:51 UTC 5h 43min ago certbot.timer certbot.serviceNamecheap ConfigurationAPI Requirements: Namecheap requires specific conditions to create API keys: At least 20 domains under your account Minimum $50 account balance At least $50 spent within the last 2 years If you don't meet these requirements, contact Namecheap support for a waiver.Create Namecheap API KeyNavigate to Namecheap → Profile → Tools → Manage API Access KeysCreate your API credentials and note: Your username Your API key Install Certbot and Dependenciesapt update apt install certbot python3-pip -yInstall the Namecheap DNS plugin:pip install certbot-dns-namecheapConfigure API CredentialsCreate the credentials file:nano /etc/letsencrypt/namecheap.iniAdd your API credentials:dns_namecheap_username = your_username dns_namecheap_api_key = your_api_keySecure the file:chmod 600 /etc/letsencrypt/namecheap.iniGenerate SSL CertificateRequest the certificate with wildcard support:certbot certonly --dns-namecheap \\ --dns-namecheap-credentials /etc/letsencrypt/namecheap.ini \\ -d example.com -d *.example.comExpected output:Successfully received certificate. Certificate is saved at: /etc/letsencrypt/live/example.com/fullchain.pem Key is saved at: /etc/letsencrypt/live/example.com/privkey.pemTest Automatic RenewalPerform a dry run to validate the renewal process:certbot renew --dry-runExpected output:Saving debug log to /var/log/letsencrypt/letsencrypt.log - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - Processing /etc/letsencrypt/renewal/example.com.conf - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - Account registered. Simulating renewal of an existing certificate for example.com and *.example.com Waiting 10 seconds for DNS changes to propagate - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - Congratulations, all simulated renewals succeeded: /etc/letsencrypt/live/example.com/fullchain.pem (success)Verify Automatic Renewal TimerCheck that the certbot timer is active:systemctl list-timers | grep certbotExpected output:Tue 2024-11-05 02:22:10 UTC 3h 21min Mon 2024-11-04 17:16:51 UTC 5h 43min ago certbot.timer certbot.service Post-Installation Steps After successfully setting up automatic SSL renewal: Update Gateway Configuration Configure your AR.IO Gateway to use the new certificates. Update your gateway's SSL configuration to point to: Certificate: /etc/letsencrypt/live/your-domain.com/fullchain.pem Private Key: /etc/letsencrypt/live/your-domain.com/privkey.pem Reload Web Server (Optional) If you're using nginx or another web server, reload it to apply the new certificates: systemctl reload nginx Monitor Renewal Process Certbot automatically sets up a systemd timer for renewal. Certificates will be renewed when they have 30 days or less remaining. To manually check renewal status: certbot certificates Troubleshooting Common Issues DNS propagation delays: Wait 5-10 minutes for DNS changes to propagate API rate limits: Check your DNS provider's API rate limits Permission errors: Ensure credential files have correct permissions (600) Logs and Debugging Check certbot logs for detailed error information: tail -f /var/log/letsencrypt/letsencrypt.log Next Steps With SSL certificates automated, consider: Setting up monitoring to track certificate expiration Configuring gateway filters for optimal performance Implementing content moderation policies How is this guide?GoodBadUpgrading your GatewayStep-by-step guide to upgrading your AR.IO Gateway to the latest version safely without losing data or progressGateway FiltersComprehensive guide to configuring AR.IO Gateway filters for efficient data processing and indexing","estimatedWords":886,"lastModified":"2025-10-20T12:14:00.124Z","breadcrumbs":["build","run a gateway","manage","ssl certs"],"siteKey":"ario","siteName":"AR-IO Network","depth":4,"crawledAt":"2025-10-20T12:14:00.124Z"},{"url":"https://docs.ar.io/build/run-a-gateway/manage/content-moderation","title":"Content Moderation","content":"Run a GatewayManage your GatewayContent ModerationCopy MarkdownOpenOverview Arweave is a network designed for permanent storage of data. It is a practical impossibility for data to be wholly removed from the network once it has been uploaded. The AR.IO Network has adopted Arweave's voluntary content moderation model, whereby every participant of the network has the autonomy to decide which content they want to (or can legally) store, serve, and see. Each gateway operating on the network has the right and ability to blocklist any content, ArNS name, or address that is deemed in violation of its content policies or is non-compliant with local regulations. Gateway operators may set content to be blocked by their gateway by submitting a PUT request to their gateway defining the content to be blocked. This requires that the ADMIN_API_KEY environmental variable to be set in order to authenticate the moderation request. The simplest method for submitting moderation requests to a gateway is to use curl in a terminal. Quick Start Set Up Admin API KeyConfigure your admin API key in your .env file:# Set a secure admin API key ADMIN_API_KEY=your_secure_admin_key_hereSecurityChoose a strong, unique admin API key. This key provides administrative access to your gateway and should be kept secure.Test API AccessVerify your admin API key is working:# Test admin endpoint access curl -H \"Authorization: Bearer your_secure_admin_key_here\" \\ http:","estimatedWords":222,"lastModified":"2025-10-20T12:14:01.135Z","breadcrumbs":["build","run a gateway","manage","content moderation"],"siteKey":"ario","siteName":"AR-IO Network","depth":4,"crawledAt":"2025-10-20T12:14:01.136Z"},{"url":"https://docs.ar.io/build/run-a-gateway/manage/index-snapshots","title":"Importing SQLite Database Snapshots","content":"Run a GatewayManage your GatewayImporting SQLite Database SnapshotsCopy MarkdownOpenOverview One of the challenges of running an AR.IO Gateway is the initial synchronization time as your gateway builds its local index of the Arweave network. This process can take days or even weeks, depending on your hardware and the amount of data you want to index. To accelerate this process, you can import a pre-synchronized SQLite database snapshot that contains transaction and data item records already indexed. This guide will walk you through the process of importing a database snapshot into your AR.IO Gateway. NoteThe below instructions are designed to be used in a linux environment. Windows and MacOS users must modify the instructions to use the appropriate package manager/ command syntax for their platform.Unless otherwise specified, all commands should be run from the root directory of the gateway. Quick Start Download Database SnapshotDownload the latest database snapshot using BitTorrent:transmission-cli \"magnet:?xt=urn:btih:62ca6e05248e6df59fac9e38252e9c71951294ed&dn=2025-04-23-sqlite.tar.gz&tr=udp%3A%2F%2Ftracker.opentrackr.org%3A1337%2Fannounce&tr=http%3A%2F%2Ftracker.opentrackr.org%3A1337%2Fannounce&tr=udp%3A%2F%2Fopen.demonii.com%3A1337%2Fannounce&tr=udp%3A%2F%2Ftracker.torrent.eu.org%3A451%2Fannounce&tr=udp%3A%2F%2Fp4p.arenabg.com%3A1337%2Fannounce&tr=https%3A%2F%2Ftracker.bt4g.com%3A443%2Fannounce\"This downloads a 42.8GB snapshot current to April 23, 2025.Extract the SnapshotExtract the downloaded tarball:tar -xzf 2025-04-23-sqlite.tar.gzThis creates a directory with the extracted database files.Import the SnapshotReplace your existing database with the snapshot:# Stop the gateway docker compose down # Backup existing database (optional) mkdir sqlite-backup mv data/sqlite/* sqlite-backup/ # Remove old database rm data/sqlite/* # Import new snapshot mv 2025-04-23-sqlite/* data/sqlite/ # Start the gateway docker compose up -d Detailed Instructions Download MethodsExtraction ProcessImport ProcessVerificationObtaining a Database SnapshotSQLite database snapshots are very large and not easy to incrementally update. For these reasons, AR.IO is distributing them using BitTorrent.Install Torrent ClientInstall a BitTorrent client. We recommend transmission-cli:# Ubuntu/Debian sudo apt-get install transmission-cli # CentOS/RHEL sudo yum install transmission-cli # macOS brew install transmission-cliDownload SnapshotDownload the latest snapshot using the magnet link:transmission-cli \"magnet:?xt=urn:btih:62ca6e05248e6df59fac9e38252e9c71951294ed&dn=2025-04-23-sqlite.tar.gz&tr=udp%3A%2F%2Ftracker.opentrackr.org%3A1337%2Fannounce&tr=http%3A%2F%2Ftracker.opentrackr.org%3A1337%2Fannounce&tr=udp%3A%2F%2Fopen.demonii.com%3A1337%2Fannounce&tr=udp%3A%2F%2Ftracker.torrent.eu.org%3A451%2Fannounce&tr=udp%3A%2F%2Fp4p.arenabg.com%3A1337%2Fannounce&tr=https%3A%2F%2Ftracker.bt4g.com%3A443%2Fannounce\"This will download a snapshot, current to April 23, 2025, of an unbundled data set that includes all data items uploaded via an ArDrive product, including Turbo. The file will be named 2025-04-23-sqlite.tar.gz and be approximately 42.8GB in size.Consider SeedingSeeding RecommendationWhile continuing to seed the torrent after download is not required, it is highly recommended to help ensure the continued availability of the snapshot for others, as well as the integrity of the data. Seeding this file should not cause any issues with your internet service provider.Extracting the Database SnapshotOnce the file has downloaded, you can extract it using the following command.Verify DownloadCheck that the file downloaded completely:ls -lh 2025-04-23-sqlite.tar.gz # Should show approximately 42.8GBExtract the ArchiveExtract the tarball:tar -xzf 2025-04-23-sqlite.tar.gzBe sure to replace the filename with the actual filename of the snapshot you are using, if not using the example above.Verify ExtractionCheck that the extraction was successful:ls -la 2025-04-23-sqlite/ # Should show SQLite database filesThis will extract the file into a directory matching the filename, minus the .tar.gz extension.Importing the Database SnapshotOnce you have an extracted database snapshot, you can import it into your AR.IO gateway by replacing the existing SQLite database files.IMPORTANTImporting a database snapshot will delete your existing database and replace it with the snapshot you are importing.Stop the GatewayStop your AR.IO gateway:docker compose downBackup Existing Database(Optional) Backup your existing SQLite database files:mkdir sqlite-backup mv data/sqlite/* sqlite-backup/Remove Old DatabaseDelete the existing SQLite database files:rm data/sqlite/*Import New SnapshotMove the snapshot files into the data/sqlite directory:mv 2025-04-23-sqlite/* data/sqlite/Be sure to replace 2025-04-23-sqlite with the actual directory name of the extracted snapshot you are using.Start the GatewayStart your AR.IO gateway:docker compose up -dVerifying the ImportThe simplest way to verify the import is to check the gateway logs to see what block number is being imported.Check Gateway LogsView the gateway logs to see the current block height:docker compose logs -f gatewayLook for messages indicating the current block being processed.Verify Block HeightThe 2025-04-23 snapshot was taken at block 1645229, so the gateway will start importing blocks after this height if the snapshot was imported successfully.You should see logs showing blocks being processed starting from block 1645230 or higher.Use Grafana (Optional)You can also use the Grafana Extension to view the last block imported in a more human readable format.How is this guide?GoodBadContent ModerationGateway operators have the right and ability to blocklist any content or ArNS name that is deemed in violation of its content policies or is non-compliant with local regulations.Setting Apex Domain ContentComplete guide to configuring your AR.IO Gateway to serve custom content from the apex domain","estimatedWords":705,"lastModified":"2025-10-20T12:14:02.227Z","breadcrumbs":["build","run a gateway","manage","index snapshots"],"siteKey":"ario","siteName":"AR-IO Network","depth":4,"crawledAt":"2025-10-20T12:14:02.227Z"},{"url":"https://docs.ar.io/build/run-a-gateway/manage/setting-apex-domain","title":"Setting Apex Domain Content","content":"Run a GatewayManage your GatewaySetting Apex Domain ContentCopy MarkdownOpenConfigure your AR.IO Gateway to serve custom content from the apex domain instead of the default Arweave network information. This allows you to customize your gateway's root domain with useful information, project details, or any content you wish to share. Overview Prior to gateway Release 28, the apex domain of a gateway would only display information about the Arweave network. Release 28 introduced two new environment variables that allow a gateway to serve custom content from the apex domain: APEX_TX_ID: Set to serve content from a specific transaction ID APEX_ARNS_NAME: Set to serve content from an ArNS name These variables enable gateway operators to customize their gateway's apex domain with useful information, details about the operator or associated projects, or any other content they wish to share. Quick Start Choose Your Content SourceDecide how you want to serve your content:Option 1: Direct Transaction ID Upload your content to Arweave Use the transaction ID directly Option 2: ArNS Name (Recommended) Upload your content to Arweave Assign your content's transaction ID to an ArNS name Use the ArNS name for easier management Upload Your ContentUpload your dApp, website, or other content to Arweave using your preferred method: ArDrive - For simple file uploads Turbo - For application bundles Direct upload - For advanced users Configure Environment VariableAdd one of these variables to your .env file:# Option 1: Direct transaction ID APEX_TX_ID=your-transaction-id # Option 2: ArNS name (recommended) APEX_ARNS_NAME=your-arns-nameIMPORTANTYou cannot set both variables simultaneously. Providing both variables will result in an error.Restart Your GatewayRestart your gateway to apply the changes:docker compose down docker compose up -dVerify ConfigurationVisit your gateway's apex domain to confirm the custom content is being served correctly. Configuration Methods Transaction ID MethodArNS Name MethodAdvanced ConfigurationUsing Direct Transaction IDUpload ContentUpload your content to Arweave and note the transaction ID:# Example: Upload using ArDrive CLI ardrive upload-file --file-path ./my-website.html # Note the returned transaction ID # Example: abc123...def789Set Environment VariableAdd the transaction ID to your .env file:APEX_TX_ID=abc123...def789Restart GatewayRestart your gateway to apply the configuration:docker compose down docker compose up -dUpdate ContentTo update your content: Upload new content to Arweave Update APEX_TX_ID with the new transaction ID Restart your gateway Advantages: Direct control over content No additional ArNS setup required Simple for one-time content Disadvantages: Requires gateway restart for updates Less flexible for content management Using ArNS Name (Recommended)Upload ContentUpload your content to Arweave:# Upload your website or dApp ardrive upload-file --file-path ./my-dapp.html # Note the transaction ID: xyz789...abc123Register ArNS NameRegister an ArNS name pointing to your content: Visit ArNS App Connect your wallet Choose your desired name (e.g., my-gateway-content) Set the transaction ID: xyz789...abc123 Pay the registration fee Configure Environment VariableAdd the ArNS name to your .env file:APEX_ARNS_NAME=my-gateway-contentRestart GatewayRestart your gateway to apply the configuration:docker compose down docker compose up -dUpdate ContentTo update your content: Upload new content to Arweave Update the ArNS name to point to the new transaction ID No gateway restart required! Advantages: No restart required for content updates Easy content management Professional domain naming Can be updated independently Disadvantages: Requires ArNS setup Additional cost for ArNS registration Advanced Setup OptionsCustom Content TypesConfigure different types of content:Static Website:# Upload HTML/CSS/JS files APEX_ARNS_NAME=my-gateway-websiteSingle Page Application:# Upload SPA bundle APEX_ARNS_NAME=my-dappDocumentation Site:# Upload documentation APEX_ARNS_NAME=my-gateway-docsContent Management WorkflowImplement a content management workflow: Development - Test content locally Upload - Deploy to Arweave Register - Create/update ArNS name Verify - Check content on gateway Monitor - Track performance and usage Use Cases and Examples Gateway InformationProject ShowcaseDocumentationCommunity ExamplesDisplay Gateway Service InformationPerfect for showcasing your gateway service:Content Ideas: Gateway operator information Service capabilities and features Contact information Status and uptime statistics Network participation details Example Structure: My AR.IO Gateway AR.IO Gateway Service Reliable gateway infrastructure for the permanent web High availability Fast response times Global CDN Contact: operator@example.com Showcase Associated ProjectsHighlight your projects and services:Content Ideas: Project portfolio Service offerings Recent updates and news Links to other projects Integration examples Example Structure: My Projects - AR.IO Gateway My Projects Project Alpha Description of project and its features Visit Project Project Beta Another project description Visit Project Host DocumentationProvide comprehensive documentation:Content Ideas: Gateway setup guides API documentation Integration tutorials Troubleshooting guides FAQ sections Example Structure: Gateway Documentation .nav { float: left; width: 200px; } .content { margin-left: 220px; } Navigation Setup Guide API Reference Troubleshooting Gateway Documentation Setup Guide Step-by-step setup instructions... Real-World ExamplesSeveral gateway operators have implemented this feature:arweave.tech Serves a custom landing page with gateway service information Professional presentation of capabilities arlink.xyz Serves the permaDapp for the Arlink project Demonstrates integration with existing projects frostor.xyz / love4src.com Serves information about the Memetic Block Software Guild Showcases community and project information vilenarios.com Serves personalized portfolio/link tree information Personal branding and contact information permagate.io Serves personalized link tree information Professional operator presence These examples demonstrate the flexibility of the apex domain feature and how different operators use it to create unique, personalized experiences for their users. Troubleshooting Configuration IssuesContent ProblemsArNS IssuesFix Configuration ProblemsCheck Environment VariablesVerify your .env file configuration:# Check if variables are set correctly grep -E \"APEX_(TX_ID|ARNS_NAME)\" .env # Should show only one of: # APEX_TX_ID=your-transaction-id # APEX_ARNS_NAME=your-arns-nameEnsure you have only ONE of the APEX variables set, not both.Verify Gateway RestartEnsure your gateway has been restarted after configuration changes:# Check if gateway is running docker compose ps # Restart if needed docker compose down docker compose up -dCheck Gateway LogsReview logs for any error messages:docker compose logs ar-io-core | grep -i apexResolve Content IssuesVerify Content AccessibilityTest if your content is accessible:# Test transaction ID directly curl -I https:","estimatedWords":921,"lastModified":"2025-10-20T12:14:03.513Z","breadcrumbs":["build","run a gateway","manage","setting apex domain"],"siteKey":"ario","siteName":"AR-IO Network","depth":4,"crawledAt":"2025-10-20T12:14:03.513Z"},{"url":"https://docs.ar.io/build/run-a-gateway/manage/environment-variables","title":"Environment Variables","content":"BuildRun a gatewayManageEnvironment VariablesCopy MarkdownOpenDefault Values: Most environment variables have sensible defaults. Only set variables when you need to override the default behavior. Core AR.IO Node The main AR.IO Gateway service that handles data retrieval, indexing, and serving. Server Configuration VariableTypeDefaultDescriptionPORTnumber4000HTTP server portNODE_ENVstringproductionNode.js environmentLOG_LEVELstringinfoLogging level (error, warn, info, debug)LOG_FORMATstringsimpleLog format (simple, json)LOG_FILTERstring{\"always\":true}Log filtering configurationLOG_ALL_STACKTRACESbooleanfalseInclude full stack traces in logsINSTANCE_IDstring-Unique instance identifier Authentication & Security VariableTypeDefaultDescriptionADMIN_API_KEYStringGeneratedAPI key for admin endpoints (auto-generated if not set)ADMIN_API_KEY_FILEString-Path to file containing admin API key Network Configuration VariableTypeDefaultDescriptionTRUSTED_NODE_URLstringhttps:","estimatedWords":82,"lastModified":"2025-10-20T12:14:04.703Z","breadcrumbs":["build","run a gateway","manage","environment variables"],"siteKey":"ario","siteName":"AR-IO Network","depth":4,"crawledAt":"2025-10-20T12:14:04.704Z"},{"url":"https://docs.ar.io/sdks/wayfinder/wayfinder-core","title":"Wayfinder Core","content":"IntroductionWayfinderWayfinder CoreCopy MarkdownOpenFor AI and LLM users: Access the complete Wayfinder Core documentation in plain text format at llm.txt for easy consumption by AI agents and language models. Wayfinder Core Please refer to the source code for SDK details.How is this guide?GoodBad","estimatedWords":42,"lastModified":"2025-10-20T12:14:05.415Z","breadcrumbs":["sdks","wayfinder","wayfinder core"],"siteKey":"ario","siteName":"AR-IO Network","depth":4,"crawledAt":"2025-10-20T12:14:05.415Z"},{"url":"https://docs.ar.io/sdks/wayfinder/wayfinder-react","title":"Wayfinder React","content":"IntroductionWayfinderWayfinder ReactCopy MarkdownOpenFor AI and LLM users: Access the complete Wayfinder React documentation in plain text format at llm.txt for easy consumption by AI agents and language models. Wayfinder React Please refer to the source code for SDK details.How is this guide?GoodBad","estimatedWords":42,"lastModified":"2025-10-20T12:14:06.754Z","breadcrumbs":["sdks","wayfinder","wayfinder react"],"siteKey":"ario","siteName":"AR-IO Network","depth":4,"crawledAt":"2025-10-20T12:14:06.754Z"},{"url":"https://docs.ar.io/sdks/wayfinder/wayfinder-core/preferredwithfallbackroutingstrategy","title":"PreferredWithFallbackRoutingStrategy","content":"IntroductionWayfinderWayfinder CoreRouting StrategiesPreferredWithFallbackRoutingStrategyCopy MarkdownOpenUses a preferred gateway, with a fallback strategy if the preferred gateway is not available. This is useful for builders who run their own gateways and want to use their own gateway as the preferred gateway, but also want to have a fallback strategy in case their gateway is not available. This strategy is built using CompositeRoutingStrategy internally. It first attempts to ping the preferred gateway (using PingRoutingStrategy with StaticRoutingStrategy), and if that fails, it falls back to the specified fallback strategy. import { PreferredWithFallbackRoutingStrategy, FastestPingRoutingStrategy } from '@ar.io/wayfinder-core'; How is this guide?GoodBad","estimatedWords":96,"lastModified":"2025-10-20T12:14:07.599Z","breadcrumbs":["sdks","wayfinder","wayfinder core","preferredwithfallbackroutingstrategy"],"siteKey":"ario","siteName":"AR-IO Network","depth":4,"crawledAt":"2025-10-20T12:14:07.599Z"}],"lastCrawled":"2025-10-20T12:15:30.422Z","stats":{"totalPages":147,"averageWords":442,"duration":178201,"requestCount":166,"averageResponseTime":1043,"pagesPerSecond":0.8249111957845354}},"arweave":{"name":"Arweave Cookbook","baseUrl":"https://cookbook.arweave.net","pages":[{"url":"https://cookbook.arweave.net/tooling/specs/ans/ANS-105.html","title":"ANS-105 License Tagsopen in new window","content":"Contributors: Dylan ShadeLast Updated: Edit# ANS-105: License Tagsopen in new windowAuthors: Sam Williams sam@arweave.org, Abhav Kedia abhav@arweave.org# AbstractThis standard outlines a mechanism for arbitrarily tagging any transaction on the permaweb with the license under which it is published. The standard focuses on simplicity and ease of use in order to encourage broad adoption across the permaweb ecosystem.# MotivationThe goal of the Arweave ecosystem is to create a permanent, collective commons of all valuable knowledge and history, available to all people at any time. Just like the original web, by its open nature this ecosystem encourages wide reuse and copying of data. On the traditional web, this has led to a broad corpus of widely shared information that lacks associated licensing data.By making simple use of the tagging system exposed by Arweave transactions and data entries we can avoid a repeat of this situation, and instead build a web where it is common practice to communicate the license of any piece of data. Due to the inseparable nature of Arweave tags from their associated data, the licensing data of all complying transactions will be atomically communicated to uses -- it is impossible to communicate a link to the data, without also transmitting its licensing information. Due to the nature of Arweave's tagging system, this is achieved without any mandatory or noticeable effect on user experience (when the data is rendered in browsers, etc).# SpecificationThe license tagging format is composed of a single additional tag on any transaction on the Arweave network, as well as two additional transaction formats for defining license types.# Assigning a License to Transaction DataIn order to publish the data of a transaction under a given transaction format, simply add the following tags:Tag NameOptional?Tag ValueLicenseFalseTXID of License DefinitionTitleTrueThe title of the workCreatorTrueThe name/identifier of the creator(s) of the workSourceTrueA link to the source where the material was obtained, if applicable# License AssertionIn order to assert that a previously uploaded transaction has a given license, provide the following tags:Tag NameOptional?Tag ValueApp-NameFalse\"License-Assertion\"OriginalFalseTXID of original uploadLicenseFalseTXID of updated License Definition# License or Legal Tool DefinitionsIn order to define a license or related legal tool (such as CC0open in new window), submit a transaction with the following tags:Tag NameOptional?Tag ValueApp-NameTrue\"License-Definition\"LogoTrueTXID of License LogoShort-NameTrueA short (max 64 characters) 'ticker' with which to refer to the licenseContent-TypeTrueThe MIME type of the contained file, describing the licenseThe body of the transaction must contain the legal text of the license being defined.# License LogosLicense Logo transactions should be defined as follows:Tag NameOptional?Tag ValueContent-TypeFalseThe MIME type of the contained fileThe body of the transaction must contain the visual representation of the license's logo. ANS-104: Bundled Data - Binary ANS-106: Do-Not-Store","estimatedWords":443,"lastModified":"2025-10-20T12:14:11.387Z","breadcrumbs":["tooling","specs","ans","ans 105"],"siteKey":"arweave","siteName":"Arweave Cookbook","depth":4,"crawledAt":"2025-10-20T12:14:11.388Z"},{"url":"https://cookbook.arweave.net/tooling/specs/ans/ANS-104.html","title":"ANS-104 Bundled Data v20 - Binary Serializationopen in new window","content":"Contributors: Dylan ShadeLast Updated: Edit# ANS-104: Bundled Data v2.0 - Binary Serializationopen in new windowStatus: Standard# AbstractThis document describes the data format and directions for reading and writing bundled binary data. Bundled data is a way of writing multiple independent data transactions (referred to as DataItems in this document) into one top level transaction. A DataItem shares many of the same properties as a normal data transaction, in that it has an owner, data, tags, target, signature, and id. It differs in that is has no ability to transfer tokens, and no reward, as the top level transaction pays the reward for all bundled data.# MotivationBundling multiple data transactions into one transaction provides a number of benefits:Allow delegation of payment for a DataItem to a 3rd party, while maintaining the identity and signature of the person who created the DataItem, without them needing to have a wallet with fundsAllow multiple DataItems to be written as a groupIncrease the throughput of logically independent data-writes to the Arweave network# Reference ImplementationThere is a reference implementation for the creation, signing, and verification of DataItems and working with bundles in TypeScriptopen in new window# Specification# 1. Transaction Format# 1.1 Transaction TagsA bundle of DataItems MUST have the following two tags present:Bundle-Format a string describing the bundling format. The format for this standard is binaryBundle-Version a version string. The version referred to in this standard is 2.0.0Version changes may occur due to a change in encoding algorithm in the future# 1.2 Transaction Body FormatThis format for the transaction body is binary data in the following bytes formatN = number of DataItemsBytesPurpose32Numbers of data itemsN x 64Pairs of size and entry ids [size (32 bytes), entry ID (32 bytes)]Remaining bytesBinary encoded data items in bundle# 1.3 DataItem FormatA DataItem is a binary encoded object that has similar properties to a transactionFieldDescriptionEncodingLength (in bytes)Optionalsignature typeType of key format used for the signatureBinary2❌signatureA signature produced by ownerBinaryDepends on signature type❌ownerThe public key of the ownerBinary512❌targetAn address that this DataItem is being sent toBinary32 (+ presence byte)✔️anchorA value to prevent replay attacksBinary32 (+ presence byte)✔️number of tagsNumber of tagsBinary8❌number of tag bytesNumber of bytes used for tagsBinary8❌tagsAn avro array of tag objectsBinaryVariable❌dataThe data contentsBinaryVariable❌All optional fields will have a leading byte which describes whether the field is present (1 for present, 0 for not present). Any other value for this byte makes the DataItem invalid.A tag object is an Apache Avro encoded stream representing an object { name: string, value: string }. Prefixing the tags objects with their bytes length means decoders may skip them if they wish.The anchor and target fields in DataItem are optional. The anchor is an arbitrary value to allow bundling gateways to provide protection from replay attacks against them or their users.# 1.3.1 Tag formatParsing the tags is optional, as they are prefixed by their bytes length.To conform with deployed bundles, the tag format is Apache Avroopen in new window with the following schema:{ \"type\": \"array\", \"items\": { \"type\": \"record\", \"name\": \"Tag\", \"fields\": [ { \"name\": \"name\", \"type\": \"bytes\" }, { \"name\": \"value\", \"type\": \"bytes\" } ] } } Usually the name and value fields are UTF-8 encoded strings, in which case \"string\" may be specified as the field type rather than \"bytes\", and avro will automatically decode them.To encode field and list sizes, avro uses a long datatype that is first zig-zag encoded, and then variable-length integer encoded, using existing encoding specifications. When encoding arrays, avro provides for a streaming approach that separates the content into blocks.# 1.3.1.1 ZigZag codingZigZagopen in new window is an integer format where the sign bit is in the 1s place, such that small negative numbers have no high bits set. In surrounding code, normal integers are almost always stored in a twos-complement manner instead, which can be converted as below.Converting to ZigZag:zigzag = twos_complement > 1; # 1.3.1.2 Variable-length integer codingVariable-length integeropen in new window is a 7-bit little-endian integer format, where the 8th bit of each byte indicates whether another byte (of 7 bits greater significance) follows in the stream.Converting to VInt:// writes 'zigzag' to 'vint' buffer offset = 0; do { vint_byte = zigzag zigzag >>= 7; if (zigzag) vint_byte |= 0x80; vint.writeUInt8(vint_byte, offset); offset += 1; } while(zigzag); Converting from VInt:// constructs 'zigzag' from 'vint' buffer zigzag = 0; offset = 0; do { vint_byte = vint.readUInt8(offset); zigzag |= (vint_byte vint_byte offset += 1; } while(vint_byte); # 1.3.1.3 Avro tag array formatAvro arraysopen in new window may arrive split into more than one sequence of items. Each sequence is prefixed by its length, which may be negative, in which case a byte length is inserted between the length and the sequence content. This is used in schemas of larger data to provide for seeking. The end of the array is indicated by a sequence of length zero.The complete tags format is a single avro array, consisting solely of blocks of the below format. The sequence is terminated by a block with a count of 0. The size field is only present if the count is negative, in which case its absolute value should be used.FieldDescriptionEncodingLengthOptionalcountNumber of items in blockZigZag VIntVariable❌sizeNumber of bytes if count<0ZigZag VIntVariable✔️blockConcatenated tag itemsBinarysize❌# 1.3.1.4 Avro tag item formatEach item of the avro array is a pair of avro strings or bytes objects, a name and a value, each prefixed by their length.FieldDescriptionEncodingLengthOptionalname_sizeNumber of bytes in nameZigZag VIntVariable❌nameName of the tagBinaryname_size❌value_sizeNumber of bytes in valueZigZag VIntVariable❌valueValue of the tagBinaryvalue_size❌# 2. DataItem signature and idThe signature and id for a DataItem is built in a manner similar to Arweave 2.0 transaction signing. It uses the Arweave 2.0 deep-hash algorithm. The 2.0 deep-hash algorithm operates on arbitrarily nested arrays of binary data, i.e a recursive type of DeepHashChunk = Uint8Array | DeepHashChunk[].There are reference implementations for the deep-hash algorithm in TypeScriptopen in new window and Erlangopen in new windowTo generate a valid signature for a DataItem, the contents of the DataItem and static version tags are passed to the deep-hash algorithm to obtain a message. This message is signed by the owner of the DataItem to produce the signature. The id of the DataItem, is the SHA256 digest of this signature.The exact structure and content passed into the deep-hash algorithm to obtain the message to sign is as follows:[ utf8Encoded(\"dataitem\"), utf8Encoded(\"1\"), owner, target, anchor, [ ... [ tag.name, tag.value ], ... [ tag.name, tag.value ], ... ], data ] # 2.1 Verifying a DataItemDataItem verification is a key to maintaining consistency within the bundle standard. A DataItem is valid iff.1:id matches the signature (via SHA-256 of the signature)signature matches the owner's public keytags are all validan anchor isn't more than 32 bytesA tag object is valid iff.:there are <= 128 tagseach key is <= 1024 byteseach value is <= 3072 bytesonly contains a key and valueboth the key and value are non-empty strings# 3. Writing a Bundle of DataItemsTo write a bundle of DataItems, each DataItem should be constructed, signed, encoded, and placed in a transaction with the transaction body format and transaction tags specified in Section 1.# 3.1 Nested bundleArweave Transactions and DataItems have analogous specifications for tagging and bearing of a binary payload. As such, the ANS-104 Bundle Transaction tagging and binary data format specification can be applied to the tags and binary data payload of a DataItem. Assembling a DataItem this way provides for the nesting of ANS-104 Bundles with one-to-many relationships between \"parent\" and \"child\" bundles and theoretically unbounded levels of nesting. Additionally, nested DataItem Bundles can be mixed heterogeneously with non-Bundle DataItems at any depth in the Bundle tree.To construct an ANS-104 DataItem as a nested Bundle:Add tags to the DataItem as described by the specification in section 1.1Provide a binary payload for the DataItem matching the Bundle Transaction Body Format described in section 1.2, i.e. the Bundle header outlining the count, size, and IDs of the subsequent, nested DataItems, each of which should be verifiable using the method described in section 2.1.Gateway GQL queries for DataItem headers should, upon request, contain a bundledIn field whose value indicates the parent-child relationship of the DataItem to its immediate parent. Any nested bundle should be traceable to a base layer Arweave Transaction by recursively following the bundledIn field up through the chain of parents.# 4. Reading a Bundle of DataItemsTo read a bundle of DataItems, the list of bytes representing the DataItems can be partitioned using the offsets in each pair. Subsequently, each partition can be parsed to a DataItem object (struct in languages such as Rust/Go etc. or JSON in TypeScript).This allows for querying of a singleton or a bundle as a whole.# 4.1 Indexing DataItemsThis format allows for indexing of specific fields in O(N) time. Some form of caching or indexing could be performed by gateways to improve lookup times.1 - if and only if ANS-103: Succinct Proofs ANS-105: License Tags","estimatedWords":1473,"lastModified":"2025-10-20T12:14:11.992Z","breadcrumbs":["tooling","specs","ans","ans 104"],"siteKey":"arweave","siteName":"Arweave Cookbook","depth":4,"crawledAt":"2025-10-20T12:14:11.992Z"},{"url":"https://cookbook.arweave.net/tooling/specs/ans/ANS-103.html","title":"ANS-103 Succinct Proofs of Random Accessopen in new window","content":"Contributors: Dylan ShadeLast Updated: Edit# ANS-103: Succinct Proofs of Random Accessopen in new windowStatus: Draft Authors: Sam Williams sam@arweave.org, Lev Berman ldmberman@protonmail.com# AbstractThis document describes the new consensus mechanism for the Arweave network based on the competition to find a chunk of the past data in a set of historical chunks inferred from the latest agreed-upon blockweave state.# MotivationAt the time when this specification is written, the consensus mechanism employed by the Arweave network is a classical Proof of Work with an additional requirement of including a chunk (up to 256 KiB) of the past data into the hash preimage where the chunk is deterministically determined by the latest state of the blockweave.While this approach incentivizes the network to keep historical data, it does not impose any significant restrictions on the speed of access to data a miner needs to be competitive. Specifically, miners can benefit from using a remote storage pool. Moreover, combined with a computation pool, it can serve Proof of Work preimages to millions of clients per second via a Gbit Internet link. A storage and computation pool has been evidenced in the Arweave network by a decreased number of public nodes and a simultaneous increase of the network hashpower and people claiming to be part of the pool.Therefore, the new consensus algorithm's first goal is to make the mining advantage grow sharply with the growing speed of access to data, promoting replication more aggressively.The second but not less important goal is to reduce the energy consumed by the network. Proof of Work networks are knownopen in new window for their energy-intensity. Reduced energy consumption reduces the carbon footprint, widely believed to be very harmful to the planet. We believe the environment-friendliness of the consensus mechanism is crucial for the long-term sustainability of the Arweave platform.To sum up, the consensus algorithm described here aims at two major goals.Disincentivize miners from retrieving data on demand from the network. In other words, incentivize miners to store data as close to the mining machine as possible.Reduce network's energy consumption.# Reference ImplementationLinkopen in new window.# Specification# PrerequsitesIndexed DatasetThe core of the new mechanism - continuous retrieval of chunks of the past data. Every chunk is identified by a global offset, as we want to make the possession of any byte equally incentivized. Therefore, the whole weave has to be indexed so that every chunk is quickly accessible by its global offset.Starting from the release 2.1, the Arweave Erlang client maintains such an index.Slow HashThe consensus mechanism needs a deterministic but unpredictable way to choose candidate chunks, to make the miners continuously access storage. However, choosing candidate chunks cannot be too easy as it would allow miners to reduce the number of chunks they ever work with. There are two threats associated with that. One threat is the lower cost of the computation expenditure, as compared to the cost of storing extra data required for the same probability of a reward. The second threat is the existing computation technology that, although being more expensive than storing the entire weave, is so efficient that it outperforms even the most efficient data retrieval based client.Starting from release 1.7, the Arweave protocol relies on RandomXopen in new window, a proof-of-work algorithm optimized for general-purpose CPUs.# Algorithm DescriptionEvery block uniquely determines a Search Space - a set of offsets (Recall Bytes) on the [0, weave size at a certain block (Search Space Upper Bound)] interval.# Miner StepsGenerate a random nonce and generate a slow hash (H0) of the hash of a Merkle tree containing the current state, the candidate block, and the nonce.Compute the unique recall byte from H0, the hash of the previous block (PrevH), and Search Space Upper Bound.Search the local storage for the chunk containing the computed Recall Byte. If not found, repeat from the first step.Compute a fast hash of the hash of the Merkle tree containing the slow hash computed at the first step and the located chunk.Check whether the computed hash is bigger than the current mining difficulty (the number computed from its binary digit big-endian representation is bigger). If not, repeat from the first step. If yes, pack and distribute the block (the block contains the nonce and the chunk).The solution chunk together with the Merkle proofs of its inclusion in the blockweave we call Succinct Proof of Random Access (or SPoRA) and use as a name for the new consensus mechanism.# Verifier StepsPerform one iteration over the miner steps where the nonce and the chunk are found in the verified block.# Rationale# Search Space ConstraintsSearch Space needs to be big enough for two reasons:make it prohibitively expensive to download the entire search space on demand; note that the prior consensus mechanism can be viewed as an edge case of SPoRA where the search space consists of a single chunk; the efficiency of serving data to the computing agents on demand depends on the network bandwidth, which grows over timeopen in new window.make it prohibitively expensive to compensate for the lack of data by hashing.On the other hand, Search Space needs to be small enough to incentivize miners to replicate the rarer parts of the weave, which would give them and advantage over miners who replicated less of the corresponding area at the corresponding blocks.We choose the Search Space size to be 10% of the weave. In this case, 10% of the miners storing unique 10% of the weave in the network where everybody stores 90% of the weave are roughly 1.2 times more efficient than the rest of the miners. It holds for various ratios of the time it takes to compute a RandomX hash and the time it takes to look up a chunk.# PseudocodeThe ar_deep_hash definitionopen in new window.mine(Nonce, BlockPreimage, PrevH, SearchSpaceUpperBound): // Compute a slow hash. H0 := randomx_hash(concat(Nonce, BlockPreimage)) RecallByte := pick_recall_byte(H0, PrevH, SearchSpaceUpperBound) // Search the local storage for the chunk containing Recall Byte. SPoA := get_spoa_by_byte(RecallByte) if SPoA == not_found mine(random_nonce(), BlockPreimage, PrevH, SearchSpaceUpperBound) Chunk := get_chunk(SPoA) SolutionHash := randomx_hash(concat(H0, PrevH, Timestamp, Chunk)) if validate(Candidate, Diff) return mine(random_nonce(), BlockPreimage, PrevH, SearchSpaceUpperBound) pick_recall_byte(H0, PrevH, SearchSpaceUpperBound): SubspaceNumber := remainder(hash_to_number(H0), SUBSPACES_COUNT) SearchSpaceSize := integer_division(SearchSpaceUpperBound, 10) EvenSubspaceSize := integer_division(SearchSpaceUpperBound, SUBSPACES_COUNT) SearchSubspaceSize := integer_division(SearchSpaceSize, SUBSPACES_COUNT) SubspaceStart := SubspaceNumber * EvenSubspaceSize SubspaceSize := min(SearchSpaceUpperBound - SubspaceStart, EvenSubspaceSize) EncodedSubspaceNumber := number_to_binary(SubspaceNumber) SearchSubspaceSeed := hash_to_number(sha256(concat(PrevH, EncodedSubspaceNumber))) SearchSubspaceStart := remainder(SearchSubspaceSeed, SubspaceSize) SearchSubspaceByteSeed := hash_to_number(sha256(H)) SearchSubspaceByte := remainder(SearchSubspaceByteSeed, SearchSubspaceSize) return AbsoluteSubspaceStart + remainder(SearchSubspaceStart + SearchSubspaceByte, SubspaceSize) # Related WorkThe work was heavily inspired by Permacoin: Repurposing Bitcoin Work for Data Preservationopen in new window by Andrew Miller, Ari Juels, Elaine Shi, Bryan Parno, and Jonathan Katz from Microsoft Research.We suggest to use the existing, growing, and always accessible Arweave dataset instead of one pre-generated by a trusted dealer. We use a slow specialized hardware-resistant hash to make it prohibitively expensive to compensate for the lack of local data with computation. Finally, we provide the network with the incentive to replicate data uniformly. ANS-102: Bundled Data - JSON ANS-104: Bundled Data - Binary","estimatedWords":1160,"lastModified":"2025-10-20T12:14:12.984Z","breadcrumbs":["tooling","specs","ans","ans 103"],"siteKey":"arweave","siteName":"Arweave Cookbook","depth":4,"crawledAt":"2025-10-20T12:14:12.984Z"},{"url":"https://cookbook.arweave.net/tooling/specs/ans/ANS-102.html","title":"ANS-102 Bundled Data - JSON Serializationopen in new window","content":"Contributors: Dylan ShadeLast Updated: Edit# ANS-102: Bundled Data - JSON Serializationopen in new windowStatus: Standard# AbstractThis document describes the data format and directions for reading and writing bundled data. Bundled data is a way of writing multiple logical data transactions (referred to as DataItems in this document) into one top level transaction. A DataItem shares many of the same properties as normal data transaction, in that it has an owner, data, tags, target, signature, and id. It differs in that it has no ability to transfer tokens, and no reward, as the top level transaction pays the reward for all bundled data.# MotivationBundling multiple logical data transactions into one transaction provides a number of benefits:To allow delegation of payment for a DataItem to a 3rd party, while maintaining the identity and signature of person who created the DataItem, without them needing to have a wallet with funds.Allow multiple DataItems to be written as a group.To increase the throughput of logically independent data writes to the Arweave network# Reference ImplementationThere is a reference implementation for the creation, signing, and verification of DataItems and working with bundles in TypeScriptopen in new window# Specification# 1. Transaction Format# 1.1 Transaction TagsA bundle of DataItems MUST have the following two tags presentBundle-Format a string describing the bundling format. The format for this standard is jsonBundle-Version a version string. The version referred to in this standard is 1.0.0# 1.2 Transaction Body FormatThis format for the transaction body is a JSON object in the following format{ items: [ { DataItem }, { DataItem } ] } # 1.3 DataItem FormatA DataItem is a JSON object that has similar properties to a transaction:B64U Encoding indicates the field is Base64Url encoded binary.All properties MUST be present, for optional values the value in 'Empty Value' MUST be used.FieldDescriptionEncodingEmpty ValueownerThe public key of the ownerB64UtargetAn address that this DataItem is being sent toB64UEmpty StringnonceA value to prevent replay attacksB64UEmpty StringtagsAn array of tag objectsJson ArrayEmpty Json ArraydataThe data contentsB64UsignatureA signature produced by ownerB64UidThe id the itemB64UA tag object is a JSON object with the following two keys. A tag object MUST NOT have any other keys.FieldDescriptionEncodingEmpty ValuenameName of the tagB64UvalueValue of the tagB64UThe fields in the DataItem and Tags objects can be handled in an identical way as their counterpart in a regular Arweave transaction.The nonce field in DataItem is optional, and is an arbitrary value to allow bundling gateways to provide protection from replay attacks against them or their users.# 2. DataItem signature and idThe signature, and id for a DataItem is built in a manner similar to Arweave 2.0 transaction signing. It uses the Arweave 2.0 deep-hash algorithm. The 2.0 deep-hash algorithm operates on arbitrarily nested arrays of binary data, i.e a recursive type of DeepHashChunk = Uint8Array | DeepHashChunk[].There is reference implementations for the deep-hash algorithm in TypeScriptopen in new window and Erlangopen in new windowTo generate a valid signature for a DataItem, the contents of the DataItem and static version tags, are passed to the deep-hash algorithm to obtain a message. This message is signed by the owner of the DataItem to produce the signature. The id of the DataItem, is the SHA256 digest of this signature.The exact structure and content passed into the deep-hash algorithm to obtain the message to sign is as follows:[ utf8Encoded(\"dataitem\"), utf8Encoded(\"1\"), owner, target, nonce, [ ... [ tag.name, tag.value ], ... [ tag.name, tag.value ], ... ], data ] # 3. Expanding a bundle of DataItemsTo read and expand a bundle of DataItems, each DataItem in the items should be verified using the verification algorithm. Individual items that fail verification MUST be discarded.In rare cases, an identical DataItem may exist in more that one transaction. That is, the contents and id of the DataItem are identical but exist in multiple Arweave transactions. Since they are identical, any of the copies can be discarded.# 4. Writing a bundle of DataItemsTo write a bundle of DataItems, each DataItem should constructed and signed, and placed in a transaction with the transaction body format and transaction tags specified in Section 1. Transaction Format. ANS-101: Gateway Capabilities ANS-103: Succinct Proofs","estimatedWords":684,"lastModified":"2025-10-20T12:14:13.919Z","breadcrumbs":["tooling","specs","ans","ans 102"],"siteKey":"arweave","siteName":"Arweave Cookbook","depth":4,"crawledAt":"2025-10-20T12:14:13.919Z"},{"url":"https://cookbook.arweave.net/tooling/specs/ans/ANS-101.html","title":"ANS-101 Gateway Capabilities Endpointopen in new window","content":"Contributors: Dylan ShadeLast Updated: Edit# ANS-101: Gateway Capabilities Endpointopen in new windowStatus: DraftVersion: -# AbstractThis document describes a HTTP endpoint and schema that Arweave Gateways should expose to allow users of the gateway to determine the capabilities of the gateway.# MotivationAs the ecosystem of gateways onto the Arweave blockchain grows, these gateways will provide different capabilities. Users of the gateway, need to be able to determine if the gateway supports the capabilities they need to run a particular app or use for a particular purpose.# Specification# 1. HTTP EndpointGateways that conforming to this specification MUST expose a HTTP endpoint that responds to a GET request at the path /info/capabilities with a JSON document conforming to the schema in section 2, which accurately lists the capabilities supported by the gateway.# 2. Response SchemaThe JSON document returned by the endpoint MUST be an object with a capabilities key, which is an array of capability objects.A capability object has the following schema at a minimum -name capability name. This MUST be a globally unique name.version capability version. This MUST be a semver version string.Capability objects MAY conform to additional schema particular to that capability.Example response body: ( names, versions, and capability objects are examples only. ){ \"capabilities\": [ { \"name\": \"arql\", \"version\": \"1.0.0\" }, { \"name\": \"graphql\", \"version\": \"1.0.0\" }, { \"name\": \"arweave-id-lookup\", \"verson\": \"1.0.0\" }, { \"name\": \"post-bundled-tx-json\", \"version\": \"1.0.0\" }, { \"name\": \"post-delegated-tx\", \"version\": \"1.0.0\", \"maxFee\": \"0.00125\" }, { \"name\": \"push-on-event-api\", \"version\": \"1.0.0\", \"platforms\": [\"push-android\", \"push-ios\", \"web-push-firefox\", \"webhook\" ] } ] } ANS-102: Bundled Data - JSON","estimatedWords":256,"lastModified":"2025-10-20T12:14:14.897Z","breadcrumbs":["tooling","specs","ans","ans 101"],"siteKey":"arweave","siteName":"Arweave Cookbook","depth":4,"crawledAt":"2025-10-20T12:14:14.898Z"},{"url":"https://cookbook.arweave.net/tooling/deployment/github-action.html","title":"Github Action","content":"Contributors: Dylan ShadeLast Updated: Edit# Github ActionWARNINGThis guide is for educational purposes only, and you should use to learn options of how you might want to deploy your application. In this guide, we are trusting a 3rd party resource github owned by microsoft to protect our secret information, in their documentation they encrypt secrets in their store using libsodium sealed box, you can find more information about their security practices here. https://docs.github.com/en/actions/security-guides/encrypted-secretsGithub Actions are CI/CD pipelines that allows developers to trigger automated tasks via events generated from the github workflow system. These tasks can be just about anything, in this guide we will show how you can use github actions to deploy your permaweb application to the permaweb using permaweb-deploy and ArNS.TIPThis guide requires understanding of github actions, and you must have some Turbo Credits and an ArNS name. Go to https://ar.io/arns/ for more details on acquiring an ArNS name.WARNINGThis guide does not include testing or any other checks you may want to add to your production workflow.# PrerequisitesBefore setting up GitHub Actions deployment, you'll need:An Arweave wallet with sufficient Turbo Credits for deploymentAn ArNS name that you ownA built application (e.g., in a ./dist folder)# Install permaweb-deployAdd permaweb-deploy as a development dependency to your project:npm install --save-dev permaweb-deploy # Configure Deployment ScriptAdd a deployment script to your package.json that builds your application and deploys it using permaweb-deploy:{ \"scripts\": { \"dev\": \"vuepress dev src\", \"build\": \"vuepress build src\", \"deploy\": \"npm run build && permaweb-deploy --arns-name YOUR_ARNS_NAME\" } } Replace YOUR_ARNS_NAME with your actual ArNS name (e.g., my-app).# Advanced ConfigurationYou can customize the deployment with additional options:{ \"scripts\": { \"deploy\": \"npm run build && permaweb-deploy --arns-name my-app --deploy-folder ./dist --undername @\" } } Available options:--arns-name (required): Your ArNS name--deploy-folder: Folder to deploy (default: ./dist)--undername: ANT undername to update (default: @)--ario-process: ARIO process (default: mainnet)# Create GitHub ActionCreate a .github/workflows/deploy.yml file in your repository:name: Deploy to Permaweb on: push: branches: - \"main\" jobs: deploy: runs-on: ubuntu-latest steps: - uses: actions/checkout@v4 - uses: actions/setup-node@v4 with: node-version: 20.x - run: npm install - run: npm run deploy env: DEPLOY_KEY: ${{ secrets.DEPLOY_KEY }} # Setup GitHub Secrets# 1. Prepare Your WalletFirst, encode your Arweave wallet as base64:base64 -i wallet.json Copy the output (it will be a long base64 string).# 2. Add Secret to GitHubGo to your repository on GitHubNavigate to Settings → Secrets and variables → ActionsClick New repository secretName: DEPLOY_KEYValue: Paste the base64 encoded wallet stringClick Add secret# Fund Your WalletEnsure your deployment wallet has sufficient Turbo Credits. You can fund it using:# Check current balance npx @ardrive/turbo-cli balance --wallet-file wallet.json # Add credits (amount in Winston - 1 AR = 1,000,000,000,000 Winston) npx @ardrive/turbo-cli top-up --value 500000000000 --wallet-file wallet.json Security Best PracticesUse a dedicated wallet solely for deploymentsKeep minimal funds in the deployment walletNever commit wallet files to your repositoryRegularly rotate deployment keys# Test Your Deployment# Local TestingTest your deployment locally before pushing:DEPLOY_KEY=$(base64 -i wallet.json) npm run deploy # Verify DeploymentAfter a successful GitHub Action run:Check the action logs for the deployment transaction IDWait 10-20 minutes for ArNS propagationVisit your ArNS name: https://YOUR_ARNS_NAME.arweave.net# TroubleshootingCommon Issues:Insufficient Credits: Ensure your wallet has enough Turbo CreditsArNS Propagation: Wait 10-20 minutes after deployment for changes to appearBuild Failures: Ensure your build command works locally firstSecret Issues: Verify the DEPLOY_KEY secret is properly set and base64 encodedCheck Deployment Status:Monitor your deployments through:GitHub Actions logsArNS resolver: https://arns.arweave.net/resolve/YOUR_ARNS_NAME🎉 You now have automated permaweb deployment with GitHub Actions!Your application will automatically deploy to the permaweb whenever you push to the main branch, and your ArNS name will point to the latest version. arkb (CLI)","estimatedWords":593,"lastModified":"2025-10-20T12:14:15.527Z","breadcrumbs":["tooling","deployment","github action"],"siteKey":"arweave","siteName":"Arweave Cookbook","depth":3,"crawledAt":"2025-10-20T12:14:15.527Z"},{"url":"https://cookbook.arweave.net/tooling/deployment/arkb.html","title":"arkb","content":"Contributors: Dylan ShadeLast Updated: Edit# arkb# RequirementsAn Arweave wallet is required to deploy using arkb for covering the data transaction costs.# InstallationTo install arkb runNPMYARNnpm install -g arkb yarn add ar-gql # DeployingWhen uploading a directory of files or a Permaweb application, by default arkb deploys each file separately as an L1 transaction, with the option to bundle the transactions using Bundlr.# Static BuildPermaweb applications are statically generated, meaning that the code and content are generated ahead of time and stored on the network.Below is an example of a static site. To deploy this to the Permaweb, the build directory will be passed in as the argument for the deploy flag.|- build |- index.html |- styles.css |- index.js # Default DeploymentDeploying as an L1 transaction can take longer to confirm as it is directly uploaded to the Arweave network.arkb deploy [folder] --wallet [path to wallet] # Bundled DeploymentTo deploy using Bundlr you will need to fund a Bundlr node.Bundlr node2 allows free transactions under 100kb.You can add custom identifiable tags to the deployment using tag-name/tag-value syntax.arkb deploy [folder] --use-bundler [bundlr node] --wallet [path to wallet] --tag-name [tag name] --tag-value [tag value] # Other Commands# Fund Bundlrarkb fund-bundler [amount] --use-bundler [bundlr node] * Funding a Bundlr instance can take up to 30 minutes to process# Saving Keyfilearkb wallet-save [path to wallet] After saving your key you can now run commands without the --wallet-file option, like thisarkb deploy [path to directory] # Check Wallet Balancearkb balance Permaweb Deploy GitHub Actions","estimatedWords":248,"lastModified":"2025-10-20T12:14:16.102Z","breadcrumbs":["tooling","deployment","arkb"],"siteKey":"arweave","siteName":"Arweave Cookbook","depth":3,"crawledAt":"2025-10-20T12:14:16.102Z"},{"url":"https://cookbook.arweave.net/tooling/deployment/permaweb-deploy.html","title":"Permaweb Deploy","content":"Contributors: Dylan ShadeLast Updated: Edit# Permaweb Deploypermaweb-deploy is a Node.js command-line tool that streamlines deployment of web applications and files to the Permaweb using Arweave. It uploads your build folder or single files, creates Arweave manifests, and automatically updates ArNS (Arweave Name Service) records with the new transaction ID.# FeaturesTurbo SDK Integration: Fast, reliable file uploads to ArweaveArweave Manifest v0.2.0: Creates manifests with fallback support for SPAsArNS Updates: Automatically updates ArNS records via ANT with new transaction IDsAutomated Workflow: Integrates seamlessly with GitHub ActionsGit Hash Tagging: Automatically tags deployments with Git commit hashes404 Fallback Detection: Automatically detects and sets 404.html as fallbackNetwork Support: Supports mainnet, testnet, and custom ARIO process IDsFlexible Deployment: Deploy folders or single files# Installationnpm install permaweb-deploy --save-dev For Yarn users:yarn add permaweb-deploy --dev --ignore-engines # Prerequisites# Wallet ConfigurationFor Arweave signer (default): Encode your Arweave wallet key in base64 format:base64 -i wallet.json | pbcopy Set the encoded wallet as the DEPLOY_KEY environment variable.For Ethereum/Polygon/KYVE signers: Use your raw private key (no encoding needed) as the DEPLOY_KEY.Security Best PracticeUse a dedicated wallet for deployments to minimize security risks. Ensure your wallet has sufficient Turbo Credits for uploads.# Basic UsageAdd deployment scripts to your package.json:{ \"scripts\": { \"build\": \"vite build\", \"deploy\": \"npm run build && permaweb-deploy --arns-name my-app\" } } Deploy your application:npm run deploy # CLI OptionsOptionAliasDescriptionDefault--arns-name-nRequired. The ArNS name to update---ario-process-pARIO process (mainnet, testnet, or process ID)mainnet--deploy-folder-dFolder to deploy./dist--deploy-file-fDeploy a single file instead of a folder---undername-uANT undername to update@--ttl-seconds-tTTL in seconds for the ANT record (60-86400)3600--sig-type-sSigner type (arweave, ethereum, polygon, kyve)arweave# ExamplesDeploy ApplicationDEPLOY_KEY=$(base64 -i wallet.json) npx permaweb-deploy --arns-name my-app Deploy Specific FolderDEPLOY_KEY=$(base64 -i wallet.json) npx permaweb-deploy --arns-name my-app --deploy-folder ./build Deploy Single FileDEPLOY_KEY=$(base64 -i wallet.json) npx permaweb-deploy --arns-name my-app --deploy-file ./script.lua Deploy to UndernameDEPLOY_KEY=$(base64 -i wallet.json) npx permaweb-deploy --arns-name my-app --undername staging Deploy with Custom TTLDEPLOY_KEY=$(base64 -i wallet.json) npx permaweb-deploy --arns-name my-app --ttl-seconds 7200 Deploy with Ethereum SignerDEPLOY_KEY= npx permaweb-deploy --arns-name my-app --sig-type ethereum # Network ConfigurationsMainnet (Default){ \"scripts\": { \"deploy\": \"npm run build && permaweb-deploy --arns-name my-app\" } } Testnet{ \"scripts\": { \"deploy:test\": \"npm run build && permaweb-deploy --arns-name my-app --ario-process testnet\" } } Custom Process ID{ \"scripts\": { \"deploy:custom\": \"npm run build && permaweb-deploy --arns-name my-app --ario-process PROCESS_ID\" } } # GitHub Actions IntegrationCreate .github/workflows/deploy.yml:name: Deploy to Permaweb on: push: branches: - main jobs: deploy: runs-on: ubuntu-latest steps: - uses: actions/checkout@v4 - uses: actions/setup-node@v4 with: node-version: 20 - run: npm install - run: npm run deploy env: DEPLOY_KEY: ${{ secrets.DEPLOY_KEY }} # Deployment OutputAfter successful deployment, you'll see output similar to:-------------------- DEPLOY DETAILS -------------------- Tx ID: abc123def456ghi789jkl012mno345pqr678stu901v ArNS Name: my-app Undername: @ ANT: xyz789abc012def345ghi678jkl901mno234pqr567s AR IO Process: bh9l1cy0aksiL_x9M359faGzM_yjralacHIUo8_nQXM TTL Seconds: 3600 -------------------------------------------------------- Deployed TxId [abc123def456ghi789jkl012mno345pqr678stu901v] to name [my-app] for ANT [xyz789abc012def345ghi678jkl901mno234pqr567s] using undername [@] # Security Best PracticesUse dedicated wallets: Create deployment-specific wallets to minimize security risksSecure secret management: Never commit your DEPLOY_KEY to version controlBuild verification: Always check your build for exposed secrets before deploymentSufficient credits: Ensure your wallet has enough Turbo Credits before deploymentBase64 encoding: Arweave wallets must be base64 encoded for the deployment script# Troubleshooting# Common Errors\"ARNS_NAME not configured\"Ensure you're passing the --arns-name flag with a valid ArNS name\"DEPLOY_KEY not configured\"Verify your base64 encoded wallet is set as the DEPLOY_KEY environment variable\"deploy-folder does not exist\"Check that your build folder exists and the path is correctRun your build command first\"ARNS name does not exist\"Verify the ArNS name is correct and exists in the specified network\"Upload timeouts\"Files have a 10-second upload timeoutLarge files may fail and require optimization\"Insufficient Turbo Credits\"Check your wallet balance and add more credits if needed# Debug InformationEnable verbose logging by setting the DEBUG environment variable:DEBUG=permaweb-deploy* npm run deploy # Dependencies@ar.io/sdk: ANT operations and ArNS management@ardrive/turbo-sdk: Fast file uploads to Arweave@permaweb/aoconnect: AO network connectivityyargs: CLI argument parsing# Next StepsArNS Setup: ArNS NamesTurbo Credits: Turbo SDKopen in new windowGitHub Actions: CI/CD Integration# ResourcesGitHub Repository: permaweb/permaweb-deployopen in new windowTurbo SDK Documentation: docs.ardrive.io/turboopen in new windowArNS Documentation: ar.io/arnsopen in new windowArweave Ecosystem: arweave.orgopen in new window arkb (CLI)","estimatedWords":654,"lastModified":"2025-10-20T12:14:16.939Z","breadcrumbs":["tooling","deployment","permaweb deploy"],"siteKey":"arweave","siteName":"Arweave Cookbook","depth":3,"crawledAt":"2025-10-20T12:14:16.939Z"},{"url":"https://cookbook.arweave.net/tooling/graphql/ar-gql.html","title":"ar-gql","content":"Contributors: Dylan ShadeLast Updated: Edit# ar-gqlThis package is a minimal layer on top of GraphQL, it supports parameterized queries with query variables. It also implements management of paged results.# InstallationTo install `ar-gql runNPMYARNnpm i ar-gql yarn add ar-gql # Exampleimport { arGql } from \"ar-gql\" const argql = arGql() (async () => { let results = await argql.run(`query( $count: Int ){ transactions( first: $count, tags: [ { name: \"App-Name\", values: [\"PublicSquare\"] }, { name: \"Content-Type\", values: [\"text/plain\"] }, ] ) { edges { node { id owner { address } data { size } block { height timestamp } tags { name, value } } } } }`, {count: 1}); console.log(results); })(); # Resourcesar-gql github pageopen in new window Goldsky Search Gateway","estimatedWords":122,"lastModified":"2025-10-20T12:14:17.603Z","breadcrumbs":["tooling","graphql","ar gql"],"siteKey":"arweave","siteName":"Arweave Cookbook","depth":3,"crawledAt":"2025-10-20T12:14:17.603Z"},{"url":"https://cookbook.arweave.net/tooling/graphql/search-indexing-service.html","title":"Goldsky Search GraphQL Gateway","content":"Contributors: Dylan ShadeLast Updated: Edit# Goldsky Search GraphQL GatewayA specialized GraphQL gateway that extends Arweave's querying capabilities with advanced search features.# TL;DRBackwards compatible syntax with Arweave GraphQLFaster response times for complex queries (ie multi-tag search)More query options including fuzzy and wildcard searchEnhanced filtering capabilitiesGoldskyopen in new window's free search service is a GraphQL gateway that uses an optimized backend to provide faster searches for complex queries across arweave blocks and transactions, and also introduces additional querying syntax for fuzzy and wildcard search use-cases.The Search GraphQL syntax is a superset of the Arweave GraphQL syntax. It's fully backwards compatible and will return the same results for the same queries, but has some additional modifiers that can be useful.Flexible tag filters Search for just a tag name or valueAdvanced tag filters Fuzzy searchWildcard searchFilter for L1 transactions onlyResult set total countsFor any custom needs or feature ideas, feel free to contact the Goldsky team through email or on discord!# Search Gateway EndpointsCurrently, the only service with this syntax is hosted Goldsky. If anybody is interested in hosting their own gateway with the same syntax, feel free to contact the Goldskyopen in new window for help.Goldsky Search Serviceopen in new window# Features# Flexible Tag FiltersThe Search Gateway Syntax is less strict, and allows for searching just for the Tag name or value# ExamplesSearch for transactions with the tag value 'cat'query just_values { transactions( first: 10, tags: [ { values: [\"cat\"] } ] ) { edges { node { id tags { name value } } } } } Search for transactions that have an In-Response-To-IDquery just_name { transactions( first: 10, tags: [ { name: \"In-Response-To-ID\" } ] ) { edges { node { id tags { name value } } } } } # Advanced tag filtersThe Search Gateway Syntax offers an additional parameter to the tag filter, match.Match valueDescriptionEXACT(default) exact matches only.WILDCARDEnables * to match any amount of characters, ie. text/*FUZZY_ANDFuzzy match containing all search termsFUZZY_ORFuzzy match containing at least one search termOpen up the playground and try some of the following queries!Searching all transactions with an image content type using a wildcard{ transactions( tags: [ { name: \"Content-Type\", values: \"image/*\", match: WILDCARD} ] first: 10 ) { edges { cursor node { id tags { name value } block { height } bundledIn {id} } } } } # Fuzzy SearchFuzzy search is very powerful, and can search for 'similar' text with many variations.Searching all transactions with 'cat' OR 'dog' (or CAT or doG or cAts or CAAts etcs). So the tag could contain at least of cat-like or dog-like term.{ transactions( tags: [ { name: \"Content-Type\", values: [\"cat\", \"dog\"], match: \"FUZZY_OR\"} ] first: 10 ) { edges { cursor node { id tags { name value } block { height } bundledIn {id} } } } } Search for transactions that have cat-like AND dog-like tag values{ transactions( tags: [ { name: \"Content-Type\", values: [\"cat\", \"dog\"], match: \"FUZZY_AND\"} ] first: 10 ) { edges { cursor node { id tags { name value } block { height } bundledIn {id} } } } } # Exclude Bundled (L2) TransactionsSimply set bundledIn: NULLquery just_l1 { transactions( first: 10, bundledIn: null ) { edges { node { id signature owner { address } block { height } } } } } # Getting total counts given a queryIf you'd like to understand how many transactions fit a certain set of filters, just use the count field. This will trigger an additional optimized count operation. This will likely double the time it would take to return the query, so use only when needed.query count_mirror { { transactions(tags:{values:[\"MirrorXYZ\"]}) { count } } } Querying Arweave ar-gql (Library)","estimatedWords":613,"lastModified":"2025-10-20T12:14:18.186Z","breadcrumbs":["tooling","graphql","search indexing service"],"siteKey":"arweave","siteName":"Arweave Cookbook","depth":3,"crawledAt":"2025-10-20T12:14:18.186Z"},{"url":"https://cookbook.arweave.net/tooling/querying-arweave.html","title":"Querying Arweave with GraphQL","content":"Contributors: Dylan ShadeLast Updated: Edit# Querying Arweave with GraphQLArweave provides a simple way of querying for transactions and filtering them by tags. Arweave GraphQL-compatible indexing services provide endpoints users can post GraphQL queries to, and also provide a playground for trying queries.GraphQLopen in new window is a flexible query language that services can use to build a customized data schema for clients to query. GraphQL also allows clients to specify which elements of the available data structure they would like to see in the results.# Public Indexing Servicesarweave.net graphqlopen in new window the original graphql endpoint, managed by ar.ioopen in new windowgoldsky search serviceopen in new window a public service specifically optimized for search using a superset of the graphql syntax, managed by goldskyopen in new windowar.io decentralized indexingopen in new window A decentralized network for indexing services. Currently in testing with L1 transactions available.# Executing a GraphQL QueryTo query arweave we’ll need to access it through an indexing service that supports GraphQL. Use one of the GraphQL playgrounds listed above to get started!Copy and paste in the following queryquery { transactions(tags: [{ name: \"App-Name\", values: [\"PublicSquare\"] }]) { edges { node { id tags { name value } } } } } If you’re not familiar with GraphQL it can seem a little overwhelming at first but once you know the structure, it’s fairly easy to read and understand.query { ( ) { } } In the example query we pasted our is transactions but we could also query for blocks. A full description of Arweave's GraphQL schema is written up in the Arweave GraphQL Guideopen in new window. The guide refers to the filter criteria as “Query Structures” and the complete data structure definition of transactions and blocks as “Data Structures”.When it comes to the , the thing to note is that you can specify a subset of the complete data structure you’re interested in. For example, the complete data structure for a transactions schema is listed hereopen in new window.In our case we’re interested in the id and complete list of tags for any transaction matching our filter criteria.Hit the big “Play” button in the middle of the playground to run the query.You’ll notice we get back a list of transactions in the results data structure we specified in our original query.If you’re new to blockchains this is unexpected, we haven’t built anything, why do these results exist? It turns out, the “PublicSquare”: “App-Name” tag we’ve filtered for has been in use for a while.Arweave protocol's founder, Sam Williams, proposed the transaction format a few years ago in a github code snippetopen in new window. Since then builders in the ecosystem have been building on and around it, experimenting, posting transactions with those tags.Back to querying Arweave. You’ll notice in the GraphQL results that there are no readable post messages, just tags and information about posts.This is because the GraphQL indexing service is concerned with indexing and retrieving header data for transactions and blocks but not their associated data.To get the data of a transaction we need to look it up using another HTTP endpoint.https://arweave.net/ Copy and paste one of the id’s in your query results and modify the above link, appending the id. It should look something like this…https://arweave.net/eaUAvulzZPrdh6_cHwUYV473OhvCumqT3K7eWI8tArkThe result of navigating to that URL in the browser (HTTP GET) would be retrieving the content of the post (stored in the transactions data). In this example it’s…Woah that's pretty cool 😎 (For a complete listing arweave HTTP endpoints visit the HTTP APIopen in new window documentation.)# Posting a Query From JavaScriptPosting a GraphQL query from javascript isn't much different than posting it in the playground.First install the arweave-js package for easy access to a GraphQL endpoint.npm install --save arweave Then enter a slightly more advanced version of the example query from above and await the results of posting it.import Arweave from 'arweave'; // initialize an arweave instance const arweave = Arweave.init({}); // create a query that selects tx data the first 100 tx with specific tags const queryObject = { query: `{ transactions( first:100, tags: [ { name: \"App-Name\", values: [\"PublicSquare\"] }, { name: \"Content-Type\", values: [\"text/plain\"] } ] ) { edges { node { id tags { name value } } } } }` }; const results = await arweave.api.post('/graphql', queryObject); # Multiple QueriesIt is possible to post multiple queries in a single round-trip to the GraphQL endpoint. This example queries the name transaction (each as a separate query) for two wallet addresses using the now obsolete (replaced by ar-profile) but still permanent arweave-id protocol.query { account1: transactions(first: 1, owners:[\"89tR0-C1m3_sCWCoVCChg4gFYKdiH5_ZDyZpdJ2DDRw\"], tags: [ { name: \"App-Name\", values: [\"arweave-id\"] }, { name: \"Type\", values: [\"name\"] } ] ) { edges { node { id owner { address } } } } account2: transactions(first: 1, owners:[\"kLx41ALBpTVpCAgymxPaooBgMyk9hsdijSF2T-lZ_Bg\"], tags: [ { name: \"App-Name\", values: [\"arweave-id\"] }, { name: \"Type\", values: [\"name\"] } ] ) { edges { node { id owner { address } } } } } # ResourcesArweave GQL ReferenceArDB packagear-gql packageSearch Indexing Service Goldsky Search Gateway","estimatedWords":839,"lastModified":"2025-10-20T12:14:18.790Z","breadcrumbs":["tooling","querying arweave"],"siteKey":"arweave","siteName":"Arweave Cookbook","depth":2,"crawledAt":"2025-10-20T12:14:18.790Z"},{"url":"https://cookbook.arweave.net/tooling/graphql/index.html","title":"GraphQL Tools","content":"Contributors: Dylan ShadeLast Updated: Edit# GraphQL ToolsThis section covers the tools and libraries available for querying Arweave data using GraphQL. GraphQL provides a powerful and flexible way to retrieve exactly the data you need from the Arweave network.# Core GraphQL Toolsar-gql - JavaScript LibraryLightweight GraphQL client for ArweaveTypeScript supportEasy integration with web applicationsComprehensive query buildingQuerying Arweave - Comprehensive GuideComplete overview of Arweave querying methodsGraphQL query examples and patternsBest practices for data retrieval# Advanced QueryingGoldsky Search Gateway - Search & IndexingAdvanced search capabilitiesFull-text search across Arweave dataIndexing and aggregation featuresHigh-performance querying# Getting Started# Basic GraphQL Queryimport { gql } from 'ar-gql' const query = gql` query { transactions( owners: [\"YOUR_WALLET_ADDRESS\"] first: 10 ) { edges { node { id block { height } tags { name value } } } } } ` # Query PatternsTransaction QueriesFilter by owner, recipient, or tagsRetrieve transaction metadata and contentSearch across time rangesBlock QueriesGet block information and statisticsQuery network state at specific heightsAnalyze network activityBundle QueriesAccess bundled transactionsQuery bundle metadataRetrieve nested transaction data# Best PracticesUse Specific Queries: Request only the data you needImplement Pagination: Handle large result sets efficientlyCache Results: Store frequently accessed data locallyError Handling: Implement robust error handling for network issuesRate Limiting: Respect API rate limits and implement backoff strategies# Next StepsStart with ar-gql: ar-gql LibraryLearn Querying: Querying ArweaveAdvanced Search: Goldsky Search GatewayExplore Examples: Zero to Deployed App","estimatedWords":225,"lastModified":"2025-10-20T12:14:19.372Z","breadcrumbs":["tooling","graphql","index"],"siteKey":"arweave","siteName":"Arweave Cookbook","depth":3,"crawledAt":"2025-10-20T12:14:19.373Z"},{"url":"https://cookbook.arweave.net/tooling/bundlers.html","title":"Bundling Services","content":"Contributors: Dylan ShadeLast Updated: Edit# Bundling ServicesWith bundling services users can post their data transactions to a bundling service to have it \"bundled\" together with other users transactions and posted as a single Arweave transaction in an upcoming Arweave block.# What is a bundle?A description of transaction bundles and their benefits can be found here.Bundles follow the ANS-104 standardopen in new window, which defines how multiple data items can be efficiently packaged together into a single Arweave transaction. This enables:Cost Efficiency: Multiple small transactions can be bundled together, reducing overall transaction costsScalability: Higher throughput by processing many data items in a single base layer transactionFlexibility: Support for different payment tokens while the bundler handles AR token payments# What is a Bundler node?A bundler is a node which is responsible for accepting transactions or data items from users, bundling them, and posting them to the Arweave network (with a guarantee they will be uploaded with a specific transaction ID).# Key Bundling Services:# TurboRepository: Turbo Upload Serviceopen in new windowDescription: A high-performance bundling service developed by ArDriveFeatures: Fast upload processing, reliable data persistence, and efficient bundlingIntegration: Widely used across the Arweave ecosystem for production applications# How Bundlers WorkData Acceptance: Bundlers receive data items from users along with paymentValidation: Each data item is validated for proper formatting and signaturesBundling: Multiple data items are packaged together using ANS-104 standardNetwork Submission: The bundle is submitted to Arweave as a single base layer transactionPersistence Guarantee: Bundlers ensure data is stored until confirmed on-chain# Supporting Multiple CurrenciesA key feature of bundling services is that because they pay for the base Arweave transaction to be posted (using AR tokens) they can choose to enable payments of storage fees on a variety of different tokens. This is the main entry point for other chains to enable Arweave's permanent storage for their users.# Payment Token SupportBundlers can accept various cryptocurrencies for payment while handling the AR token requirements internally:Native AR tokens: Direct payment in Arweave's native currencyEthereum tokens: ETH, USDC, DAI, and other ERC-20 tokensSolana tokens: SOL and SPL tokensOther chains: Support varies by bundler implementation# Benefits for DevelopersSimplified Integration: Developers don't need to manage AR tokens directlyUser Experience: Users can pay with familiar tokens from their preferred chainsCost Predictability: Bundlers often offer fixed pricing in stable currenciesAutomatic Handling: Bundlers manage all Arweave network interactions# Data Verification and IntegrityModern bundling services implement comprehensive verification systems:Signature Verification: All data items must be properly signedData Root Verification: Bundle contents are cryptographically verified against Arweave chain dataBackground Verification: Continuous validation ensures data integrity over timeChunk-based Retrieval: Data can be retrieved and verified through Arweave's chunk system# Code Examples# Using Turbo SDK# Basic Upload with TurboBasic Upload Exampleimport { TurboFactory, ArweaveSigner } from '@ardrive/turbo-sdk'; // Initialize Turbo client const turbo = TurboFactory.authenticated({ privateKey: yourPrivateKey, // JWK or raw private key }); // Upload data async function uploadData() { const data = Buffer.from('Hello, Arweave!'); try { const uploadResult = await turbo.uploadFile({ fileStreamFactory: () => Readable.from(data), fileSizeFactory: () => data.length, signal: AbortSignal.timeout(10_000), // 10 second timeout dataItemOpts: { tags: [ { name: 'Content-Type', value: 'text/plain' }, { name: 'App-Name', value: 'MyApp' }, ], }, }); console.log('Upload successful:', uploadResult.id); return uploadResult.id; } catch (error) { console.error('Upload failed:', error); } } # Upload with Payment (ETH)Upload with Ethereum Paymentimport { TurboFactory } from '@ardrive/turbo-sdk'; import { EthereumSigner } from 'arbundles'; // Using Ethereum wallet for payment const turbo = TurboFactory.authenticated({ signer: new EthereumSigner(ethereumPrivateKey), token: 'ethereum', // Pay with ETH }); async function uploadWithEthPayment() { const data = JSON.stringify({ message: 'Hello from Ethereum!' }); // Check balance first const balance = await turbo.getBalance(); console.log('Current balance:', balance); // Get upload cost const cost = await turbo.getUploadCosts({ bytes: [Buffer.byteLength(data)], }); console.log('Upload cost:', cost); // Fund account if needed if (balance.winc Readable.from(Buffer.from(data)), fileSizeFactory: () => Buffer.byteLength(data), dataItemOpts: { tags: [ { name: 'Content-Type', value: 'application/json' }, { name: 'Payment-Method', value: 'ethereum' }, ], }, }); return result.id; } # Using arbundles Library# Create and Upload Bundle ManuallyManual Bundle Creation and Uploadimport { DataItem, createData, bundleAndSignData } from 'arbundles'; import Arweave from 'arweave'; // Initialize Arweave client const arweave = Arweave.init({ host: 'arweave.net', port: 443, protocol: 'https' }); async function createAndUploadBundle() { // Create multiple data items const dataItems = []; // First data item const item1 = createData('Hello World!', yourSigner, { tags: [ { name: 'Content-Type', value: 'text/plain' }, { name: 'App-Name', value: 'BundleExample' } ] }); await item1.sign(yourSigner); dataItems.push(item1); // Second data item const item2 = createData(JSON.stringify({ foo: 'bar' }), yourSigner, { tags: [ { name: 'Content-Type', value: 'application/json' }, { name: 'Data-Type', value: 'metadata' } ] }); await item2.sign(yourSigner); dataItems.push(item2); // Create bundle const bundle = await bundleAndSignData(dataItems, yourSigner); // Upload to Arweave const transaction = await arweave.createTransaction({ data: bundle.getRaw() }); transaction.addTag('Bundle-Format', 'binary'); transaction.addTag('Bundle-Version', '2.0.0'); await arweave.transactions.sign(transaction, yourWallet); await arweave.transactions.post(transaction); console.log('Bundle uploaded:', transaction.id); return { bundleId: transaction.id, items: dataItems.map(item => item.id) }; } # React Component ExampleReact File Uploader Componentimport React, { useState } from 'react'; import { TurboFactory } from '@ardrive/turbo-sdk'; function FileUploader() { const [file, setFile] = useState(null); const [uploading, setUploading] = useState(false); const [uploadResult, setUploadResult] = useState(null); const turbo = TurboFactory.authenticated({ privateKey: process.env.REACT_APP_ARWEAVE_KEY, }); const handleUpload = async () => { if (!file) return; setUploading(true); try { const result = await turbo.uploadFile({ fileStreamFactory: () => file.stream(), fileSizeFactory: () => file.size, dataItemOpts: { tags: [ { name: 'Content-Type', value: file.type }, { name: 'File-Name', value: file.name }, { name: 'Upload-Timestamp', value: Date.now().toString() }, ], }, }); setUploadResult(result); } catch (error) { console.error('Upload error:', error); } finally { setUploading(false); } }; return ( setFile(e.target.files[0])} /> {uploading ? 'Uploading...' : 'Upload to Arweave'} {uploadResult } # Node.js Batch UploadNode.js Batch Directory Uploadimport fs from 'fs'; import path from 'path'; import { TurboFactory } from '@ardrive/turbo-sdk'; async function batchUploadDirectory(directoryPath) { const turbo = TurboFactory.authenticated({ privateKey: process.env.ARWEAVE_PRIVATE_KEY, }); const files = fs.readdirSync(directoryPath); const results = []; for (const filename of files) { const filePath = path.join(directoryPath, filename); const stats = fs.statSync(filePath); if (stats.isFile()) { console.log(`Uploading: ${filename}`); try { const result = await turbo.uploadFile({ fileStreamFactory: () => fs.createReadStream(filePath), fileSizeFactory: () => stats.size, dataItemOpts: { tags: [ { name: 'File-Name', value: filename }, { name: 'Content-Type', value: getContentType(filename) }, { name: 'File-Size', value: stats.size.toString() }, { name: 'Upload-Batch', value: 'directory-upload' }, ], }, }); results.push({ filename, transactionId: result.id, size: stats.size, }); console.log(`✓ ${filename} uploaded: ${result.id}`); } catch (error) { console.error(`✗ Failed to upload ${filename}:`, error); } } } return results; } function getContentType(filename) { const ext = path.extname(filename).toLowerCase(); const types = { '.txt': 'text/plain', '.json': 'application/json', '.html': 'text/html', '.css': 'text/css', '.js': 'application/javascript', '.png': 'image/png', '.jpg': 'image/jpeg', '.jpeg': 'image/jpeg', '.gif': 'image/gif', '.pdf': 'application/pdf', }; return types[ext] || 'application/octet-stream'; } # Check Upload StatusUpload Status Verificationasync function checkUploadStatus(transactionId) { const turbo = TurboFactory.unauthenticated(); try { // Check if data item exists const status = await turbo.getUploadStatus({ id: transactionId }); console.log('Upload status:', status); // Alternative: Check directly with Arweave const arweave = Arweave.init({ host: 'arweave.net', port: 443, protocol: 'https' }); const txStatus = await arweave.transactions.getStatus(transactionId); console.log('Transaction status:', txStatus); return { exists: status !== null, confirmed: txStatus.confirmed, blockHeight: txStatus.confirmed?.block_height, }; } catch (error) { console.error('Status check failed:', error); return { exists: false, confirmed: false }; } } # Integration with AR.IO GatewaysAR.IO gateways provide enhanced support for bundled data:Automatic Unbundling: Gateways can automatically extract and index data items from bundlesOptimistic Indexing: Data items can be made available before final confirmationPeer-to-Peer Retrieval: Enhanced data availability through gateway networksCaching: Intelligent caching systems improve data access performance# Accessing Bundled Data via GatewaysGateway Data Access// Access data through AR.IO gateway const gatewayUrl = 'https://arweave.net'; // or your preferred gateway const transactionId = 'your-transaction-id'; // Direct access const response = await fetch(`${gatewayUrl}/${transactionId}`); const data = await response.text(); // With metadata const headResponse = await fetch(`${gatewayUrl}/${transactionId}`, { method: 'HEAD' }); const contentType = headResponse.headers.get('Content-Type'); const bundleFormat = headResponse.headers.get('Bundle-Format');","estimatedWords":1284,"lastModified":"2025-10-20T12:14:20.081Z","breadcrumbs":["tooling","bundlers"],"siteKey":"arweave","siteName":"Arweave Cookbook","depth":2,"crawledAt":"2025-10-20T12:14:20.081Z"},{"url":"https://cookbook.arweave.net/tooling/index.html","title":"Tooling","content":"Contributors: Dylan ShadeLast Updated: Edit# ToolingTools and services organized by the problems they solve for Permaweb developers.# Data Upload & BundlingTools for efficiently uploading data to Arweave:Bundlers - Services that bundle multiple transactions together# Data Querying & IndexingTools for discovering and retrieving data:GraphQL - Powerful query interfaces for transaction data# Deployment & PublishingTools for deploying applications to the Permaweb:CLI Tools - Command-line deployment utilities# Framework IntegrationDevelopment kits are available in the Kits section for:React applicationsVue applicationsSvelte applications# When to Use Each ToolBundlers - When posting multiple transactions or need guaranteed settlementGraphQL - For complex data queries and analytics (optional for most apps)CLI Tools - For automated deployment and CI/CD integrationFramework Kits - When building frontend applications with modern frameworks Guides References","estimatedWords":121,"lastModified":"2025-10-20T12:14:20.657Z","breadcrumbs":["tooling","index"],"siteKey":"arweave","siteName":"Arweave Cookbook","depth":2,"crawledAt":"2025-10-20T12:14:20.657Z"},{"url":"https://cookbook.arweave.net/kits/vue/create-vue.html","title":"Create Vue Starter Kit","content":"Contributors: Dylan ShadeLast Updated: Edit# Create Vue Starter KitThis guide will provide step-by-step instructions to configure your development environment and build a permaweb Vue application.# PrerequisitesBasic Typescript Knowledge (Not Mandatory) - Learn Typescriptopen in new windowNodeJS v16.15.0 or greater - Download NodeJSopen in new windowKnowledge of Vue.js (preferably Vue 3) - Learn Vue.jsopen in new windowKnow git and common terminal commands# Development DependenciesTypeScript (Optional)NPM or Yarn Package Manager# Steps# Create ProjectThe following command installs and launches create-vue, the official scaffolding tool for Vue projects.NPMYARNnpm init vue@latest yarn create vue During the process, you'll be prompted to select optional features such as TypeScript and testing support. I recommend selecting the Vue Router with yes, the rest can be selected as per your preference.✔ Project name: … ✔ Add TypeScript? … No / Yes ✔ Add JSX Support? … No / Yes ✔ Add Vue Router for Single Page Application development? … No / *Yes* ✔ Add Pinia for state management? … No / Yes ✔ Add Vitest for Unit testing? … No / Yes ✔ Add Cypress for both Unit and End-to-End testing? … No / Yes ✔ Add ESLint for code quality? … No / Yes ✔ Add Prettier for code formatting? … No / Yes # Change into the Project Directorycd # Install DependenciesNPMYARNnpm install yarn # Setup RouterVue Router is the official router for Vue.js and seamlessly integrates with Vue. To make it work with Permaweb, switch from a browser history router to a hash router as the URL cannot be sent to the server. Change createWebHistory to createWebHashHistory in your src/router/index.ts or src/router/index.js file.import { createRouter, createWebHashHistory } from \"vue-router\"; import HomeView from \"../views/HomeView.vue\"; const router = createRouter({ history: createWebHashHistory(import.meta.env.BASE_URL), routes: [ { path: \"/\", name: \"home\", component: HomeView, }, { path: \"/about\", name: \"about\", component: () => import(\"../views/AboutView.vue\"), }, ], }); export default router; # Setup BuildConfigure the build process in the vite.config.ts or vite.config.js file. To serve Permaweb apps from a sub-path (https://[gateway]/[TX_ID]), update the base property to ./ in the config file.export default defineConfig({ base: './', ... }) # Run the AppBefore moving forward, it is crucial to verify that everything is working correctly. Run a check to ensure smooth progress.NPMYARNnpm run dev yarn dev it will start a new development server locally on your machine by default it uses `PORT 5173`. If this PORT is already in use it may increase the PORT number by 1 (`PORT 5174`) and try again. # Deploy Permanently# Generate WalletWe need the arweave package to generate a walletNPMYARNnpm install --save arweave yarn add arweave -D then run this command in the terminalnode -e \"require('arweave').init({}).wallets.generate().then(JSON.stringify).then(console.log.bind(console))\" > wallet.json # Fund WalletYou will need to fund your wallet with ArDrive Turbo credits. To do this, enter ArDriveopen in new window and import your wallet. Then, you can purchase turbo credits for your wallet.# Setup Permaweb-DeployNPMYARNnpm install --global permaweb-deploy yarn global add permaweb-deploy # Fund Your WalletTurbo uses Turbo Credits to upload data to Arweave. You can purchase Turbo Credits with a variety of fiat currencies or crypto tokens. Below is an example for funding your wallet with 10 USD. It will open a browser window to complete the purchase using Stripe.npm install @ardrive/turbo-sdk turbo top-up --wallet-file wallet.json --currency USD --value 10 Be sure to replace wallet.json with the path to your Arweave wallet.# Update package.json{ ... \"scripts\": { ... \"deploy\": \"DEPLOY_KEY=$(base64 -i wallet.json) permaweb-deploy --ant-process > --deploy-folder build\" } ... } Replace > with your ANT process id.# Run buildNow it is time to generate a build, runNPMYARNnpm run build yarn build # Run deployFinally we are good to deploy our first Permaweb ApplicationNPMYARNnpm run deploy yarn deploy ERRORIf you receive an error Insufficient funds, make sure you remembered to fund your deployment wallet with ArDrive Turbo credits.# ResponseYou should see a response similar to the following:Deployed TxId [>] to ANT [>] using undername [>] Your Vue app can be found at https://arweave.net/>.SUCCESSYou should now have a Vue Application on the Permaweb! Great Job!# SummaryThis guide provides a simple step-by-step method to publish a Vue.js app on the Permaweb using Create Vue. If you need additional features Tailwind, consider exploring alternative starter kits listed in the guide to find a suitable solution for your requirements.","estimatedWords":705,"lastModified":"2025-10-20T12:14:21.278Z","breadcrumbs":["kits","vue","create vue"],"siteKey":"arweave","siteName":"Arweave Cookbook","depth":3,"crawledAt":"2025-10-20T12:14:21.278Z"},{"url":"https://cookbook.arweave.net/kits/vue/index.html","title":"Vue Starter Kits","content":"Contributors: Dylan ShadeLast Updated: Edit# Vue Starter KitsVue.js is a progressive JavaScript framework that allows building user interfaces. Unlike other frameworks, it compiles the template into JavaScript during runtime, resulting in a smaller file size and faster performance. Vue is ideal for building performant and scalable single-page applications, making it a popular choice among front-end developers.Vue Starter Kit Guides:Note: - Since npm init vue@latestalredy uses vite, we have not included a vite guide for Vue.Create Vue App - Use Create Vue to efficiently build a Vue.js-based with TypeScript and Vite modern permaweb applicationPermaweb Application Constraints100% Front-end application (No Server-Side Backend)Applications are served from a sub-path (https://[gateway]/[TX_ID]) Svelte","estimatedWords":107,"lastModified":"2025-10-20T12:14:21.916Z","breadcrumbs":["kits","vue","index"],"siteKey":"arweave","siteName":"Arweave Cookbook","depth":3,"crawledAt":"2025-10-20T12:14:21.916Z"},{"url":"https://cookbook.arweave.net/kits/svelte/vite.html","title":"SvelteVite Starter Kit","content":"Contributors: Dylan ShadeLast Updated: Edit# Svelte/Vite Starter KitSvelte is the framework that compiles out of the way, that results is small packages, which is perfect for the permaweb. As developers, we value Dev Experience as much as we value User Experience. This kit uses the vite bundle system to give developers a great DX experience.# Installing vite with svelte and typescriptNPM v6NPM v7YARNPNPMnpm create vite@latest my-perma-app --template svelte-ts npm create vite@latest my-perma-app -- --template svelte-ts yarn create vite my-perma-app --template svelte-ts pnpm create vite my-perma-app --template svelte-ts # Project InfoThe vite build system places your index.html file in the root directory, this is where you would include any css or global script dependencies if needed. For more information about the vite project layout check out the vite documentationopen in new window# Setup hash-routerTo setup the hash-router we will use tinroopen in new window. tinro is a tiny declarative routing library, that is similar to React Router.NPMYARNnpm install --save-dev tinro yarn add -D tinro # Telling Svelte to use hash routingIn the src/App.svelte file, you want to configure the router to use the hash routing mode. import { Route, router } from \"tinro\"; router.mode.hash(); router.subscribe((_) => window.scrollTo(0, 0)); The router.mode.hash function turns on hash router mode. The router.subscribe callback is nice to reset the page to the top on page transfers# Adding some transition componentsThese component will manage the transition between one page to another page when routing.Create a directory under the src directory called components and add these two files:# announcer.svelte import { router } from \"tinro\"; $: current = $router.path === \"/\" ? \"Home\" : $router.path.slice(1); {#key current} Navigated to {current} {/key} div { position: absolute; left: 0; top: 0; clip: rect(0 0 0 0); clip-path: inset(50%); overflow: hidden; white-space: nowrap; width: 1px; height: 1px; } This component is for screen readers announcing when a page changes# transition.svelte import { router } from \"tinro\"; import { fade } from \"svelte/transition\"; {#key $router.path} {/key} This component adds a fade to the page transition# Adding Routes to the app ... import Announcer from \"./components/announcer.svelte\"; import Transition from \"./components/transition.svelte\"; ... Adding the Announcer and Transition components to our routing system will handle announcing page transitions as well as animating the transition.# Create some pages# home.svelte let count = 0; function inc() { count += 1; } Hello Permaweb Inc Count: {count} About # about.svelteAbout Page Svelte/Vite About Page Home # Modify App.svelte ... import Home from './home.svelte' import About from './about.svelte' ... # Deploy Permanently# Generate WalletWe need the arweave package to generate a walletNPMYARNnpm install --save arweave yarn add arweave -D then run this command in the terminalnode -e \"require('arweave').init({}).wallets.generate().then(JSON.stringify).then(console.log.bind(console))\" > wallet.json # Fund WalletYou will need to fund your wallet with ArDrive Turbo credits. To do this, enter ArDriveopen in new window and import your wallet. Then, you can purchase turbo credits for your wallet.# Setup Permaweb-DeployNPMYARNnpm install --global permaweb-deploy yarn global add permaweb-deploy # Update vite.config.tsimport { defineConfig } from 'vite' import { svelte } from '@sveltejs/vite-plugin-svelte' export default defineConfig({ plugins: [svelte()], base: './' }) # Update package.json# Update package.json{ ... \"scripts\": { ... \"deploy\": \"DEPLOY_KEY=$(base64 -i wallet.json) permaweb-deploy --ant-process > --deploy-folder build\" } ... } Replace > with your ANT process id.# Run buildNow it is time to generate a build, runNPMYARNnpm run build yarn build # Run deployFinally we are good to deploy our first Permaweb ApplicationNPMYARNnpm run deploy yarn deploy ERRORIf you receive an error Insufficient funds, make sure you remembered to fund your deployment wallet with ArDrive Turbo credits.# ResponseYou should see a response similar to the following:Deployed TxId [>] to ANT [>] using undername [>] Your Svelte app can be found at https://arweave.net/>.SUCCESSYou should now have a Svelte Application on the Permaweb! Great Job!# SummaryThis is a minimal version of publishing a Svelte application on the permaweb, but you may want more features, like hot-reloading and tailwind, etc. Check out hypar for a turnkey starter kit. HypARopen in new window Minimal","estimatedWords":657,"lastModified":"2025-10-20T12:14:22.738Z","breadcrumbs":["kits","svelte","vite"],"siteKey":"arweave","siteName":"Arweave Cookbook","depth":3,"crawledAt":"2025-10-20T12:14:22.739Z"},{"url":"https://cookbook.arweave.net/kits/svelte/minimal.html","title":"Minimal Svelte Starter Kit","content":"Contributors: Dylan ShadeLast Updated: Edit# Minimal Svelte Starter KitThis guide will walk you through in a step by step flow to configure your development environment to build and deploy a permaweb application.# PrerequisitesKnow typescriptNodeJS v18 or greaterKnow Svelte - https://svelte.devopen in new windowKnow git and common terminal commands# Development DependenciesTypeScriptesbuildw3# Steps# Create ProjectNPMYARNmkdir myproject cd myproject npm init -y npm install -D svelte esbuild typescript esbuild-svelte tinro svelte-preprocess mkdir myproject cd myproject yarn init -y yarn add -D svelte esbuild typescript esbuild-svelte tinro svelte-preprocess # Create buildscript.jsimport fs from \"fs\"; import esbuild from \"esbuild\"; import esbuildSvelte from \"esbuild-svelte\"; import sveltePreprocess from \"svelte-preprocess\"; //make sure the directoy exists before stuff gets put into it if (!fs.existsSync(\"./dist/\")) { fs.mkdirSync(\"./dist/\"); } esbuild .build({ entryPoints: [`./src/main.ts`], bundle: true, outdir: `./dist`, mainFields: [\"svelte\", \"browser\", \"module\", \"main\"], // logLevel: `info`, splitting: true, write: true, format: `esm`, plugins: [ esbuildSvelte({ preprocess: sveltePreprocess(), }), ], }) .catch((error, location) => { console.warn(`Errors: `, error, location); process.exit(1); }); //use a basic html file to test with fs.copyFileSync(\"./index.html\", \"./dist/index.html\"); # Modify package.jsonSet type to module, add a build script{ \"type\": \"module\" ... \"scripts\": { \"build\": \"node buildscript.js\" } } # Create src directory and some src filesmkdir src touch src/main.ts touch src/app.svelte touch src/counter.svelte touch src/about.svelte # Main.tsimport App from \"./app.svelte\"; new App({ target: document.body, }); # app.svelte import { Route, router } from \"tinro\"; import Counter from \"./counter.svelte\"; import About from \"./about.svelte\"; // add hash routing for permaweb support router.mode.hash(); Home | About Hash RoutingYou will notice the router.mode.hash() setting in the script session, this is important to configure your application to use hash based routing, which will enable url support when running that application on a path, like https://[gateway]/[TX]# counter.svelte let count = 0; function inc() { count += 1; } Hello Permaweb Inc Count: {count} # about.svelteAbout Page Minimal About Page Home # Add index.html Vite + Svelte + TS # Deploy Permanently# Generate WalletWe need the arweave package to generate a walletNPMYARNnpm install --save arweave yarn add arweave -D then run this command in the terminalnode -e \"require('arweave').init({}).wallets.generate().then(JSON.stringify).then(console.log.bind(console))\" > wallet.json # Fund WalletYou will need to fund your wallet with ArDrive Turbo credits. To do this, enter ArDriveopen in new window and import your wallet. Then, you can purchase turbo credits for your wallet.# Setup Permaweb-DeployNPMYARNnpm install --global permaweb-deploy yarn global add permaweb-deploy # Update vite.config.tsimport { defineConfig } from 'vite' import { svelte } from '@sveltejs/vite-plugin-svelte' export default defineConfig({ plugins: [svelte()], base: './' }) # Update package.json{ ... \"scripts\": { ... \"deploy\": \"DEPLOY_KEY=$(base64 -i wallet.json) permaweb-deploy --ant-process > --deploy-folder build\" } ... } Replace > with your ANT process id.# Run buildNow it is time to generate a build, runNPMYARNnpm run build yarn build # Run deployFinally we are good to deploy our first Permaweb ApplicationNPMYARNnpm run deploy yarn deploy ERRORIf you receive an error Insufficient funds, make sure you remembered to fund your deployment wallet with ArDrive Turbo credits.# ResponseYou should see a response similar to the following:Deployed TxId [>] to ANT [>] using undername [>] Your Svelte app can be found at https://arweave.net/>.SUCCESSYou should now have a Svelte Application on the Permaweb! Great Job!# SummaryThis is a minimal version of publishing a Svelte application on the permaweb, but you may want more features, like hot-reloading and tailwind, etc. Check out hypar for a turnkey starter kit. HypARopen in new window Vite","estimatedWords":557,"lastModified":"2025-10-20T12:14:23.469Z","breadcrumbs":["kits","svelte","minimal"],"siteKey":"arweave","siteName":"Arweave Cookbook","depth":3,"crawledAt":"2025-10-20T12:14:23.469Z"},{"url":"https://cookbook.arweave.net/kits/svelte/index.html","title":"Svelte Starter Kits","content":"Contributors: Dylan ShadeLast Updated: Edit# Svelte Starter KitsSvelte is a framework that compiles to a JavaScript bundle and in the process removes the framework from the distribution of the app. This results in a much smaller footprint than other frameworks. Svelte is the perfect framework for Permaweb Applications. A Permaweb Application is built on the principles of a Single Page Application, but lives on the Arweave network and is distributed by Permaweb gateways.Svelte Starter Kit Guides:Minimal - the minimum required to build a svelte permaweb appVite - Svelte, Typescript and VitePermaweb Application Constraints100% Front-end application (No Server-Side Backend)Applications are served from a sub-path (https://[gateway]/[TX_ID]) React Vue","estimatedWords":106,"lastModified":"2025-10-20T12:14:24.049Z","breadcrumbs":["kits","svelte","index"],"siteKey":"arweave","siteName":"Arweave Cookbook","depth":3,"crawledAt":"2025-10-20T12:14:24.049Z"},{"url":"https://cookbook.arweave.net/kits/react/create-react-app.html","title":"Create React App Starter Kit","content":"Contributors: Dylan ShadeLast Updated: Edit# Create React App Starter KitThis guide will walk you through in a step by step flow to configure your development environment to build and deploy a permaweb react application.# PrerequisitesBasic Typescript Knowledge (Not Mandatory) - [https://www.typescriptlang.org/docs/](Learn Typescript)NodeJS v16.15.0 or greater - [https://nodejs.org/en/download/](Download NodeJS)Knowledge of ReactJS - [https://reactjs.org/](Learn ReactJS)Know git and common terminal commands# Development DependenciesTypeScriptNPM or Yarn Package Manager# Steps# Create ProjectIf you are not familiar with typescript you can exclude the extra check --template typescriptNPMYARNnpx create-react-app permaweb-create-react-app --template typescript yarn create react-app permaweb-create-react-app --template typescript # Change into the Project Directorycd permaweb-create-react-app # Install react-router-domYou have to install this package to manage routing between different pagesNPMYARNnpm install react-router-dom --save yarn add react-router-dom -D # Run the AppNow we need to check if everything is working before jumping into next step, runNPMYARNnpm start yarn start This will start a new development server locally on your machine. By default it uses `PORT 3000`, if this PORT is already in use it may ask you to switch to another available PORT in Terminal # Modify the package.json to contain the following config{ ... \"homepage\": \".\", } # Setup RoutingNow modify the application and add a new route such as an about page, first create 2 more .tsx files. (if you have exluceded the extra check --template typescript, then your component file extension should be .jsx or .js)touch src/HomePage.tsx touch src/About.tsx # HomePage.tsximport { Link } from \"react-router-dom\"; function HomePage() { return ( Welcome to the Permaweb! About ); } export default HomePage; # About.tsximport { Link } from \"react-router-dom\"; function About() { return ( Welcome to the About page! Home ); } export default About; # Modify App.tsxWe need to update the App.tsx to manage the different pagesimport { HashRouter } from \"react-router-dom\"; import { Routes, Route } from \"react-router-dom\"; import HomePage from \"./HomePage\"; import About from \"./About\"; function App() { return ( } /> } /> ); } export default App; Hash RoutingNote that we are wrapping the routes in a HashRouter and using the react-router-dom Link component to build links. This is important on the permaweb in its current state, it will ensure the routes work properly because applications are served on a path like https://[gateway]/[TX]# Deploy Permanently# Generate WalletExisting WalletThis step will generate a new, empty, Arweave wallet. If you already have an existing Arweave wallet you may provide its keyfile and skip this step.We need the arweave package to generate a walletNPMYARNnpm install --save arweave yarn add arweave -D then run this command in the terminalnode -e \"require('arweave').init({}).wallets.generate().then(JSON.stringify).then(console.log.bind(console))\" > wallet.json It is very important to make sure that your wallet file is not included in any folder you want uploaded to Arweave.# Setup TurboWe need Turbo to deploy our app to the Permaweb.# Fund WalletYou will need to fund your wallet with ArDrive Turbo credits. To do this, enter ArDriveopen in new window and import your wallet. Then, you can purchase turbo credits for your wallet.# Setup Permaweb-DeployNPMYARNnpm install --global permaweb-deploy yarn global add permaweb-deploy # Fund Your WalletTurbo uses Turbo Credits to upload data to Arweave. You can purchase Turbo Credits with a variety of fiat currencies or crypto tokens. Below is an example for funding your wallet with 10 USD. It will open a browser window to complete the purchase using Stripe.npm install @ardrive/turbo-sdk turbo top-up --wallet-file wallet.json --currency USD --value 10 Be sure to replace wallet.json with the path to your Arweave wallet.# Update package.json{ ... \"scripts\": { ... \"deploy\": \"turbo upload-folder --folder-path ./build --wallet-file wallet.json > latest-manifest.json\" } ... } This will upload your build folder to the permaweb, and save all of the details of the upload to a file named \"latest-manifest.json\". That way, you'll have a reference for the manifest TxId to use later.# Run buildNow it is time to generate a build, runNPMYARNnpm run build yarn build # Run deployFinally we are good to deploy our first Permaweb ApplicationNPMYARNnpm run deploy yarn deploy ERRORIf you receive an error Insufficient funds, make sure you remembered to fund your deployment wallet with ArDrive Turbo credits.# ResponseYou should see a response similar to the following:Deployed TxId [>] to ANT [>] using undername [>] Your React app can be found at https://arweave.net/>.SUCCESSYou should now have a React Application on the Permaweb! Great Job!# SummaryThis is a Create React App version of publishing a React app on the permaweb. You may discover new ways to deploy an app on the permaweb or checkout other starter kits in this guide! Vite + permaweb-deploy","estimatedWords":751,"lastModified":"2025-10-20T12:14:24.880Z","breadcrumbs":["kits","react","create react app"],"siteKey":"arweave","siteName":"Arweave Cookbook","depth":3,"crawledAt":"2025-10-20T12:14:24.880Z"},{"url":"https://cookbook.arweave.net/kits/react/turbo.html","title":"React Starter Kit wvite  ArDrive","content":"Contributors: Dylan ShadeLast Updated: Edit# React Starter Kit w/vite function Home() { return ( Welcome to the Permaweb! About ); } export default Home; src/About.tsximport { Link } from \"react-router-dom\"; function About() { return ( Welcome to the About page! Home ); } export default About; # Modify App.tsxWe need to update the App.tsx to manage different pagesimport { HashRouter } from \"react-router-dom\"; import { Routes, Route } from \"react-router-dom\"; import Home from \"./Home\"; import About from \"./About\"; function App() { return ( } /> } /> ); } export default App; # Modify index.cssAlter the body selectorbody { margin: 0; padding-top: 200px; display: flex; flex-direction: column; place-items: center; min-width: 100%; min-height: 100vh; } Run the projectNPMYARNnpm run dev yarn dev # Building React App# Modify vite.config.tsimport { defineConfig } from 'vite' import react from '@vitejs/plugin-react' // https://vitejs.dev/config/ export default defineConfig({ base: \"\", plugins: [react()], }) # Build Appyarn build # Deploy Permanently# Generate WalletWe need the arweave package to generate a walletNPMYARNnpm install --save arweave yarn add arweave -D then run this command in the terminalnode -e \"require('arweave').init({}).wallets.generate().then(JSON.stringify).then(console.log.bind(console))\" > wallet.json # Fund WalletYou will need to fund your wallet with ArDrive Turbo credits. To do this, enter ArDriveopen in new window and import your wallet. Then, you can purchase turbo credits for your wallet.# Setup Permaweb-DeployNPMYARNnpm install --save-dev permaweb-deploy yarn add permaweb-deploy --dev --ignore-engines You will need to add AR to your wallet and fund it with Turbo credits to be able to upload this app. See Turbo SDKopen in new window for more information.# Update package.json{ ... \"scripts\": { ... \"deploy\": \"npm run build && permaweb-deploy --arns-name my-react-app\" } ... } Replace my-react-app with your actual ArNS name. You can also add additional options like --undername staging for staging deployments.# Run buildNow it is time to generate a build, runNPMYARNnpm run build yarn build # Run deployFinally we are good to deploy our first Permaweb ApplicationNPMYARNnpm run deploy yarn deploy Insufficient FundsIf you receive an error Insufficient funds, make sure you remembered to fund your deployment wallet with Turbo credits. See Turbo SDKopen in new window for more information.# ResponseYou should see a response similar to the following:-------------------- DEPLOY DETAILS -------------------- Tx ID: abc123def456ghi789jkl012mno345pqr678stu901v ArNS Name: my-react-app Undername: @ ANT: xyz789abc012def345ghi678jkl901mno234pqr567s AR IO Process: bh9l1cy0aksiL_x9M359faGzM_yjralacHIUo8_nQXM TTL Seconds: 3600 -------------------------------------------------------- Deployed TxId [abc123def456ghi789jkl012mno345pqr678stu901v] to name [my-react-app] for ANT [xyz789abc012def345ghi678jkl901mno234pqr567s] using undername [@] Your React app can be found at https://my-react-app.arweave.net (if using ArNS) or https://arweave.net/abc123def456ghi789jkl012mno345pqr678stu901v.SUCCESSYou should now have a React Application on the Permaweb! Great Job!# Congrats!You just published a react application on the Permaweb! This app will be hosted forever! Create React App","estimatedWords":435,"lastModified":"2025-10-20T12:14:25.503Z","breadcrumbs":["kits","react","turbo"],"siteKey":"arweave","siteName":"Arweave Cookbook","depth":3,"crawledAt":"2025-10-20T12:14:25.504Z"},{"url":"https://cookbook.arweave.net/kits/react/index.html","title":"React Starter Kits","content":"Contributors: Dylan ShadeLast Updated: Edit# React Starter KitsReact is a popular library used for building user interfaces. Alongside other popular tools such as create-react-app, a React project can be compiled into a bundle. This bundle can be uploaded as a transaction to the permaweb where it will serve as a single page application.React Starter Kit Guides:Vite - React + Vite, publish with permaweb-deployCreate React App - utilize Create React App to build a React permaweb appPermaweb Application Constraints100% Front-end application (No Server-Side Backend)Applications are served from a sub-path (https://[gateway]/[TX_ID]) Svelte","estimatedWords":90,"lastModified":"2025-10-20T12:14:26.244Z","breadcrumbs":["kits","react","index"],"siteKey":"arweave","siteName":"Arweave Cookbook","depth":3,"crawledAt":"2025-10-20T12:14:26.244Z"},{"url":"https://cookbook.arweave.net/fundamentals/decentralized-computing/hyperbeam/hyperbeam-devices.html","title":"HyperBEAM Devices","content":"Contributors: Dylan ShadeLast Updated: Edit# HyperBEAM DevicesHyperBEAM devices are the modular building blocks that power the AO Computer. Think of them as specialized engines or services that can be plugged into the AO framework to provide specific computational capabilities. This modularity is key to AO's flexibility, extensibility, and ability to evolve with new technologies.# Understanding the Device Architecture# What Are Devices?In AO-Core and HyperBEAM, Devices are modular components responsible for processing and interpreting Messages. They define the specific logic for how computations are performed, data is handled, or interactions occur within the AO ecosystem.Each device is essentially an Erlang module that implements a specific interface, allowing it to:Define computation logic - Dictate how message instructions are executedEnable specialization - Allow nodes to focus on specific computational tasksPromote modularity - Add new functionality without altering the core protocolDistribute workload - Handle different parts of complex tasks in parallelHyperBEAM Device Architecture:HTTP Request ↓ HyperBEAM Router ↓ Device Selection ↓ ┌─────────────────────────────────────────┐ │ Device Types: │ │ • ~process@1.0 → Process State Mgmt │ │ • ~lua@5.3a → Lua Script Execution │ │ • ~wasm64@1.0 → WebAssembly Execution│ │ • ~json@1.0 → JSON Processing │ └─────────────────────────────────────────┘ ↓ Processing Results ↓ HTTP Response Device Ecosystem: ┌─────────────────────────────────────────┐ │ • Security Devices (authentication) │ │ • Utility Devices (routing, caching) │ │ • Custom Devices (domain-specific) │ │ • Communication Devices (relays) │ │ • Storage Devices (state management) │ └─────────────────────────────────────────┘ This modular architecture allows HyperBEAM to handle diverse computational tasks by routing requests to specialized devices, each optimized for specific types of processing.# Device Naming and VersioningDevices follow a consistent naming convention that makes them easy to identify and use:Format: ~name@version or dev_name (for internal devices)Examples:~process@1.0 - Primary process management device~lua@5.3a - Lua 5.3 execution device~wasm64@1.0 - WebAssembly 64-bit execution devicedev_router - Internal routing device (development prefix)The tilde (~) indicates a primary, user-facing device, while the dev_ prefix is used for internal or utility devices in the source code.# Versioning StrategyVersioning indicates the specific interface and behavior of the device:Semantic versioning - Major.minor.patch formatBackward compatibility - Breaking changes increment major versionFeature additions - New features increment minor versionBug fixes - Patches increment patch version# Core HyperBEAM Devices# Process Management Devices# ~process@1.0 - Process State ManagerThe process device manages persistent, shared computational states similar to traditional smart contracts, but with greater flexibility.# Access current process state GET /PROCESS_ID~process@1.0/now # Get cached state (faster) GET /PROCESS_ID~process@1.0/compute # Access specific state fields GET /PROCESS_ID~process@1.0/now/balance GET /PROCESS_ID~process@1.0/compute/users/USER_ADDRESS Key Functions:now - Calculates real-time process state by processing all messagescompute - Returns the latest known state without checking for new messagesState persistence - Automatic state snapshots to ArweaveMessage ordering - Ensures deterministic state transitionsUse Cases:Token contracts and DeFi applicationsVoting and governance systemsGame state managementDecentralized databases# ~scheduler@1.0 - Message SchedulingHandles the ordering and execution timing of messages within processes.# Query scheduler status GET /PROCESS_ID~scheduler@1.0/status # Get message queue information GET /PROCESS_ID~scheduler@1.0/queue/pending Responsibilities:Message ordering and consensusExecution timing coordinationLoad balancing across compute unitsFault tolerance and recovery# Execution Devices# ~lua@5.3a - Lua Script ExecutionExecutes Lua scripts for serverless functions and data processing.# Simple calculation GET /~lua@5.3a&script=return 2 + 3 * 4/result # With parameters GET /~lua@5.3a&script=return Args.name .. \" is \" .. Args.age .. \" years old\"&name=\"Alice\"&age+integer=25/result # Using modules GET /~lua@5.3a&module=MODULE_TX_ID&script=return math_utils.factorial(Args.n)&n+integer=5/result Capabilities:Full Lua 5.3 language supportModule loading from Arweave transactionsJSON processing and manipulationString processing and regexMathematical computationsHTTP client functionality (via libraries)Performance Characteristics:Lightweight execution overheadFast startup time (no cold starts)Memory efficient for small to medium computationsExcellent for data transformation and business logic# ~wasm64@1.0 - WebAssembly ExecutionExecutes WebAssembly code for high-performance computations written in languages like Rust, C++, Go, and others.# Execute WASM module GET /~wasm64@1.0&module=WASM_MODULE_TX_ID/function_name # With parameters GET /~wasm64@1.0&module=WASM_MODULE_TX_ID&arg1+integer=100&arg2=\"test\"/compute Advantages:High performance - Near-native execution speedMultiple languages - Support for Rust, C++, Go, AssemblyScriptSandboxed execution - Secure isolated environmentPredictable performance - No garbage collection pausesUse Cases:Cryptographic operations (hashing, signatures, ZK proofs)Image and video processingMachine learning inferenceScientific computingGame engines and simulationsExample WASM Module (Rust):// Compile to WASM and deploy to Arweave use wasm_bindgen::prelude::*; #[wasm_bindgen] pub fn fibonacci(n: u32) -> u32 { match n { 0 | 1 => n, _ => fibonacci(n - 1) + fibonacci(n - 2), } } #[wasm_bindgen] pub fn hash_data(data: &str) -> String { use sha2::{Sha256, Digest}; let mut hasher = Sha256::new(); hasher.update(data); format!(\"{:x}\", hasher.finalize()) } # Data Processing Devices# ~json@1.0 - JSON ManipulationProvides JSON data structure access and manipulation capabilities.# Format process state as JSON GET /PROCESS_ID~process@1.0/now~json@1.0 # Pretty-print JSON GET /PROCESS_ID~process@1.0/compute~json@1.0&pretty=true # Extract specific JSON fields GET /~json@1.0&data={\"users\":{\"alice\":{\"balance\":100}}}/users/alice/balance Features:JSON serialization and deserializationPath-based field accessPretty printing and formattingSchema validation (when configured)Type conversion and casting# ~message@1.0 - Message ProcessingThe default device that resolves keys to their literal values within messages.# Create temporary message with data GET /~message@1.0&greeting=\"Hello\"&count+integer=42/count # Response: 42 # Complex data structures GET /~message@1.0&config+map=host=\"localhost\";port+integer=3000&items+list=\"a\",\"b\",\"c\"/config/port # Response: 3000 Type Casting Support:+integer - Convert to integer+float - Convert to floating point+boolean - Convert to boolean+list - Parse comma-separated values+map - Parse key-value pairs+binary - Treat as binary string (default)# Communication Devices# ~relay@1.0 - Message RelayForwards messages between AO nodes or to external HTTP endpoints.# Relay GET request to external API GET /~relay@1.0/call?method=GET&path=https://api.example.com/data # Relay POST with data POST /~relay@1.0/call?method=POST&path=https://webhook.site/your-webhook Content-Type: application/json {\"message\": \"Hello from AO\"} # Relay to another AO process GET /~relay@1.0/process/TARGET_PROCESS_ID?action=GetBalance&user=ALICE Use Cases:Cross-chain bridges - Connect to other blockchain networksExternal API integration - Fetch data from Web2 servicesInter-process communication - Route messages between AO processesWebhook delivery - Send notifications to external services# Security and Verification Devices# ~snp@1.0 - Secure Enclave VerificationHandles Trusted Execution Environment (TEE) attestation and verification.# Get TEE attestation report GET /~snp@1.0/attestation # Verify node is running in genuine TEE GET /~snp@1.0/verify Security Features:AMD SEV-SNP attestationIntel TXT supportHardware security verificationRemote attestation protocolsCryptographic proof generation# dev_codec_httpsig - HTTP Signature ProcessingManages HTTP message signing and verification for authentication.Capabilities:HTTP signature generation and verificationMultiple signature algorithms (RSA, ECDSA, EdDSA)Request/response integrity verificationAuthentication and authorization# Utility and System Devices# ~meta@1.0 - Node ConfigurationConfigures the HyperBEAM node itself including hardware specs, supported devices, and payment information.# Get node capabilities GET /~meta@1.0/capabilities # Get supported devices GET /~meta@1.0/devices # Get node status GET /~meta@1.0/status Configuration Options:Hardware specificationsAvailable compute resourcesSupported device listPayment and billing informationNetwork connectivity options# dev_cron - Task SchedulingCoordinates scheduled task execution and workflow management.Features:Cron-like task schedulingRecurring job managementEvent-driven automationWorkflow orchestration# dev_monitor - System MonitoringMonitors process activity, performance metrics, and system health.Monitoring Capabilities:Process execution metricsResource utilization trackingError rate monitoringPerformance benchmarkingAlert generation# Financial and Access Control Devices# ~p4@1.0 - Payment ProcessingManages metering, billing, and micropayments for node services.# Check payment status GET /~p4@1.0/balance/USER_ADDRESS # Get pricing information GET /~p4@1.0/pricing/compute Payment Features:Micropayment processingUsage-based billingMulti-token supportPayment channel managementRevenue sharing protocols# ~faff@1.0 - Access ControlHandles authorization and access control for protected resources.Access Control Features:Role-based access control (RBAC)Attribute-based access control (ABAC)Token-based authenticationMulti-signature authorizationTemporary access grants# Data Storage and Management# ~patch@1.0 - State ManagementApplies state updates directly to processes, often used for data migration and management.# Apply state patch to process POST /PROCESS_ID~patch@1.0/apply Content-Type: application/json { \"operation\": \"update\", \"path\": \"/users/alice/balance\", \"value\": 1500 } # Get patch history GET /PROCESS_ID~patch@1.0/history Patch Operations:State updates and migrationsData consistency maintenanceVersion control for process stateRollback and recovery operations# Advanced Device Concepts# Device Composition and PipelinesDevices can be chained together to create sophisticated processing pipelines:# Multi-device pipeline: # 1. Get process state # 2. Transform with Lua # 3. Format as JSON # 4. Apply template GET /TOKEN_PROCESS~process@1.0/now/~lua@5.3a&module=ANALYTICS_MODULE/calculateMetrics/~json@1.0/format~template@1.0&type=dashboard # Device SpecializationNodes can choose which devices to support, allowing for specialization:Compute-Optimized Nodes:Focus on ~wasm64@1.0 and ~lua@5.3a devicesHigh-performance processors and memoryOptimized for CPU-intensive workloadsStorage-Optimized Nodes:Specialize in ~process@1.0 and ~patch@1.0 devicesLarge storage capacity and fast I/OOptimized for state management and data persistenceSecurity-Focused Nodes:Run ~snp@1.0 and security-related devicesHardware security modules (HSMs)Trusted Execution Environments (TEEs)# Custom Device DevelopmentWhile HyperBEAM comes with a comprehensive set of built-in devices, you can create custom devices in Erlang to extend functionality for specialized use cases. This is an advanced topic that allows you to build domain-specific functionality tailored to your exact needs.For detailed guidance on building custom devices, see the HyperBEAM Device Development Guideopen in new window.# Device Discovery and RoutingHyperBEAM automatically routes requests to the appropriate devices based on the URL path. You can discover available devices on any node:# List all available devices GET /~meta@1.0/devices # Get information about a specific device GET /~meta@1.0/device/~lua@5.3a Devices are automatically load-balanced across available instances, with HyperBEAM handling routing optimization internally.# Performance ConsiderationsDifferent devices have varying performance characteristics:~lua@5.3a - Fast startup, low resource usage, ideal for simple logic~wasm64@1.0 - Higher performance for complex computations~process@1.0 - Use /compute for cached state, /now for real-time updates~json@1.0 - Very lightweight for data serializationOptimization Tips:Use device pipelines to chain operations in a single requestCache frequently accessed data at the application levelChoose the right device for your workload (Lua for simple logic, WASM for computation)# Extensible Device EcosystemThe modular nature of HyperBEAM devices enables endless possibilities for expansion. The community and ecosystem are continuously developing new devices for:Specialized Hardware - GPU computing, AI/ML acceleration, quantum computingDomain-Specific Logic - DeFi protocols, scientific computing, media processingCross-Chain Integration - Bridges to other blockchain networksIndustry Solutions - Custom devices for specific business needsThis extensibility ensures HyperBEAM can adapt to new technologies and use cases without requiring changes to the core protocol.# Security ConsiderationsHyperBEAM devices run in isolated environments with built-in security features:Sandboxing - Each device operates in its own isolated environmentResource Limits - Automatic memory and execution time constraintsVerification - Device signatures and integrity checkingAccess Control - Permission-based device accessBest Practices:Always specify device versions (e.g., ~lua@5.3a not just ~lua)Validate inputs when building applications that use devicesUse TEE-enabled nodes (~snp@1.0) for sensitive computations# Next StepsExplore the broader HyperBEAM ecosystem:Build Custom Devices: Device Development Guideopen in new windowLua Programming: Lua Serverless FunctionsProcess Integration: AO Process DevelopmentProduction Deployment: Builder's Journey# ResourcesHyperBEAM Device Documentation: Official Device Docsopen in new windowErlang/OTP Documentation: Erlang Referenceopen in new window Lua Serverless Functions","estimatedWords":1608,"lastModified":"2025-10-20T12:14:27.137Z","breadcrumbs":["fundamentals","decentralized computing","hyperbeam","hyperbeam devices"],"siteKey":"arweave","siteName":"Arweave Cookbook","depth":4,"crawledAt":"2025-10-20T12:14:27.138Z"},{"url":"https://cookbook.arweave.net/fundamentals/decentralized-computing/hyperbeam/lua-serverless.html","title":"Lua Serverless Functions","content":"Contributors: Dylan ShadeLast Updated: Edit# Lua Serverless FunctionsHyperBEAM's ~lua@5.3a device enables you to run serverless Lua functions with permanent availability, instant execution, and cryptographic verification. Unlike traditional serverless platforms, your functions are deployed permanently to Arweave and executed on the decentralized HyperBEAM network.# How HyperBEAM Lua WorksHyperBEAM executes Lua functions through HTTP requests using a specific URL structure:https://forward.computer/~lua@5.3a/{function_name}/~json@1.0/serialize?param1=value1 queryParams.append('module', moduleId); for (const [key, value] of Object.entries(params)) { if (typeof value === 'number') { queryParams.append(`${key}+${Number.isInteger(value) ? 'integer' : 'float'}`, value.toString()); } else { queryParams.append(key, value.toString()); } } const url = `https://forward.computer/~lua@5.3a/${functionName}/~json@1.0/serialize?${queryParams}`; const response = await fetch(url); if (!response.ok) { throw new Error(`HTTP ${response.status}: ${response.statusText}`); } return await response.json(); } // Usage const result = await callLuaFunction('YOUR_MODULE_ID', 'add', { a: 10, b: 20 }); console.log(result); // { a: 10, b: 20, result: 30, operation: \"addition\" } Getting AO Process State HyperBEAM Devices","estimatedWords":140,"lastModified":"2025-10-20T12:14:27.767Z","breadcrumbs":["fundamentals","decentralized computing","hyperbeam","lua serverless"],"siteKey":"arweave","siteName":"Arweave Cookbook","depth":4,"crawledAt":"2025-10-20T12:14:27.767Z"},{"url":"https://cookbook.arweave.net/fundamentals/decentralized-computing/hyperbeam/getting-ao-state.html","title":"Exposing Process State to HyperBEAM","content":"Contributors: Dylan ShadeLast Updated: Edit# Exposing Process State to HyperBEAMHyperBEAM introduces a powerful feature for exposing parts of a process's state for immediate reading over HTTP. This improves performance for web frontends and data services by replacing the need for dryrun calls.# The Patch DeviceThe ~patch@1.0 device is the mechanism that allows AO processes to make parts of their internal state readable via direct HTTP GET requests.# How it WorksExposing state is a four-step process:Process Logic: Send an outbound message to the ~patch@1.0 device from your process.Patch Message Format: The message must include device and cache tags.Send({ Target = ao.id, device = 'patch@1.0', cache = { mydatakey = MyValue } }) HyperBEAM Execution: HyperBEAM's dev_patch module processes this message, mapping the key-value pairs from the cache table to a URL path.HTTP Access: The exposed data is then immediately available via a standard HTTP GET request:GET /~process@1.0/now/cache/ # Initial State Sync (Optional)To make data available immediately on process creation:-- Place this logic at the top level of your process script Balances = { token1 = 100, token2 = 200 } -- A table of balances TotalSupply = 1984 -- A single total supply value -- 1. Initialize Flag: InitialSync = InitialSync or 'INCOMPLETE' -- 2. Check Flag: if InitialSync == 'INCOMPLETE' then -- 3. Patch State: Send({ device = 'patch@1.0', cache = { balances = Balances, totalsupply = TotalSupply } }) -- 4. Update Flag: InitialSync = 'COMPLETE' end # Practical ExampleHere's a complete example of a token contract that exposes its balance state:-- Token Process with State Exposure Balances = Balances or { [ao.id] = 1000000 } TotalSupply = TotalSupply or 1000000 -- Initial state sync InitialSync = InitialSync or 'INCOMPLETE' if InitialSync == 'INCOMPLETE' then Send({ device = 'patch@1.0', cache = { balances = Balances, totalsupply = TotalSupply }}) InitialSync = 'COMPLETE' end -- Transfer handler Handlers.add(\"transfer\", \"Transfer\", function(msg) local from = msg.From local to = msg.Tags.Recipient or msg.Recipient local amount = tonumber(msg.Tags.Quantity or msg.Quantity) if Balances[from] and Balances[from] >= amount then Balances[from] = Balances[from] - amount Balances[to] = (Balances[to] or 0) + amount -- Update exposed state after transfer Send({ device = 'patch@1.0', cache = { balances = Balances }}) ao.send({ Target = from, Data = \"Transfer successful\" }) else ao.send({ Target = from, Data = \"Insufficient balance\" }) end end) # Accessing Exposed DataOnce state is exposed via the patch device, you can query it directly over HTTP:# Get all balances curl https://hyperbeam-node.arweave.net/~process@1.0/compute/cache/balances # Get total supply curl https://hyperbeam-node.arweave.net/~process@1.0/compute/cache/totalsupply # BenefitsPerformance: Direct HTTP access is significantly faster than traditional dryrun calls.Simplicity: Standard REST-like patterns instead of complex message handling.Real-time Updates: State changes are immediately reflected in HTTP responses.Caching: HyperBEAM can cache frequently accessed data for even better performance. HyperBEAM Introduction Lua Serverless Functions","estimatedWords":456,"lastModified":"2025-10-20T12:14:28.356Z","breadcrumbs":["fundamentals","decentralized computing","hyperbeam","getting ao state"],"siteKey":"arweave","siteName":"Arweave Cookbook","depth":4,"crawledAt":"2025-10-20T12:14:28.356Z"},{"url":"https://cookbook.arweave.net/fundamentals/decentralized-computing/hyperbeam/hyperbeam-introduction.html","title":"HyperBEAM Introduction","content":"Contributors: Dylan ShadeLast Updated: Edit# HyperBEAM IntroductionHyperBEAM is the primary, production-ready implementation of the AO-Core protocol, built on the robust Erlang/OTP framework. It serves as a decentralized operating system, powering the AO Computer—a scalable, trust-minimized, distributed supercomputer built on the permanent storage of Arweave.For the most current technical specifications and implementation details, refer to the official HyperBEAM documentationopen in new window.# What is HyperBEAM?Think of HyperBEAM as your \"Swiss Army knife\" for decentralized development. It's not a single-purpose application, but rather a powerful, extensible engine that transforms the abstract concepts of AO-Core into a concrete, operational system.HyperBEAM provides the runtime environment and essential services to execute computations across a network of distributed nodes, making the AO Computer accessible through familiar web technologies like HTTP.HyperBEAM serves as the bridge between standard HTTP interfaces and the AO Computer, managing message routing, device execution, state queries, and cryptographic verification.# Core AO-Core Concepts in HyperBEAMHyperBEAM implements the three fundamental components of AO-Core:# Messages: Modular Data PacketsIn HyperBEAM, every interaction within the AO Computer is handled as a message. Messages are cryptographically-linked data units that form the foundation for communication, allowing processes to trigger computations, query state, and transfer value.// Example message structure in HyperBEAM const message = { Id: \"MESSAGE_TX_ID\", Process: \"TARGET_PROCESS_ID\", Owner: \"SENDER_ADDRESS\", Data: \"Hello, AO Computer!\", Tags: [ { name: \"Action\", value: \"Greet\" }, { name: \"Device\", value: \"~lua@5.3a\" } ] }; HyperBEAM nodes are responsible for routing and processing these messages according to the rules of the AO-Core protocol, ensuring reliable delivery and execution.# Devices: Extensible Execution EnginesHyperBEAM introduces a uniquely modular architecture centered around Devices. These pluggable components are Erlang modules that define specific computational logic—like running WASM, managing state, or relaying data—allowing for unprecedented flexibility.Common HyperBEAM Devices:~process@1.0: Manages persistent, shared computational states (like smart contracts)~lua@5.3a: Executes Lua scripts for serverless functions~wasm64@1.0: Executes WebAssembly code for high-performance computing~json@1.0: Provides JSON data structure manipulation~relay@1.0: Forwards messages between nodes or external HTTP endpoints~scheduler@1.0: Handles message ordering and execution timingFor a complete list of devices and their latest specifications, see the HyperBEAM Devices documentationopen in new window.# Using the Lua device to execute a calculation GET /~lua@5.3a _ -> {error, unknown_action, State} end. # Hardware AbstractionHyperBEAM abstracts away underlying hardware differences, allowing diverse nodes to contribute resources without compatibility issues. Whether running on:Consumer laptopsEnterprise serversCloud instancesEdge devicesSpecialized hardware (GPUs, TPUs)All nodes can participate in the AO Computer through the common HyperBEAM interface.# Use Cases and Applications# Serverless Computing with Trustless GuaranteesReplace traditional cloud functions with permanently available, cryptographically verifiable compute:// Traditional serverless function exports.handler = async (event) => { return { statusCode: 200, body: \"Hello World\" }; }; // HyperBEAM equivalent - permanently available on AO // Accessible via: /PROCESS~process@1.0/hello Handlers.add( \"hello\", function() { return true; }, function(msg) { ao.send({ Target = msg.From, Data = \"Hello from the permanent web!\" }) } ) # Hybrid Smart Contract + Serverless ApplicationsCombine persistent state management with on-demand compute:# Smart contract state management GET /TOKEN_CONTRACT~process@1.0/now/balance # + Serverless data processing GET /DATA_PROCESSOR~lua@5.3a // Example: Query process state from a HyperBEAM node const processId = 'YOUR_PROCESS_ID'; const nodeUrl = 'YOUR_HYPERBEAM_NODE_URL'; // Replace with verified node URL const response = await fetch(`${nodeUrl}/${processId}~process@1.0/now`); const state = await response.json(); # Basic OperationsQuery Process State:GET /PROCESS_ID~process@1.0/compute # Returns the latest known state (faster) GET /PROCESS_ID~process@1.0/now # Returns real-time state (slower, more accurate) Execute Lua Code:GET /~lua@5.3a const ao = connect({ MU_URL: \"YOUR_HYPERBEAM_NODE_URL\" // Replace with verified HyperBEAM node }); await ao.message({ process: \"PROCESS_ID\", tags: [ { name: \"Action\", value: \"Transfer\" }, { name: \"Recipient\", value: \"RECIPIENT_ADDRESS\" }, { name: \"Quantity\", value: \"100\" } ], signer: createDataItemSigner(wallet) }); # Security and Trust Model# Cryptographic VerificationEvery HyperBEAM response includes cryptographic proofs:Signatures: All responses signed by the executing nodeHashPaths: Verifiable computation trailsState Hashes: Merkle proofs of state integrityMessage IDs: Tamper-evident message identification# Trusted Execution Environments (TEEs)Many HyperBEAM nodes run in TEEs for additional security:# Verify node is running in genuine TEE GET /~snp@1.0/attestation # Returns cryptographic attestation report # Decentralized TrustNo single point of failure or control:Computation can be verified independentlyMultiple nodes can execute the same requestConsensus mechanisms for critical operationsOpen network - anyone can run a node# Performance Characteristics# ConcurrencyMillions of concurrent processes per nodeLightweight message passing between processesNon-blocking I/O for network operationsParallel device execution for complex pipelines# ScalabilityHorizontal scaling through additional nodesLoad balancing across node networkCaching layers for frequently accessed dataEventual consistency model for global state# EfficiencyCompiled code execution via BEAM VMMemory management with garbage collectionHot code reloading for zero-downtime updatesOptimized message serialization for network transport# Network EffectsAs more nodes join the HyperBEAM network:Increased Resilience: More redundancy and fault toleranceBetter Performance: Load distribution and edge computingEnhanced Security: More verification nodes and cryptographic diversityExpanded Capabilities: New devices and specialized servicesLower Costs: Competition drives down execution costs# Next StepsExplore HyperBEAM's capabilities in detail:Learn Querying: Querying AO Process StateBuild Serverless Functions: Lua Serverless FunctionsUnderstand Devices: HyperBEAM DevicesStart Building: Builder's Journey# ResourcesHyperBEAM Official Documentation: HyperBEAM Docsopen in new windowAO Cookbook: AO Documentationopen in new windowHyperBEAM Migration Guide: Migration from AO Connectopen in new windowNote: Node endpoints and availability may change over time. Always verify node status and select reliable endpoints for production use. Consider running your own HyperBEAM node for maximum reliability and control. AO Processes","estimatedWords":860,"lastModified":"2025-10-20T12:14:29.059Z","breadcrumbs":["fundamentals","decentralized computing","hyperbeam","hyperbeam introduction"],"siteKey":"arweave","siteName":"Arweave Cookbook","depth":4,"crawledAt":"2025-10-20T12:14:29.059Z"},{"url":"https://cookbook.arweave.net/fundamentals/decentralized-computing/ao-processes/what-are-ao-processes.html","title":"What are AO Processes","content":"Contributors: Dylan ShadeLast Updated: Edit# What are AO ProcessesAO processes are autonomous compute units that run on the Arweave network, enabling decentralized applications to execute complex logic permanently and trustlessly. Think of them as serverless functions that never go down and can maintain state across invocations.# Core ArchitectureAO processes represent a paradigm shift from traditional smart contracts. Unlike Ethereum's synchronous execution model, AO processes operate asynchronously, communicating through message passing in a distributed network.AO Process Architecture:User/Application ↓ (Message) AO Process ←→ Another Process ↓ ↑ (Messages) State Update/Read ↓ ↑ Arweave Storage This architecture demonstrates how AO processes communicate through asynchronous message passing while maintaining persistent state on Arweave. Each process operates independently while being able to interact with other processes in the network.# Key ComponentsProcess InstanceUnique process ID (43-character string)Lua-based execution environmentPersistent state storage on ArweaveMessage inbox for receiving communicationsMessage SystemAsynchronous message passingTagged messages for routing and filteringCryptographic signatures for authenticationPermanent message history on ArweaveState ManagementDeterministic state transitionsImmutable state snapshotsConflict-free replicated data types (CRDTs)Rollback and replay capabilities# Process Lifecycle# 1. Process CreationCreating an AO process involves deploying Lua code to the network:import { connect } from \"@permaweb/aoconnect\"; const ao = connect(); // Deploy a new process const processId = await ao.spawn({ module: \"MODULE_TX_ID\", // Pre-compiled Lua module scheduler: \"SCHEDULER_ADDRESS\", // Network scheduler signer: createDataItemSigner(wallet), // Wallet signer tags: [ { name: \"App-Name\", value: \"MyApp\" }, { name: \"App-Version\", value: \"1.0.0\" } ] }); console.log(\"Process created:\", processId); # 2. Process InitializationOnce spawned, the process can be initialized with initial state:// Send initialization message await ao.message({ process: processId, tags: [ { name: \"Action\", value: \"Initialize\" } ], data: JSON.stringify({ owner: \"USER_ADDRESS\", name: \"My Process\", version: \"1.0.0\" }), signer: createDataItemSigner(wallet) }); # 3. Message ProcessingProcesses receive and handle messages according to their Lua handlers:-- Example Lua handler in the process Handlers.add( \"Initialize\", Handlers.utils.hasMatchingTag(\"Action\", \"Initialize\"), function(msg) local data = json.decode(msg.Data) State.owner = data.owner State.name = data.name State.initialized = true ao.send({ Target = msg.From, Data = \"Process initialized successfully\" }) end ) # State Management Patterns# Deterministic State UpdatesAO processes maintain deterministic state through ordered message processing:-- State variables Balance = Balance or 0 Transactions = Transactions or {} -- Handler for balance updates Handlers.add( \"UpdateBalance\", Handlers.utils.hasMatchingTag(\"Action\", \"UpdateBalance\"), function(msg) local amount = tonumber(msg.Tags.Amount) local operation = msg.Tags.Operation if operation == \"credit\" then Balance = Balance + amount elseif operation == \"debit\" and Balance >= amount then Balance = Balance - amount else ao.send({ Target = msg.From, Data = \"Insufficient balance\" }) return end -- Record transaction table.insert(Transactions, { id = msg.Id, from = msg.From, amount = amount, operation = operation, timestamp = msg.Timestamp, balance = Balance }) ao.send({ Target = msg.From, Data = json.encode({ success = true, balance = Balance, transactionId = msg.Id }) }) end ) # State PersistenceState is automatically persisted to Arweave through the process lifecycle:-- State checkpoint handler Handlers.add( \"SaveCheckpoint\", Handlers.utils.hasMatchingTag(\"Action\", \"SaveCheckpoint\"), function(msg) local checkpoint = { balance = Balance, transactions = Transactions, lastUpdate = msg.Timestamp, version = \"1.0.0\" } -- State is automatically persisted ao.send({ Target = msg.From, Data = \"Checkpoint saved\", Tags = { { name = \"Checkpoint-Data\", value = json.encode(checkpoint) } } }) end ) # Common Use Cases# 1. Token ContractsAO processes excel at implementing token logic:-- Token contract implementation Name = \"MyToken\" Ticker = \"MTK\" Denomination = 12 TotalSupply = 1000000 * 10^Denomination Balances = { [Owner] = TotalSupply } Handlers.add( \"Transfer\", Handlers.utils.hasMatchingTag(\"Action\", \"Transfer\"), function(msg) local target = msg.Tags.Recipient local quantity = tonumber(msg.Tags.Quantity) if Balances[msg.From] and Balances[msg.From] >= quantity then Balances[msg.From] = Balances[msg.From] - quantity Balances[target] = (Balances[target] or 0) + quantity -- Emit events ao.send({ Target = msg.From, Data = \"Transfer successful\" }) ao.send({ Target = target, Data = \"Tokens received\" }) else ao.send({ Target = msg.From, Data = \"Insufficient balance\" }) end end ) # 2. Decentralized ApplicationsBuild complex dApps with multiple interacting processes:-- DAO voting process Proposals = Proposals or {} Votes = Votes or {} Handlers.add( \"CreateProposal\", Handlers.utils.hasMatchingTag(\"Action\", \"CreateProposal\"), function(msg) local proposalId = msg.Id Proposals[proposalId] = { title = msg.Tags.Title, description = msg.Data, creator = msg.From, created = msg.Timestamp, status = \"active\", votesFor = 0, votesAgainst = 0 } ao.send({ Target = msg.From, Data = \"Proposal created: \" .. proposalId }) end ) # 3. Data Processing PipelinesChain processes together for complex workflows:-- Data processing handler Handlers.add( \"ProcessData\", Handlers.utils.hasMatchingTag(\"Action\", \"ProcessData\"), function(msg) local data = json.decode(msg.Data) -- Process the data local processed = transformData(data) -- Send to next process in pipeline ao.send({ Target = msg.Tags.NextProcess, Data = json.encode(processed), Tags = { { name = \"Action\", value = \"ReceiveProcessedData\" }, { name = \"Source\", value = ao.id } } }) end ) # Process Communication Patterns# Direct MessagingProcesses communicate directly through tagged messages:// Send message to specific process await ao.message({ process: targetProcessId, tags: [ { name: \"Action\", value: \"GetBalance\" }, { name: \"Account\", value: userAddress } ], signer: createDataItemSigner(wallet) }); // Receive response const result = await ao.result({ message: messageId, process: targetProcessId }); # Pub/Sub PatternsImplement publish-subscribe messaging:-- Subscriber registration Subscribers = Subscribers or {} Handlers.add( \"Subscribe\", Handlers.utils.hasMatchingTag(\"Action\", \"Subscribe\"), function(msg) local topic = msg.Tags.Topic if not Subscribers[topic] then Subscribers[topic] = {} end table.insert(Subscribers[topic], msg.From) ao.send({ Target = msg.From, Data = \"Subscribed to \" .. topic }) end ) -- Broadcast messages Handlers.add( \"Publish\", Handlers.utils.hasMatchingTag(\"Action\", \"Publish\"), function(msg) local topic = msg.Tags.Topic if Subscribers[topic] then for _, subscriber in ipairs(Subscribers[topic]) do ao.send({ Target = subscriber, Data = msg.Data, Tags = { { name = \"Topic\", value = topic }, { name = \"Publisher\", value = msg.From } } }) end end end ) # Development Best Practices# Error HandlingImplement robust error handling in your processes:-- Comprehensive error handling Handlers.add( \"SafeOperation\", Handlers.utils.hasMatchingTag(\"Action\", \"SafeOperation\"), function(msg) local success, result = pcall(function() -- Your operation logic here local data = json.decode(msg.Data) if not data.required_field then error(\"Missing required field\") end return processData(data) end) if success then ao.send({ Target = msg.From, Data = json.encode({ success = true, result = result }) }) else ao.send({ Target = msg.From, Data = json.encode({ success = false, error = result, timestamp = msg.Timestamp }) }) end end ) # Access ControlImplement proper authorization:-- Role-based access control Roles = { [Owner] = \"admin\", -- Add other role assignments } local function hasRole(address, requiredRole) return Roles[address] == requiredRole end Handlers.add( \"AdminOnly\", Handlers.utils.hasMatchingTag(\"Action\", \"AdminOnly\"), function(msg) if not hasRole(msg.From, \"admin\") then ao.send({ Target = msg.From, Data = \"Access denied: Admin role required\" }) return end -- Admin logic here end ) # Testing StrategiesUse AOS (AO Studio) for local development and testing:# Install AOS for local testing npm install -g https://get_ao.g8way.io # Start AOS REPL aos # Load your process code .load process.lua # Test message handling Send({ Action = \"Test\", Data = \"test data\" }) # Performance Considerations# Message OptimizationStructure messages for efficient processing:-- Batch operations for efficiency Handlers.add( \"BatchTransfer\", Handlers.utils.hasMatchingTag(\"Action\", \"BatchTransfer\"), function(msg) local transfers = json.decode(msg.Data) local results = {} for i, transfer in ipairs(transfers) do local success = executeTransfer(transfer.to, transfer.amount) table.insert(results, { index = i, success = success, to = transfer.to, amount = transfer.amount }) end ao.send({ Target = msg.From, Data = json.encode(results) }) end ) # State Management OptimizationKeep state lean and efficient:-- Use efficient data structures -- Instead of storing full transaction history: -- Transactions = {} -- Can grow very large -- Use rolling window or summary data: RecentTransactions = {} -- Last 100 transactions TransactionSummary = { total_count = 0, total_volume = 0, last_updated = 0 } # Security Considerations# Input ValidationAlways validate incoming data:local function validateTransfer(msg) local recipient = msg.Tags.Recipient local quantity = tonumber(msg.Tags.Quantity) if not recipient or recipient == \"\" then return false, \"Invalid recipient\" end if not quantity or quantity <= 0 then return false, \"Invalid quantity\" end if not Balances[msg.From] or Balances[msg.From] < quantity then return false, \"Insufficient balance\" end return true, \"Valid\" end # Reentrancy ProtectionProtect against message replay attacks:ProcessedMessages = ProcessedMessages or {} local function isProcessed(messageId) return ProcessedMessages[messageId] ~= nil end local function markProcessed(messageId) ProcessedMessages[messageId] = true end Handlers.add( \"IdempotentHandler\", Handlers.utils.hasMatchingTag(\"Action\", \"IdempotentHandler\"), function(msg) if isProcessed(msg.Id) then return -- Already processed end -- Process the message -- ... handler logic ... markProcessed(msg.Id) end ) # Monitoring and Debugging# Process Health ChecksImplement health monitoring:Handlers.add( \"HealthCheck\", Handlers.utils.hasMatchingTag(\"Action\", \"HealthCheck\"), function(msg) local health = { status = \"healthy\", uptime = msg.Timestamp - (StartTime or 0), balance = Balance, message_count = #ProcessedMessages, last_activity = LastActivity or StartTime } ao.send({ Target = msg.From, Data = json.encode(health) }) end ) # Debugging ToolsUse logging for debugging:-- Debug logging handler local DEBUG_MODE = true local function debugLog(message, data) if DEBUG_MODE then print(\"DEBUG [\" .. os.date() .. \"]: \" .. message) if data then print(\"Data: \" .. json.encode(data)) end end end Handlers.add( \"DebugHandler\", function() return DEBUG_MODE end, function(msg) debugLog(\"Received message\", { action = msg.Tags.Action, from = msg.From, id = msg.Id }) end ) # Next StepsNow that you understand AO processes fundamentals:Learn process communication - Process CommunicationMaster state management - State ManagementExplore HyperBEAM - HyperBEAM IntroductionBuild your first process - Builder's Journey# ResourcesAO Documentation: Official AO Docsopen in new windowAOS (AO Studio): Development Environmentopen in new windowCode Examples: AO Cookbook Repositoryopen in new windowCommunity: AO Discord Channelopen in new window HyperBEAM","estimatedWords":1495,"lastModified":"2025-10-20T12:14:29.898Z","breadcrumbs":["fundamentals","decentralized computing","ao processes","what are ao processes"],"siteKey":"arweave","siteName":"Arweave Cookbook","depth":4,"crawledAt":"2025-10-20T12:14:29.898Z"},{"url":"https://cookbook.arweave.net/fundamentals/decentralized-computing/index.html","title":"Decentralized Computing","content":"Contributors: Dylan ShadeLast Updated: Edit# Decentralized ComputingThe Permaweb enables a new paradigm of decentralized computing that goes beyond simple data storage. Through technologies like AO and HyperBEAM, developers can build sophisticated applications that run permanently and trustlessly on a global, decentralized computer.# OverviewTraditional computing relies on centralized servers and cloud providers, creating single points of failure and control. Decentralized computing on the Permaweb distributes computation across a network of nodes, ensuring permanence, censorship resistance, and trustless execution.# Key ComponentsAO Processes - Autonomous computational units that maintain state and execute logic permanently on Arweave HyperBEAM - The production implementation of AO-Core that provides HTTP access to decentralized computing resourcesDecentralized Computing Architecture:User/Application ↓ (HTTP Request) HyperBEAM Node ←──────→ HTTP Response ↓ (Route Message) AO Process ←──────→ Other AO Process ↓ (Execute Logic) (Messages) Lua/WASM Runtime ↓ (Update State) Arweave Storage Decentralized Computing Network: ┌─────────────────────────────────────┐ │ Node 1 Node 2 Node 3 ... Node N │ │ ↑ ↑ ↑ ↑ │ │ └───────┼───────┼──────────┘ │ │ └───────┘ │ │ (Distributed across network) │ └─────────────────────────────────────┘ This system enables permanent, trustless computation where HyperBEAM nodes provide HTTP access to AO processes that execute on various runtimes and persist state to Arweave.# Core Concepts# Permanent ComputationUnlike traditional serverless functions that can be shut down or modified, AO processes run permanently on the Arweave network. Once deployed, they continue executing indefinitely without requiring maintenance or hosting fees.# Trustless ExecutionAll computation is cryptographically verifiable. Results can be independently verified by anyone, eliminating the need to trust centralized providers or intermediaries.# Message-Based ArchitectureAO processes communicate through asynchronous message passing, enabling sophisticated distributed computing patterns while maintaining consistency and ordering.# Device ModularityHyperBEAM's device architecture allows different computational engines (Lua, WebAssembly, custom modules) to be plugged in as needed, creating a flexible and extensible computing environment.# Why Decentralized Computing Matters# PermanenceApplications never go offline or disappearNo vendor lock-in or platform dependenciesPermanent accessibility for users worldwide# Censorship ResistanceNo single authority can shut down applicationsGlobal network distribution prevents blockingPermissionless participation in the network# Economic EfficiencyPay once for permanent deploymentNo ongoing hosting or maintenance costsCompetitive pricing through decentralized markets# Trust MinimizationCryptographically verifiable executionOpen source and auditable codeMathematical guarantees instead of institutional trust# Architecture ComparisonAspectTraditional CloudDecentralized ComputingUptime99.9% SLA100% (permanent)ControlPlatform controlledUser controlledCostsMonthly recurringOne-time deploymentScalabilityManual scalingAutomatic network scalingVerificationTrust-basedCryptographically provableCensorshipPlatform policiesCensorship resistant# Learning Path# 1. Start with AO ProcessesUnderstand the fundamental building blocks of decentralized computing:What are AO Processes - Learn the core concepts and architectureProcess Communication - Master message passing and inter-process communicationState Management - Understand persistent state and data consistency# 2. Explore HyperBEAMLearn how to interact with and leverage the AO Computer:HyperBEAM Introduction - Understand the HTTP gateway to AOQuerying AO Process State - Master the HTTP API for data accessLua Serverless Functions - Build serverless functions with permanent availabilityHyperBEAM Devices - Understand the modular device architecture# 3. Build ApplicationsApply your knowledge to real-world projects:Builder's Journey - End-to-end development workflowZero-deployed Full Stack App - Quick start guideAdvanced Patterns - Sophisticated application architectures# Use Cases# Decentralized Applications (dApps)Token contracts and DeFi protocolsVoting and governance systemsSocial media and content platformsGaming and virtual worlds# Serverless ComputingAPI endpoints with permanent availabilityData processing and transformationScheduled tasks and automationMicroservices architecture# Data ProcessingETL pipelines and analyticsMachine learning inferenceContent delivery and cachingDatabase and storage systems# Integration and AutomationCross-chain bridges and oraclesWebhook endpoints and notificationsWorkflow automationThird-party API integration# Technical Benefits# Developer ExperienceFamiliar interfaces - HTTP APIs and standard programming languagesNo infrastructure management - Deploy and forgetInstant scaling - Network automatically handles loadBuilt-in persistence - State management included# Performance CharacteristicsLow latency - Global edge networkHigh availability - No single points of failureElastic scaling - Resources scale with demandPredictable costs - One-time deployment fees# Security ModelCryptographic verification - All execution is provableIsolated execution - Sandboxed environmentsImmutable code - Deployed logic cannot be changedTransparent operations - All activity is publicly auditable# Getting Started# PrerequisitesBasic understanding of blockchain conceptsFamiliarity with HTTP APIsProgramming experience (JavaScript/Lua preferred)# Quick StartDeploy your first process - Zero-deployed App GuideQuery process state - HyperBEAM QueryingBuild serverless functions - Lua Functions Guide# Development ToolsAOS - AO Studio development environmentao-connect - JavaScript SDK for AO interactionHyperBEAM nodes - HTTP gateways to the AO Computer# Future of Decentralized Computing# Emerging CapabilitiesAI/ML integration - Permanent machine learning modelsAdvanced cryptography - Zero-knowledge proofs and privacyCross-chain bridges - Seamless blockchain interoperabilityIoT integration - Edge computing with global state# Network EffectsAs the decentralized computing network grows:More computational resources become availableSpecialized devices and capabilities emergeCosts decrease through competitionInnovation accelerates through composability# Industry ImpactDecentralized computing enables:Platform independence - Applications that outlive their creatorsGlobal accessibility - Permanent availability worldwideEconomic inclusion - Lower barriers to application deploymentInnovation freedom - Censorship-resistant development# Community and Resources# DocumentationAO Computer Docs - ao.arweave.netopen in new windowHyperBEAM Documentation - hyperbeam.arweave.netopen in new windowArweave Developer Docs - docs.arweave.orgopen in new window# CommunityDiscord - discord.gg/arweaveopen in new windowGitHub - github.com/permawebopen in new windowTwitter - @ArweaveEcoopen in new window# Tools and SDKsAO Connect - JavaScript SDK for AO processesAOS - Local development environmentPermaweb Deploy - Deployment tools and utilitiesReady to build? Start with What are AO Processes to understand the fundamentals, then move to HyperBEAM Introduction to learn how to interact with the AO Computer. Accessing Arweave Data","estimatedWords":844,"lastModified":"2025-10-20T12:14:30.540Z","breadcrumbs":["fundamentals","decentralized computing","index"],"siteKey":"arweave","siteName":"Arweave Cookbook","depth":3,"crawledAt":"2025-10-20T12:14:30.540Z"},{"url":"https://cookbook.arweave.net/fundamentals/accessing-arweave-data/arns.html","title":"ArNS - Arweave Name System","content":"Contributors: Dylan ShadeLast Updated: Edit# ArNS - Arweave Name System# OverviewThe Arweave Name System (ArNS) is the phonebook of the Permaweb.It is a decentralized and censorship-resistant naming system that is enabled by AR.IO Gateways and used to connect friendly names to Permaweb apps, pages and data.This system works similarly to traditional DNS, where a user can purchase a name in a registry and DNS Name servers resolve these names to IP addresses.With ArNS, the registry is decentralized, permanent and stored on Arweave (via AO), and each AR.IO gateway acts as both cache and name resolver. Users can register a name within the ArNS Registry, like \"my-name\" and set a pointer to any Arweave Transaction ID.AR.IO Gateways will resolve that name as one of their own subdomains, eg. https://laserilla.arweave.net and proxy all requests to the associated Arweave Transaction ID. Each registered name can also have under names associated with it that each point to an Arweave Transaction ID, like https://v1_laserilla.arweave.net, giving even more flexibility and control to its owner.# The ArNS RegistryArNS uses AO to manage its name records. Each record, or name, is leased by a user or bought permanently and tied to an ANT token. You can register multiple ArNS names to a single ANT, but you cannot register multiple ANTs to a single ArNS name - the gateways wouldn't know where to point the routing ID.ArNS names can be up to 32 characters, including numbers [0-9], letters [a-z], and dashes [-]. The dashes cannot be trailing dashes, e.g. -myname.# ANTs (Arweave Name Tokens)ANTs are a crucial part of the ArNS ecosystem - they are the actual key to owning an ArNS name. When you register an ArNS name to an ANT, the ANT then becomes the transfer method for that name. The ArNS registry does not care who owns the ANT, it simply knows what name ANT it belongs to.Within ANTs you can build out whatever functionality you wish, within the scope ArNS registry approved source code transaction list.# Under_NamesUndernames are records held and managed by your ANT (Arweave Name Token). These records can be created and managed without even owning an ARNS name, and will be transferred along with the ant when sent to a new owner. Likewise if your ArNS name expires, and you register your ANT to a new ArNS name, all your undername will remain intact.Example: you own oldName.arweave.net.then: You create the undername \"my\" - my_oldName.arweave.net.then: oldName.arweave.net expires, and you register newName.arweave.net to your ANT.now: my_ undername is accessable on newName - my_newName.arweave.net.Below is an example of an ANT contract State:{ balances:{ QGWqtJdLLgm2ehFWiiPzMaoFLD50CnGuzZIPEdoDRGQ : 1 }, controller: \"QGWqtJdLLgm2ehFWiiPzMaoFLD50CnGuzZIPEdoDRGQ\", evolve: null, name: \"ArDrive OG Logo\", owner: \"QGWqtJdLLgm2ehFWiiPzMaoFLD50CnGuzZIPEdoDRGQ\", records:{ @:{ transactionId: \"xWQ7UmbP0ZHDY7OLCxJsuPCN3wSUk0jCTJvOG1etCRo\" }, undername1:{ transactionId: \"usOLUmbP0ZHDY7OLCxJsuPCN3wSUk0jkdlvOG1etCRo\" } }, ticker:\"ANT-ARDRIVE-OG-LOGO\" } the base \"@\" record is the initial routing id for the ANT. if you registered 'my-name' to this ANT, and tried to access it via my-name.arweave.net, you would be redirected to the @ record's transactionId.if you tried to access undername1_my-name.arweave.net, you would get 'undername1's transactionId.ANT's, in theory, have an UNLIMITED number of undernames. However, how many will be served depends on which tier is used with your ArNS name.# ResourcesArNS Appopen in new windowArNS Docsopen in new window Manifests & Path Resolution","estimatedWords":536,"lastModified":"2025-10-20T12:14:31.213Z","breadcrumbs":["fundamentals","accessing arweave data","arns"],"siteKey":"arweave","siteName":"Arweave Cookbook","depth":3,"crawledAt":"2025-10-20T12:14:31.213Z"},{"url":"https://cookbook.arweave.net/fundamentals/accessing-arweave-data/manifests.html","title":"Path Manifests","content":"Contributors: Dylan ShadeLast Updated: Edit# Path Manifests# OverviewWhen uploading files to Arweave each file is assigned its own unique transaction ID. By default these ID's aren't grouped or organized in any particular manner.One picture of your cat might be stored with a transaction ID of bVLEkL1SOPFCzIYi8T_QNnh17VlDp4RylU6YTwCMVRwopen in new window, while another with FguFk5eSth0wO8SKfziYshkSxeIYe7oK9zoPN2PhSc0open in new window as its transaction ID.Cat1Cat2bVLEkL1SOPFCzIYi8T_QNnh17VlDp4...FguFk5eSth0wO8SKfziYshkSxeIYe7oK9zoPN2PhSc0These transaction ID's are a bit unwieldy and make it difficult to find all of your relevant files. Without a path manifest, if you uploaded 100 pictures of your cat you would need to keep track of 100 different IDs and links!Path Manifests are a way to link multiple transactions together under a single base transaction ID and give them human readable file names. In relation to the cat example, you could have one base transaction ID to remember and use it like a folder - accessing your cat pictures with more memorable filenames like {base id}/cat1.jpgopen in new window, {base id}/cat2.jpgopen in new window, etc.Creating grouped sets of readable file names is essential for creating practical applications on Arweave, and unlocks the ability to host websites or other file collections as explored in the examples below.# What Can You Use Manifests For?Any time you need to group files in a hierarchical way, manifests can be useful. For example:Storing NFT collections:https://arweave.net/X8Qm…AOhA/0.pngopen in new windowhttps://arweave.net/X8Qm…AOhA/1.pngopen in new windowThis mirrors the common base path approach used by NFT collections when linking to NFT images and metadata on a storage API or IPFS.Hosting websites:https://arweave.net/X8Qm…AOhA/index.htmlhttps://arweave.net/X8Qm…AOhA/styles.csshttps://arweave.net/X8Qm…AOhA/public/favicon.png# Manifest StructurePath Manifests are a special format of transaction created and posted to Arweave using the Tags:{ name: \"Content-type\", value: \"application/x.arweave-manifest+json\" }and having JSON formatted transaction data that matches the example below.{ \"manifest\": \"arweave/paths\", \"version\": \"0.2.0\", \"index\": { \"path\": \"index.html\" }, \"fallback\": { \"id\": \"cG7Hdi_iTQPoEYgQJFqJ8NMpN4KoZ-vH_j7pG4iP7NI\" }, \"paths\": { \"index.html\": { \"id\": \"cG7Hdi_iTQPoEYgQJFqJ8NMpN4KoZ-vH_j7pG4iP7NI\" }, \"js/style.css\": { \"id\": \"fZ4d7bkCAUiXSfo3zFsPiQvpLVKVtXUKB6kiLNt2XVQ\" }, \"css/style.css\": { \"id\": \"fZ4d7bkCAUiXSfo3zFsPiQvpLVKVtXUKB6kiLNt2XVQ\" }, \"css/mobile.css\": { \"id\": \"fZ4d7bkCAUiXSfo3zFsPiQvpLVKVtXUKB6kiLNt2XVQ\" }, \"assets/img/logo.png\": { \"id\": \"QYWh-QsozsYu2wor0ZygI5Zoa_fRYFc8_X1RkYmw_fU\" }, \"assets/img/icon.png\": { \"id\": \"0543SMRGYuGKTaqLzmpOyK4AxAB96Fra2guHzYxjRGo\" } } } fallback:Manifest version 0.2.0 introduced the fallback attribute. fallback is an object that accepts the sub attribute id, which defines an Arweave data item transaction id for the resolver to fall back to if it fails to correctly resolve a requested path.Source and Further Reading in the official Arweave Path Manifest docs: Arweave Docsopen in new window GraphQL Queries ArNS Introduction","estimatedWords":390,"lastModified":"2025-10-20T12:14:31.793Z","breadcrumbs":["fundamentals","accessing arweave data","manifests"],"siteKey":"arweave","siteName":"Arweave Cookbook","depth":3,"crawledAt":"2025-10-20T12:14:31.793Z"},{"url":"https://cookbook.arweave.net/fundamentals/accessing-arweave-data/graphql.html","title":"GraphQL Queries","content":"Contributors: Dylan ShadeLast Updated: Edit# GraphQL Queries# OverviewOver time, indexing services that implement a GraphQL interface have became the preferred method for querying transaction data on Arweave. An indexing service reads transaction and block headers as they are added to the network (usually from a full Arweave node which the service operates). Once read, the header info is inserted into a database where it can be indexed and efficiently queried. The indexing service uses this database to provide a GraphQL endpoint for clients to query.GraphQL has a few advantages that make it ideal for retrieving query data sets. It enables indexing services to create a single endpoint that can then be used to query all types data. The service is able to return multiple resources in a single request as opposed to making an HTTP request for each resource (like one would with a REST API). With GraphQL, clients can batch multiple requests in a single round-trip and specify exactly what data is needed which increases performance.# Basic Query ExampleThe following GraphQL example queries all the transaction ids from a given owners wallet address that have a \"Type\" tag with a value of \"manifest\". For more information about tags, read the guide on Transaction Tags.const queryObject = { query: `{ transactions ( owners:[\"${address}\"], tags: [ { name: \"Type\", values: [\"manifest\"] } ] ) { edges { node { id } } } }` }; const results = await arweave.api.post('/graphql', queryObject); # Public Indexing Serviceshttps://arweave.net/graphqlopen in new windowhttps://arweave-search.goldsky.com/graphqlopen in new window# ResourcesQuerying Arweave Guidear-gql packageGraphQL Reference Gateways & Access Manifests & Path Resolution","estimatedWords":262,"lastModified":"2025-10-20T12:14:32.545Z","breadcrumbs":["fundamentals","accessing arweave data","graphql"],"siteKey":"arweave","siteName":"Arweave Cookbook","depth":3,"crawledAt":"2025-10-20T12:14:32.545Z"},{"url":"https://cookbook.arweave.net/fundamentals/accessing-arweave-data/gateways.html","title":"Gateways in the Arweave Network","content":"Contributors: Dylan ShadeLast Updated: Edit# Gateways in the Arweave NetworkGateways serve as the interface between the Arweave network and end-users, making permaweb data easily accessible through standard web browsers. Often described as the \"front door to the permaweb,\" these services allow users to interact with blockchain-stored content in a familiar web-like experience.When you access content on Arweave, you typically use a URL structure like:https:/// This allows HTML files to render as web pages, images to display properly, and other data types to be served appropriately—creating an experience similar to the traditional web despite the content being stored on a decentralized network.# Key Functions of GatewaysGateways provide several critical services beyond basic content delivery:Content Caching: Store frequently accessed transactions to improve performanceData Indexing: Provide GraphQL interfaces for querying transactions by tags and metadataNetwork Seeding: Help distribute transactions throughout the Arweave networkContent Moderation: Apply content policies to determine which data is served# Relationship to Core ProtocolIt's important to understand that gateways are not part of the core Arweave protocol. This distinction has several implications:Operating a gateway is separate from running a node that secures the networkThere is no built-in protocol-level incentive structure for gateway operatorsGateway services can implement their own economic models and incentivesApplications can operate their own gateways for improved performanceThis separation allows for a more flexible and decentralized ecosystem where different gateway operators can experiment with various service models.# Popular Gateway ServicesSeveral gateway services currently serve the Arweave ecosystem:arweave.netopen in new window - Operated by the Arweave teamarweave.worldopen in new windowarweave.asiaopen in new windowarweave.liveopen in new windowg8way.ioopen in new windowThe AR.IO project is working to make gateway operation more accessible, potentially increasing the decentralization of access points to the network.# Further ReadingArWiki Gateway Documentationopen in new windowAR.IO Projectopen in new window GraphQL Queries","estimatedWords":293,"lastModified":"2025-10-20T12:14:32.906Z","breadcrumbs":["fundamentals","accessing arweave data","gateways"],"siteKey":"arweave","siteName":"Arweave Cookbook","depth":3,"crawledAt":"2025-10-20T12:14:32.906Z"},{"url":"https://cookbook.arweave.net/fundamentals/accessing-arweave-data/index.html","title":"Accessing Arweave Data","content":"Contributors: Dylan ShadeLast Updated: Edit# Accessing Arweave DataOne of Arweave's core strengths is permanent data storage, but stored data is only valuable if it can be efficiently discovered and retrieved. Arweave provides multiple methods for accessing data, each optimized for different use cases and requirements.# Data Access FundamentalsAll data on Arweave is accessible through gateways - HTTP endpoints that serve as bridges between users and the Arweave network. These gateways expose various interfaces and methods for retrieving data:Direct data retrieval - Access data by transaction IDQuery interfaces - Search and filter data based on metadataName resolution - Access data through human-readable namesPath-based access - Navigate data collections like websites# Access Methods# 1. HTTP APIThe simplest way to access Arweave data is through direct HTTP requests using a transaction ID:https://arweave.net/{transaction-id} This method is ideal for:Retrieving known data by IDSimple integrationsDirect data access without queriesLearn more about Gateways & Access →# 2. GraphQL QueriesFor more complex data discovery and filtering, Arweave gateways provide GraphQL endpoints that enable sophisticated queries based on:Owner addressesTransaction tagsBlock heightsTime rangesGraphQL is the preferred method for:Searching large datasetsBuilding applications that need to discover dataComplex filtering based on metadataLearn more about GraphQL →# 3. Path ManifestsPath manifests enable organizing multiple files into navigable collections, similar to traditional websites:https://arweave.net/{manifest-id}/{path/to/file} Use manifests for:Hosting websites and web applicationsCreating browsable file collectionsOrganizing related contentLearn more about Manifests →# 4. ArNS (Arweave Name System)ArNS provides human-readable names that resolve to Arweave transaction IDs:https://{name}.arweave.dev ArNS is perfect for:User-friendly URLsUpdatable references to contentBuilding permanent web applicationsLearn more about ArNS →# Choosing the Right MethodMethodBest ForExample Use CaseHTTP APIDirect access by IDLoading a specific image or fileGraphQLData discovery and filteringFinding all transactions with specific tagsManifestsMulti-file collectionsHosting a website or applicationArNSHuman-readable addressingCreating a permanent blog or dApp# Gateway InfrastructureAll these access methods are provided by Arweave gateways. Gateways can be:Public gateways like arweave.net - Open for anyone to usePrivate gateways - Run by individuals or organizations for their own useAR.IO gateways - Decentralized gateway network providing enhanced features# Next StepsSet up gateway access for your applicationQuery data with GraphQL to build dynamic applicationsCreate path manifests for organizing contentRegister ArNS names for user-friendly addressing# Additional ResourcesQuerying Arweave GuideGraphQL Reference Transactions Decentralized Computing","estimatedWords":363,"lastModified":"2025-10-20T12:14:33.532Z","breadcrumbs":["fundamentals","accessing arweave data","index"],"siteKey":"arweave","siteName":"Arweave Cookbook","depth":3,"crawledAt":"2025-10-20T12:14:33.532Z"},{"url":"https://cookbook.arweave.net/fundamentals/transactions/bundles.html","title":"Transaction Bundles","content":"Contributors: Dylan ShadeLast Updated: Edit# Transaction Bundles# What is a Bundle?A transaction bundle is a special type of Arweave transaction. It enables multiple other transactions and/or data items to be bundled inside it. Because transaction bundles contain many nested transactions they are key to Arweave's ability to scale to thousands of transactions per second.Users submit transactions to a bundling service, such as turboopen in new window, which combines them into a 'bundle' with other transactions and posts them to the network.# How Do Bundles Help Arweave?# AvailabilityBundling services guarantee that bundled transactions are reliably posted to Arweave without dropping.Transaction IDs of the bundled transactions are immediately made available, meaning the data can instantly be accessed as if it was already on the Arweave network.# ReliabilityTransactions posted to Arweave can occasionally fail to confirm (resulting in a dropped transaction) due to a number of reasons, such as high network activity. In these instances transactions can become orphaned, i.e. stuck in the mempool and eventually removed.Bundlers solve this problem by continually attempting to post bundled data to Arweave, assuring that it does not fail or get stuck in the mempool.# ScalabilityBundles can store up to 2256 transactions, each of which are settled as a single transaction on Arweave. This makes Arweave blockspace scale to support almost any use case.# What are Nested Bundles?Bundles can include data items for uploading to Arweave and those data item can themselves be a bundle.This means it is possible to upload a bundle of bundles, or in other words nested bundles.Nested bundles have no theoretical limit on nesting depth, meaning that transaction throughput can be increased drastically.Nested bundles might be useful for when you have different groups of bundled data that you want to guarantee reach Arweave altogether, and at the same time.Sources and Further Reading:Ardrive Turboopen in new windowANS-104 Standardopen in new window Metadata (Tags)","estimatedWords":309,"lastModified":"2025-10-20T12:14:34.088Z","breadcrumbs":["fundamentals","transactions","bundles"],"siteKey":"arweave","siteName":"Arweave Cookbook","depth":3,"crawledAt":"2025-10-20T12:14:34.088Z"},{"url":"https://cookbook.arweave.net/fundamentals/transactions/tags.html","title":"Transaction Metadata (Tags)","content":"Contributors: Dylan ShadeLast Updated: Edit# Transaction Metadata (Tags)Arweave can be thought of as a permanent append-only hard drive where each entry on the drive is its own unique transaction. Transactions have a unique ID, signature, and owner address for the address that signed and paid for the transaction to be posted. Along with those header values, the Arweave protocol allows users to tag transactions with custom tags. These are specified as a collection name value pairs appended to the transaction. These tags make it possible to query Arweave and find all the Transactions that include a particular tag or tags. The ability to query and filter transactions is critical to supporting apps built on Arweave.# What are Transaction Tags?Transaction tags are key-value pairs, where the combination of base64URL keys and values must be less than the maximum of 2048 bytes for an arweave native transaction.Some common examples of transaction tags include:Content-Type: Used to specify the MIME type of content for render on the permaweb.App-Name: This tag describes the app that is writing the dataApp-Version: This tag is the version of the app, paired with App-NameUnix-Time: This tag is the a unix timestamp, seconds since epoch.Title: Used to give a name or brief description of the content stored in the transaction.Description: Used to provide a longer description of the content.Transaction tags can be used for a variety of purposes, such as indexing transactions for search, organizing transactions into categories, or providing metadata about the content stored in a transaction.# Some good things to know about Transaction TagsTransaction tags are encoded as Base64URL encoded strings for both the key and value. This makes it possible to post arrays of bytes as keys or values and transfer them safely over http. While it's not human readable without decoding, it shouldn't be considered encryption.The max total size of Transaction tags for transaction posted directly to Arweave is 2048 bytes. This size is determined by the concatenation of all keys and all values of the transaction tags.Transaction tags can be used in GraphQL queries to return a filtered set of transaction items.# Common Tags used in the communityTag NameDescriptionUse CasesApp-NameMost commonly used to identify applications using ArweaveCommon uses are the project's name, sometimes also used in specific ANS transactionsApp-VersionThe version of this data, it may represent the app consuming this informationE.g. 0.3.0Content-TypeMIME Type to identify the data contained in the transactiontext/html, application/json, image/pngUnix-TimeThis tag is the a unix timestamp, seconds since epochThe time the transaction is submittedTitleANS-110 Standard for describing contentProviding a name for an Atomic AssetTypeANS-110 Standard for categorization of dataa type can classify a permaweb asset# Examplesconst tx = await arweave.createTransaction({ data: mydata }); tx.addTag(\"Content-Type\", \"text/html\"); tx.addTag(\"Title\", \"My incredible post about Transaction Tags\"); tx.addTag(\"Description\", \"This is one post you do not want to miss!\"); tx.addTag(\"Topic:Amazing\", \"Amazing\"); tx.addTag(\"Type\", \"blog-post\"); await arweave.transactions.sign(tx, jwk); await arweave.transactions.post(tx); # SummaryUnderstanding how Transaction Tags factor into the Arweave tech stack can provide context on how to solve problems using the Permaweb as an application platform. Tags provide a tool to consume and create common data standards and patterns to encourage a non-rivalous data experience on the Permaweb. The result gives users of the ecosystem the choice of applications to consume and create content as their data is always with the user not the application. Posting Transactions Bundles","estimatedWords":548,"lastModified":"2025-10-20T12:14:34.853Z","breadcrumbs":["fundamentals","transactions","tags"],"siteKey":"arweave","siteName":"Arweave Cookbook","depth":3,"crawledAt":"2025-10-20T12:14:34.853Z"},{"url":"https://cookbook.arweave.net/fundamentals/transactions/post-transactions.html","title":"Posting Transactions","content":"Contributors: Dylan ShadeLast Updated: Edit# Posting TransactionsThere are several ways to post transactions to Arweave. Each has its own unique affordances and constraints. The diagram below illustrates the four main approaches to posting transactions.Direct to Peer,Direct to Gateway, Bundled, and Dispatched.Guaranteed TransactionsWhen posting a large quantity of transactions or when fast settlement time is desireable consider using a bundling service. Bundlers settle large volumes of transactions immediately and make the transaction data available within milliseconds. The bundling service holds onto posted transactions until they are confirmed on-chain. If the transactions are not included in the most recent block the bundling service re-posts them with each new block until they are recorded on chain with a sufficient number of confirmations.# Direct TransactionsTransactions posted directly to Arweave come in two varieties wallet-to-wallet transactions and data transactions. The first transfers AR tokens between wallet addresses. The second posts data to Arweave and pays the associated storage costs.Interestingly, data transactions may also transfer AR tokens to a wallet address while paying storage costs at the same time.All transactions allow the user to specify up to 2KB worth of metadata in the form of custom tags.# Direct to PeerTransactions may be posted directly to an Arweave peer (mining node). This is perhaps the most decentralized means of posting a transaction as clients can choose what peer they wish to post to.This approach is not without drawbacks. Peers may come and go making it difficult to reliably post transactions from an app. While it's possible to query a list of active peers and choose one before posting it adds overhead and friction to the process. Additionally, transactions posted to peers are only queryable at the gateway after being mined in a block. This introduces a 1-2 minute delay between posting the transaction to a peer and it being available to read in a browser from a gateway.For the above reasons, developers tend to configure arweave-js to point to a gateway when posting direct transactions as the optimistic cache at the gateway makes the transaction available almost immediately.# Direct to GatewayGateways sit between clients and Arweave's network of peers. One of the primary functions of the gateway is to index transactions and optimistically cache the data posted to the network while waiting for it to be included in a block. This makes the transaction queryable in a \"Pending\" state almost instantly which allows applications built on top of a gateway to be more responsive. There is still a risk of transactions dropping out of the optimistic cache if they are not mined in a block by the peers.An example of how to post a direct transaction using arweave-js can be found in this guide.# Bundled TransactionsServices built on top of Arweave that provide additional utility for Permaweb builders are sometimes called Permaweb Services. A bundler is one such service. Bundlers take multiple individual transactions and bundle them together into a single transaction that is posted directly to Arweave. In this way a single transaction at the protocol level can contain tens of thousands of bundled transactions. There is one restriction, however, only data transactions can be included in a bundle. Wallet-to-wallet transactions (that transfer AR tokens between wallet addresses) must be done as individual transactions posted directly to Arweave.# Dispatched TransactionsAnother way to post bundled transactions is from the browser. While browsers enforce some constraints around the size of data that can be uploaded, browser based wallets are able to post transactions to bundlers. Arweave browser wallets implement a dispatch() API method. If you are posting small transactions (100KB or less) you can use the wallets dispatch() method to take advantage of bundled transactions.An example of how to post a 100KB or less bundled transaction with an Arweave wallets dispatch() method can be found in this guide.# Resourcesarweave-js exampledispatch exampleTurbo SDK example Metadata (Tags)","estimatedWords":637,"lastModified":"2025-10-20T12:14:35.613Z","breadcrumbs":["fundamentals","transactions","post transactions"],"siteKey":"arweave","siteName":"Arweave Cookbook","depth":3,"crawledAt":"2025-10-20T12:14:35.613Z"},{"url":"https://cookbook.arweave.net/fundamentals/transactions/transaction-types.html","title":"Transactions","content":"Contributors: Dylan ShadeLast Updated: Edit# TransactionsTransactions are the fundamental building blocks of the Arweave network. This section covers how transactions work, how to post them, and the different types available for various use cases.# Core Transaction ConceptsAll data on Arweave is stored as transactions. Each transaction contains data and metadata (tags) that can be queried and retrieved. Understanding these concepts is essential for building on the permaweb.# Transaction Guides# Posting TransactionsLearn the different ways to post transactions to Arweave, including direct posting, bundling services, and dispatched transactions. Understand the trade-offs between settlement time, cost, and reliability.# Transaction TagsDiscover how to use tags to organize and query transaction data. Tags are key-value pairs that make transactions discoverable and enable complex applications on Arweave.# Transaction BundlesUnderstand how multiple transactions can be bundled together for improved scalability and guaranteed settlement. Bundles are crucial for high-throughput applications. Wallets and Keys Accessing Arweave Data","estimatedWords":149,"lastModified":"2025-10-20T12:14:36.547Z","breadcrumbs":["fundamentals","transactions","transaction types"],"siteKey":"arweave","siteName":"Arweave Cookbook","depth":3,"crawledAt":"2025-10-20T12:14:36.547Z"},{"url":"https://cookbook.arweave.net/fundamentals/wallets-and-keyfiles/creating-a-wallet.html","title":"Cooking with the Permaweb","content":"Contributors: longviewoorLast Updated: Edit# Creating a WalletUsers can create Arweave and AO wallets without requiring any technical knowledge by using wallets like Wanderopen in new window or Beaconopen in new window.These are third-party applications, so as with most of crypto DYOR before choosing a wallet.# Generating a wallet programmaticallyArweave wallets can also be generated programatically.# Creating a wallet with arweave-jsnpm install arweave arweave.wallets.generate().then((key) => { console.log(key); // { // \"kty\": \"RSA\", // \"n\": \"3WquzP5IVTIsv3XYJjfw5L-t4X34WoWHwOuxb9V8w...\", // \"e\": ... }); # Creating a wallet from the command lineIf you would prefer to create an Arweave wallet through a command-line application, you can use the ArDrive CLI.npm install -g ardrive-cli You can generate a seed phrase with the command generate-seedphrase:# Generate seed-phrase ardrive generate-seedphrase \"this is an example twelve word seed phrase that you could use\" Or, you can generate a wallet file using generate wallet:# Generate a wallet and store it in a chosen output file ardrive generate-wallet > /path/to/wallet/file.json","estimatedWords":158,"lastModified":"2025-10-20T12:14:37.119Z","breadcrumbs":["fundamentals","wallets and keyfiles","creating a wallet"],"siteKey":"arweave","siteName":"Arweave Cookbook","depth":3,"crawledAt":"2025-10-20T12:14:37.119Z"},{"url":"https://cookbook.arweave.net/fundamentals/wallets-and-keyfiles/index.html","title":"Wallets and Keys","content":"Contributors: longviewoorLast Updated: Edit# Wallets and KeysArweave wallets serve as the gateway to interact with the Arweave blockchain network. They don't physically store tokens but instead manage the cryptographic keys needed to access and control your on-chain assets and data.# What is an Arweave wallet?A wallet on Arweave is a cryptographic tool that secures your unique blockchain address. This address tracks your $AR token balance and enables network interactions such as sending transactions or working with AO Processesopen in new window.It's important to understand that wallets don't actually \"hold\" tokens. Instead, they store the cryptographic public-private key pair that allows you to sign transactions and manage your on-chain assets. Token balances exist on the blockchain itself, linked to your wallet's address.# Key PointsWallets contain the cryptographic keys needed to sign transactions and access funds on the Arweave networkOnly the wallet owner (with access to the private key) can authorize transactions for their addressArweave uses 4096-bit RSA-PSS key-pairs stored in JWK (JSON Web Keys) formatWallet addresses are derived from the public key using SHA-256 hashing and Base64URL encodingPrivate keys must be kept secure at all times, as they control access to your funds# Keypair and Wallet FormatArweave utilizes 4096-bit RSA-PSS key-pairs stored in the JWK (JSON Web Keys) format. A typical JWK file for an Arweave wallet looks like this (with abbreviated values):{ \"d\": \"cgeeu66FlfX9wVgZr5AXKlw4MxTlxSuSwMtTR7mqcnoE...\", \"dp\": \"DezP9yvB13s9edjhYz6Dl...\", \"dq\": \"SzAT5DbV7eYOZbBkkh20D...\", \"e\": \"AQAB\", \"ext\": true, \"kty\": \"RSA\", \"n\": \"o4FU6y61V1cBLChYgF9O37S4ftUy4newYWLApz4CXlK8...\", \"p\": \"5ht9nFGnpfW76CPW9IEFlw...\", \"q\": \"tedJwzjrsrvk7o1-KELQxw...\", \"qi\": \"zhL9fXSPljaVZ0WYhFGPU...\" } In this JWK file:The n value represents your wallet's public key, which can be safely sharedThe d value (along with other fields) comprises your wallet's private key, which must be kept confidentialThese JWK files can be created and exported from wallet applications like Arweave.appopen in new window or generated programmatically using arweave-jsopen in new windowWhen using certain wallet applications, your private key may also be represented as a mnemonic seed phrase, which can be used to sign transactions or recover your wallet.# Wallet AddressesArweave wallet addresses are derived from the public key through a deterministic process:The SHA-256 hash of the public key is calculatedThis hash is then Base64URL encodedThe result is a 43-character wallet address that's more convenient to use than the full 4096-bit public keyThis process creates a secure and verifiable link between your wallet address and public key, while providing a more human-readable format for everyday use.# Wallet SecurityYour private key grants complete control over your wallet and funds. Anyone with access to your private key can transfer tokens from your address. As a developer, exercise extreme caution:Never include your keyfile in public GitHub repositoriesDon't store your private key on unsecured devices or cloud servicesBack up your private key or seed phrase securelyConsider using hardware wallets for significant holdings# Available WalletsSeveral wallet options are available for interacting with the Arweave network:Wanderopen in new window - Browser extension and mobile wallet for Arweave and AOBeaconopen in new window - Browser extension and mobile wallet for Arweave and AOArweave.appopen in new window - Web wallet for deploying permanent data, connecting to dApps, and navigating the weave (Limited AO/HyperBEAM support)# Further ReadingArweave Docsopen in new windowJSON Web Key Format (RFC 7517)open in new window Transactions","estimatedWords":527,"lastModified":"2025-10-20T12:14:37.925Z","breadcrumbs":["fundamentals","wallets and keyfiles","index"],"siteKey":"arweave","siteName":"Arweave Cookbook","depth":3,"crawledAt":"2025-10-20T12:14:37.925Z"},{"url":"https://cookbook.arweave.net/fundamentals/index.html","title":"Core Concepts","content":"Contributors: Dylan ShadeLast Updated: Edit# Core ConceptsUnderstanding the fundamental concepts of Arweave and the Permaweb is essential for building robust decentralized applications.# Transaction FundamentalsTransactions - How data is stored and transactions work Posting TransactionsBundlesTransaction Types# Core ArchitectureManifests & Path Resolution - How files and applications are organizedQuerying Fundamentals - How to find and retrieve dataWallets & Keys - Identity and authenticationGateways & Access - How to access the network# Advanced TopicsArNS Introduction - Decentralized naming systemVouch Protocol - Content moderation and reputation# File System SpecificationsArFS Specification - Arweave File System standard Data ModelEntity TypesContent TypesPrivacySchema Diagrams Getting Started Guides","estimatedWords":99,"lastModified":"2025-10-20T12:14:38.573Z","breadcrumbs":["fundamentals","index"],"siteKey":"arweave","siteName":"Arweave Cookbook","depth":2,"crawledAt":"2025-10-20T12:14:38.573Z"},{"url":"https://cookbook.arweave.net/getting-started/quick-starts/hw-no-code.html","title":"Hello World (No Code)","content":"Contributors: Dylan ShadeLast Updated: Edit# Hello World (No Code)In this quick start we are going to upload an image to the Permaweb with no code.# RequirementsComputerInternetModern web browser# Create a wallethttps://arweave.app/addopen in new window or https://wander.appopen in new window# Send some data to ArweaveGo to https://hello_cookbook.arweave.netopen in new windowEnter some data and click publish, connect your wallet and \"BAM\"# Congratulations!You just published some data on Arweave using zero code.To check out the project -> https://github.com/twilson63/pw-no-code-hello Code","estimatedWords":76,"lastModified":"2025-10-20T12:14:39.163Z","breadcrumbs":["getting started","quick starts","hw no code"],"siteKey":"arweave","siteName":"Arweave Cookbook","depth":3,"crawledAt":"2025-10-20T12:14:39.163Z"},{"url":"https://cookbook.arweave.net/getting-started/quick-starts/hw-code.html","title":"Hello World (Code)","content":"Contributors: Dylan Shade, longviewoorLast Updated: Edit# Hello World (Code)This guide walks you through a quick way to get a static HTML, CSS and JavaScript webpage onto the Permaweb using a few lines of code and a command-line interface (CLI).# RequirementsNodeJSopen in new window LTS or greaterBasic knowledge of HTML, CSS and JavaScriptA text editor (VS Code, Sublime, or similar)# DescriptionUsing a terminal/console window create a new folder called hello-world.# Setupcd hello-world npm init -y mkdir src background-color: #4caf50; } index.jsClick to view JSfunction changeColor() { const header = document.getElementById(\"main\"); header.style.color === \"\" ? (header.style.color = \"red\") : (header.style.color = \"\"); } Now that there is a static site to deploy, it can be checked to ensure it all functions properly by typing open src/index.html in your console/terminal. If everything is working as expected it is time to deploy to Arweave!# Upload using permaweb-deployInstall and configure permaweb-deploy for deployment:npm install --save-dev permaweb-deploy Add a deployment script to your package.json:{ \"scripts\": { \"deploy\": \"permaweb-deploy --arns-name my-hello-world --deploy-folder src\" } } Deploy your application:DEPLOY_KEY=$(base64 -i wallet.json) npm run deploy For detailed deployment instructions, see Permaweb Deploy.# Congrats!!You just published a static site on Arweave using a few commands and a few lines of code! CLI No Code","estimatedWords":204,"lastModified":"2025-10-20T12:14:39.762Z","breadcrumbs":["getting started","quick starts","hw code"],"siteKey":"arweave","siteName":"Arweave Cookbook","depth":3,"crawledAt":"2025-10-20T12:14:39.762Z"},{"url":"https://cookbook.arweave.net/getting-started/quick-starts/hw-cli.html","title":"Hello World (CLI)","content":"Contributors: Dylan Shade, longviewoorLast Updated: Edit# Hello World (CLI)This guide walks you through the most simple way to get data on to the Permaweb using a command-line interface (CLI).# RequirementsNodeJSopen in new window LTS or greater# DescriptionUsing a terminal/console window create a new folder called hw-permaweb-1.# Setupcd hw-permaweb-1 npm init -y npm install arweave ardrive-cli # Generate a walletnpx -y @permaweb/wallet > ~/.demo-arweave-wallet.json # Create a web pageecho \"Hello Permaweb\" > index.html # Upload using Ardrive CLI# Create a Drive FOLDER_ID=$(npx ardrive create-drive -n public -w ~/.demo-arweave-wallet.json --turbo | jq -r '.created[] | select(.type == \"folder\") | .entityId') # Upload file TX_ID=$(npx ardrive upload-file -l index.html --content-type text/html -w ~/.demo-arweave-wallet.json --turbo -F ${FOLDER_ID} | jq -r '.created[] | select(.type == \"file \") | .dataTxId') # open file from ar.io gateway open https://arweave.net/${TX_ID} Code","estimatedWords":133,"lastModified":"2025-10-20T12:14:40.382Z","breadcrumbs":["getting started","quick starts","hw cli"],"siteKey":"arweave","siteName":"Arweave Cookbook","depth":3,"crawledAt":"2025-10-20T12:14:40.383Z"},{"url":"https://cookbook.arweave.net/getting-started/welcome.html","title":"Developing on the Permaweb","content":"Contributors: Dylan Shade, longviewoorLast Updated: Edit# Developing on the Permaweb# Welcome to the PermawebCreating applications on the Permaweb, which is built on the Arweave protocol, is similar to building traditional web applications but with some key differences.One major difference is that data is stored on the Permaweb permanently, as the name suggests, rather than on a centralized server. This means that once data is uploaded to the Permaweb, it cannot be deleted or altered. This can be beneficial for applications that require tamper-proof data storage, such as supply chain management or voting systems.Another difference is that the Permaweb is decentralized, meaning there is no central point of control or failure. This can provide increased security and reliability for applications.Additionally, the Permaweb uses a unique token, called AR, to pay for the storage of data on the network. This can add a new layer of complexity to application development, as developers need to consider how to integrate AR into their applications and handle payments.Overall, the experience of creating applications on the Permaweb can be challenging, but it can also be rewarding as it offers unique benefits over traditional web development.# Hello WorldsHello World (No Code)Hello World (CLI)","estimatedWords":196,"lastModified":"2025-10-20T12:14:40.516Z","breadcrumbs":["getting started","welcome"],"siteKey":"arweave","siteName":"Arweave Cookbook","depth":2,"crawledAt":"2025-10-20T12:14:40.516Z"},{"url":"https://cookbook.arweave.net/index.html","title":"Build on the Permaweb","content":"Contributors: Dylan ShadeLast Updated: Edit# Cooking with the PermawebThe Permaweb Cookbook is a developer resource that provides the essential concepts and references for buiding applications on the Permaweb. Each concept and reference will focus on specific aspects of the Permaweb development ecosystem while providing additional details and usage examples.# DevelopersWelcome to the Arweave development community, where the past is forever etched in the blockchain and the future is full of endless possibilities. Let's build the decentralized web together!Read More# ContributingThe Cookbook is designed in a way that makes it easy for new Permaweb developers to contribute. Even if you don't know how to do something, contributing to the cookbook is a great way to learn!Check out all open issues here. Contribution guidelines here. if you find the cookbook is missing a concept, guide or reference, please add an issue.Read More# How to Read the CookbookThe Permaweb Cookbook is split into different sections, each aimed at a different goal.SectionDescriptionCore ConceptsBuilding blocks of the Permaweb that are good to know for developmentGuidesSnack-sized guides about different tools for developmentReferencesReferences to commonly needed code snippetsStarter KitsFront-end Framework Starters to get you started building on the Permaweb in no time# Quick StartsThese are small guides to help developers from every experience level to ship code the the permaweb.Hello World (No Code)Hello World (CLI)","estimatedWords":218,"lastModified":"2025-10-20T12:14:40.880Z","breadcrumbs":["index"],"siteKey":"arweave","siteName":"Arweave Cookbook","depth":1,"crawledAt":"2025-10-20T12:14:40.880Z"},{"url":"https://cookbook.arweave.net/references/index.html","title":"References","content":"Contributors: Dylan ShadeLast Updated: Edit# ReferencesReference documentation, specifications, and resources for Permaweb development.# Quick ReferencesGlossary - Definitions of key terms and conceptsLLMs.txt - Machine-readable documentation for AI assistants# Technical SpecificationsArFS Specification - Arweave File System standard Data ModelEntity TypesContent TypesPrivacySchema Diagrams# External ResourcesArweave Documentationopen in new windowCommunity Forumopen in new windowDeveloper Discordopen in new window# API ReferencesFor specific tool and service APIs, see the Tooling section. Tooling Community","estimatedWords":68,"lastModified":"2025-10-20T12:14:40.993Z","breadcrumbs":["references","index"],"siteKey":"arweave","siteName":"Arweave Cookbook","depth":2,"crawledAt":"2025-10-20T12:14:40.993Z"},{"url":"https://cookbook.arweave.net/guides/index.html","title":"Guides","content":"Contributors: Dylan ShadeLast Updated: Edit# GuidesStep-by-step tutorials and practical guides for building on the Permaweb. Guides are currently being reorganized for better user experience.# Coming SoonThis section is being revamped to organize guides by user personas and use cases:Builder - Frontend and full-stack development guidesExplorer - Data querying and analysis tutorialsGamer - Gaming and NFT developmentQuant - Advanced data analysis techniquesNode Operator - Infrastructure and network participationJack of All Trades - No-code and low-code solutions# Current ResourcesWhile we reorganize, you can find existing guides in:Posting Transactions - How to send data to ArweaveDeploying Manifests - Publishing applications and websitesDeployment Tools - CLI tools and automationQuerying Arweave - Finding and retrieving data# Framework IntegrationDevelopment kits and framework integrations are being moved to dedicated sections for better organization. Fundamentals Tooling","estimatedWords":128,"lastModified":"2025-10-20T12:14:41.111Z","breadcrumbs":["guides","index"],"siteKey":"arweave","siteName":"Arweave Cookbook","depth":2,"crawledAt":"2025-10-20T12:14:41.111Z"},{"url":"https://cookbook.arweave.net/getting-started/index.html","title":"Getting Started","content":"Contributors: Dylan ShadeLast Updated: Edit# Getting StartedWelcome to the Permaweb Cookbook! This guide will help you get started building decentralized applications on Arweave.# Start HereWelcome & Overview - Introduction to Arweave and the PermawebZero-deployed Minimal Full Stack App - Build your first app without writing codeContributing - How to contribute to this cookbook# What is the Permaweb?The Permaweb is a global, community-owned web that anyone can contribute to or get paid to maintain. It's built on top of Arweave, a decentralized storage network that permanently stores data.# Quick Start OptionsChoose your path based on your experience level:No-Code - Use visual tools to deploy your first appFrontend Developer - Deploy a React/Vue/Svelte appFull-Stack - Build applications with data storageBlockchain Developer - Integrate with smart contracts# Next StepsOnce you've completed the getting started guide, explore:Core Concepts to understand how Arweave worksGuides for step-by-step tutorialsTooling for development tools Fundamentals","estimatedWords":146,"lastModified":"2025-10-20T12:14:41.443Z","breadcrumbs":["getting started","index"],"siteKey":"arweave","siteName":"Arweave Cookbook","depth":2,"crawledAt":"2025-10-20T12:14:41.443Z"},{"url":"https://cookbook.arweave.net/tooling/specs/ans/ANS-106.html","title":"ANS-106 Do-Not-Store Requestopen in new window","content":"Contributors: Dylan ShadeLast Updated: Edit# ANS-106: Do-Not-Store Requestopen in new windowStatus: Draft-1Authors: Abhav Kedia abhav@arweave.org, Sam Williams sam@arweave.org# AbstractThis document describes a transaction format that users of the Arweave network can use to request miners to not store certain kinds of data, for various reasons. In order to request non-storage, upload a transaction with the transaction ID of the data in question, along with tags as specified here. Upon receiving this data, nodes in the network will independently decide whether or not to accept the request.# MotivationThe Arweave permanent storage protocol enables a truly permanent digital store of media and documents.There might be various reasons why persons or entities might wish to remove some data from the network. These include privacy, government regulation and copyright violations. By uploading a transaction that adheres to this standard, users of the network can ensure that their request is broadcast to all relevant storage node operators.# Specification# Transaction FormatA Do-Not-Store Request transaction MUST be a transaction with the following tags:Tag NameOptional?Tag ValueApp-NameFalseDo-Not-StoreDo-Not-StoreFalseArweave Txn ID of the data in questionCategoryTrueCategory-TagGeographyTrueIf requesting removal in particular countries, include the 2 letter Country-Code (Alpha-2, ISO 3166)Content-TypeTrueA guide to the type of content included in the body of the transaction, in order to aid rendering.The body of the transaction must then include a text description of the reason for the case describing why the data should not be stored. Multiple Do-Not-Store tags may be added to request non-storage of multiple data items at once.# Category-TagsCategory tags are a short-hand way of specifying the reason for removal. Common tags include Private, Regulation, Copyright. Custom category tags may be used as appropriate. Custom tags must not exceed 50 characters.# Future workThere may be a need to request removal based on various properties of data or tags associated with them. This standard may be extended to allow for lists of transactions and/or associated filtering criteria. ANS-105: License Tags ANS-109: Vouch-For","estimatedWords":318,"lastModified":"2025-10-20T12:14:42.682Z","breadcrumbs":["tooling","specs","ans","ans 106"],"siteKey":"arweave","siteName":"Arweave Cookbook","depth":4,"crawledAt":"2025-10-20T12:14:42.682Z"},{"url":"https://cookbook.arweave.net/tooling/specs/ans/ANS-109.html","title":"ANS-109 Vouch-For (Assertion of Identity)open in new window","content":"Contributors: Dylan ShadeLast Updated: Edit# ANS-109: Vouch-For (Assertion of Identity)open in new windowStatus: Draft (Version 0.1)Authors: Abhav Kedia (abhav@arweave.org), Sam Williams (sam@arweave.org), Tom Wilson (tom@hyper.io)# AbstractThis document specifies a transaction format that allows addresses to vouch for the identity of other addresses on the permaweb.# MotivationSybil resistance is a necessary component of most applications on the permaweb. A transaction format that allows addresses to vouch for the identity of other addresses enables human and programmatic Verifiers to confirm the humanity of addresses. All other applications can then utilize this information as a primitive for identity verification, with various safeguards that can be built on top.One example of abstractions and safeguards built on top of such a system could be a \"VouchDAO\" - a community that specifies which human \"Verifiers\" or \"Verification Services\" they deem to be trustworthy at a given point in time.# Specfication# Transaction FormatA Verifier can assert the identity of an address using the Vouch-For standard by sending a transaction with the following tags.Tag NameOptional?Tag ValueApp-NameFalseVouchVouch-ForFalseArweave address that is being vouched for in this transactionApp-VersionTrue0.1Verification-MethodTrueMethod of verification of identity for the person. Example - Twitter/In-Person/Gmail/FacebookUser-IdentifierTrueAn identifier for the user based on the Verification Method. Example - abhav@arweave.org# UsageUsers of this standard can run a graphql query on the arweave network with transactions of the Vouch-For standard that vouch for a particular address to be verified. For example,query { transactions( tags:{name:\"Vouch-For\", values:[\"0L_z90sYv36VDoDhrRBffo9KrADWpCaaGQz7hJhhP9g\"]} ) { edges { node { id tags { name value } } } } } This query returns all vouches for the address 0L_z90sYv36VDoDhrRBffo9KrADWpCaaGQz7hJhhP9g. Additional filters (such as those by an implementation of \"VouchDAO\" as outlined above) can be applied by filtering for specific owners that have been designated as Verifiers. ANS-106: Do-Not-Store ANS-110: Asset Discoverability","estimatedWords":290,"lastModified":"2025-10-20T12:14:43.478Z","breadcrumbs":["tooling","specs","ans","ans 109"],"siteKey":"arweave","siteName":"Arweave Cookbook","depth":4,"crawledAt":"2025-10-20T12:14:43.478Z"},{"url":"https://cookbook.arweave.net/tooling/specs/ans/ANS-110.html","title":"ANS-110 Asset Discoverabilityopen in new window","content":"Contributors: Dylan ShadeLast Updated: Edit# ANS-110: Asset Discoverabilityopen in new windowStatus: DraftAuthors: Tom Wilson (tom@hyper.io), Sam Williams (sam@arweave.org), Abhav Kedia (abhav@arweave.org)# AbstractThis document specifies a data protocol that allows assets to be discovered and uniformly displayed on the permaweb.# MotivationThe permaweb is a rich collection of various kinds of data -- media, content, functions, and applications. A standard protocol for identifying assets enables discoverability for dashboards, exchanges, and higher-level permaweb services. By providing an extensible data protocol for transactions, creators and publishers can harness the power of permaweb-wide content discoverability -- making it easily appear in various applications and contexts across the permaweb.This protocol would enable, for example, a marketplace or exchange where users trade media assets to render them in a user-friendly way. By extending asset transactions using these identifiers, creators provide a clear and composable set of identifiers that marketplaces or exchanges can use to give a detailed description of the asset. Another example would be a search engine service that may want to index specific types of assets.# Specfication# Transaction FormatTo utilize Asset Discoverability, a creator or publisher can dispatch or post an Arweave transaction specifying the following tags.Tag NameOptional?Tag ValueTitleFalseA maximum of 150 characters used to identify the content, this title can provide a quick eye catching description of the assetType*FalseType of asset. One or more of: meme, image, video, podcast, blog-post, social-post, music, token, web-page, profileTopic*TrueZero to many topics that can be used to locate assets of a given type by a specific topic. For example: an asset of type meme might have the following two topics, Funny, Sports.DescriptionTrueA longer description of 300 characters that can provide a set of details further describing the asset# UsageThe primary purpose of these tags is to allow content devs to leverage GraphQL to find asset transactions of a specific type or topic for use in their applications.query { transactions( first: 100, tags: [ { name: \"Type\", values: [\"meme\", \"blog-post\"] }, { name: \"Topic:Funny\", values: [\"Funny\"] }, { name: \"Topic:Jokes\", values: [\"Jokes\"] } ]) { edges { node { id owner { address } tags { name value } }} } } This GraphQL query filters based on Type and Topic tags to filter Assets for an aggregate list display. ANS-109: Vouch-For","estimatedWords":373,"lastModified":"2025-10-20T12:14:44.070Z","breadcrumbs":["tooling","specs","ans","ans 110"],"siteKey":"arweave","siteName":"Arweave Cookbook","depth":4,"crawledAt":"2025-10-20T12:14:44.070Z"},{"url":"https://cookbook.arweave.net/tooling/specs/arfs/arfs.html","title":"ArFS Protocol A Decentralized File System on Arweave","content":"Contributors: Dylan ShadeLast Updated: Edit# ArFS Protocol: A Decentralized File System on ArweaveArweave File System, or “ArFS” is a data modeling, storage, and retrieval protocol designed to emulate common file system operations and to provide aspects of mutability to your data hierarchy on Arweaveopen in new window's otherwise permanent, immutable data storage blockweave.Due to Arweave's permanent, immutable and public nature traditional file system operations such as permissions, file/folder renaming and moving, and file updates cannot be done by simply updating the on-chain data model.ArFS works around this by implementing a privacy and encryption pattern and defining an append-only transaction data model using tags within Arweave Transaction headersopen in new window.# Key Features# File StructureArFS organizes files and folders using a hierarchical structure. Files are stored as individual transactions on the Arweave blockchain, while folders are metadata that reference these file transactions.# MetadataEach file and folder has associated metadata, such as the name, type, size, and modification timestamp. ArFS leverages Arweave's tagging system to store this metadata in a standardized format, which allows for easy querying and organization.# File PermissionsArFS supports public and private file permissions. Public files can be accessed by anyone on the network, while private files are encrypted using the owner's private key, ensuring only they can decrypt and access the content.# File VersioningArFS supports versioning of files, allowing users to store multiple versions of a file and access previous versions at any time. This is achieved by linking new file transactions to previous versions through the use of metadata tags.# Data DeduplicationTo minimize storage redundancy and costs, ArFS employs data deduplication techniques. If a user tries to store a file that already exists on the network, the protocol will simply create a new reference to the existing file instead of storing a duplicate copy.# Search and DiscoveryArFS enables users to search and discover files based on their metadata, such as file names, types, and tags. This is made possible by indexing the metadata stored within the Arweave blockchain.# InteroperabilityArFS is designed to be interoperable with other decentralized applications and services built on the Arweave network. This allows for seamless integration and collaboration between different applications and users.# Getting StartedTo start using ArFS, you'll need to familiarize yourself with the Arweave ecosystem, acquire AR tokens to cover storage costs, and choose a compatible client or library to interact with the ArFS protocol.# ResourcesFor more information, documentation, and community support, refer to the following resources:Arweave Official Websiteopen in new windowArweave Developer Documentationopen in new windowArweave Community Forumsopen in new window Data Model","estimatedWords":423,"lastModified":"2025-10-20T12:14:44.658Z","breadcrumbs":["tooling","specs","arfs","arfs"],"siteKey":"arweave","siteName":"Arweave Cookbook","depth":4,"crawledAt":"2025-10-20T12:14:44.658Z"},{"url":"https://cookbook.arweave.net/tooling/specs/arfs/data-model.html","title":"Data Model","content":"Contributors: Dylan ShadeLast Updated: Edit# Data ModelBecause of Arweave's permanent and immutable nature, traditional file structure operations such as renaming and moving files or folders cannot be accomplished by simply updating on-chain data. ArFS works around this by defining an append-only transaction data model based on the metadata tags found in the Arweave Transaction Headers.open in new windowThis model uses a bottom-up reference method, which avoids race conditions in file system updates. Each file contains metadata that refers to the parent folder, and each folder contains metadata that refers to its parent drive. A top-down data model would require the parent model (i.e. a folder) to store references to its children.These defined entities allow the state of the drive to be constructed by a client to look and feel like a file systemDrive Entities contain folders and filesFolder Entities contain other folders or filesFile Entities contain both the file data and metadataSnapshot entities contain a state rollups of all files and folder metadata within a drive# Entity relationshipsThe following diagram shows the high level relationships between drive, folder, and file entities, and their associated data. More detailed information about each Entity Type can be found here.Entity Relationship DiagramAs you can see, each file and folder contains metadata which points to both the parent folder and the parent drive. The drive entity contains metadata about itself, but not the child contents. So clients must build drive states from the lowest level and work their way up.# Metadata FormatMetadata stored in any Arweave transaction tag will be defined in the following manner:{ \"name\": \"Example-Tag\", \"value\": \"example-data\" } Metadata stored in the Transaction Data Payload will follow JSON formatting like below:{ \"exampleField\": \"exampleData\" } fields with a ? suffix are optional.{ \"name\": \"My Project\", \"description\": \"This is a sample project.\", \"version?\": \"1.0.0\", \"author?\": \"John Doe\" } Enumerated field values (those which must adhere to certain values) are defined in the format \"value 1 | value 2\".All UUIDs used for Entity-Ids are based on the Universally Unique Identifieropen in new window standard.There are no requirements to list ArFS tags in any specific order. ArFS Protocol: A Decentralized File System on Arweave Entity Types","estimatedWords":359,"lastModified":"2025-10-20T12:14:45.419Z","breadcrumbs":["tooling","specs","arfs","data model"],"siteKey":"arweave","siteName":"Arweave Cookbook","depth":4,"crawledAt":"2025-10-20T12:14:45.419Z"},{"url":"https://cookbook.arweave.net/tooling/specs/arfs/entity-types.html","title":"Entity Types","content":"Contributors: Dylan ShadeLast Updated: Edit# Entity Types# OverviewArweave transactions are composed of transaction headers and data payloads.ArFS entities, therefore, have their data split between being stored as tags on their transaction header and encoded as JSON and stored as the data of a transaction. In the case of private entities, JSON data and file data payloads are always encrypted according to the protocol processes defined below.Drive entities require a single metadata transaction, with standard Drive tags and encoded JSON with secondary metadata.Folder entities require a single metadata transaction, with standard Folder tags and an encoded JSON with secondary metadata.File entities require a metadata transaction, with standard File tags and an encoded Data JSON with secondary metadata relating to the file.File entities also require a second data transaction, which includes a limited set of File tags and the actual file data itself.Snapshot entities require a single transaction. which contains a Data JSON with all of the Drive’s rolled up ArFS metadata and standard Snapshot GQL tags that identify the Snapshot.# DriveA drive is the highest level logical grouping of folders and files. All folders and files must be part of a drive, and reference the Drive ID of that drive.When creating a Drive, a corresponding folder must be created as well. This will act as the root folder of the drive. This separation of drive and folder entity enables features such as folder view queries, renaming, and linking.ArFS: \"0.13\" Cipher?: \"AES256-GCM\" Cipher-IV?: \"\" Content-Type: \"\" Drive-Id: \"\" Drive-Privacy: \"\" Drive-Auth-Mode?: \"password\" Entity-Type: \"drive\" Unix-Time: \"\" Data JSON { \"name\": \"\", \"rootFolderId\": \"\" } Drive Entity Transaction Example# FolderA folder is a logical grouping of other folders and files. Folder entity metadata transactions without a parent folder id are considered the Drive Root Folder of their corresponding Drives. All other Folder entities must have a parent folder id. Since folders do not have underlying data, there is no Folder data transaction required.ArFS: \"0.13\" Cipher?: \"AES256-GCM\" Cipher-IV?: \"\" Content-Type: \"\" Drive-Id: \"\" Entity-Type: \"folder\" Folder-Id: \"\" Parent-Folder-Id?: \"\" Unix-Time: \"\" Data JSON { \"name\": \"\" } Folder Entity Transaction Example# FileA File contains uploaded data, like a photo, document, or movie.In the Arweave File System, a single file is broken into 2 parts - its metadata and its data.In the Arweave File System, a single file is broken into 2 parts - its metadata and its data.A File entity metadata transaction does not include the actual File data. Instead, the File data must be uploaded as a separate transaction, called the File Data Transaction. The File JSON metadata transaction contains a reference to the File Data Transaction ID so that it can retrieve the actual data. This separation allows for file metadata to be updated without requiring the file itself to be reuploaded. It also ensures that private files can have their JSON Metadata Transaction encrypted as well, ensuring that no one without authorization can see either the file or its metadata.ArFS: \"0.13\" Cipher?: \"AES256-GCM\" Cipher-IV?: \"\" Content-Type: \"\" Drive-Id: \"\" Entity-Type: \"file\" File-Id: \"\" Parent-Folder-Id: \"\" Unix-Time: \"\" Data JSON { \"name\": \"\", \"size\": \"\", \"lastModifiedDate\": \"\", \"dataTxId\": \"\", \"dataContentType\": \"\", \"pinnedDataOwner\": \"\" # Optional } Pin Files Since the version v0.13, ArFS suports Pins. Pins are files whose data may be any transaction uploaded to Arweave, that may or may not be owned by the wallet that created the pin.When a new File Pin is created, the only created transaction is the Metadata Transaction. The dataTxId field will point it to any transaction in Arweave, and the optional pinnedDataOwner field is gonna hold the address of the wallet that owns the original copy of the data transaction.File Data Transaction ExampleThe File Data Transaction contains limited information about the file, such as the information required to decrypt it, or the Content-Type (mime-type) needed to view in the browser.Cipher?: \"AES256-GCM\" Cipher-IV?: \"\" Content-Type: \"\" { File Data - Encrypted if private } File Metadata Transaction ExampleThe the File Metadata Transaction contains the GQL Tags necessary to identify the file within a drive and folder.Its data contains the JSON metadata for the file. This includes the file name, size, last modified date, data transaction id, and data content type.ArFS: \"0.13\" Cipher?: \"AES256-GCM\" Cipher-IV?: \"\" Content-Type: \"\" Drive-Id: \"\" Entity-Type: \"file\" File-Id: \"\" Parent-Folder-Id: \"\" Unix-Time: \"\" { File JSON Metadata - Encrypted if private } # SnapshotArFS applications generate the latest state of a drive by querying for all ArFS transactions made relating to a user's particular Drive-Id. This includes both paged queries for indexed ArFS data via GQL, as well as the ArFS JSON metadata entries for each ArFS transaction.For small drives (less than 1000 files), a few thousand requests for very small volumes of data can be achieved relatively quickly and reliably. For larger drives, however, this results in long sync times to pull every piece of ArFS metadata when the local database cache is empty. This can also potentially trigger rate-limiting related ArWeave Gateway delays.Once a drive state has been completely, and accurately generated, in can be rolled up into a single snapshot and uploaded as an Arweave transaction. ArFS clients can use GQL to find and retrieve this snapshot in order to rapidly reconstitute the total state of the drive, or a large portion of it. They can then query individual transactions performed after the snapshot.This optional method offers convenience and resource efficiency when building the drive state, at the cost of paying for uploading the snapshot data. Using this method means a client will only have to iterate through a few snapshots instead of every transaction performed on the drive.# Snapshot Entity TagsSnapshot entities require the following tags. These are queried by ArFS clients to find drive snapshots, organize them together with any other transactions not included within them, and build the latest state of the drive.ArFS: \"0.13\" Drive-Id: \"\" Entity-Type: \"snapshot\" Snapshot-Id: \"\" Content-Type: \"\" Block-Start: \"\" Block-End: \"\" Data-Start: \"\" Snapshot Transaction GQL tags example# Snapshot Entity DataA JSON data object must also be uploaded with every ArFS Snapshot entity. THis data contains all ArFS Drive, Folder, and File metadata changes within the associated drive, as well as any previous Snapshots. The Snapshot Data contains an array txSnapshots. Each item includes both the GQL and ArFS metadata details of each transaction made for the associated drive, within the snapshot's start and end period.A tsSnapshot contains a gqlNode object which uses the same GQL tags interface returned by the Arweave Gateway. It includes all of the important block, owner, tags, and bundledIn information needed by ArFS clients. It also contains a dataJson object which stores the correlated Data JSON for that ArFS entity.For private drives, the dataJson object contains the JSON-string-escaped encrypted text of the associated file or folder. This encrypted text uses the file's existing Cipher and Cipher-IV. This ensures clients can decrypt this information quickly using the existing ArFS privacy protocols.{ \"txSnapshots\": [ { \"gqlNode\": { \"id\": \"bWCvIc3cOzwVgquD349HUVsn5Dd1_GIri8Dglok41Vg\", \"owner\": { \"address\": \"hlWRbyJ6WUoErm3b0wqVgd1l3LTgaQeLBhB36v2HxgY\" }, \"bundledIn\": { \"id\": \"39n5evzP1Ip9MhGytuFm7F3TDaozwHuVUbS55My-MBk\" }, \"block\": { \"height\": 1062005, \"timestamp\": 1669053791 }, \"tags\": [ { \"name\": \"Content-Type\", \"value\": \"application/json\" }, { \"name\": \"ArFS\", \"value\": \"0.11\" }, { \"name\": \"Entity-Type\", \"value\": \"drive\" }, { \"name\": \"Drive-Id\", \"value\": \"f27abc4b-ed6f-4108-a9f5-e545fc4ff55b\" }, { \"name\": \"Drive-Privacy\", \"value\": \"public\" }, { \"name\": \"App-Name\", \"value\": \"ArDrive-App\" }, { \"name\": \"App-Platform\", \"value\": \"Web\" }, { \"name\": \"App-Version\", \"value\": \"1.39.0\" }, { \"name\": \"Unix-Time\", \"value\": \"1669053323\" } ] }, \"dataJson\": \"{\\\"name\\\":\\\"november\\\",\\\"rootFolderId\\\":\\\"71dfc1cb-5368-4323-972a-e9dd0b1c63a0\\\"}\" } ] } Snapshot Transaction JSON data example# Schema DiagramsThe following diagrams show complete examples of Drive, Folder, and File entity Schemas.# Public DrivePublic Drive Schema# Private DrivePrivate Drive Schema Data Model Content Types","estimatedWords":1263,"lastModified":"2025-10-20T12:14:45.509Z","breadcrumbs":["tooling","specs","arfs","entity types"],"siteKey":"arweave","siteName":"Arweave Cookbook","depth":4,"crawledAt":"2025-10-20T12:14:45.510Z"},{"url":"https://cookbook.arweave.net/tooling/specs/arfs/content-types.html","title":"Content Types","content":"Contributors: Dylan ShadeLast Updated: Edit# Content TypesAll transaction types in ArFS leverage a specific metadata tag for the Content-Type (also known as mime-type) of the data that is included in the transaction. ArFS clients must determine what the mime-type of the data is, in order for Arweave gateways and browswers to render this content appropriately.All public drive, folder, and file (metadata only) entity transactions all use a JSON standard, therefore they must have the following content type tag:Content-Type: '' However, a file's data transaction must have its mime-type determined. This is stored in the file's corresponding metadata transaction JSON's dataContentType as well as the content type tag in the data transaction itself.Content-Type: \"\" All private drive, folder, and file entity transactions must have the following content type, since they are encrypted:Content-Type: '' ArDrive-Coreopen in new window includes methods to determine a file's content type.# Other TagsArFS enabled clients should include the following tags on their transactions to identify their applicationApp-Name: \"<defined application name eg. ArDrive\" App-Version: \"<defined version of the app eg. 0.5.0\" Client?: \"<if the application has multiple clients, they should be specified here eg. Web\" Entity Types Privacy","estimatedWords":190,"lastModified":"2025-10-20T12:14:45.643Z","breadcrumbs":["tooling","specs","arfs","content types"],"siteKey":"arweave","siteName":"Arweave Cookbook","depth":4,"crawledAt":"2025-10-20T12:14:45.643Z"},{"url":"https://cookbook.arweave.net/tooling/specs/arfs/privacy.html","title":"Privacy","content":"Contributors: Dylan ShadeLast Updated: Edit# PrivacyThe Arweave blockweave is inherently public. But with apps that use ArFS, like ArDrive, your private data never leaves your computer without using military grade (and quantum resistantopen in new window) encryption. This privacy layer is applied at the Drive level, and users determine whether a Drive is public or private when they first create it. Private drives must follow the ArFS privacy model.Every file within a Private Drive is symmetrically encrypted using AES-256-GCMopen in new window. Every Private drive has a master \"Drive Key\" which uses a combination of the user's Arweave wallet signature, a user defined drive password, and a unique drive identifier (uuidv4open in new window). Each file has its own \"File Key\" derived from the \"Drive Key\". This allows for single files to be shared without exposing access to the other files within the Drive.Once a file is encrypted and stored on Arweave, it is locked forever and can only be decrypted using its file key.# Deriving KeysPrivate drives have a global drive key, D, and multiple file keys F, for encryption. This enables a drive to have as many uniquely encrypted files as needed. One key is used for all versions of a single file (since new file versions use the same File-Id)D is used for encrypting both Drive and Folder metadata, while F is used for encrypting File metadata and the actual stored data. Having these different keys, D and F, allows a user to share specific files without revealing the contents of their entire drive.D is derived using HKDF-SHA256 with an unsaltedopen in new window RSA-PSS signature of the drive's id and a user provided password.F is also derived using HKDF-SHA256 with the drive key and the file's id.Other wallets (like Wanderopen in new window) integrate with this Key Derivation protocol just exposing an API to collect a signature from a given Arweave Wallet in order to get the SHA-256 signature needed for the HKDFopen in new window to derive the Drive Key.An example implementation, using Dart, is available hereopen in new window, with a Typescript implementation hereopen in new window.# Private DrivesDrives can store either public or private data. This is indicated by the Drive-Privacy tag in the Drive entity metadata.Drive-Privacy: \"\" If a Drive entity is private, an additional tag Drive-Auth-Mode must also be used to indicate how the Drive Key is derived. ArDrive clients currently leverage a secure password along with the Arweave Wallet private key signature to derive the global Drive Key.Drive-Auth-Mode?: 'password' On every encrypted Drive Entity, a Cipher tag must be specified, along with the public parameters for decrypting the data. This is done by specifying the parameter with a Cipher-* tag. eg. Cipher-IV. If the parameter is byte data, it must be encoded as Base64 in the tag.ArDrive clients currently leverage AES256-GCM for all symmetric encryption, which requires a Cipher Initialization Vector consisting of 12 random bytes.Cipher?: \"AES256-GCM\" Cipher-IV?: \"\" Additionally, all encrypted transactions must have the Content-Type tag application/octet-stream as opposed to application/jsonPrivate Drive Entities and their corresponding Root Folder Entities will both use these keys and ciphers generated to symmetrically encrypt the JSON files that are included in the transaction. This ensures that only the Drive Owner (and whomever the keys have been shared with) can open the drive, discover the root folder, and continue to load the rest of the children in the drive.# Private FilesWhen a file is uploaded to a private drive, it by default also becomes private and leverages the same drive keys used for its parent drive. Each unique file in a drive will get its own set of file keys based off of that file's unique FileId. If a single file gets a new version, its File-Id will be reused, effectively leveraging the same File Key for all versions in that file's history.These file keys can be shared by the drive's owner as needed.Private File entities have both its metadata and data transactions encrypted using the same File Key, ensuring all facets of the data is truly private. As such, both the file's metadata and data transactions must both have a unique Cipher-IV and Cipher tag:Cipher?: \"AES256-GCM\" Cipher-IV?: \"\" Just like drives, private files must have the Content-Type tag set as application/octet-stream in both its metadata and data transactions:Content-Type: \"application/octet-stream\" Content Types schema-diagrams.md","estimatedWords":720,"lastModified":"2025-10-20T12:14:46.208Z","breadcrumbs":["tooling","specs","arfs","privacy"],"siteKey":"arweave","siteName":"Arweave Cookbook","depth":4,"crawledAt":"2025-10-20T12:14:46.208Z"},{"url":"https://cookbook.arweave.net/tooling/specs/arfs/schema-diagrams.html","title":"Cooking with the Permaweb","content":"Contributors: Dylan ShadeLast Updated: Edit# Schema DiagramsThe following diagrams show complete examples of Drive, Folder, and File entity Schemas.# Public DrivePublic Drive Schema# Private DrivePrivate Drive SchemaArweave GQL Tag Byte Limit is restricted to 2048. There is no determined limit on Data JSON custom metadata, though more data results in a higher upload cost. Privacy","estimatedWords":55,"lastModified":"2025-10-20T12:14:46.782Z","breadcrumbs":["tooling","specs","arfs","schema diagrams"],"siteKey":"arweave","siteName":"Arweave Cookbook","depth":4,"crawledAt":"2025-10-20T12:14:46.782Z"},{"url":"https://cookbook.arweave.net/references/glossary.html","title":"Glossary","content":"Contributors: Dylan ShadeLast Updated: Edit# GlossaryThe following is an embedded glossary toolopen in new window for the permaweb ecosystem. LLMs.txt","estimatedWords":20,"lastModified":"2025-10-20T12:14:47.402Z","breadcrumbs":["references","glossary"],"siteKey":"arweave","siteName":"Arweave Cookbook","depth":4,"crawledAt":"2025-10-20T12:14:47.402Z"},{"url":"https://cookbook.arweave.net/references/llms-txt.html","title":"LLMstxt","content":"Contributors: Dylan ShadeLast Updated: Edit# LLMs.txtThe following is a toolopen in new window that allows you to build your own LLMs.txt files based on docs from the permaweb ecosystem. Glossary Contributing","estimatedWords":31,"lastModified":"2025-10-20T12:14:47.979Z","breadcrumbs":["references","llms txt"],"siteKey":"arweave","siteName":"Arweave Cookbook","depth":4,"crawledAt":"2025-10-20T12:14:47.979Z"},{"url":"https://cookbook.arweave.net/getting-started/contributing.html","title":"Contributing Workflow","content":"Contributors: Dylan ShadeLast Updated: Edit# Contributing WorkflowAnyone in the community is welcome to contribute to the Permaweb Cookbook, as community members we want a high quality reference guide of little snack bite sized nuggets of information. Below is a step by step workflow of how anyone can contribute to this project.# What do you need to know?Git and Github - publishes content to github.com.Markdown - Markdown is a text based markup language that can be transformed into HTMLArweave and the Permaweb - Have some knowledge about the Permaweb that should be shared# Steps to Contribute# Commiting workWe are using conventional commitsopen in new window for this repository.General flow for making a contribution:Fork the repo on GitHubClone the project to your own machineCommit changes to your own branchPush your work back up to your forkSubmit a Pull request so that we can review your changesNOTE: Be sure to merge the latest from \"upstream\" before making a pull request!# StyleHere are some suggestions on tone and style from some contributors:TIPIn writing them, I'm getting a feeling for the tone that's appropriate for each. CoreConcepts should be rather textbook like, neutral voice, objective. \"This is how Arweave works\" For Guides, I think it's ok to have a more personal voice. Refer to the reader as \"you\" and speak in the collaborative voice \"next we'll take a look at...\" This may just be personal preference, but in general I feel this tone much more supportive and accessible when following a longer form guide. Indeed, its the voice that most popular tutorials from other ecosystems are written in. For Resources, I think it shares the same voice as core concepts, with a preference for brevity.dmacTIPConceptual and referencial data should have a more cold scientific tone and guides should be a supportive or even humorous tone. Longer form content needs to pull readers in without them zoning out.Arch_Druid# More information about contributing check out the repo style guideCONTRIBUTINGopen in new window LLMs.txt","estimatedWords":326,"lastModified":"2025-10-20T12:14:48.839Z","breadcrumbs":["getting started","contributing"],"siteKey":"arweave","siteName":"Arweave Cookbook","depth":4,"crawledAt":"2025-10-20T12:14:48.839Z"},{"url":"https://cookbook.arweave.net/community/index.html","title":"Community","content":"Contributors: Dylan ShadeLast Updated: Edit# CommunityResources for the Permaweb developer community and archived content.# Developer ResourcesForums and discussion channelsCommunity projects and collaborationsDeveloper meetups and eventsContribution guidelines# Archive (Deprecated Content)Historical documentation for reference:# SmartWeave (Legacy)SmartWeave - Legacy smart contract systemProfit Sharing Tokens (PSTs) - Legacy token standardAtomic Tokens - Legacy NFT implementation# Warp SDK (Legacy)IntroductionDeploying ContractsReading StateWrite InteractionsContract Evolution# Deprecated GuidesAtomic Tokens Guide# Migration NotesThe archived content represents older approaches and tools that have been superseded by newer, more efficient solutions. Refer to the current Concepts and Guides for up-to-date information. References","estimatedWords":91,"lastModified":"2025-10-20T12:14:49.628Z","breadcrumbs":["community","index"],"siteKey":"arweave","siteName":"Arweave Cookbook","depth":4,"crawledAt":"2025-10-20T12:14:49.628Z"},{"url":"https://cookbook.arweave.net/tooling/deployment.html","title":"Deployment  Publishing Tools","content":"Contributors: Dylan ShadeLast Updated: Edit# Deployment & Publishing ToolsTools for deploying applications and publishing content to the permaweb.# CLI Tools# arkbA command-line tool for deploying static websites and applications to Arweave.# permaweb-deployModern deployment tool with built-in bundling and optimization.# CI/CD Integration# GitHub ActionsAutomated deployment workflows for continuous integration.# Other CI PlatformsIntegration guides for popular CI/CD platforms.","estimatedWords":56,"lastModified":"2025-10-20T12:14:54.408Z","breadcrumbs":["tooling","deployment"],"siteKey":"arweave","siteName":"Arweave Cookbook","depth":3,"crawledAt":"2025-10-20T12:14:54.408Z"}],"lastCrawled":"2025-10-20T12:15:30.422Z","stats":{"totalPages":62,"averageWords":443,"duration":64025,"requestCount":116,"averageResponseTime":527.6896551724138,"pagesPerSecond":0.9683717297930495}},"wao":{"name":"WAO Documentation","baseUrl":"https://docs.wao.eco","pages":[{"url":"https://docs.wao.eco/hyperbeam/payment-system","title":"Payment System","content":"Payment System faff@1.0 faff@1.0 restricts node access to whitelisted accounts. You can pass a list of allowed accounts to your test HyperBEAM node. It only restricts POST requests. GET still works for all accounts. The node operator can update faff_allow_list via /~meta@1.0/info to manage the allowed accounts. File/test/payment-system.test.jsCopyimport assert from \"assert\" import { describe, it, before, after } from \"node:test\" import { HyperBEAM, acc } from \"wao/test\" import HB from \"wao\" import { rsaid, hmacid } from \"hbsig\" describe(\"Payment System faff@1.0\", function () { let hbeam, hb, operator let allowed_user = acc[0] let disallowed_user = acc[1] before(async () ).ready() operator = hbeam allowed_user.hb = new HB({ jwk: allowed_user.jwk }) disallowed_user.hb = new HB({ jwk: disallowed_user.jwk }) }) after(async () => hbeam.kill()) it(\"should test faff@1.0\", async () } We can get the 2 hashes required to construct commitments. File/test/payment-system.test.jsCopyimport { rsaid, hmacid } from \"wao/utils\" const hmacId = hmacid(lua_msg.headers) const rsaId = rsaid(lua_msg.headers) Now, we can construct commitments and the wrapped message. File/test/payment-system.test.jsCopyconst committed_lua_msg = { commitments: { [ rsaId ]: { alg: \"rsa-pss-sha512\", \"commitment-device\": \"httpsig@1.0\", committer: operator.addr, signature: lua_msg.headers.signature, \"signature-input\": lua_msg.headers[\"signature-input\"] }, [ hmacId ]: { alg: \"hmac-sha256\", \"commitment-device\": \"httpsig@1.0\", signature: lua_msg.headers.signature, \"signature-input\": lua_msg.headers[\"signature-input\"] } }, ...obj } WAO has, of course, a convenient method to create commitment. Copyconst committed_lua_msg = await operator.hb.commit(obj, { path: true }) Finally, we can send it to /ledger~node-process@1.0/schedule. File/test/payment-system.test.jsCopyawait operator.hb.post({ path: \"/ledger~node-process@1.0/schedule\", body: committed_lua_msg, }) Let's check the balance of the user. File/test/payment-system.test.jsCopyconst { out: balance } = await operator.hb.get({ path: `/ledger~node-process@1.0/now/balance/${user.addr}`, }) assert.equal(balance, 100) Now try executing some messages with the user. File/test/payment-system.test.jsCopy","estimatedWords":260,"lastModified":"2025-10-20T12:15:19.985Z","breadcrumbs":["hyperbeam","payment system"],"siteKey":"wao","siteName":"WAO Documentation","depth":2,"crawledAt":"2025-10-20T12:15:19.985Z"},{"url":"https://docs.wao.eco/hyperbeam/legacynet-aos","title":"Legacynet Compatible AOS","content":"Legacynet Compatible AOS Legacynet compatible AOS uses genesis-wasm@1.0 to delegate compute to an external local CU. You can use wasm modules stored on the Arweave Mainnet storage, or you could create a helper method to locally store wasm modules for testing. You should also pass as = [\"genesis_wasm\"] to the test HyperBEAM node to auto-start a local CU server with HyperBEAM. Let's use the production AOS2.0.6 module stored at ISShJH1ij-hPPt9St5UFFr_8Ys3Kj5cyg7zrMGt7H9s for now. ExclamationNesting the schduling message is the cleaner way to schedule messages, but we will learn it in the next chapter and keep it unnested here. When not nesting messages, HyperBEAM automatically replaces path with schedule for process-based messages, which invalidates signatures. To circumvent this, we need to exclude path from the signature base with { path: false }. File/test/legacynet-aos.test.jsCopyimport assert from \"assert\" import { describe, it, before, after } from \"node:test\" import { HyperBEAM } from \"wao/test\" i { hbeam = await new HyperBEAM({ reset: true, as: [\"genesis_wasm\"] }).ready() hb = hbeam.hb }) after(async () => hbeam.kill()) it(\"should spawn a legacynet AOS process\", async () = await hb.p( \"/schedule\", { device: \"process@1.0\", type: \"Process\", \"data-protocol\": \"ao\", variant: \"ao.TN.1\", scheduler: hb.addr, \"scheduler-location\": hb.addr, authority: hb.addr, \"random-seed\": seed(16), module: \"ISShJH1ij-hPPt9St5UFFr_8Ys3Kj5cyg7zrMGt7H9s\", \"scheduler-device\": \"scheduler@1.0\", \"execution-device\": \"stack@1.0\", \"device-stack\": [\"genesis-wasm@1.0\", \"patch@1.0\"], \"push-device\": \"push@1.0\", \"patch-from\": \"/results/outbox\", }, { path: false } ) await hb.p( `/${pid}/schedule`, { type: \"Message\", target: pid, action: \"Eval\", data }, { path: false } ) await hb.p( `/${pid}/schedule`, { type: \"Message\", target: pid, action: \"Inc\" }, { path: false } ) const { slot } = await hb.p( `/${pid}/schedule`, { type: \"Message\", target: pid, action: \"Inc\" }, { path: false } ) const { results } = await hb.g(`/${pid}/compute`, { slot }) assert.equal(\"Count: 2\", results.outbox[\"1\"].data) const { body } = await hb.post({ path: \"/~relay@1.0/call\", method: \"POST\", \"relay-path\": `http:","estimatedWords":297,"lastModified":"2025-10-20T12:15:20.437Z","breadcrumbs":["hyperbeam","legacynet aos"],"siteKey":"wao","siteName":"WAO Documentation","depth":2,"crawledAt":"2025-10-20T12:15:20.437Z"},{"url":"https://docs.wao.eco/hyperbeam/processes-scheduler","title":"Processes and Scheduler","content":"Processes and Scheduler Most things we've learned so far to do manually, such as device method composition, message caching, and commitments for message verification, are handled automatically by message@1.0, process@1.0, and scheduler@1.0. You can spawn a process, schedule messages to process slots, and compute the process state going through the allocated messages using multiple execution devices with stack@1.0. Let's create a new custom device dev_inc.erl. The minimum viable device to be compatible with process@1.0 requires 4 methods: init, normalize, compute, and snapshot. File/HyperBEAM/src/dev_inc.erlCopy-module(dev_inc). -export([ compute/3, init/3, snapshot/3, normalize/3 ]). -include_lib(\"eunit/include/eunit.hrl\"). -include(\"include/hb.hrl\"). compute(Msg1, Msg2, Opts) -> Num = maps:get(>, Msg1), {ok, hb_ao:set( Msg1, #{ > => Num + 1 }, Opts )}. init(Msg, Msg2, Opts) -> {ok, hb_ao:set(Msg, #{ > => 0 }, Opts)}. snapshot(Msg, _Msg2, _Opts) -> {ok, Msg}. normalize(Msg, _Msg2, _Opts) -> {ok, Msg}. Don't forget to add it to preloaded_devices in hb_opts.erl. File/HyperBEAM/src/hb_opts.erlCopypreloaded_devices => [ ... #{ > => >, > => dev_inc } ], Add message@1.0, process@1.0, scheduler@1.0, and inc@1.0 to our HyperBEAM node in the test file. You'll also need a random seed generation function, since message IDs are deterministic content hashes and it generates the same message ID if you send the same initialization message. To avoid this, you need to include a randomized value in the message content to spawn a new process. Also you need to nest the message in body without specifying path in the inner message. You can use the following paths to spawn, schedule, and compute: /~process@1.0/schedule : to spawn a process, requires scheduler=[operator_wallet_address] body device=\"process@1.0\" scheduler=[operator_wallet_address] type=\"Process\" execution-device /[pid]/schedule : to schedule a message to the pid body type=\"Message\" /[pid]/compute?slot=[slot] : to compute the pid state up to the slot pid is the process message ID returned by the spawn message. At this point, you could simply remove the devices parameter and preload all existing devices in our test file. File/test/processes-scheduler.test.jsCopyimport assert from \"assert\" import { describe, it, before, after } from \"node:test\" import { HyperBEAM } from \"wao/test\" i { hbeam = await new HyperBEAM({ reset: true }).ready() hb = hbeam.hb }) after(async () => hbeam.kill()) it(\"should spawn a process\", async () = await hb.p(\"/~process@1.0/schedule\", { scheduler: hb.addr, body: { device: \"process@1.0\", type: \"Process\", scheduler: hb.addr, \"random-seed\": seed(16), \"execution-device\": \"inc@1.0\", }, }) console.log(`Process ID: ${pid}`) const { slot } = await hb.p(`/${pid}/schedule`, { body: { type: \"Message\" }, }) console.log(`Allocated Slot: ${slot}`) const out = await hb.g(`/${pid}/compute`, { slot }) assert.equal(out.num, 2) const { slot: slot2 } = await hb.p(`/${pid}/schedule`, { body: { type: \"Message\" }, }) console.log(`Allocated Slot: ${slot2}`) const out2 = await hb.g(`/${pid}/compute`, { slot: slot2 }) assert.equal(out2.num, 3) }) }) now /[pid]/now gives you the latest process state. File/test/processes-scheduler.test.jsCopyconst { slot: slot3 } = await hb.p(`/${pid}/schedule`, { body: { type: \"Message\" }, }) console.log(`Allocated Slot: ${slot3}`) const out3 = await hb.g(`/${pid}/now`) assert.equal(out3.num, 4) WAO SDK WAO has convenient APIs for process management. File/test/processes-scheduler.test.jsCopyconst { pid } = await hb.spawn({ \"execution-device\": \"inc@1.0\" }) const { slot } = await hb.schedule({ pid }) const { num } = await hb.compute({ pid, slot }) assert.equal(num, 2) const { res: { num: num2 }, } = await hb.message({ pid })","estimatedWords":521,"lastModified":"2025-10-20T12:15:20.606Z","breadcrumbs":["hyperbeam","processes scheduler"],"siteKey":"wao","siteName":"WAO Documentation","depth":2,"crawledAt":"2025-10-20T12:15:20.606Z"},{"url":"https://docs.wao.eco/hyperbeam/device-composition","title":"Device Composition","content":"Device Composition So far, we've learned about HyperBEAM devices and URL pathing, the core codecs, HTTP message signatures, and hashpaths. You already know the fundamentals of how HyperBEAM works. Chaining Device Methods with URL Path Let's play around with device composition to build something powerful. We can access any cached messages with an ID or a hashpath at /[id | hashpath]. And we can also chain device methods like /~meta@1.0/info/~json@1.0/serialize. Could we chain our own device methods like the following? /[hashpath]/~mydev@1.0/inc/~mydev@1.0/double/~mydev@1.0/square Let's find out! Our goal is to pass an existing message with num, and compute num through the device method chaining. So if the initial message with a hashpath has num=6, => /~mydev@1.0/inc => 6 + 1 => num=7 => /~mydev@1.0/double => 7 * 2 => num=14 => /~mydev@1.0/square => 14 * 14 => num=196 is what we need to end up with. File/HyperBEAM/src/dev_mydev.erlCopy-export([ inc/3, double/3, square/3 ]). inc(Msg1, Msg2, Opts)-> Num = maps:get(>, Msg1), {ok, #{ > => Num + 1 }}. double(Msg1, Msg2, Opts)-> Num = maps:get(>, Msg1), {ok, #{ > => Num * 2 }}. square(Msg1, Msg2, Opts)-> Num = maps:get(>, Msg1), {ok, #{ > => Num * Num }}. We can use the resolve3 method from the previous chapter to create the base num with the hashpath cached. /~mydev@1.0/resolve3 returns num=6 with out.hashpath_7. File/test/device-composition.test.jsCopyconst out = await hb.p(\"/~mydev@1.0/resolve3\") const { num } = await hb.g( `/${out.hashpath_7}/~mydev@1.0/inc/~mydev@1.0/double/~mydev@1.0/square` ) assert.equal(num, 196) Voila! It works! But there are 3 caveats to this. First of all, during this pipeline, the Msg2 passed to each device method of inc/3, double/3, and square/3 stays the same and is the original committed Msg2 to the first method in the chain, which in this case is deviceless since we start the pipeline with /${out.hashpath_7}. If we were to start the chain with /~mydev@1.0/inc/~mydev@1.0/double/~mydev@1.0/square, the Msg2 would always be the same as what is passed to /~mydev@1.0/inc. So to evolve the state, you need to use the values from Msg1. Secondly, as we learned in an earlier chapter, Msg1 contains inter-decoded values, and not the final decoded values, which means even if you pass integer, Msg1 will have stringified num. You need to take the initial values from Msg2. Lastly, during the pipeline, you cannot overwrite the fields initially passed to Msg2. So you cannot pass num and update num during the pipeline. The initial Msg2 always overwrites the updated num and it ends up unchanged. So you need to pass something other than num, then update num during the pipeline. The case with /[hashpath]/~mydev@1.0/inc/~mydev@1.0/double/~mydev@1.0/square works since we're not passing num to the initial /[hashpath] execution. One way to solve this is to create an entry method like calc to take a different field such as init_num from Msg2, then pass it down to the pipeline as num. File/HyperBEAM/src/dev_mydev.erlCopy-export([ calc/3 ]). calc(Msg1, Msg2, Opts)-> Num = maps:get(>, Msg2), {ok, #{ > => Num}}. Now we can POST to /~mydev@1.0/calc/~mydev@1.0/inc/~mydev@1.0/double/~mydev@1.0/square, and get the correct output. File/test/device-composition.test.jsCopyconst { num } = await hb.p( \"/~mydev@1.0/calc/~mydev@1.0/inc/~mydev@1.0/double/~mydev@1.0/square\", { init_num: 1 } ) assert.equal(num, 16) Stacking Devices There is a built-in device called stack@1.0 to make device composition easy. It's supposed to be used with process@1.0, so it's limited in a certain way, but we can still use it without processes. Let's modify our methods to make them compatible with stack@1.0. We just need to forward device-stack from Msg1. File/HyperBEAM/src/dev_mydev.erlCopy-export([ inc2/3, double2/3, square2/3 ]). inc2(Msg1, Msg2, Opts)-> io:format(\"Inc: ~p~n\", [Msg1]), Num = maps:get(>, Msg1), {ok, #{ > => Num + 1, > => maps:get(>, Msg1) }}. double2(Msg1, Msg2, Opts)-> Num = maps:get(>, Msg1), {ok, #{ > => Num * 2, > => maps:get(>, Msg1) }}. square2(Msg1, Msg2, Opts)-> Num = maps:get(>, Msg1), {ok, #{ > => Num * Num, > => maps:get(>, Msg1) }}. Add the stack@1.0 device to the HyperBEAM class in our test file. File/test/device-composition.test.jsCopyimport assert from \"assert\" import { describe, it, before, after } from \"node:test\" import { HyperBEAM } from \"wao/test\" import { id } from \"hbsig\" const devices = [ \"json\", \"structured\", \"httpsig\", \"flat\", \"meta\", \"stack\", { name: \"mydev@1.0\", module: \"dev_mydev\" }, ] describe(\"Device Composition\", function () { let hbeam, hb before(async () ).ready() hb = hbeam.hb }) after(async () => hbeam.kill()) it(\"should stack devices\", async () , mode: \"Fold\", num: 3, } const out = await hb.p(\"inc2\", msg_base) assert.equal(out.num, 6)","estimatedWords":716,"lastModified":"2025-10-20T12:15:21.009Z","breadcrumbs":["hyperbeam","device composition"],"siteKey":"wao","siteName":"WAO Documentation","depth":2,"crawledAt":"2025-10-20T12:15:21.009Z"},{"url":"https://docs.wao.eco/hyperbeam/hashpaths","title":"Hashpaths","content":"Hashpaths Hashpath is a mechanism to make compute steps verifiable with chained hashes. Message ID Each message has an ID. For signed messages, the ID is the sha256 hash of all commitment IDs except for hmac joined with , . For unsigned messages, the ID is the hmac-sha256 hash of the message content with ao as the key. You can use hb_message:id on HyperBEAM. CopyID = hb_message:id(Msg) Or you can use id from hbsig. Copyimport { id } from \"hbisg\" const msg_id = id(msg) Message Resolving As we've learned so far, URLs like http:","estimatedWords":94,"lastModified":"2025-10-20T12:15:21.249Z","breadcrumbs":["hyperbeam","hashpaths"],"siteKey":"wao","siteName":"WAO Documentation","depth":2,"crawledAt":"2025-10-20T12:15:21.249Z"},{"url":"https://docs.wao.eco/hyperbeam/http-message-signatures","title":"HTTP Message Signatures","content":"HTTP Message Signatures So far, we've been virtually testing the encoding process by exposing internal codec methods and sending JSON stringified messages. But in practice, these methods are not available and we need to sign the httpsig encoded message before sending it to a remote node. AO Core / HyperBEAM uses the web standard protocol of HTTP Message Signatures (RFC-9421). Let's go back to the message tested in an earlier chapter and encode it with the pipeline from the previous chapter. File/test/http-message-signatures.test.jsCopyname=\"map\"\\r\\n' + '--x8jUsRrtoRCInzE6Nwgl_uoK-D2Oe-9i_RKeFskZk8c--', 'body-keys': '\"body\", \"map\"', bool: '\"true\"', 'content-digest': 'sha-256=:yrY11i+3uYmjCLzOaOeIijrNL/dyPWHNHJTJwvsKvsc=:', 'content-type': 'multipart/form-data; boundary=\"x8jUsRrtoRCInzE6Nwgl_uoK-D2Oe-9i_RKeFskZk8c\"', key: 'abc', list: '\"(ao-type-integer) 1\", \"(ao-type-integer) 2\", \"(ao-type-integer) 3\"', path: '/~mydev@1.0/forward' } You can sign it with hb.signEncoded. File/test/http-message-signatures.test.jsCopy boundary=\"x8jUsRrtoRCInzE6Nwgl_uoK-D2Oe-9i_RKeFskZk8c\"', key: 'abc', list: '\"(ao-type-integer) 1\", \"(ao-type-integer) 2\", \"(ao-type-integer) 3\"', path: '/~mydev@1.0/forward', 'content-length': '236', signature: 'http-sig-bba7e22451416f77=:D8jgRQXCC0WIdTaKs4v9cjvmzZy13VdoFNzWeXxT8ErP8inUeLyXQy0V4aUaodbAueMSG0sk8Ut5EmMaeV6AX4mHO6YtZHWj7TL7x8h2Sa8dlvcYNHjauNlygs0URoeKIaE0eZoWM1LWD6F+qqLPL4mm2C4Ex5UttPkNb8kT4UI5AuxlGWmIgOBZZngWT4xoRsFIlanr2bz4Px4IeiTJgLAS2QwRsJTAJ60EumZ5xCBTU6Ir98W45PrHUf2MUjVxmcaVFZb3nrB4mC/3IjXIBlHmMkjD4lRr7/FuIr8JwuBDzlpA+/VUmfx+0L2qVp+F0rL0VhBiFB7KiRCpPqNDXO5bw2bei1cxoQHmcwhAxO+BIisJrBrbylHo+7yw4LLzAGebunMgsfVzl5DIxZRcjxNCF/4vSYllB+oybEgqTw9MP0Iwip59yCnzsnCrvo1m8PvQ9izJBVL7OrasjFTj5i+iOwOJt7YAZ8Yea93m7c2lMFJNocEme3Otj3oWU9MwPv+qyId5Q59A9uSzKD5wKMrUcICRYduw3NmGzcxKzPMHoC4lZQEDtzZvIvsjm/EVPQpIt/oPLMp3y5ZHEuPUoZx1xT9ahMWyg8eNw01+TQPsDK6QDE7eeRvWleBuaIMkBVH6mXSiR5J+vteSXox1xK0cV9evc10inWZd9LtPfxM=:', 'signature-input': 'http-sig-bba7e22451416f77=(\"ao-types\" \"bool\" \"content-digest\" \"content-type\" \"key\" \"list\" \"content-length\");alg=\"rsa-pss-sha512\";keyid=\"o1kvTqZQ0wbS_WkdwX70TFCk7UF76ldnJ85l8iRV7t6mSlzkXBYCecb-8RXsNEQQmO0KergtHOvhuBJmB6YXaYe_UftI_gendojfIa6jlTgw-qmH6g4_oErI8djDRbQSm-5nCfGVRuYxsNZLYDeqw4gFb9K3b1h7tuMoLd6-d5pkaLfTMUNcvs2OqpkLo0i_av746FieaURdWozwFqO0APtdA7pLHDqQZDMNdTmsUBJFszL6SOa1bKe5cUWnrq4uaW4NAN3JAQniILKGsKZENeKtfXwiKVaFJtriWWsbhOaNT0JLcuBAwXQAP59RXzcr8bRY6XFn8zBmEmZBGszOD9c9ssDENRFDa5uyVhk8XgIgQjErAWYd9T6edrYcIp3R78jhNK_nLiIBBz8_Oz3bLjL5i_aiV2gpfIbd44DCHihuuxSWRAPJxhEy9TS0_QbVOIWhcDTIeEJE3aRPTwSTMt1_Fec7i9HJWN0mvMbAAJw8k6HxjA3pFZiCowZJw7FBwMAeYgEwIeB82f-S2-PtFLwR9i0tExo36hEBHqaS4Y-O3NGgQ8mKnhT7Z1EfxEbA2BpR9oL8rJFEnPIrHHu7B88OHDDfnfRD3D79fKktnisC7XOuwbHG3TQo0_j4_mElH7xj_7IyAbmCUHDd-eRa482wOYXBB01DGnad901qaHU\"' }, body: Blob { size: 236, type: '' } } The signing added signature and signature-input to headers. Let's break down signature-input. You can also verify the signature with hbsig, which gives you the decomposition of the message if you ever need it. File/test/http-message-signatures.test.jsCopyimport { verify } from \"hbsig\" const { valid,","estimatedWords":189,"lastModified":"2025-10-20T12:15:21.606Z","breadcrumbs":["hyperbeam","http message signatures"],"siteKey":"wao","siteName":"WAO Documentation","depth":2,"crawledAt":"2025-10-20T12:15:21.606Z"},{"url":"https://docs.wao.eco/hyperbeam/codec-httpsig","title":"Httpsig Codec","content":"Httpsig Codec httpsig@1.0 turns structured encoded objects into HTTP signature-ready objects. It flattens map structures into strings with the flat@1.0 device, and puts complex structures into the multipart body format. Create custom methods to expose dev_codec_httpsig:from and dev_codec_httpsig:to. File/HyperBEAM/src/dev_mydev.erlCopy-export([ httpsig_to/3, httpsig_from/3 ]). httpsig_to(Msg1, Msg2, Opts) -> Body = maps:get(>, Msg1), TABM = dev_codec_json:from(Body), HTTPSIG = dev_codec_httpsig:to(TABM), JSON = dev_codec_json:to(HTTPSIG), {ok, JSON}. httpsig_from(Msg1, Msg2, Opts) -> Body = maps:get(>, Msg1), HTTPSIG = dev_codec_json:from(Body), TABM = dev_codec_httpsig:from(HTPSIHG), JSON = dev_codec_json:to(TABM), {ok, JSON}. Let's convert one complex object. { a: { b: [1, 2, 3]}, c: { d: [3.14, true, \"str\"] } } The structured encoded representation is the following. File/test/codec-httpsig.test.jsCopyname=\"a\"\\r\\n' + '--rqDK_isKBhMozuATy4K6NFgdADGNHedXoUEDN10AANo\\r\\n' + 'ao-types: d=\"list\"\\r\\n' + 'content-disposition: form-data;name=\"c\"\\r\\n' + 'd: \"(ao-type-float) 3.14\", \"(ao-type-atom) \\\\\"true\\\\\"\", \"str\"\\r\\n' + '--rqDK_isKBhMozuATy4K6NFgdADGNHedXoUEDN10AANo--', 'body-keys': '\"a\", \"c\"', 'content-digest': 'sha-256=:mv08FUN7TpjmiHhagrxwqgjS7kQ/HY2+If2hIUq/y54=:', 'content-type': 'multipart/form-data; boundary=\"rqDK_isKBhMozuATy4K6NFgdADGNHedXoUEDN10AANo\"' } The encoding gives you 3 pieces of metadata. Fields other than body go into the HTTP headers. content-digest : the sha256 hash of body content, only required if body exists content-type : multipart/form-data with boundary body-keys : allocated key of each body part So you can split the body by the boundary of rqDK_isKBhMozuATy4K6NFgdADGNHedXoUEDN10AANo. content-disposition: form-data; : tells which path the part falls into name could be a flattened path like a/b/c Copyname=\"a\"\\r\\n', c: 'ao-types: d=\"list\"\\r\\n' + 'content-disposition: form-data;name=\"c\"\\r\\n' + 'd: \"(ao-type-float) 3.14\", \"(ao-type-atom) \\\\\"true\\\\\"\", \"str\"\\r\\n` } You can decode the encoded value with dev_codec_structured:to. File/test/codec-httpsig.test.jsCopyname=\"a\"\\r\\n' + '--rqDK_isKBhMozuATy4K6NFgdADGNHedXoUEDN10AANo\\r\\n' + 'ao-types: d=\"list\"\\r\\n' + 'content-disposition: form-data;name=\"c\"\\r\\n' + 'd: \"(ao-type-float) 3.14\", \"(ao-type-atom) \\\\\"true\\\\\"\", \"str\"\\r\\n' + '--rqDK_isKBhMozuATy4K6NFgdADGNHedXoUEDN10AANo--', 'body-keys': '\"a\", \"c\"', 'content-digest': 'sha-256=:mv08FUN7TpjmiHhagrxwqgjS7kQ/HY2+If2hIUq/y54=:', 'content-type': 'multipart/form-data; boundary=\"rqDK_isKBhMozuATy4K6NFgdADGNHedXoUEDN10AANo\"' } ] for (const v of cases) { const { body } = await hb.post({ path: \"/~mydev@1.0/httpsig_from\", body: JSON.stringify(v), }) console.log(JSON.parse(body)) } Another example reveals a couple of special fields. { data: \"abc\", \"Tbun4iRRQW93gUiSAmTmZJ2PGI-_yYaXsX69ETgzSRE\": 123 } The encoded value is: Copy{ 'ao-ids': 'Tbun4iRRQW93gUiSAmTmZJ2PGI-_yYaXsX69ETgzSRE=\"123\"', 'ao-types': '%54bun4i%52%52%51%5793g%55i%53%41m%54m%5a%4a2%50%47%49-_y%59a%58s%5869%45%54gz%53%52%45=\"integer\"', body: 'abc', 'content-digest': 'sha-256=:ungWv48Bz+pBQUDeXa4iI7ADYaOWF3qctBD/YfIAFa0=:', 'inline-body-key': 'data' } ao-ids : all keys get lower-cased during the encoding, but Arweave addresses are kept case-sensitive inline-body-key : the entire body will become the value of the specified key Encoding / Decoding Pipeline You can validate the encoding-decoding of any value with the following pipeline. File/test/codec-httpsig.test.jsCopyconst cases = [ { list: [1, true, \"abc\"] }, { nested_list: [1, [2, 3]] }, { a: { b: [1, 2, 3] } }, { a: [1, 2], b: [3, 4] }, { empty_list: [], empty_binary: \"\", empty_message: {} }, { data: \"abc\", [hb.addr]: 123 }, { list: [1, 2, 3], map: { a: { b: { c: 4 } } } }, ] for(const json of cases){ const res = await hb.post({ path: \"/~mydev@1.0/structured_from\", body: JSON.stringify(json), }) const structured = JSON.parse(res.body) console.log(structured) const res2 = await hb.post({ path: \"/~mydev@1.0/httpsig_to\", body: JSON.stringify(structured), }) const encoded = JSON.parse(res2.body) console.log(encoded) const res3 = await hb.post({ path: \"/~mydev@1.0/httpsig_from\", body: JSON.stringify(encoded), })","estimatedWords":460,"lastModified":"2025-10-20T12:15:21.821Z","breadcrumbs":["hyperbeam","codec httpsig"],"siteKey":"wao","siteName":"WAO Documentation","depth":2,"crawledAt":"2025-10-20T12:15:21.821Z"},{"url":"https://docs.wao.eco/hyperbeam/codec-structured","title":"Structured Codec","content":"Structured Codec structured@1.0 turns complex objects into strings with extended ao-types according to the HTTP Structured Field Values (RFC-9651) specification. This object type is called TABM (Type Annotated Binary Message) in HyperBEAM. ao-types has the following types: integer : 123 float : 3.14 binary : \"abc\" | Buffer.from([1,2,3]) atom : true | false | null | Symbol(\"abc\") list : [1, 2, 3] empty-binary : \"\" | Buffer.from([]) empty-list : [] empty-message : {} File/HyperBEAM/src/dev_mydev.erlCopy-export([ structured_to/3, structured_from/3 ]). structured_to(Msg1, Msg2, Opts) -> Body = maps:get(>, Msg1), OBJ = dev_codec_json:from(Body), TABM = dev_codec_structured:to(OBJ), JSON = dev_codec_json:to(TABM), {ok, JSON}. structured_from(Msg1, Msg2, Opts) -> Body = maps:get(>, Msg1), TABM = dev_codec_json:from(Body), OBJ = dev_codec_structured:from(TABM), JSON = dev_codec_json:to(OBJ), {ok, JSON}. Encode JSON with dev_codec_structured:from. File/test/codec-structured.test.jsCopyconst cases = [ { list: [1, true, \"abc\"] }, { nested_list: [1, [2, 3]] }, { a: { b: [1, 2, 3] } }, { a: [1, 2], b: [3, 4] }, { empty_list: [], empty_binary: \"\", empty_message: {} }, ] for (const v of cases) { const { out } = await hb.post({ path: \"/~mydev@1.0/structured_from\", body: JSON.stringify(v), }) console.log(JSON.parse(out)) } { list: [1, true, \"abc\"] } Copy{ 'ao-types': 'list=\"list\"', list: '\"(ao-type-integer) 1\", \"(ao-type-atom) \\\\\"true\\\\\"\", \"abc\"' } { nested_list: [1, [2, 3]] } Copy{ 'ao-types': 'nested_list=\"list\"', nested_list: '\"(ao-type-integer) 1\", \"(ao-type-list) \\\\\"(ao-type-integer) 2\\\\\", \\\\\"(ao-type-integer) 3\\\\\"\"' } { a: { b: [1, 2, 3] } } Copy{ a: { 'ao-types': 'b=\"list\"', b: '\"(ao-type-integer) 1\", \"(ao-type-integer) 2\", \"(ao-type-integer) 3\"' } } { a: [1, 2], b: [3, 4] } Copy{ a: '\"(ao-type-integer) 1\", \"(ao-type-integer) 2\"', 'ao-types': 'a=\"list\", b=\"list\"', b: '\"(ao-type-integer) 3\", \"(ao-type-integer) 4\"' } { empty_list: [], empty_binary: \"\", empty_message: {} } Copy{ 'ao-types': 'empty_binary=\"empty-binary\", empty_list=\"empty-list\", empty_message=\"empty-message\"' } You can specify ao-types of the values at the same level, annotate keys with (ao-type-[type]), and join multiple entries with , . Let's decode the encoded values. File/test/codec-structured.test.jsCopyconst cases = [ { 'ao-types': 'list=\"list\"', list: '\"(ao-type-integer) 1\", \"(ao-type-atom) \\\\\"true\\\\\"\", \"abc\"' }, { 'ao-types': 'nested_list=\"list\"', nested_list: '\"(ao-type-integer) 1\", \"(ao-type-list) \\\\\"(ao-type-integer) 2\\\\\", \\\\\"(ao-type-integer) 3\\\\\"\"' }, { a: { 'ao-types': 'b=\"list\"', b: '\"(ao-type-integer) 1\", \"(ao-type-integer) 2\", \"(ao-type-integer) 3\"' } }, { a: '\"(ao-type-integer) 1\", \"(ao-type-integer) 2\"', 'ao-types': 'a=\"list\", b=\"list\"', b: '\"(ao-type-integer) 3\", \"(ao-type-integer) 4\"' }, { 'ao-types': 'empty_binary=\"empty-binary\", empty_list=\"empty-list\", empty_message=\"empty-message\"' } ] for (const v of cases) { const { out } = await hb.post({ path: \"/~mydev@1.0/structured_to\", body: JSON.stringify(v), }) console.log(JSON.parse(out)) } Running Tests You can find the working test file for this chapter here: codec-structured.test.js Run tests: TerminalTerminalCopyyarn test test/codec-structured.test.js References Specs Structured Field Values for HTTP [RFC-9651] Device API dev_codec_structured.erl WAO API HyperBEAM Class API HB Class API","estimatedWords":422,"lastModified":"2025-10-20T12:15:22.318Z","breadcrumbs":["hyperbeam","codec structured"],"siteKey":"wao","siteName":"WAO Documentation","depth":2,"crawledAt":"2025-10-20T12:15:22.318Z"},{"url":"https://docs.wao.eco/hyperbeam/codec-flat","title":"Flat Codec","content":"Flat Codec flat@1.0 is a simple codec device to flatten/unflatten object paths since HTTP headers and body cannot handle nested object structures. It's internally used by httpsig@1.0 to resolve object paths. Codec devices have to and from methods to encode and decode, but they are not exposed to external URLs. We can create custom methods for our custom device to expose them. File/HyperBEAM/src/dev_mydev.erlCopy-export([ flat_to/3, flat_from/3 ]). flat_to(Msg1, Msg2, Opts) -> Body = maps:get(>, Msg1), OBJ = dev_codec_json:from(Body), FLAT = dev_codec_flat:to(OBJ), JSON = dev_codec_json:to(FLAT), {ok, JSON}. flat_from(Msg1, Msg2, Opts) -> Body = maps:get(>, Msg1), OBJ = dev_codec_json:from(Body), FLAT = dev_codec_flat:from(OBJ), JSON = dev_codec_json:to(FLAT), {ok, JSON}. One thing to note is that HTTP header keys cannot contain /, so if you ever need to send keys with / you need to push them into multipart body. We will handle this in the next chapter with Httpsig Codec. Flat codec only handles map structures. List structures are handled by Structured Codec. Also Flat Codec is an intermediary step used by Httpsig Codec, so values also have to be strings with dev_codec_flat:to. File/test/codec-flat.test.jsCopyconst cases = [ { a: { b: \"v\" } }, { a: \"v\", b: { c: \"v2\", d: \"v3\" } }, { a: { b: { c: { d: \"v\" } } } }, ] for (const v of cases) { const { body } = await hb.post({ path: \"/~mydev@1.0/flat_to\", body: JSON.stringify(v), }) console.log(JSON.parse(body)) } { a: { b: \"v\" } } -> { \"a/b\": \"v\" } { a: \"v\", b: { c: \"v2\", d: \"v3\" } } -> { a: \"v\", \"b/c\": \"v2\", \"b/d\": \"v3\" } { a: { b: { c: { d: \"v\" } } } } -> { \"a/b/c/d\": \"v\" } File/test/codec-flat.test.jsCopyconst cases = [ { \"a/b\": \"v\" }, { a: \"v\", \"b/c\": \"v2\", \"b/d\": \"v3\" }, { \"a/b/c/d\": \"v\" }, ] for (const v of cases) { const { body } = await hb.post({ path: \"/~mydev@1.0/flat_from\", body: JSON.stringify(v), }) console.log(JSON.parse(body)) } Running Tests You can find the working test file for this chapter here: codec-flat.test.js Run tests: TerminalTerminalCopyyarn test test/codec-flat.test.js References General HyperBEAM Class API HB Class API Device API dev_codec_flat.erl WAO API HyperBEAM Class API HB Class API","estimatedWords":363,"lastModified":"2025-10-20T12:15:22.491Z","breadcrumbs":["hyperbeam","codec flat"],"siteKey":"wao","siteName":"WAO Documentation","depth":2,"crawledAt":"2025-10-20T12:15:22.491Z"},{"url":"https://docs.wao.eco/hub","title":"WAO Hub","content":"WAO Hub WAO Hub is an ultra-lightweight ephemeral proxy that seamlessly connects different types of servers and browsers. Anyone can launch it both locally and remotely. WAO Hub: Connects the browser to remote HyperBEAM nodes via WebSockets Syncs your local file system to the browser via WebSockets Relays local AO tests to the browser via WebSockets Connects browsers with other browsers via WebRTC to create P2P mesh networks Copynpx wao hub Usage Coming Soon!","estimatedWords":74,"lastModified":"2025-10-20T12:15:23.016Z","breadcrumbs":["hub"],"siteKey":"wao","siteName":"WAO Documentation","depth":1,"crawledAt":"2025-10-20T12:15:23.016Z"},{"url":"https://docs.wao.eco/web","title":"AO The Web","content":"AO The Web AO The Web brings AO units directly into the browser—completely standalone, with no external dependencies. It includes an integrated AOS terminal, a code editor, and a local AO explorer for seamless development and debugging. WAO is both a standalone AO unit emulator and a lightweight SDK, enabling you to embed fully functional AO units into any web application with just a single line of code. Copyimport { AO } from \"wao/web\" preview.wao.eco","estimatedWords":75,"lastModified":"2025-10-20T12:15:23.368Z","breadcrumbs":["web"],"siteKey":"wao","siteName":"WAO Documentation","depth":1,"crawledAt":"2025-10-20T12:15:23.368Z"},{"url":"https://docs.wao.eco/legacynet","title":"Legacynet AOS","content":"Legacynet AOS WAO is still actively being developed; please use it at your discretion. Installation Copyyarn add wao Drop-in aoconnect Replacement for Tests By replacing aoconnect with WAO connect, everything runs in memory with zero latency and your tests are executed 1000x faster. The APIs are identical. So, there's no need to change anything else in your code. Copy","estimatedWords":59,"lastModified":"2025-10-20T12:15:23.652Z","breadcrumbs":["legacynet"],"siteKey":"wao","siteName":"WAO Documentation","depth":1,"crawledAt":"2025-10-20T12:15:23.652Z"},{"url":"https://docs.wao.eco/api/hbsig","title":"HBSig","content":"HBSig HyperBEAM / AO-Core requires complex encoding and signing involving multiple codecs and HTTP Message Signatures. However, it is not 100% compatible with the standard http-message-signatures libraries, and aoconnect only provides basic encoding for AOS messages. hbsig handles encoding of arbitrarily complex objects, which works on HyperBEAM. It is built by emulating the HyperBEAM codec devices as well as creating workaround encoding strategies using a custom device for extensive tests with LLMs. Installation hbsig is a standalone package providing utility methods for HyperBEAM codecs, signatures, encoding and hash algorithms. Copyyarn add hbsig Sign Message createSigner Copyimport { createSigner } from \"hbsig\" const hyperbeam_url = \"http:","estimatedWords":105,"lastModified":"2025-10-20T12:15:23.939Z","breadcrumbs":["api","hbsig"],"siteKey":"wao","siteName":"WAO Documentation","depth":2,"crawledAt":"2025-10-20T12:15:23.939Z"},{"url":"https://docs.wao.eco/api/hyperbeam","title":"HyperBEAM","content":"HyperBEAM HperBEAM class can start and manage a HyperBEAM node from within JS code for testing. You should first install HyperBEAM on your local machine by following the official docs. Basic Usage Copyimport { HyperBEAM } from \"wao/test\" import { describe, it, before, after } from \"node:test\" describe(\"HyperBEAM\", function () { let hbeam before(async () , operator: addr }).ready() }) after(() => hbeam.kill()) it(\"should run\", async () ) }) hbeam.kill() Parameters NameDefaultDescriptionport10001Port number for the HyperBEAM nodecwd../HyperBEAMnode directory relative to current working directorywallet.wallet.jsonnode operator jwk location relative to cwdresetfalseclear storage to start freshbundler-bundler service portgateway-gateway service portlogstrueset false to disable HyperBEAM logsshelltruefalse to not auto-start rebar3 shellas[]rocksdb, genesis_wasm, http3devices-array of preloaded devices, undefined to load everythingfaff-array of addresses (likely for funding/faucet)simple_payfalseenable simple-pay@1.0simple_pay_price-base price for transactionsp4_lua-{ processor: pid, client: cid }operator-payment operator address Environment Variables If your installation requires environment variables to run rebar3, define then in .env.hyperbeam. cwd can also be set in .env.hyperbeam to apply globally across all test files. File.env.hyperbeamCopyCWD=./HyperBEAM CC=gcc-12 CXX=g++-12 CMAKE_POLICY_VERSION_MINIMUM=3.5 Preloaded Devices KeyNameModulemetameta@1.0dev_metajsonjson@1.0dev_codec_jsonflatflat@1.0dev_codec_flathttpsighttpsig@1.0dev_codec_httpsigstructuredstructured@1.0dev_codec_structuredprocessprocess@1.0dev_processmessagemessage@1.0dev_messageschedulerscheduler@1.0dev_schedulerdelegated-computedelegated-compute@1.0dev_delegated_computegenesis-wasmgenesis-wasm@1.0dev_genesis_wasmlualua@5.3adev_luawasiwasi@1.0dev_wasiwasm-64wasm-64@1.0dev_wasmjson-ifacejson-iface@1.0dev_json_ifacetest-devicetest-device@1.0dev_testpatchpatch@1.0dev_patchpushpush@1.0dev_pushstackstack@1.0dev_stackmultipassmultipass@1.0dev_multipassfafffaff@1.0dev_faffp4p4@1.0dev_p4node-processnode-process@1.0dev_node_processsimple-paysimple-pay@1.0dev_simple_paycroncron@1.0dev_cronrelayrelay@1.0dev_relayrouterrouter@1.0dev_routercachecache@1.0dev_cachelocal-namelocal-name@1.0dev_local_namelookuplookup@1.0dev_lookupnamename@1.0dev_namecomputecompute@1.0dev_cudedupdedup@1.0dev_dedupmanifestmanifest@1.0dev_manifestmonitormonitor@1.0dev_monitorsnpsnp@1.0dev_snpvolumevolume@1.0dev_volumepodapoda@1.0dev_podagreenzonegreenzone@1.0dev_green_zonehyperbuddyhyperbuddy@1.0dev_hyperbuddyans104ans104@1.0dev_codec_ans104cachevizcacheviz@1.0dev_cachevizwaowao@1.0dev_wao If the device is not listed, you need to define it yourself. device = { name, module } Copyconst hbeam = await new HyperBEAM({ devices: [ \"flat\", \"structured\", \"httpsig\", \"json\", \"meta\", { name: \"mydev@1.0\", module: \"dev_mydev\" } ] }).ready() Node Operator Address You can use HyperBEAM.OPERATOR as a placeholder for the node operator address before instantiation. Copyconst hbeam = await new HyperBEAM({ operator: HyperBEAM.OPERATOR, faff: [ HyperBEAM.OPERATOR, addr2, addr3 ] }).ready() HyperBEAM.OPERATOR will be replaced with the actual node operator address on instantiation. Eunit Testing You can run Erlang eunit tests from JS. Copyimport assert from \"assert\" import { after, describe, it, before, beforeEach } from \"node:test\" import HyperBEAM from \"../../src/hyperbeam.js\" describe(\"Hyperbeam Eunit\", function () { let hbeam before(async () )) }) beforeEach(async () => (hb = hbeam.hb)) it(\"should run a single module test\", async () ) it(\"should run multiple module tests\", async () ) it(\"should run a specific test\", async () ) it(\"should run multiple tests\", async () ) }) Reading Local Files You can read local files under the HyperBEAM cwd directory with the file method. The following is reading lua scripts and caching them to the HyperBEAM node, then starting another node process on port 10002 with the p4 payment service using the cached Lua scripts. Copyconst process = hbeam.file(\"scripts/p4-payment-process.lua\") const pid = await hb.cacheScript(process) const client = hbeam.file(\"scripts/p4-payment-client.lua\") const cid = await hb.cacheScript(client) const hbeam2 = await new HyperBEAM({ port: 10002, operator: hb.addr, p4_lua: { processor: pid, client: cid }, }).ready()","estimatedWords":413,"lastModified":"2025-10-20T12:15:24.223Z","breadcrumbs":["api","hyperbeam"],"siteKey":"wao","siteName":"WAO Documentation","depth":2,"crawledAt":"2025-10-20T12:15:24.224Z"},{"url":"https://docs.wao.eco/api/armem","title":"ArMem","content":"ArMem ArMem stands for Arweave in memory and is a class to emulate an Arweave node and AO units in memory, which is internally used in the WAO testing framework. You can instantiate ArMem and control multiple emulators by passing it between other classes. Instantiate Instantiate When you instantiate WAO connect or AO from wao/test, it automatically and internally instantiates ArMem. Copyimport { connect } from \"wao/test\" const { spawn, message, dryrun, assign, result, mem } = connect()","estimatedWords":78,"lastModified":"2025-10-20T12:15:24.761Z","breadcrumbs":["api","armem"],"siteKey":"wao","siteName":"WAO Documentation","depth":2,"crawledAt":"2025-10-20T12:15:24.761Z"},{"url":"https://docs.wao.eco/api/gql","title":"GQL","content":"GQL GQL simplifies the Arwave GraphQL operations to query blocks and transactions. Instantiate You can instantiate the GQL class with an endpoint url. Copyimport { GQL } from \"wao\" const gql = new GQL({ url: \"https:","estimatedWords":36,"lastModified":"2025-10-20T12:15:25.103Z","breadcrumbs":["api","gql"],"siteKey":"wao","siteName":"WAO Documentation","depth":2,"crawledAt":"2025-10-20T12:15:25.103Z"},{"url":"https://docs.wao.eco/api/ar","title":"AR","content":"AR AR handles operations on the base Arweave Storage layer as well as wallet connections. Instantiate Copyimport { AR } from \"wao\" ({ err, jwk } = await ar.checkWallet({ jwk })) if(!err){","estimatedWords":32,"lastModified":"2025-10-20T12:15:25.317Z","breadcrumbs":["api","ar"],"siteKey":"wao","siteName":"WAO Documentation","depth":2,"crawledAt":"2025-10-20T12:15:25.317Z"},{"url":"https://docs.wao.eco/api/function-piping","title":"Function Piping","content":"Function Piping pipe Most functions return in the format of { err, res, out, pid, mid, id }, and these function can be chained with pipe, which makes executing multiple messages a breeze. For example, the following is how deploy uses pipe internally. The execution will be immediately aborted if any of the functions in fns produces an error. Copylet fns = [ { fn: \"spwn\", args: { module, scheduler, tags, data }, then: { \"args.pid\": \"pid\" }, }, { fn: \"wait\", then: { \"args.pid\": \"pid\" } }, { fn: \"load\", args: { src, fills }, then: { \"args.pid\": \"pid\" } } ] const { err, res, out, pid } = await this.pipe({ jwk, fns }) bind If the function comes from other instances rather than AO, use bind. Copyconst fns = [{ fn: \"post\", bind: this.ar, args: { data, tags }}] then You can pass values between functions with then. For instance, passing the result from the previous functions to the next function's arguments is a common operation. Copyconst fns = [ { fn: \"post\", bind: ao.ar, args: { data, tags }, then: ({ id, args, out })}, { fn: \"msg\", args: { pid, tags }}, ] const { out: { txid } } = await ao.pipe({ fns, jwk }) If then returns a value, pipe will immediately return with that single value. You can also use err to abort pipe with an error. Copyconst fns = [ { fn: \"msg\", args: { pid, tags }, then: ({ inp })}, { fn: \"msg\", args: { pid, tags }, err: ({ inp })}, ] const val = await ao.pipe({ jwk, fns }) then has many useful parameters. res : res from the previous result args : args for the next function out : the final out result from the pipe sequence inp : out from the previous result _ : if values are assigned to the _ fields, pipe returns them as top-level fields in the end pid : pid will be passed if any previous functions return pid ( e.g. deploy ) mid : mid will be passed if any previous functions return mid ( e.g. msg ) id : id will be passed if any previous functions return id ( e.g. post ) then can be a simplified hashmap object. Copylet fns = [ { fn: \"msg\", args: { tags }, then: { \"args.mid\": \"mid\", \"out.key\": \"inp.a\", \"_.val\": \"inp.b\" }, }, { fn: \"some_func\", args: {} }","estimatedWords":409,"lastModified":"2025-10-20T12:15:25.653Z","breadcrumbs":["api","function piping"],"siteKey":"wao","siteName":"WAO Documentation","depth":2,"crawledAt":"2025-10-20T12:15:25.653Z"},{"url":"https://docs.wao.eco/api/process","title":"Process","content":"Process You can go for even more concise syntax with Process class. Instantiate Copyconst p = ao.p(pid) or Copyconst { p, pid } = await ao.deploy({ data, tags, src, fills }) msg The first argument is Action, the second argument is Tags, and the third argument is the rest of the options. Copyconst { mid, res, out, err } = await p.msg( \"Action\", { Tag1: \"value1\", Tag2: \"value2\" }, { get: true, check: { TagA: \"valueA\" }, jwk } ) The default third argument is { get: false } to return the text Data. Copyconst { mid, out } = await p.msg(\"Action\", { Tag1: \"value1\", Tag2: \"value2\" }) The third parameter defaults to get if it's not an object. Copyconst { mid, out } = await p.msg(\"Action\", { Tag1: \"value1\" }, \"TagA\") is equivalent to Copyconst { mid, out } = await p.msg(\"Action\", { Tag1: \"value1\" }, \"TagA\") You can omit the second argument if there is no tag to pass to. Copyconst { mid, out } = await p.msg(\"Action\", { check: \"success!\" }} m You can only get out with m. This is the most extreme form. Copyconst out = await p.m(\"Action\", { Tag1: \"value1\", Tag2: \"value2\" }) This is a quite common pattern during testing. Doing the same with aoconnect requires an enormous amount of code, especially if it involves async/await receive(). Copyconst { p } = await ao.deploy({ tags, src_data, fills }) const out = await p.m(\"Action\", { Tag1: \"value1\", Tag2: \"value2\" })","estimatedWords":246,"lastModified":"2025-10-20T12:15:25.929Z","breadcrumbs":["api","process"],"siteKey":"wao","siteName":"WAO Documentation","depth":2,"crawledAt":"2025-10-20T12:15:25.929Z"},{"url":"https://docs.wao.eco/api/ao","title":"AO","content":"AO Instantiate You can initialize AO in the same way as AR. Copyimport { AO } from \"wao\" const ao = await new AO().init(jwk || arweaveWallet) If you need to pass AR settings, use ar. ao.ar will be automatically available. Copyconst ao = await new AO({ ar: { port: 4000 }}).init(jwk || arweaveWallet) const addr = ao.ar.addr await ao.ar.post({ data, tags }) AO Core Functions deploy Spawn a process, get a Lua source, and eval the script. src is an Arweave txid of the Lua script. Copyconst { err, res, pid, p } = await ao.deploy({ data, tags, src, fills }) You can directly pass the Lua script with src_data instead of src. Copyconst { err, res, pid, p } = await ao.deploy({ data, tags, src_data, fills }) boot will use On-Boot tag to initialize the process instead of Eval action. You can set either true to use src_data, or set a txid of an existing script. In case of true, data should be undefined so the src_data can fill it with spawn. Copyconst { err, res, pid, p } = await ao.deploy({ boot: true, tags, src_data, fills }) fills replace the Lua source script from src. Copylocal replace_me = '' local replace_me_again = '' local replace_me_with_hello_again = '' Copyconst fills = { REPLACE_ME: \"hello\", REPLACE_ME_AGAIN: \"world\" } This will end up in the following lua script. Copylocal replace_me = 'hello' local replace_me_again = 'world' local replace_me_with_hello_again = 'hello' In case you have multiple scripts, use loads and pass src and fills respectively. Copyawait ao.deploy({ tags, loads: [ { src, fills }, { src: src2, fills: fills2 } ] }) You can also pass an array of string data to loads. Copyconst num = `num = 0` const inc = `Handlers.add(\"Inc\", \"Inc\", function () num = num + 1 end)` const get = `Handlers.add(\"Get\", \"Get\", function (m) m.reply({ Data = num }) end)` const { p } = await ao.deploy({ tags, loads: [ num, inc, get ] }) await p.m(\"Inc\") assert.equal(await p.d(\"Get\"), 1) msg Send a message. Copyconst { err, mid, res, out } = await ao.msg({ pid, data, act, tags, check, get, mode, limit }) check determins if the message call is successful by checking through Tags in Messages in res. When using from either in check or get, mode needs to be set gql. mode defaults to aoconnect which uses the aoconnect.results function to track down results, which cannot tell where results come from. gql mode doesn't sometimes catch all results if used with AO/Arweave mainnet since there are lags due to the block finality time. limit specifies how many transactions or results to fetch for the check. Copyconst check = { Status : \"Success\" }","estimatedWords":448,"lastModified":"2025-10-20T12:15:26.267Z","breadcrumbs":["api","ao"],"siteKey":"wao","siteName":"WAO Documentation","depth":2,"crawledAt":"2025-10-20T12:15:26.267Z"},{"url":"https://docs.wao.eco/tutorials/running-llms","title":"Running LLMs on AOS (Highly Experimental)","content":"Running LLMs on AOS (Highly Experimental) You can run LLMs on top of AOS using the right module. First create a test project. Copynpx wao create llm && cd llm Create a directory and download one of the tiny models from Hugging Face. We will try TinyLlama-1.1B-Chat-v1.0-GGUF for this tutorial. Copymkdir test/models curl -L -o test/models/tinyllama.gguf \"https:","estimatedWords":57,"lastModified":"2025-10-20T12:15:26.480Z","breadcrumbs":["tutorials","running llms"],"siteKey":"wao","siteName":"WAO Documentation","depth":2,"crawledAt":"2025-10-20T12:15:26.480Z"},{"url":"https://docs.wao.eco/tutorials/devices-cpp","title":"Custom Devices in C","content":"Custom Devices in C++ Erlang can natively execute C++ functions with almost no overhead via NIF (Native Implemented Function). Creating a C++ Device Go to HyperBEAM/native directory and create a new directory for your C++ device. Copymkdir -p dev_mul_nif/include } The extern \"C\" linkage is crucial for making C++ functions callable from C/Erlang NIFs. Step 2: Implement the C++ Logic Create dev_mul.cpp: Copy#include \"include/dev_mul.h\" int multiply(const int a, const int b) { return a * b; } Step 3: Create the NIF Wrapper Create dev_mul_nif.cpp: Copy#include #include \"include/dev_mul.h\" static int load(ErlNifEnv* env, void** priv_data, ERL_NIF_TERM load_info) { return 0; } static void unload(ErlNifEnv* env, void* priv_data) {} static ERL_NIF_TERM mul_nif(ErlNifEnv* env, int argc, const ERL_NIF_TERM argv[]) { int a, b; if (!enif_get_int(env, argv[0], } int result = multiply(a, b); return enif_make_int(env, result); } static ErlNifFunc nif_funcs[] = { {\"multiply\", 2, mul_nif} }; ERL_NIF_INIT(dev_mul_nif, nif_funcs, load, NULL, NULL, unload) Step 4: Configure Build in rebar.config Add the C++ compilation settings to HyperBEAM/rebar.config: Copy{port_env, [ {\"(linux|darwin|solaris)\", \"CXX\", \"g++\"}, {\"(linux|darwin|solaris)\", \"CXXFLAGS\", \"$CXXFLAGS -std=c++17 -I${REBAR_ROOT_DIR}/native/dev_mul_nif/include -I/usr/local/lib/erlang/usr/include/\"}, {\"(linux|darwin|solaris)\", \"LDFLAGS\", } ]}. Add the port specification: Copy{port_specs, [ ... {\"./priv/dev_mul.so\", [ \"./native/dev_mul_nif/dev_mul_nif.cpp\", \"./native/dev_mul_nif/dev_mul.cpp\" ]} ... ]}. Add cleanup hooks: Copy{post_hooks, [ ... { compile, \"rm -f native/dev_mul_nif/*.o native/dev_mul_nif/*.d\"} ... ]}. Step 5: Create the Erlang NIF Module Create HyperBEAM/src/dev_mul_nif.erl: Copy-module(dev_mul_nif). -export([multiply/2]). -include(\"include/hb.hrl\"). -include_lib(\"eunit/include/eunit.hrl\"). -on_load(init/0). -define(NOT_LOADED, not_loaded(?LINE)). not_loaded(Line) -> erlang:nif_error({not_loaded, [{module, ?MODULE}, {line, Line}]}). init() -> PrivDir = code:priv_dir(hb), Path = filename:join(PrivDir, \"dev_mul\"), case erlang:load_nif(Path, 0) of ok -> ok; {error, Reason} -> exit({load_failed, Reason}) end. multiply(_A, _B) -> not_loaded(?LINE). Step 6: Create the Erlang Device Module Create HyperBEAM/src/dev_mul.erl: Copy-module(dev_mul). -export([mul/3]). -include(\"include/hb.hrl\"). -include_lib(\"eunit/include/eunit.hrl\"). mul(_, M2, Opts) -> A = hb_ao:get(>, M2, Opts), B = hb_ao:get(>, M2, Opts), Product = dev_mul_nif:multiply(A, B), {ok, #{ > => Product, > => A, > => B }}. multiply_test() -> M1 = #{> => >}, M2 = #{ > => >, > => 2, > => 3 }, {ok, Product} = hb_ao:resolve(M1, M2, #{}), ?assertEqual(6, maps:get(>, Product)). Step 7: Register the Device Add the device to HyperBEAM/hb_opt.erl: Copypreloaded_devices => [ ... #{> => >, > => dev_mul}, ... ], Step 8: Build and Test Run the unit tests: Copyrebar3 eunit --module=dev_mul Test the device with WAO: Copyit(\"should test mul@1.0\", async () ) assert.equal(res.headers.get(\"product\"), \"20\") })","estimatedWords":373,"lastModified":"2025-10-20T12:15:26.835Z","breadcrumbs":["tutorials","devices cpp"],"siteKey":"wao","siteName":"WAO Documentation","depth":2,"crawledAt":"2025-10-20T12:15:26.835Z"},{"url":"https://docs.wao.eco/tutorials/devices-rust","title":"Custom Devices in Rust","content":"Custom Devices in Rust Erlang can natively execute Rust functions with almost no overhead via NIF (Native Implemented Function). Creating a Rust Device Go to HyperBEAM/native directory and create a new Rust project. Copycargo new dev_add_nif --lib use rustler::types::atom::ok; #[rustler::nif] fn add(env: Env, a: i64, b: i64) -> NifResult> { Ok((ok(), a + b).encode(env)) } rustler::init!(\"dev_add_nif\", [add]); Step 3: Build the Rust Library Compile the Rust code: Copycargo build For release builds with optimizations: Copycargo build --release Step 4: Configure rebar.config Add the device to cargo_opts in HyperBEAM/rebar.config: Copy{cargo_opts, [ {src_dir, \"native/dev_add_nif\"}, {src_dir, \"native/dev_snp_nif\"} ]}. Step 5: Create the Erlang NIF Module Create HyperBEAM/src/dev_add_nif.erl: Copy-module(dev_add_nif). -export([add/2]). -on_load(init/0). -include(\"include/cargo.hrl\"). -include_lib(\"eunit/include/eunit.hrl\"). init() -> ?load_nif_from_crate(dev_add_nif, 0). add(_, _) -> erlang:nif_error(nif_not_loaded). Step 6: Create the Erlang Device Module Create HyperBEAM/src/dev_add.erl: Copy-module(dev_add). -export([add/3]). -include(\"include/hb.hrl\"). -include_lib(\"eunit/include/eunit.hrl\"). add(_M1, M2, _Opts) -> A = maps:get(>, M2), B = maps:get(>, M2), {ok, Sum} = dev_add_nif:add(A, B), {ok, #{ > => Sum }}. add_test() -> M1 = #{ > => > }, M2 = #{ > => >, > => 2, > => 3 }, {ok, #{ > := 5 }} = hb_ao:resolve(M1, M2, #{}). Step 7: Register the Device Add the device to HyperBEAM/hb_opt.erl: Copypreloaded_devices => [ ... #{> => >, > => dev_add}, ... ], Step 8: Build and Test Run the unit tests: Copyrebar3 eunit --module=dev_add Test the device with WAO: Copyit(\"should test add@1.0\", async () ) assert.equal(res.headers.get(\"sum\"), \"5\") })","estimatedWords":233,"lastModified":"2025-10-20T12:15:27.040Z","breadcrumbs":["tutorials","devices rust"],"siteKey":"wao","siteName":"WAO Documentation","depth":2,"crawledAt":"2025-10-20T12:15:27.040Z"},{"url":"https://docs.wao.eco/tutorials/creating-devices","title":"Creating Custom HyperBEAM Devices","content":"Creating Custom HyperBEAM Devices You can create your own HyperBEAM devices with Erlang, Rust or C++, and test them using WAO JS SDK. You should write unit tests in device's own language such as Erlang with eunit. Minimum Viable Device You can add an arbitrary device in the HyperBEAM/src directory. Copy-module(dev_foo). -export([ info/3 ]). -include_lib(\"eunit/include/eunit.hrl\"). -include(\"include/hb.hrl\"). info(Msg, _, Opts) -> {ok, hb_ao:set(Msg, #{ > => > }, Opts)}. Then add your device to the preloaded_devices list in HyperBEAM/src/hb_ops. Copypreloaded_devices => [ #{> => >, > => dev_codec_ans104}, #{> => >, > => dev_cu}, ... #{> => >, > => dev_foo} ], Now you can execute the functions using the WAO HB class. Copyimport assert from \"assert\" import { describe, it, before, after, beforeEach } from \"node:test\" import { HyperBEAM } from \"wao\" const cwd = \"../HyperBEAM\"","estimatedWords":136,"lastModified":"2025-10-20T12:15:27.393Z","breadcrumbs":["tutorials","creating devices"],"siteKey":"wao","siteName":"WAO Documentation","depth":2,"crawledAt":"2025-10-20T12:15:27.393Z"},{"url":"https://docs.wao.eco/tutorials/legacynet-aos","title":"Legacynet AOS on HyperBEAM","content":"Legacynet AOS on HyperBEAM Currently, testing legacynet aos processes works much better without HyperBEAM. You can check out Legacynet AOS guide to test with legacy AO units. Legacynet AOS on HyperBEAM uses genesis-wasm@1.0 device and an external CU for computation. The standalone WAO server works as a local CU. spawnLegacy, schedule, and computeLegacy manages the process for you. Copyimport assert from \"assert\" import { describe, it, before, after, beforeEach } from \"node:test\" import { HyperBEAM } from \"wao\" const cwd = \"../HyperBEAM\"","estimatedWords":82,"lastModified":"2025-10-20T12:15:27.936Z","breadcrumbs":["tutorials","legacynet aos"],"siteKey":"wao","siteName":"WAO Documentation","depth":2,"crawledAt":"2025-10-20T12:15:27.936Z"},{"url":"https://docs.wao.eco/hyperbeam/custom-devices-codecs","title":"Custom Devices and Codecs","content":"Custom Devices and Codecs We are going to learn the core codecs such as path flattening, HTTP message signatures, and AO types while building a custom HyperBEAM device. Accessible Device Methods If you look into any dev_ prefixed Erlang files under HyperBEAM/src, device methods are defined and exported in method_name/arity format. HyperBEAM automatically routes HTTP requests to device methods defined with arity of 3. So any methods exported as method_name/3 are automatically accessible. For instance, the dev_meta.erl file has these lines, which make /~meta@1.0/info and /~meta@1.0/build accessible via URL endpoints. File/HyperBEAM/src/dev_meta.erlCopy-export([info/1, info/3, build/3, handle/2, adopt_node_message/2, is/2, is/3]). info(_, Request, NodeMsg) -> build(_, _, _NodeMsg) -> Another example is dev_codec_json.erl with deserialize/3 and serialize/3 exposed. File/HyperBEAM/src/dev_codec_json.erlCopy-export([deserialize/3, serialize/3]). deserialize(Base, Req, Opts) -> serialize(Base, _Msg, _Opts) -> The device names are defined in hb_opt.erl under preloaded_devices. File/HyperBEAM/src/hb_opts.erlCopypreloaded_devices => [ ... #{> => >, > => dev_codec_json}, ... #{> => >, > => dev_meta}, ... ], This is exactly how you can build your own custom devices. Building Custom Devices Create dev_mydev.erl under /HyperBEAM/src, and define the info/3 method. The following is the minimum viable HyperBEAM device implementation. File/HyperBEAM/src/dev_mydev.erlCopy-module(dev_mydev). -export([ info/3 ]). -include_lib(\"eunit/include/eunit.hrl\"). -include(\"include/hb.hrl\"). info(Msg1, Msg2_, Opts) -> {ok, #{ > => > }}. Also, add the device to preloaded_devices in hb_opts.erl. File/HyperBEAM/src/hb_opts.erlCopypreloaded_devices => [ ... #{> => >, > => dev_codec_json}, ... #{> => >, > => dev_meta}, ... #{> => >, > => dev_mydev} ], Now you can test your device using WAO. Don't forget to preload your mydev device. File/test/custom-devices-codecs.test.jsCopyimport assert from \"assert\" import { describe, it, before, after } from \"node:test\" import { HyperBEAM } from \"wao/test\" alg=\"rsa-pss-sha512\";keyid=\"o1kvTqZQ0wbS_WkdwX70TFCk7UF76ldnJ85l8iRV7t6mSlzkXBYCecb-8RXsNEQQmO0KergtHOvhuBJmB6YXaYe_UftI_gendojfIa6jlTgw-qmH6g4_oErI8djDRbQSm-5nCfGVRuYxsNZLYDeqw4gFb9K3b1h7tuMoLd6-d5pkaLfTMUNcvs2OqpkLo0i_av746FieaURdWozwFqO0APtdA7pLHDqQZDMNdTmsUBJFszL6SOa1bKe5cUWnrq4uaW4NAN3JAQniILKGsKZENeKtfXwiKVaFJtriWWsbhOaNT0JLcuBAwXQAP59RXzcr8bRY6XFn8zBmEmZBGszOD9c9ssDENRFDa5uyVhk8XgIgQjErAWYd9T6edrYcIp3R78jhNK_nLiIBBz8_Oz3bLjL5i_aiV2gpfIbd44DCHihuuxSWRAPJxhEy9TS0_QbVOIWhcDTIeEJE3aRPTwSTMt1_Fec7i9HJWN0mvMbAAJw8k6HxjA3pFZiCowZJw7FBwMAeYgEwIeB82f-S2-PtFLwR9i0tExo36hEBHqaS4Y-O3NGgQ8mKnhT7Z1EfxEbA2BpR9oL8rJFEnPIrHHu7B88OHDDfnfRD3D79fKktnisC7XOuwbHG3TQo0_j4_mElH7xj_7IyAbmCUHDd-eRa482wOYXBB01DGnad901qaHU\";tag=\"PBF4RF2dpjDPZ_uujtBb--DcOx_Z4EyeUdjAerMpsAw/jNI0FLgi9Lz2UT_l1sK2TCPZMCIwFpPcOK3e3cqdRwo\"', status: '200', 'transfer-encoding': 'chunked', version: '1.0' } Erlang JSON One way to purify the return value is to return stringified JSON using the json@1.0 device internally. The device has a dev_codec_json:to/1 method to convert an Erlang object to JSON. The internal device names are defined with preloaded_devices in hb_opts.erl. #{> => >, > => dev_codec_json} And you can internally execute all exposed methods. File/HyperBEAM/src/dev_codec_json.erlCopy-module(dev_codec_json). -export([to/1, from/1, commit/3, verify/3, committed/1, content_type/1]). -export([deserialize/3, serialize/3]). When you define a new method, don't forget to export it from mydev@1.0. From this point on, we'll always assume you're correctly exporting new methods. File/HyperBEAM/src/dev_mydev.erlCopy-export([ info_json/3 ]). info_json(Msg1, Msg2_, Opts) -> JSON = dev_codec_json:to(#{ > => > }), {ok, JSON}. Now stringified JSON is returned in body, and you can just JSON.parse(body) to get exactly what you return. File/test/custom-devices-codecs.test.jsCopyconst { body } = await hb.get({ path: \"/~mydev@1.0/info_json\" }) boundary=\\\"eyja4UA4reu5SLEKVqY67NG8Q-jMdqEFmleD-hhSJKM\\\"\", \"device\": \"mydev@1.0\", \"key\": \"abc\", \"list\": \"\\\"(ao-type-integer) 1\\\", \\\"(ao-type-integer) 2\\\", \\\"(ao-type-integer) 3\\\"\", \"map\": { \"abc\": \"123\" }, \"method\": \"POST\" }, \"msg2\": { \"body\": \"test_body\", \"bool\": true, \"commitments\": { \"ovgxZflZZcY_kXWpV6yWf_ilaGuMDh7PUGC1YNhJQ90\": { \"alg\": \"hmac-sha256\", \"commitment-device\": \"httpsig@1.0\", \"signature\": \"http-sig-bba7e22451416f77=:kqtatfcnCrmmJiEc1GT3hKKU6tUVRy34hDN6z1vN5UHCwNZ4f+tu9FafZ/mOx8loq11DgdV8S7Xvxk5LzytMAtV1SmAArEQ1VbMJkS+bIiNksd4qmU13JkQjz+a90FYVUDKn0uU+cRUDx+7wVh4Rco27WEBj/E5yVreKcmG0fpORHi4DMV219cb0zUAdDEqY/FdvdlC+Xr91xQzDwcwS8goeHS879P6FLRo5BubLWx/bbJXoS2BEGowkWAORP1jooWe+oNIcWbMWA1CUpPTih2VXbUQcdRto5DDjwXw90nxD3UVPLegweGOZrASuccG9oFg/++mJeFFz3W6cy3Eg84WmrMjfbzsUb6WVcKti83YZYTo7onUDwad2wVUe2WDCuMLm8TFhwP8zwU/MHSfcahRnZasnroPwxvYRjFNWa5USyqGaZ7uM/wqArGKL/2dlh3bIphEmTjtYBc9q2tlosNgPngwnPu6qbEFQpcDEzsODQQBYrnP9HA6HIqsC+dWPINw0xueFqnhu5bv3Y+Y17vFc4zOraBpVCZUEMKEDPgNHe2vMjFVfgIbKp9I9Xu+8vYd8sL+2p+lgkrXVjNCS07XYjVHj855GKCmIZHs9fZa0dfLghOdDfnbexzCpSDIVnfstZx5yniXh00Rk3g2wUAWB7Sq7nRG86BiOQP+EcHY=:\", \"signature-input\": \"http-sig-bba7e22451416f77=(\\\"key\\\" \\\"list\\\" \\\"bool\\\" \\\"ao-types\\\" \\\"content-type\\\" \\\"content-digest\\\" \\\"content-length\\\");alg=\\\"rsa-pss-sha512\\\";keyid=\\\"o1kvTqZQ0wbS_WkdwX70TFCk7UF76ldnJ85l8iRV7t6mSlzkXBYCecb-8RXsNEQQmO0KergtHOvhuBJmB6YXaYe_UftI_gendojfIa6jlTgw-qmH6g4_oErI8djDRbQSm-5nCfGVRuYxsNZLYDeqw4gFb9K3b1h7tuMoLd6-d5pkaLfTMUNcvs2OqpkLo0i_av746FieaURdWozwFqO0APtdA7pLHDqQZDMNdTmsUBJFszL6SOa1bKe5cUWnrq4uaW4NAN3JAQniILKGsKZENeKtfXwiKVaFJtriWWsbhOaNT0JLcuBAwXQAP59RXzcr8bRY6XFn8zBmEmZBGszOD9c9ssDENRFDa5uyVhk8XgIgQjErAWYd9T6edrYcIp3R78jhNK_nLiIBBz8_Oz3bLjL5i_aiV2gpfIbd44DCHihuuxSWRAPJxhEy9TS0_QbVOIWhcDTIeEJE3aRPTwSTMt1_Fec7i9HJWN0mvMbAAJw8k6HxjA3pFZiCowZJw7FBwMAeYgEwIeB82f-S2-PtFLwR9i0tExo36hEBHqaS4Y-O3NGgQ8mKnhT7Z1EfxEbA2BpR9oL8rJFEnPIrHHu7B88OHDDfnfRD3D79fKktnisC7XOuwbHG3TQo0_j4_mElH7xj_7IyAbmCUHDd-eRa482wOYXBB01DGnad901qaHU\\\"\" }, \"we4Z3weGpJUUEwgeWmkIQsRJBTCfaB1s75LfgudSC1I\": { \"alg\": \"rsa-pss-sha512\", \"commitment-device\": \"httpsig@1.0\", \"committer\": \"Tbun4iRRQW93gUiSAmTmZJ2PGI-_yYaXsX69ETgzSRE\", \"signature\": \"http-sig-bba7e22451416f77=:kqtatfcnCrmmJiEc1GT3hKKU6tUVRy34hDN6z1vN5UHCwNZ4f+tu9FafZ/mOx8loq11DgdV8S7Xvxk5LzytMAtV1SmAArEQ1VbMJkS+bIiNksd4qmU13JkQjz+a90FYVUDKn0uU+cRUDx+7wVh4Rco27WEBj/E5yVreKcmG0fpORHi4DMV219cb0zUAdDEqY/FdvdlC+Xr91xQzDwcwS8goeHS879P6FLRo5BubLWx/bbJXoS2BEGowkWAORP1jooWe+oNIcWbMWA1CUpPTih2VXbUQcdRto5DDjwXw90nxD3UVPLegweGOZrASuccG9oFg/++mJeFFz3W6cy3Eg84WmrMjfbzsUb6WVcKti83YZYTo7onUDwad2wVUe2WDCuMLm8TFhwP8zwU/MHSfcahRnZasnroPwxvYRjFNWa5USyqGaZ7uM/wqArGKL/2dlh3bIphEmTjtYBc9q2tlosNgPngwnPu6qbEFQpcDEzsODQQBYrnP9HA6HIqsC+dWPINw0xueFqnhu5bv3Y+Y17vFc4zOraBpVCZUEMKEDPgNHe2vMjFVfgIbKp9I9Xu+8vYd8sL+2p+lgkrXVjNCS07XYjVHj855GKCmIZHs9fZa0dfLghOdDfnbexzCpSDIVnfstZx5yniXh00Rk3g2wUAWB7Sq7nRG86BiOQP+EcHY=:\", \"signature-input\": \"http-sig-bba7e22451416f77=(\\\"key\\\" \\\"list\\\" \\\"bool\\\" \\\"ao-types\\\" \\\"content-type\\\" \\\"content-digest\\\" \\\"content-length\\\");alg=\\\"rsa-pss-sha512\\\";keyid=\\\"o1kvTqZQ0wbS_WkdwX70TFCk7UF76ldnJ85l8iRV7t6mSlzkXBYCecb-8RXsNEQQmO0KergtHOvhuBJmB6YXaYe_UftI_gendojfIa6jlTgw-qmH6g4_oErI8djDRbQSm-5nCfGVRuYxsNZLYDeqw4gFb9K3b1h7tuMoLd6-d5pkaLfTMUNcvs2OqpkLo0i_av746FieaURdWozwFqO0APtdA7pLHDqQZDMNdTmsUBJFszL6SOa1bKe5cUWnrq4uaW4NAN3JAQniILKGsKZENeKtfXwiKVaFJtriWWsbhOaNT0JLcuBAwXQAP59RXzcr8bRY6XFn8zBmEmZBGszOD9c9ssDENRFDa5uyVhk8XgIgQjErAWYd9T6edrYcIp3R78jhNK_nLiIBBz8_Oz3bLjL5i_aiV2gpfIbd44DCHihuuxSWRAPJxhEy9TS0_QbVOIWhcDTIeEJE3aRPTwSTMt1_Fec7i9HJWN0mvMbAAJw8k6HxjA3pFZiCowZJw7FBwMAeYgEwIeB82f-S2-PtFLwR9i0tExo36hEBHqaS4Y-O3NGgQ8mKnhT7Z1EfxEbA2BpR9oL8rJFEnPIrHHu7B88OHDDfnfRD3D79fKktnisC7XOuwbHG3TQo0_j4_mElH7xj_7IyAbmCUHDd-eRa482wOYXBB01DGnad901qaHU\\\"\" } }, \"content-length\": \"253\", \"content-type\": \"multipart/form-data; boundary=\\\"eyja4UA4reu5SLEKVqY67NG8Q-jMdqEFmleD-hhSJKM\\\"\", \"key\": \"abc\", \"list\": [ 1, 2, 3 ], \"map\": { \"abc\": \"123\" }, \"method\": \"POST\", \"path\": \"forward\" }, \"opts\": { \"mode\": \"debug\", \"store\": { \"prefix\": \"cache-mainnet\", \"store-module\": \"hb_store_fs\" }, \"hb_config_location\": \"config.flat\", ... } } TABM (Type Annotated Binary Message) We'll explain the commitments later, but now, if you strip down the 2 msgs: Copy{ \"msg1\": { \"body\": \"test_body\", \"bool\": \"\\\"true\\\"\", \"device\": \"wao@1.0\", \"key\": \"abc\", \"list\": \"\\\"(ao-type-integer) 1\\\", \\\"(ao-type-integer) 2\\\", \\\"(ao-type-integer) 3\\\"\", \"map\": { \"abc\": \"123\" }, \"method\": \"POST\", \"num\": \"123\" }, \"msg2\": { \"body\": \"test_body\", \"bool\": true, \"key\": \"abc\", \"list\": [1, 2, 3], \"map\": { \"abc\": \"123\" }, \"method\": \"POST\", \"num\": 123, \"path\": \"forward\" } } You can observe that bool, list, and num are encoded in some string form. The msg1 fields are encoded by structured@1.0 and msg2 fields are decoded. FYI, msg1 is also different from the form we sent; msg1 is already decoded by httpsig@1.0 device. The msg1 object type is called TABM (Type Annotated Binary Message) and this is what HyperBEAM internally uses to circumvent the limitation that we can only pass flattened strings in the HTTP headers. Encoding / Decoding Steps So a client encodes a message with httpsig@1.0, then structured@1.0 into TABM, then signs it with http-message-signatures, then sends it to a HyperBEAM node. DATA = { path: \"/~wao@1.0/forward\", key: \"abc\" } TABM = encode_by_structured(DATA) HTTP_MSG = encode_by_httpsig(TABM) = Headers + Body SIGNED_HTTP_MSG = sign(HTTP_MSG) send(SIGNED_HTTP_MSG) The node receives it, verifies the signature, then decodes it first with structured@1.0 (msg1), then with httpsig@1.0 (msg2). Msg0 + Commitments = dev_codec_httpsig:verify(SIGNED_HTTP_MSG) Msg1(TABM) = dev_codec_httpsig:to(Msg0) + Device Msg2 = dev_codec_structured:to(Msg1) + Commitments + Path flat@1.0 is used to flatten and unflatten nested object paths in the httpsig@1.0 device since HTTP headers and body can only handle string values, not nested structures. Running Tests You can find the working test file for this chapter here: custom-devices-codecs.test.js Run tests: TerminalTerminalCopyyarn test test/custom-devices-codecs.test.js Reference General Extending HyperBEAM with Devices Device Docs Device: ~json@1.0 Device API dev_codec_json.erl WAO API HyperBEAM Class API HB Class API","estimatedWords":813,"lastModified":"2025-10-20T12:15:28.246Z","breadcrumbs":["hyperbeam","custom devices codecs"],"siteKey":"wao","siteName":"WAO Documentation","depth":2,"crawledAt":"2025-10-20T12:15:28.246Z"},{"url":"https://docs.wao.eco/hyperbeam/devices-pathing","title":"Devices and Pathing","content":"Devices and Pathing The first thing to understand is that HyperBEAM consists of a collection of devices, and you can access specific methods on specific devices via URL endpoints. For instance, the meta@1.0 device lets you get and set node configurations with its info method. Let's get the info with bare JS fetch. http:","estimatedWords":54,"lastModified":"2025-10-20T12:15:28.575Z","breadcrumbs":["hyperbeam","devices pathing"],"siteKey":"wao","siteName":"WAO Documentation","depth":2,"crawledAt":"2025-10-20T12:15:28.576Z"},{"url":"https://docs.wao.eco/hyperbeam/installing-hb-wao","title":"Installing HyperBEAM and WAO","content":"Installing HyperBEAM and WAO Installing HyperBEAM Follow the HyperBEAM docs and install HyperBEAM on your local machine. You could use one of the existing remote nodes, but you'll miss many important details in these tutorials since we'll literally crack open the internals. Installing WAO Create a WAO project that comes with the wao SDK and testing framework: TerminalTerminalCopynpx wao create myapp && cd myapp You can also create an empty directory and install wao and hbsig: TerminalTerminalCopymkdir myapp && cd myapp && yarn init && yarn add wao hbsig mkdir test && touch test/hyperbeam.js Edit package.json to enable ESM and test commands with the --experimental-wasm-memory64 flag and disable concurrency so the test won't try running multiple HyperBEAM nodes: File/package.jsonCopy{ \"name\": \"myapp\", \"version\": \"0.0.1\", \"type\": \"module\", \"scripts\": { \"test\": \"node --experimental-wasm-memory64 --test --test-concurrency=1\", \"test-only\": \"node --experimental-wasm-memory64 --test-only --test-concurrency=1\", \"test-all\": \"node --experimental-wasm-memory64 --test --test-concurrency=1 test*.test.js\" }, \"dependencies\": { \"hbsig\": \"^0.0.7\", \"wao\": \"^0.33.3\" } } Writing Tests Import the HyperBEAM and HB classes from wao to interact with your HyperBEAM node. Make sure you have an Arweave wallet JWK at HyperBEAM/.wallet.json for the node operator account. Also, set CWD in .env.hyperbeam, which should be the HyperBEAM node directory path relative to the root directory of your app. File/.env.hyperbeamCopyCWD=../HyperBEAM Here's the minimum viable test code. The HyperBEAM class starts up a HyperBEAM node and kills it once your tests complete, creating a sandbox environment for each test suite. File/test/hyperbeam.test.jsCopyimport assert from \"assert\" import { describe, it, before, after, beforeEach } from \"node:test\" import { HyperBEAM } from \"wao/test\" describe(\"HyperBEAM\", function () { let hbeam, hb","estimatedWords":261,"lastModified":"2025-10-20T12:15:28.807Z","breadcrumbs":["hyperbeam","installing hb wao"],"siteKey":"wao","siteName":"WAO Documentation","depth":2,"crawledAt":"2025-10-20T12:15:28.808Z"},{"url":"https://docs.wao.eco/hyperbeam/decoding-from-scratch","title":"Decoding HyperBEAM from Scratch","content":"Decoding HyperBEAM from Scratch Welcome to the complete guide to understanding HyperBEAM from the ground up. What This Tutorial Is For This tutorial series takes you on a deep dive into HyperBEAM internals, teaching you everything from basic concepts to advanced implementations. You'll learn by doing - writing tests with WAO (HyperBEAM SDK in JS) that interact directly with HyperBEAM nodes. Why WAO + HyperBEAM? WAO transforms the complex world of HyperBEAM development into something approachable: JavaScript-First Development: Everything is JavaScript - no need to context-switch between languages during testing Sandbox Testing Environment: WAO automatically spins up isolated HyperBEAM nodes for each test suite, ensuring clean, reproducible tests Simplified APIs with Syntactic Sugar: WAO's succinct API abstracts away complexity while giving you full control when needed Automatic Codec & Signing Handling: Complex encoding/decoding pipelines and HTTP message signatures are handled seamlessly in the background What You'll Master By working through this tutorial series, you'll gain: Deep Protocol Knowledge: Understand how messages flow through HyperBEAM's codec pipeline, from structured encoding to HTTP signatures Device Development Skills: Build custom HyperBEAM devices that extend the platform's capabilities Process Management Expertise: Learn to spawn, schedule, and manage stateful computations AOS Integration Know-How: Connect HyperBEAM with the broader AO ecosystem Payment System Understanding: Implement complex payment flows using HyperBEAM's built-in devices This isn't just theoretical knowledge - you'll have practical tools and patterns to build production-ready applications on HyperBEAM and interact seamlessly with AOS processes. The Journey Ahead This series follows a carefully crafted path from basics to advanced topics. Each chapter builds on the previous ones, so it's important to follow them in order: Part 1: Foundations ChapterTopic1Installing HyperBEAM and WAO - Setting up your development environment2Devices and Pathing - Understanding HyperBEAM's URL routing and device system3Custom Devices and Codecs - Building your first custom device and learning about codecs Part 2: The Codec System ChapterTopic4Flat Codec - Deep dive into path flattening for HTTP compatibility5Structured Codec - Handling complex data types with AO's type system6Httpsig Codec - Preparing messages for HTTP signatures Part 3: Security & Verification ChapterTopic7HTTP Message Signatures - Implementing RFC-9421 for message verification8Hashpaths - Understanding compute verification through chained hashes Part 4: Advanced Concepts ChapterTopic9Device Composition - Combining devices for powerful workflows10Processes and Scheduler - Managing stateful computations11Legacynet Compatible AOS - Integrating with the AOS ecosystem12Payment System - Building complex payment systems with p4@1.0 Resources Working Test Suite - Complete test files for all chapters HyperBEAM Implementation with Tutorial Devices - HyperBEAM fork with all custom devices from this tutorial InfoThis tutorial series is a living document. As HyperBEAM evolves, so will these tutorials. If you find errors or have suggestions, please contribute to making this resource better for everyone.","estimatedWords":450,"lastModified":"2025-10-20T12:15:29.114Z","breadcrumbs":["hyperbeam","decoding from scratch"],"siteKey":"wao","siteName":"WAO Documentation","depth":2,"crawledAt":"2025-10-20T12:15:29.114Z"},{"url":"https://docs.wao.eco/hyperbeam","title":"HyperBEAM","content":"HyperBEAM Install WAO & HyperBEAM Copyyarn add wao In addition to the WAO SDK, you will need to install HyperBEAM on your local computer (recommended) or a cloud server. The required systems are not the only ones you can install HyperBEAM on. I was, for example, able to install HyperBEAM on Arch Linux by installing the necessary dependencies. You could throw installation errors at LLMs like ChatGPT and Claude and likely be able to figure it out. If you want to test Mainnet AOS modules, you need to install the WAO fork of HyperBEAM, which includes a custom helper device for testing. Copygit clone --branch wao https:","estimatedWords":107,"lastModified":"2025-10-20T12:15:29.399Z","breadcrumbs":["hyperbeam"],"siteKey":"wao","siteName":"WAO Documentation","depth":1,"crawledAt":"2025-10-20T12:15:29.399Z"},{"url":"https://docs.wao.eco/getting-started","title":"Get started","content":"Get started Lightning Fast AO Testing Framework WAO SDK streamlines Arweave/AO development with elegant syntax enhancements and seamless message piping for enjoyable coding experiences. GraphQL operations are also made super easy. Succinct AO SDK with Syntactic Sugar Additionally, it includes a drop-in replacement for aoconnect, allowing the testing of lua scripts 1000x faster than the mainnet by emulating AO units in memory. It's even 100x faster than testing with arlocal and ao-localnet. Standalone Local AO Units WAO is not only for in-memory testing, but also for lightweight standanoe AO units and web embeddable AO units. You can type a simple command npx wao, and you will have your own AO units on your local computer. AO in the Browser You can open up a browser at preview.wao.eco and you will have all the AO units (MU/SU/CU/Gateway) right in your browser with an AOS terminal, a coding editor, and a local AO explorer to make debugging easier. HyperBEAM SDK WAO is also compatible with HyperBEAM and Mainnet AOS processes. You can launch a HyperBEAM node from withing JS test code and create a sandbox environment for the test. HyperBEAM Custom Device Testing You can also create custom HyperBEAM devices in Erlang, Rust, and C++, then test them in JS.","estimatedWords":208,"lastModified":"2025-10-20T12:15:29.648Z","breadcrumbs":["getting started"],"siteKey":"wao","siteName":"WAO Documentation","depth":1,"crawledAt":"2025-10-20T12:15:29.648Z"}],"lastCrawled":"2025-10-20T12:15:30.422Z","stats":{"totalPages":31,"averageWords":260,"duration":17282,"requestCount":34,"averageResponseTime":284.7352941176471,"pagesPerSecond":1.7937738687651892}},"permaweb-glossary":{"name":"Permaweb Glossary","baseUrl":"https://glossary.arweave.net","pages":[{"url":"https://glossary.arweave.net/glossary.txt","title":"Permaweb Glossary","estimatedWords":737,"lastModified":"2025-10-20T12:15:30.421Z","breadcrumbs":["Permaweb Glossary"],"siteKey":"permaweb-glossary","siteName":"Permaweb Glossary","depth":0,"crawledAt":"2025-10-20T12:15:30.421Z"}],"lastCrawled":"2025-10-20T12:15:30.422Z","stats":{"totalPages":1,"averageWords":737,"duration":767,"requestCount":1,"averageResponseTime":766,"pagesPerSecond":1.303780964797914}}}}